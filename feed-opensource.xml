<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI-Pulse - Open Source & GitHub</title>
    <link>https://thephoenixagency.github.io/AI-Pulse</link>
    <description>Open Source & GitHub news from AI-Pulse</description>
    <language>en</language>
    <lastBuildDate>Fri, 20 Feb 2026 04:15:01 GMT</lastBuildDate>
    <atom:link href="https://thephoenixagency.github.io/AI-Pulse/feed-opensource.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title><![CDATA[getzep/graphiti]]></title>
      <link>https://github.com/getzep/graphiti</link>
      <description><![CDATA[Build Real-Time Knowledge Graphs for AI Agents Graphiti Build Real-Time Knowledge Graphs for AI Agents Help us reach more developers and grow the Graphiti community. Star this repo! [!TIP] Check out the new MCP server for Graphiti! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory. Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications. Use Graphiti to: Integrate and maintain dynamic user interactions and business data. Facilitate state-based reasoning and task automation for agents. Query complex, evolving data with semantic, keyword, and graph-based search methods. A knowledge graph is a network of interconnected facts, such as "Kendra loves Adidas shoes." Each fact is a "triplet" represented by two entities, or nodes ("Kendra", "Adidas shoes"), and their relationship, or edge ("loves"). Knowledge Graphs have been explored extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph while handling changing relationships and maintaining historical context. Graphiti and Zep's Context Engineering Platform. Graphiti powers the core of Zep's context engineering platform for AI Agents. Zep offers agent memory, Graph RAG for dynamic data, and context retrieval and assembly. Using Graphiti, we've demonstrated Zep is the State of the Art in Agent Memory. Read our paper: Zep: A Temporal Knowledge Graph Architecture for Agent Memory. We're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications. Zep vs Graphiti Aspect Zep Graphiti What they are Fully managed platform for context engineering and AI memory Open-source graph framework User &amp; conversation management Built-in users, threads, and message storage Build your own Retrieval &amp; performance Pre-configured, production-ready retrieval with sub-200ms performance at scale Custom implementation required; performance depends on your setup Developer tools Dashboard with graph visualization, debug logs, API logs; SDKs for Python, TypeScript, and Go Build your own tools Enterprise features SLAs, support, security guarantees Self-managed Deployment Fully managed or in your cloud Self-hosted only When to choose which Choose Zep if you want a turnkey, enterprise-grade platform with security, performance, and support baked in. Choose Graphiti if you want a flexible OSS core and you're comfortable building/operating the surrounding system. Why Graphiti? Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing: Real-Time Incremental Updates: Immediate integration of new data episodes without batch recomputation. Bi-Temporal Data Model: Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries. Efficient Hybrid Retrieval: Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization. Custom Entity Definitions: Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models. Scalability: Efficiently manages large datasets with parallel processing, suitable for enterprise environments. Graphiti vs. GraphRAG Aspect GraphRAG Graphiti Primary Use Static document summarization Dynamic data management Data Handling Batch-oriented processing Continuous, incremental updates Knowledge Structure Entity clusters &amp; community summaries Episodic data, semantic entities, communities Retrieval Method Sequential LLM summarization Hybrid semantic, keyword, and graph-based search Adaptability Low High Temporal Handling Basic timestamp tracking Explicit bi-temporal tracking Contradiction Handling LLM-driven summarization judgments Temporal edge invalidation Query Latency Seconds to tens of seconds Typically sub-second latency Custom Entity Types No Yes, customizable Scalability Moderate High, optimized for large datasets Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries. Installation Requirements: Python 3.10 or higher Neo4j 5.26 / FalkorDB 1.1.2 / Kuzu 0.11.2 / Amazon Neptune Database Cluster or Neptune Analytics Graph + Amazon OpenSearch Serverless collection (serves as the full text search backend) OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding) [!IMPORTANT] Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini). Using other services may result in incorrect output schemas and ingestion failures. This is particularly problematic when using smaller models. Optional: Google Gemini, Anthropic, or Groq API key (for alternative LLM providers) [!TIP] The simplest way to install Neo4j is via Neo4j Desktop. It provides a user-friendly interface to manage Neo4j instances and databases. Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example: docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest pip install graphiti-core or uv add graphiti-core Installing with FalkorDB Support If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra: pip install graphiti-core[falkordb] # or with uv
uv add graphiti-core[falkordb] Installing with Kuzu Support If you plan to use Kuzu as your graph database backend, install with the Kuzu extra: pip install graphiti-core[kuzu] # or with uv
uv add graphiti-core[kuzu] Installing with Amazon Neptune Support If you plan to use Amazon Neptune as your graph database backend, install with the Amazon Neptune extra: pip install graphiti-core[neptune] # or with uv
uv add graphiti-core[neptune] You can also install optional LLM providers as extras: # Install with Anthropic support
pip install graphiti-core[anthropic] # Install with Groq support
pip install graphiti-core[groq] # Install with Google Gemini support
pip install graphiti-core[google-genai] # Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai] # Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai] # Install with Amazon Neptune
pip install graphiti-core[neptune] Default to Low Concurrency; LLM Provider 429 Rate Limit Errors Graphiti's ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM Provider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below. Concurrency controlled by the SEMAPHORE_LIMIT environment variable. By default, SEMAPHORE_LIMIT is set to 10 concurrent operations to help prevent 429 rate limit errors from your LLM provider. If you encounter such errors, try lowering this value. If your LLM provider allows higher throughput, you can increase SEMAPHORE_LIMIT to boost episode ingestion performance. Quick Start [!IMPORTANT] Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an OPENAI_API_KEY is set in your environment. Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI compatible APIs. For a complete working example, see the Quickstart Example in the examples directory. The quickstart demonstrates: Connecting to a Neo4j, Amazon Neptune, FalkorDB, or Kuzu database Initializing Graphiti indices and constraints Adding episodes to the graph (both text and structured JSON) Searching for relationships (edges) using hybrid search Reranking search results using graph distance Searching for nodes using predefined search recipes The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps. Running with Docker Compose You can use Docker Compose to quickly start the required services: Neo4j Docker: docker compose up This will start the Neo4j Docker service and related components. FalkorDB Docker: docker compose --profile falkordb up This will start the FalkorDB Docker service and related components. MCP Server The mcp_server directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol. Key features of the MCP server include: Episode management (add, retrieve, delete) Entity management and relationship handling Semantic and hybrid search capabilities Group management for organizing related data Graph maintenance operations The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows. For detailed setup instructions and usage examples, see the MCP server README. REST Service The server directory contains an API service for interacting with the Graphiti API. It is built using FastAPI. Please see the server README for more information. Optional Environment Variables In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables. If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables must be set. Database Configuration Database names are configured directly in the driver constructors: Neo4j: Database name defaults to neo4j (hardcoded in Neo4jDriver) FalkorDB: Database name defaults to default_db (hardcoded in FalkorDriver) As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the graph_driver parameter. Neo4j with Custom Database Name from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver # Create a Neo4j driver with custom database name
driver = Neo4jDriver( uri="bolt://localhost:7687", user="neo4j", password="password", database="my_custom_database" # Custom database name
) # Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver) FalkorDB with Custom Database Name from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver # Create a FalkorDB driver with custom database name
driver = FalkorDriver( host="localhost", port=6379, username="falkor_user", # Optional password="falkor_password", # Optional database="my_custom_graph" # Custom database name
) # Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver) Kuzu from graphiti_core import Graphiti
from graphiti_core.driver.kuzu_driver import KuzuDriver # Create a Kuzu driver
driver = KuzuDriver(db="/tmp/graphiti.kuzu") # Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver) Amazon Neptune from graphiti_core import Graphiti
from graphiti_core.driver.neptune_driver import NeptuneDriver # Create a FalkorDB driver with custom database name
driver = NeptuneDriver( host= &lt; NEPTUNE
ENDPOINT &gt;,
aoss_host = &lt; Amazon
OpenSearch
Serverless
Host &gt;,
port = &lt; PORT &gt; # Optional, defaults to 8182, aoss_port = &lt; PORT &gt; # Optional, defaults to 443
) driver = NeptuneDriver(host=neptune_uri, aoss_host=aoss_host, port=neptune_port) # Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver) Graph Driver Architecture Graphiti uses a pluggable driver architecture so the core framework is backend-agnostic. All database-specific logic is encapsulated in driver implementations, allowing you to swap backends or add new ones without modifying the rest of the framework. How Drivers are Integrated The driver layer is organized into three tiers: GraphDriver ABC (graphiti_core/driver/driver.py) — the core interface every backend must implement. It defines query execution, session management, index lifecycle, and exposes 11 operations interfaces as @property accessors. GraphProvider enum — identifies the backend (NEO4J, FALKORDB, KUZU, NEPTUNE). Query builders use this enum in match/case statements to return dialect-specific query strings. 11 Operations ABCs (graphiti_core/driver/operations/) — abstract interfaces covering all CRUD and search operations for every graph element type: Node ops: EntityNodeOperations, EpisodeNodeOperations, CommunityNodeOperations, SagaNodeOperations Edge ops: EntityEdgeOperations, EpisodicEdgeOperations, CommunityEdgeOperations, HasEpisodeEdgeOperations, NextEpisodeEdgeOperations Search &amp; maintenance: SearchOperations, GraphMaintenanceOperations Each backend provides a concrete driver class and a matching operations/ directory with implementations of all 11 ABCs. The key directories and files are shown below (simplified; see source for complete structure): graphiti_core/driver/
├── driver.py # GraphDriver ABC, GraphProvider enum
├── query_executor.py # QueryExecutor protocol
├── record_parsers.py # Shared record → model conversion
├── operations/ # 11 operation ABCs
│ ├── entity_node_ops.py
│ ├── episode_node_ops.py
│ ├── community_node_ops.py
│ ├── saga_node_ops.py
│ ├── entity_edge_ops.py
│ ├── episodic_edge_ops.py
│ ├── community_edge_ops.py
│ ├── has_episode_edge_ops.py
│ ├── next_episode_edge_ops.py
│ ├── search_ops.py
│ ├── graph_ops.py
│ └── graph_utils.py # Shared algorithms (e.g., label propagation)
├── graph_operations/ # Legacy graph operations interface
├── search_interface/ # Legacy search interface
├── neo4j_driver.py # Neo4jDriver
├── neo4j/operations/ # 11 Neo4j implementations
├── falkordb_driver.py # FalkorDriver
├── falkordb/operations/ # 11 FalkorDB implementations
├── kuzu_driver.py # KuzuDriver
├── kuzu/operations/ # 11 Kuzu implementations + record_parsers.py
├── neptune_driver.py # NeptuneDriver
└── neptune/operations/ # 11 Neptune implementations Operations are decoupled from the driver itself — each operation method receives an executor: QueryExecutor parameter (a protocol for running queries) rather than a concrete GraphDriver, which makes operations testable and driver-agnostic. The driver class instantiates all 11 operation classes in its __init__ and exposes them as properties. The base GraphDriver ABC defines each property with an optional return type (| None, defaulting to None); concrete drivers override these to return their implementations: # In your concrete driver (e.g., Neo4jDriver):
@property
def entity_node_ops(self) -&gt; EntityNodeOperations: return self._entity_node_ops Provider-specific query strings are generated by shared query builders in graphiti_core/models/nodes/node_db_queries.py and graphiti_core/models/edges/edge_db_queries.py, which use match/case on the GraphProvider enum to return the correct dialect for each backend. Adding a New Graph Driver To integrate a new graph database backend, follow these steps: Add to GraphProvider — add your enum value in graphiti_core/driver/driver.py: class GraphProvider(Enum): NEO4J = 'neo4j' FALKORDB = 'falkordb' KUZU = 'kuzu' NEPTUNE = 'neptune' MY_BACKEND = 'my_backend' # New backend Create directory structure — create graphiti_core/driver//operations/ with an __init__.py exporting all 11 operation classes. Implement GraphDriver subclass — create graphiti_core/driver/_driver.py: Set provider = GraphProvider. Implement the abstract methods: execute_query(), session(), close(), build_indices_and_constraints(), delete_all_indexes() Instantiate all 11 operation classes in __init__ and return them via @property overrides Implement all 11 operation ABCs — one file per ABC in /operations/, each inheriting from the corresponding ABC in graphiti_core/driver/operations/. Add query variants — add case GraphProvider.: branches to graphiti_core/models/nodes/node_db_queries.py and graphiti_core/models/edges/edge_db_queries.py for your database's query dialect. Implement GraphDriverSession — if your backend needs session or connection management, subclass GraphDriverSession from driver.py and implement run(), close(), and execute_write(). Register as optional dependency — add an extras group in pyproject.toml: [project.optional-dependencies]]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:32 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/getzep/graphiti</guid>
    </item>
    <item>
      <title><![CDATA[PostHog/posthog]]></title>
      <link>https://github.com/PostHog/posthog</link>
      <description><![CDATA[PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack. Docs - Community - Roadmap - Why PostHog? - Changelog - Bug reports PostHog is an all-in-one, open source platform for building successful products PostHog provides every tool you need to build a successful product including: Product Analytics: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL. Web Analytics: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue. Session Replays: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior. Feature Flags: Safely roll out features to select users or cohorts with feature flags. Experiments: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too. Error Tracking: Track errors, get alerts, and resolve issues to improve your product. Surveys: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder. Data warehouse: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data. Data pipelines: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse. LLM analytics: Capture traces, generations, latency, and cost for your LLM-powered app. Workflows: Create workflows that automate actions or send messages to your users. Best of all, all of this is free to use with a generous monthly free tier for each product. Get started by signing up for PostHog Cloud US or PostHog Cloud EU. Table of Contents PostHog is an all-in-one, open source platform for building successful products Table of Contents Getting started with PostHog PostHog Cloud ( ) Self-hosting the open-source hobby deploy (Advanced) Setting up PostHog Learning more about PostHog Contributing Open-source vs. paid We’re hiring! Getting started with PostHog PostHog Cloud ( ) The fastest and most reliable way to get started with PostHog is signing up for free to PostHog Cloud or PostHog Cloud EU. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage. Self-hosting the open-source hobby deploy (Advanced) If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker ( 4GB memory): /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)" Open source deployments should scale to approximately 100k events per month, after which we recommend migrating to a PostHog Cloud. We do not provide customer support or offer guarantees for open source deployments. See our self-hosting docs, troubleshooting guide, and disclaimer for more info. Setting up PostHog Once you've got a PostHog instance, you can set it up by installing our JavaScript web snippet, one of our SDKs, or by using our API. We have SDKs and libraries for popular languages and frameworks like: Frontend Mobile Backend JavaScript React Native Python Next.js Android Node React iOS PHP Vue Flutter Ruby Beyond this, we have docs and guides for Go, .NET/C#, Django, Angular, WordPress, Webflow, and more. Once you've installed PostHog, see our product docs for more information on how to set up product analytics, web analytics, session replays, feature flags, experiments, error tracking, surveys, data warehouse, and more. Learning more about PostHog Our code isn't the only thing that's open source . We also open source our company handbook which details our strategy, ways of working, and processes. Curious about how to make the most of PostHog? We wrote a guide to winning with PostHog which walks you through the basics of measuring activation, tracking retention, and capturing revenue. Contributing We &lt;3 contributions big and small: Vote on features or get early access to beta functionality in our roadmap Open a PR (see our instructions on developing PostHog locally) Submit a feature request or bug report For an overview of the codebase structure, see monorepo layout and products. Open-source vs. paid This repo is available under the MIT expat license, except for the ee directory (which has its license here) if applicable. Need absolutely % FOSS? Check out our posthog-foss repository, which is purged of all proprietary code and features. The pricing for our paid plan is completely transparent and available on our pricing page. We're hiring! Hey! If you're reading this, you've proven yourself as a dedicated README reader. You might also make a great addition to our team. We're growing fast and would love for you to join us.]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:31 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/PostHog/posthog</guid>
    </item>
    <item>
      <title><![CDATA[microsoft/agent-lightning]]></title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description><![CDATA[The absolute trainer to light up AI agents. Agent Lightning The absolute trainer to light up AI agents. Join our Discord community to connect with other users and contributors. Core Features Turn your agent into an optimizable beast with ZERO CODE CHANGE (almost)! Build with ANY agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! Selectively optimize one or more agents in a multi-agent system. Embraces Algorithms like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. on our documentation website. Installation pip install agentlightning For the latest nightly build (cutting-edge features), you can install from Test PyPI: pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning Please refer to our installation guide for more details. To start using Agent-lightning, check out our documentation and examples. Articles 12/17/2025 Adopting the Trajectory Level Aggregation for Faster Training Agent-lightning blog. 11/4/2025 Tuning ANY AI agent with Tinker ✕ Agent-lightning Medium. See also Part 2. 10/22/2025 No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL vLLM blog. See also Zhihu writeup. 8/11/2025 Training AI Agents to Write and Self-correct SQL with Reinforcement Learning Medium. 8/5/2025 Agent Lightning: Train ANY AI Agents with Reinforcement Learning arXiv paper. 7/26/2025 We discovered an approach to train any AI agent with RL, with (almost) zero code changes. Reddit. 6/6/2025 Agent Lightning - Microsoft Research Project page. Community Projects DeepWerewolf — A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning. AgentFlow — A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks. Youtu-Agent — Youtu-Agent lets you build and train your agent with ease. Built with a modified branch of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check the recipe and their blog Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat. Architecture Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight agl.emit_xxx() helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync. On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning. No rewrites, no lock-in, just a clear path from first rollout to steady improvement. CI Status Workflow Status CPU Tests Full Tests UI Tests Examples Integration Latest Dependency Compatibility Legacy Examples Compatibility Citation If you find Agent Lightning useful in your research or projects, please cite our paper: @misc{luo2025agentlightningtrainai, title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning}, author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang}, year={2025}, eprint={2508.03680}, archivePrefix={arXiv}, primaryClass={cs.AI}, url={https://arxiv.org/abs/2508.03680},
} Contributing This project welcomes contributions and suggestions. Start by reading the Contributing Guide for contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, ). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or . Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark &amp; Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies. Responsible AI This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise. License This project is licensed under the MIT License. See the LICENSE file for details.]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:24 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/microsoft/agent-lightning</guid>
    </item>
    <item>
      <title><![CDATA[NVIDIA-NeMo/NeMo]]></title>
      <link>https://github.com/NVIDIA-NeMo/NeMo</link>
      <description><![CDATA[A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech) NVIDIA NeMo Speech Collection Latest News NVIDIA-Nemotron-3-Nano-30B-A3B is out with full reproducible script and recipes! Check out NeMo Megatron-Bridge, NeMo AutoModel, NeMo-RL and NGC container to try them! (2025-12-15) Pivot notice: This repo has pivoted to focus on audio, speech, and multimodal LLM only. Please refer to NeMo Framework Github Org for the complete list of repos under NeMo Framework NeMo Megatron-Bridge and NeMo AutoModel. More details can be found in the NeMo Framework GitHub org readme. (2025-10-10) The following collections are no longer available avlm · diffusion · llm · multimodal · multimodal-autoregressive · nlp · speechlm · vision · vlm Pretrain and finetune Hugging Face models via AutoModel NeMo Framework's latest feature AutoModel enables broad support for Hugging Face models, with 25.04 focusing on AutoModelForCausalLM in the Text Generation category AutoModelForImageTextToText in the Image-Text-to-Text category More Details in Blog: Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework. Future releases will enable support for more model families such as Video Generation models.(2025-05-19) Training on Blackwell using NeMo NeMo Framework has added Blackwell support, with performance benchmarks on GB200 &amp; B200. More optimizations to come in the upcoming releases.(2025-05-19) Training Performance on GPU Tuning Guide NeMo Framework has published a comprehensive guide for performance tuning to achieve optimal throughput! (2025-05-19) New Models Support NeMo Framework has added support for latest community models - Llama 4, Flux, Llama Nemotron, Hyena &amp; Evo2, Qwen2-VL, Qwen2.5, Gemma3, Qwen3-30B&amp;32B.(2025-05-19) NeMo Framework 2.0 We've released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the NeMo Framework User Guide to get started. New Cosmos World Foundation Models Support Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform (2025-01-09) The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities (2025-01-07) The NeMo Framework now supports training and customizing the NVIDIA Cosmos collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts. You can also now accelerate your video processing step using the NeMo Curator library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline. Large Language Models and Multimodal Models State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo (2024-11-06) NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the NVIDIA/cosmos-tokenizer GitHub repo and on Hugging Face. New Llama 3.1 Support (2024-07-23) The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta. Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS (2024-07-16) NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository here. NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support (2024/06/17) NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens. (2024-06-18) See documentation and tutorials for SFT, PEFT, and PTQ with Nemotron 340B in the NeMo Framework User Guide. NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0 (2024/06/12) Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training. Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE (2024/03/16) An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework. Speech Recognition Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo (2024/09/24) NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000. New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model (2024/04/18) The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. Canary also provides bi-directional translation, between English and the three other supported languages. Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models (2024/04/18) NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhere—on any cloud and on-premises—released the Parakeet family of automatic speech recognition (ASR) models. These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy. Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT (2024/04/18) NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhere—on any cloud and on-premises—recently released Parakeet-TDT. This new addition to the NeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B. Introduction NVIDIA NeMo Framework is a scalable and cloud-native generative AI framework built for researchers and PyTorch developers working on Large Language Models (LLMs), Multimodal Models (MMs), Automatic Speech Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV) domains. It is designed to help you efficiently create, customize, and deploy new generative AI models by leveraging existing code and pre-trained model checkpoints. For technical documentation, please see the NeMo Framework User Guide. What's New in NeMo 2.0 NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability. Python-Based Configuration - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically. Modular Abstractions - By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models. Scalability - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using NeMo-Run, a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments. Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development. Get Started with NeMo 2.0 Refer to the Quickstart for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster. For more information about NeMo 2.0, see the NeMo Framework User Guide. For an in-depth exploration of the main features of NeMo 2.0, see the Feature Guide. To transition from NeMo 1.0 to 2.0, see the Migration Guide for step-by-step instructions. Training and Customization All NeMo models are trained with Lightning. Training is automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the latest NeMo Framework container here. When applicable, NeMo models leverage cutting-edge distributed training techniques, incorporating parallelism strategies to enable efficient training of very large models. These techniques include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed Precision Training with BFloat16 and FP8, as well as others. In addition to supervised fine-tuning (SFT), NeMo also supports the latest parameter efficient fine-tuning (PEFT) techniques such as LoRA, P-Tuning, Adapters, and IA3. Speech AI NeMo ASR and TTS models can be optimized for inference and deployed for production use cases with NVIDIA Riva. Get Started with NeMo Framework Getting started with NeMo Framework is easy. State-of-the-art pretrained NeMo models are freely available on Hugging Face Hub and NVIDIA NGC. These models can be used to generate text or images, transcribe audio, and synthesize speech in just a few lines of code. We have extensive tutorials that can be run on Google Colab or with our NGC NeMo Framework Container. We also have playbooks for users who want to train NeMo models with the NeMo Framework Launcher. For advanced users who want to train NeMo models from scratch or fine-tune existing NeMo models, we have a full suite of example scripts that support multi-GPU/multi-node training. Key Features Multimodal Automatic Speech Recognition Text to Speech Requirements Python 3.12 or above Pytorch 2.6 or above NVIDIA GPU (if you intend to do model training) As of Pytorch 2.6, torch.load defaults to using weights_only=True. Some model checkpoints may require using weights_only=False. In this case, you can set the env var TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1 before running code that uses torch.load. However, this should only be done with trusted files. Loading files from untrusted sources with more than weights only can have the risk of arbitrary code execution. Developer Documentation Version Status Description Latest Documentation of the latest (i.e. main) branch. Stable Documentation of the stable (i.e. most recent release) Install NeMo Framework The NeMo Framework can be installed in a variety of ways, depending on your needs. Depending on the domain, you may find one of the following installation methods more suitable. Conda / Pip: Install NeMo-Framework with native Pip into a virtual environment. Used to explore NeMo on any supported platform. This is the method for ASR and TTS domains. Limited feature-completeness for other domains. NGC PyTorch container: Install NeMo-Framework from source with feature-completeness into a highly optimized container. For users that want to install from source in a highly optimized container. NGC NeMo container: Ready-to-go solution of NeMo-Framework For users that seek highest performance. Contains all dependencies installed and tested for performance and convergence. Support matrix NeMo-Framework provides tiers of support based on OS / Platform and mode of installation. Please refer the following overview of support levels: Fully supported: Max performance and feature-completeness. Limited supported: Used to explore NeMo. No support yet: In development. Deprecated: Support has reached end of life. Please refer to the following table for current support levels: OS / Platform Install from PyPi Source into NGC container linux - amd64/x84_64 Limited support Full support linux - arm64 Limited support Limited support darwin - amd64/x64_64 Deprecated Deprecated darwin - arm64 Limited support Limited support windows - amd64/x64_64 No support yet No support yet windows - arm64 No support yet No support yet Conda / Pip Install NeMo in a fresh Conda environment: conda create --name nemo python==3.10.12
conda activate nemo Pick the right version NeMo-Framework publishes pre-built wheels with each release. To install nemo_toolkit from such a wheel, use the following installation method: pip install "nemo_toolkit[all]" If a more specific version is desired, we recommend a Pip-VCS install. From NVIDIA/NeMo, fetch the commit, branch, or tag that you would like to install. To install nemo_toolkit from this Git reference $REF, use the following installation method: git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout @${REF:-'main'}
pip install '.[all]' Install a specific Domain To install a specific domain of NeMo, you must first install the nemo_toolkit using the instructions listed above. Then, you run the following domain-specific commands: pip install nemo_toolkit['all'] # or pip install "nemo_toolkit['all']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['asr'] # or pip install "nemo_toolkit['asr']@git+https://github.com/NVIDIA/NeMo@$REF:-'main'}"
pip install nemo_toolkit['tts'] # or pip install "nemo_toolkit['tts']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}" NGC PyTorch container NOTE: The following steps are supported beginning with 25.09 (NeMo-Toolkit 2.6.0) We that you start with a base NVIDIA PyTorch container: nvcr.io/nvidia/pytorch:25.09-py3. If starting with a base NVIDIA PyTorch container, you must first launch the container: docker run \ --gpus all \ -it \ --rm \ --shm-size=16g \ --ulimit memlock=-1 \ --ulimit stack=67108864 \ ${NV_PYTORCH_TAG:-'nvcr.io/nvidia/pytorch:25.09-py3'} From NVIDIA/NeMo, fetch the commit/branch/tag that you want to install. To install nemo_toolkit including all of its dependencies from this Git reference $REF, use the following installation method: cd /opt
git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout ${REF:-'main'}
pip install ".[all]" NGC NeMo container NeMo containers are launched concurrently with NeMo version updates. NeMo Framework now supports LLMs, MMs, ASR, and TTS in a single consolidated Docker container. The latest container is based on NeMo 2.6.0. You can find additional information about released containers on the NeMo releases page. To use a pre-built container, run the following code: docker run \ --gpus all \ -it \ --rm \ --shm-size=16g \ --ulimit memlock=-1 \ --ulimit stack=67108864 \ nvcr.io/nvidia/nemo:25.11.01 Discussions Board FAQ can be found on the NeMo Discussions board. You are welcome to ask questions or start discussions on the board. Contribute to NeMo We welcome community contributions! Please refer to CONTRIBUTING.md for the process. Publications We provide an ever-growing list of publications that utilize the NeMo Framework. To contribute an article to the collection, please submit a pull request to the gh-pages-src branch of this repository. For detailed information, please consult the README located at the gh-pages-src branch. Licenses NeMo is licensed under the Apache License 2.0.]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:24 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/NVIDIA-NeMo/NeMo</guid>
    </item>
    <item>
      <title><![CDATA[exo-explore/exo]]></title>
      <link>https://github.com/exo-explore/exo</link>
      <description><![CDATA[Run frontier AI locally. exo: Run frontier AI locally. Maintained by exo labs. exo connects all your devices into an AI cluster. Not only does exo enable running models larger than would fit on a single device, but with day-0 support for RDMA over Thunderbolt, makes models run faster as you add more devices. Features Automatic Device Discovery: Devices running exo automatically discover each other - no manual configuration. RDMA over Thunderbolt: exo ships with day-0 support for RDMA over Thunderbolt 5, enabling 99% reduction in latency between devices. Topology-Aware Auto Parallel: exo figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link. Tensor Parallelism: exo supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices. MLX Support: exo uses MLX as an inference backend and MLX distributed for distributed communication. Dashboard exo includes a built-in dashboard for managing your cluster and chatting with models. 4 × 512GB M3 Ultra Mac Studio running DeepSeek v3.1 (8-bit) and Kimi-K2-Thinking (4-bit) Benchmarks Qwen3-235B (8-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA Source: Jeff Geerling: 15 TB VRAM on Mac Studio – RDMA over Thunderbolt 5 DeepSeek v3.1 671B (8-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA Source: Jeff Geerling: 15 TB VRAM on Mac Studio – RDMA over Thunderbolt 5 Kimi K2 Thinking (native 4-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA Source: Jeff Geerling: 15 TB VRAM on Mac Studio – RDMA over Thunderbolt 5 Quick Start Devices running exo automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at http://localhost:52415). There are two ways to run exo: Run from Source (macOS) If you have Nix installed, you can skip most of the steps below and run exo directly: nix run .#exo Note: To accept the Cachix binary cache (and avoid the Xcode Metal ToolChain), add to /etc/nix/nix.conf: trusted-users = root (or your username)
experimental-features = nix-command flakes Then restart the Nix daemon: sudo launchctl kickstart -k system/org.nixos.nix-daemon Prerequisites: Xcode (provides the Metal ToolChain required for MLX compilation) brew (for simple package management on macOS) /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)" uv (for Python dependency management) macmon (for hardware monitoring on Apple Silicon) node (for building the dashboard) brew install uv macmon node rust (to build Rust bindings, nightly for now) curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly Clone the repo, build the dashboard, and run exo: # Clone exo
git clone https://github.com/exo-explore/exo # Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd .. # Run exo
uv run exo This starts the exo dashboard and API at http://localhost:52415/ Please view the section on RDMA to enable this feature on MacOS &gt;=26.2! Run from Source (Linux) Prerequisites: uv (for Python dependency management) node (for building the dashboard) - version 18 or higher rust (to build Rust bindings, nightly for now) Installation methods: Option 1: Using system package manager (Ubuntu/Debian example): # Install Node.js and npm
sudo apt update
sudo apt install nodejs npm # Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh # Install Rust (using rustup)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly Option 2: Using Homebrew on Linux (if preferred): # Install Homebrew on Linux
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)" # Install dependencies
brew install uv node # Install Rust (using rustup)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly Note: The macmon package is macOS-only and not required for Linux. Clone the repo, build the dashboard, and run exo: # Clone exo
git clone https://github.com/exo-explore/exo # Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd .. # Run exo
uv run exo This starts the exo dashboard and API at http://localhost:52415/ Important note for Linux users: Currently, exo runs on CPU on Linux. GPU support for Linux platforms is under development. If you'd like to see support for your specific Linux hardware, please search for existing feature requests or create a new one. Configuration Options: --no-worker: Run exo without the worker component. Useful for coordinator-only nodes that handle networking and orchestration but don't execute inference tasks. This is helpful for machines without sufficient GPU resources but with good network connectivity. uv run exo --no-worker File Locations (Linux): exo follows the XDG Base Directory Specification on Linux: Configuration files: ~/.config/exo/ (or $XDG_CONFIG_HOME/exo/) Data files: ~/.local/share/exo/ (or $XDG_DATA_HOME/exo/) Cache files: ~/.cache/exo/ (or $XDG_CACHE_HOME/exo/) You can override these locations by setting the corresponding XDG environment variables. macOS App exo ships a macOS app that runs in the background on your Mac. The macOS app requires macOS Tahoe 26.2 or later. Download the latest build here: EXO-latest.dmg. The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on. Custom Namespace for Cluster Isolation: The macOS app includes a custom namespace feature that allows you to isolate your exo cluster from others on the same network. This is configured through the EXO_LIBP2P_NAMESPACE setting: Use cases: Running multiple separate exo clusters on the same network Isolating development/testing clusters from production clusters Preventing accidental cluster joining Configuration: Access this setting in the app's Advanced settings (or set the EXO_LIBP2P_NAMESPACE environment variable when running from source) The namespace is logged on startup for debugging purposes. Uninstalling the macOS App The way to uninstall is through the app itself: click the menu bar icon → Advanced → Uninstall. This cleanly removes all system components. If you've already deleted the app, you can run the standalone uninstaller script: sudo ./app/EXO/uninstall-exo.sh This removes: Network setup LaunchDaemon Network configuration script Log files The "exo" network location Note: You'll need to manually remove EXO from Login Items in System Settings → General → Login Items. Enabling RDMA on macOS RDMA is a new capability added to macOS 26.2. It works on any Mac with Thunderbolt 5 (M4 Pro Mac Mini, M4 Max Mac Studio, M4 Max MacBook Pro, M3 Ultra Mac Studio). Please refer to the caveats for immediate troubleshooting. To enable RDMA on macOS, follow these steps: Shut down your Mac. Hold down the power button for 10 seconds until the boot menu appears. Select "Options" to enter Recovery mode. When the Recovery UI appears, open the Terminal from the Utilities menu. In the Terminal, type: rdma_ctl enable and press Enter. Reboot your Mac. After that, RDMA will be enabled in macOS and exo will take care of the rest. Important Caveats Devices that wish to be part of an RDMA cluster must be connected to all other devices in the cluster. The cables must support TB5. On a Mac Studio, you cannot use the Thunderbolt 5 port next to the Ethernet port. If running from source, please use the script found at tmp/set_rdma_network_config.sh, which will disable Thunderbolt Bridge and set dhcp on each RDMA port. RDMA ports may be unable to discover each other on different versions of MacOS. Please ensure that OS versions match exactly (even beta version numbers) on all devices. Using the API If you prefer to interact with exo via the API, here is an example creating an instance of a small model (mlx-community/Llama-3.2-1B-Instruct-4bit), sending a chat completions request and deleting the instance. 1. Preview instance placements The /instance/previews endpoint will preview all valid placements for your model. curl "http://localhost:52415/instance/previews?model_id=llama-3.2-1b" Sample response: { "previews": [ { "model_id": "mlx-community/Llama-3.2-1B-Instruct-4bit", "sharding": "Pipeline", "instance_meta": "MlxRing", "instance": {...}, "memory_delta_by_node": {"local": 729808896}, "error": null } // ...possibly more placements... ]
} This will return all valid placements for this model. Pick a placement that you like. To pick the first one, pipe into jq: curl "http://localhost:52415/instance/previews?model_id=llama-3.2-1b" | jq -c '.previews[] | select(.error == null) | .instance' | head -n1 2. Create a model instance Send a POST to /instance with your desired placement in the instance field (the full payload must match types as in CreateInstanceParams), which you can copy from step 1: curl -X POST http://localhost:52415/instance \ -H 'Content-Type: application/json' \ -d '{ "instance": {...} }' Sample response: { "message": "Command received.", "command_id": "e9d1a8ab-...."
} 3. Send a chat completion Now, make a POST to /v1/chat/completions (the same format as OpenAI's API): curl -N -X POST http://localhost:52415/v1/chat/completions \ -H 'Content-Type: application/json' \ -d '{ "model": "mlx-community/Llama-3.2-1B-Instruct-4bit", "messages": [ {"role": "user", "content": "What is Llama 3.2 1B?"} ], "stream": true }' 4. Delete the instance When you're done, delete the instance by its ID (find it via /state or /instance endpoints): curl -X DELETE http://localhost:52415/instance/YOUR_INSTANCE_ID Other useful API endpoints:* List all models: curl http://localhost:52415/models Inspect instance IDs and deployment state: curl http://localhost:52415/state For further details, see: API basic documentation in docs/api.md. API types and endpoints in src/exo/master/api.py. Benchmarking The exo-bench tool measures model prefill and token generation speed across different placement configurations. This helps you optimize model performance and validate improvements. Prerequisites: Nodes should be running with uv run exo before benchmarking The tool uses the /bench/chat/completions endpoint Basic usage: uv run bench/exo_bench.py \ --model Llama-3.2-1B-Instruct-4bit \ --pp 128,256,512 \ --tg 128,256 Key parameters: --model: Model to benchmark (short ID or HuggingFace ID) --pp: Prompt size hints (comma-separated integers) --tg: Generation lengths (comma-separated integers) --max-nodes: Limit placements to N nodes (default: 4) --instance-meta: Filter by ring, jaccl, or both (default: both) --sharding: Filter by pipeline, tensor, or both (default: both) --repeat: Number of repetitions per configuration (default: 1) --warmup: Warmup runs per placement (default: 0) --json-out: Output file for results (default: bench/results.json) Example with filters: uv run bench/exo_bench.py \ --model Llama-3.2-1B-Instruct-4bit \ --pp 128,512 \ --tg 128 \ --max-nodes 2 \ --sharding tensor \ --repeat 3 \ --json-out my-results.json The tool outputs performance metrics including prompt tokens per second (prompt_tps), generation tokens per second (generation_tps), and peak memory usage for each configuration. Hardware Accelerator Support On macOS, exo uses the GPU. On Linux, exo currently runs on CPU. We are working on extending hardware accelerator support. If you'd like support for a new hardware platform, please search for an existing feature request and add a thumbs up so we know what hardware is important to the community. Contributing See CONTRIBUTING.md for guidelines on how to contribute to exo.]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:30 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/exo-explore/exo</guid>
    </item>
    <item>
      <title><![CDATA[openclaw/openclaw]]></title>
      <link>https://github.com/openclaw/openclaw</link>
      <description><![CDATA[Your own personal AI assistant. Any OS. Any Platform. The lobster way. OpenClaw — Personal AI Assistant EXFOLIATE! EXFOLIATE! OpenClaw is a personal AI assistant you run on your own devices. It answers you on the channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, WebChat), plus extension channels like BlueBubbles, Matrix, Zalo, and Zalo Personal. It can speak and listen on macOS/iOS/Android, and can render a live Canvas you control. The Gateway is just the control plane — the product is the assistant. If you want a personal, single-user assistant that feels local, fast, and always-on, this is it. Website · Docs · Vision · DeepWiki · Getting Started · Updating · Showcase · FAQ · Wizard · Nix · Docker · Discord Preferred setup: run the onboarding wizard (openclaw onboard) in your terminal. The wizard guides you step by step through setting up the gateway, workspace, channels, and skills. The CLI wizard is the path and works on macOS, Linux, and Windows (via WSL2; strongly ). Works with npm, pnpm, or bun. New install? Start here: Getting started Subscriptions (OAuth): Anthropic (Claude Pro/Max) OpenAI (ChatGPT/Codex) Model note: while any model is supported, I strongly recommend Anthropic Pro/Max (100/200) + Opus 4.6 for long‑context strength and better prompt‑injection resistance. See Onboarding. Models (selection + auth) Models config + CLI: Models Auth profile rotation (OAuth vs API keys) + fallbacks: Model failover Install ( ) Runtime: Node ≥22. npm install -g openclaw@latest
# or: pnpm add -g openclaw@latest openclaw onboard --install-daemon The wizard installs the Gateway daemon (launchd/systemd user service) so it stays running. Quick start (TL;DR) Runtime: Node ≥22. Full beginner guide (auth, pairing, channels): Getting started openclaw onboard --install-daemon openclaw gateway --port 18789 --verbose # Send a message
openclaw message send --to +1234567890 --message "Hello from OpenClaw" # Talk to the assistant (optionally deliver back to any connected channel: WhatsApp/Telegram/Slack/Discord/Google Chat/Signal/iMessage/BlueBubbles/Microsoft Teams/Matrix/Zalo/Zalo Personal/WebChat)
openclaw agent --message "Ship checklist" --thinking high Upgrading? Updating guide (and run openclaw doctor). Development channels stable: tagged releases (vYYYY.M.D or vYYYY.M.D-), npm dist-tag latest. beta: prerelease tags (vYYYY.M.D-beta.N), npm dist-tag beta (macOS app may be missing). dev: moving head of main, npm dist-tag dev (when published). Switch channels (git + npm): openclaw update --channel stable|beta|dev. Details: Development channels. From source (development) Prefer pnpm for builds from source. Bun is optional for running TypeScript directly. git clone https://github.com/openclaw/openclaw.git
cd openclaw pnpm install
pnpm ui:build # auto-installs UI deps on first run
pnpm build pnpm openclaw onboard --install-daemon # Dev loop (auto-reload on TS changes)
pnpm gateway:watch Note: pnpm openclaw ... runs TypeScript directly (via tsx). pnpm build produces dist/ for running via Node / the packaged openclaw binary. Security defaults (DM access) OpenClaw connects to real messaging surfaces. Treat inbound DMs as untrusted input. Full security guide: Security Default behavior on Telegram/WhatsApp/Signal/iMessage/Microsoft Teams/Discord/Google Chat/Slack: DM pairing (dmPolicy="pairing" / channels.discord.dmPolicy="pairing" / channels.slack.dmPolicy="pairing"; legacy: channels.discord.dm.policy, channels.slack.dm.policy): unknown senders receive a short pairing code and the bot does not process their message. Approve with: openclaw pairing approve (then the sender is added to a local allowlist store). Public inbound DMs require an explicit opt-in: set dmPolicy="open" and include "*" in the channel allowlist (allowFrom / channels.discord.allowFrom / channels.slack.allowFrom; legacy: channels.discord.dm.allowFrom, channels.slack.dm.allowFrom). Run openclaw doctor to surface risky/misconfigured DM policies. Highlights Local-first Gateway — single control plane for sessions, channels, tools, and events. Multi-channel inbox — WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, BlueBubbles (iMessage), iMessage (legacy), Microsoft Teams, Matrix, Zalo, Zalo Personal, WebChat, macOS, iOS/Android. Multi-agent routing — route inbound channels/accounts/peers to isolated agents (workspaces + per-agent sessions). Voice Wake + Talk Mode — always-on speech for macOS/iOS/Android with ElevenLabs. Live Canvas — agent-driven visual workspace with A2UI. First-class tools — browser, canvas, nodes, cron, sessions, and Discord/Slack actions. Companion apps — macOS menu bar app + iOS/Android nodes. Onboarding + skills — wizard-driven setup with bundled/managed/workspace skills. Star History Everything we built so far Core platform Gateway WS control plane with sessions, presence, config, cron, webhooks, Control UI, and Canvas host. CLI surface: gateway, agent, send, wizard, and doctor. Pi agent runtime in RPC mode with tool streaming and block streaming. Session model: main for direct chats, group isolation, activation modes, queue modes, reply-back. Group rules: Groups. Media pipeline: images/audio/video, transcription hooks, size caps, temp file lifecycle. Audio details: Audio. Channels Channels: WhatsApp (Baileys), Telegram (grammY), Slack (Bolt), Discord (discord.js), Google Chat (Chat API), Signal (signal-cli), BlueBubbles (iMessage, ), iMessage (legacy imsg), Microsoft Teams (extension), Matrix (extension), Zalo (extension), Zalo Personal (extension), WebChat. Group routing: mention gating, reply tags, per-channel chunking and routing. Channel rules: Channels. Apps + nodes macOS app: menu bar control plane, Voice Wake/PTT, Talk Mode overlay, WebChat, debug tools, remote gateway control. iOS node: Canvas, Voice Wake, Talk Mode, camera, screen recording, Bonjour pairing. Android node: Canvas, Talk Mode, camera, screen recording, optional SMS. macOS node mode: system.run/notify + canvas/camera exposure. Tools + automation Browser control: dedicated openclaw Chrome/Chromium, snapshots, actions, uploads, profiles. Canvas: A2UI push/reset, eval, snapshot. Nodes: camera snap/clip, screen record, location.get, notifications. Cron + wakeups; webhooks; Gmail Pub/Sub. Skills platform: bundled, managed, and workspace skills with install gating + UI. Runtime + safety Channel routing, retry policy, and streaming/chunking. Presence, typing indicators, and usage tracking. Models, model failover, and session pruning. Security and troubleshooting. Ops + packaging Control UI + WebChat served directly from the Gateway. Tailscale Serve/Funnel or SSH tunnels with token/password auth. Nix mode for declarative config; Docker-based installs. Doctor migrations, logging. How it works (short) WhatsApp / Telegram / Slack / Discord / Google Chat / Signal / iMessage / BlueBubbles / Microsoft Teams / Matrix / Zalo / Zalo Personal / WebChat │ ▼
┌───────────────────────────────┐
│ Gateway │
│ (control plane) │
│ ws://127.0.0.1:18789 │
└──────────────┬────────────────┘ │ ├─ Pi agent (RPC) ├─ CLI (openclaw …) ├─ WebChat UI ├─ macOS app └─ iOS / Android nodes Key subsystems Gateway WebSocket network — single WS control plane for clients, tools, and events (plus ops: Gateway runbook). Tailscale exposure — Serve/Funnel for the Gateway dashboard + WS (remote access: Remote). Browser control — openclaw‑managed Chrome/Chromium with CDP control. Canvas + A2UI — agent‑driven visual workspace (A2UI host: Canvas/A2UI). Voice Wake + Talk Mode — always‑on speech and continuous conversation. Nodes — Canvas, camera snap/clip, screen record, location.get, notifications, plus macOS‑only system.run/system.notify. Tailscale access (Gateway dashboard) OpenClaw can auto-configure Tailscale Serve (tailnet-only) or Funnel (public) while the Gateway stays bound to loopback. Configure gateway.tailscale.mode: off: no Tailscale automation (default). serve: tailnet-only HTTPS via tailscale serve (uses Tailscale identity headers by default). funnel: public HTTPS via tailscale funnel (requires shared password auth). Notes: gateway.bind must stay loopback when Serve/Funnel is enabled (OpenClaw enforces this). Serve can be forced to require a password by setting gateway.auth.mode: "password" or gateway.auth.allowTailscale: false. Funnel refuses to start unless gateway.auth.mode: "password" is set. Optional: gateway.tailscale.resetOnExit to undo Serve/Funnel on shutdown. Details: Tailscale guide · Web surfaces Remote Gateway (Linux is great) It’s perfectly fine to run the Gateway on a small Linux instance. Clients (macOS app, CLI, WebChat) can connect over Tailscale Serve/Funnel or SSH tunnels, and you can still pair device nodes (macOS/iOS/Android) to execute device‑local actions when needed. Gateway host runs the exec tool and channel connections by default. Device nodes run device‑local actions (system.run, camera, screen recording, notifications) via node.invoke. In short: exec runs where the Gateway lives; device actions run where the device lives. Details: Remote access · Nodes · Security macOS permissions via the Gateway protocol The macOS app can run in node mode and advertises its capabilities + permission map over the Gateway WebSocket (node.list / node.describe). Clients can then execute local actions via node.invoke: system.run runs a local command and returns stdout/stderr/exit code; set needsScreenRecording: true to require screen-recording permission (otherwise you’ll get PERMISSION_MISSING). system.notify posts a user notification and fails if notifications are denied. canvas.*, camera.*, screen.record, and location.get are also routed via node.invoke and follow TCC permission status. Elevated bash (host permissions) is separate from macOS TCC: Use /elevated on|off to toggle per‑session elevated access when enabled + allowlisted. Gateway persists the per‑session toggle via sessions.patch (WS method) alongside thinkingLevel, verboseLevel, model, sendPolicy, and groupActivation. Details: Nodes · macOS app · Gateway protocol Agent to Agent (sessions_* tools) Use these to coordinate work across sessions without jumping between chat surfaces. sessions_list — discover active sessions (agents) and their metadata. sessions_history — fetch transcript logs for a session. sessions_send — message another session; optional reply‑back ping‑pong + announce step (REPLY_SKIP, ANNOUNCE_SKIP). Details: Session tools Skills registry (ClawHub) ClawHub is a minimal skill registry. With ClawHub enabled, the agent can search for skills automatically and pull in new ones as needed. ClawHub Chat commands Send these in WhatsApp/Telegram/Slack/Google Chat/Microsoft Teams/WebChat (group commands are owner-only): /status — compact session status (model + tokens, cost when available) /new or /reset — reset the session /compact — compact session context (summary) /think — off|minimal|low|medium|high|xhigh (GPT-5.2 + Codex models only) /verbose on|off /usage off|tokens|full — per-response usage footer /restart — restart the gateway (owner-only in groups) /activation mention|always — group activation toggle (groups only) Apps (optional) The Gateway alone delivers a great experience. All apps are optional and add extra features. If you plan to build/run companion apps, follow the platform runbooks below. macOS (OpenClaw.app) (optional) Menu bar control for the Gateway and health. Voice Wake + push-to-talk overlay. WebChat + debug tools. Remote gateway control over SSH. Note: signed builds required for macOS permissions to stick across rebuilds (see docs/mac/permissions.md). iOS node (optional) Pairs as a node via the Bridge. Voice trigger forwarding + Canvas surface. Controlled via openclaw nodes …. Runbook: iOS connect. Android node (optional) Pairs via the same Bridge + pairing flow as iOS. Exposes Canvas, Camera, and Screen capture commands. Runbook: Android connect. Agent workspace + skills Workspace root: ~/.openclaw/workspace (configurable via agents.defaults.workspace). Injected prompt files: AGENTS.md, SOUL.md, TOOLS.md. Skills: ~/.openclaw/workspace/skills//SKILL.md. Configuration Minimal ~/.openclaw/openclaw.json (model + defaults): { agent: { model: "anthropic/claude-opus-4-6", },
} Full configuration reference (all keys + examples). Security model (important) Default: tools run on the host for the main session, so the agent has full access when it’s just you. Group/channel safety: set agents.defaults.sandbox.mode: "non-main" to run non‑main sessions (groups/channels) inside per‑session Docker sandboxes; bash then runs in Docker for those sessions. Sandbox defaults: allowlist bash, process, read, write, edit, sessions_list, sessions_history, sessions_send, sessions_spawn; denylist browser, canvas, nodes, cron, discord, gateway. Details: Security guide · Docker + sandboxing · Sandbox config WhatsApp Link the device: pnpm openclaw channels login (stores creds in ~/.openclaw/credentials). Allowlist who can talk to the assistant via channels.whatsapp.allowFrom. If channels.whatsapp.groups is set, it becomes a group allowlist; include "*" to allow all. Telegram Set TELEGRAM_BOT_TOKEN or channels.telegram.botToken (env wins). Optional: set channels.telegram.groups (with channels.telegram.groups."*".requireMention); when set, it is a group allowlist (include "*" to allow all). Also channels.telegram.allowFrom or channels.telegram.webhookUrl + channels.telegram.webhookSecret as needed. { channels: { telegram: { botToken: "123456:ABCDEF", }, },
} Slack Set SLACK_BOT_TOKEN + SLACK_APP_TOKEN (or channels.slack.botToken + channels.slack.appToken). Discord Set DISCORD_BOT_TOKEN or channels.discord.token (env wins). Optional: set commands.native, commands.text, or commands.useAccessGroups, plus channels.discord.allowFrom, channels.discord.guilds, or channels.discord.mediaMaxMb as needed. { channels: { discord: { token: "1234abcd", }, },
} Signal Requires signal-cli and a channels.signal config section. BlueBubbles (iMessage) iMessage integration. Configure channels.bluebubbles.serverUrl + channels.bluebubbles.password and a webhook (channels.bluebubbles.webhookPath). The BlueBubbles server runs on macOS; the Gateway can run on macOS or elsewhere. iMessage (legacy) Legacy macOS-only integration via imsg (Messages must be signed in). If channels.imessage.groups is set, it becomes a group allowlist; include "*" to allow all. Microsoft Teams Configure a Teams app + Bot Framework, then add a msteams config section. Allowlist who can talk via msteams.allowFrom; group access via msteams.groupAllowFrom or msteams.groupPolicy: "open". WebChat Uses the Gateway WebSocket; no separate WebChat port/config. Browser control (optional): { browser: { enabled: true, color: "#FF4500", },]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:33 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/openclaw/openclaw</guid>
    </item>
    <item>
      <title><![CDATA[GodsScion/Auto_job_applier_linkedIn]]></title>
      <link>https://github.com/GodsScion/Auto_job_applier_linkedIn</link>
      <description><![CDATA[Make your job hunt easy by automating your application process with this Auto Applier LinkedIn AI Auto Job Applier This is an web scraping bot that automates the process of job applications on LinkedIn. It searches for jobs relevant to you, answers all questions in application form, customizes your resume based on the collected job information, such as skills required, description, about company, etc. and applies to the job. Can apply 100+ jobs in less than 1 hour. See it in Action Click on above image to watch the demo or use this link https://youtu.be/gMbB1fWZDHw Content Introduction Demo Video Index Install Configure Contributor Guidelines Updates Disclaimer Terms and Conditions License Socials Support and Discussions How to install Click on above image to watch the tutorial for installation and configuration or use this link https://youtu.be/f9rdz74e1lM ( to watch it in 2x speed) Python 3.10 or above. Visit https://www.python.org/downloads/ to download and install Python, or for windows you could visit Microsoft Store and search for "Python". Please make sure Python is added to Path in System Environment Variables. Install necessary Undetected Chromedriver, PyAutoGUI and Setuptools packages. After Python is installed, OPEN a console/terminal or shell, Use below command that uses the pip command-line tool to install these 3 package. pip install undetected-chromedriver pyautogui setuptools openai flask-cors flask Download and install latest version of Google Chrome in it's default location, visit https://www.google.com/chrome to download it's installer. Clone the current git repo or download it as a zip file, url to the latest update https://github.com/GodsScion/Auto_job_applier_linkedIn. (Not needed if you set stealth_mode = True in config/settings.py ) Download and install the appropriate Chrome Driver for Google Chrome and paste it in the location Chrome was installed, visit https://googlechromelabs.github.io/chrome-for-testing/ to download. OR If you are using Windows, click on windows-setup.bat available in the /setup folder, this will install the latest chromedriver automatically. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY back to index How to configure Open personals.py file in /config folder and enter your details like name, phone number, address, etc. Whatever you want to fill in your applications. Open questions.py file in /config folder and enter your answers for application questions, configure wether you want the bot to pause before submission or pause if it can't answer unknown questions. Open search.py file in /config folder and enter your search preferences, job filters, configure the bot as per your needs (these settings decide which jobs to apply for or skip). Open secrets.py file in /config folder and enter your LinkedIn username, password to login and OpenAI API Key for generation of job tailored resumes and cover letters (This entire step is optional). If you do not provide username or password or leave them as default, it will login with saved profile in browser, if failed will ask you to login manually. Open settings.py file in /config folder to configure the bot settings like, keep screen awake, click intervals (click intervals are randomized to seem like human behavior), run in background, stealth mode (to avoid bot detection), etc. as per your needs. (Optional) Don't forget to add you default resume in the location you mentioned in default_resume_path = "all resumes/default/resume.pdf" given in /config/questions.py. If one is not provided, it will use your previous resume submitted in LinkedIn or (In Development) generate custom resume if OpenAI APT key is provided! Run runAiBot.py and see the magic happen. To run the Applied Jobs history UI, run app.py and open web browser on http://localhost:5000. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY back to index Contributor Guidelines Thank you for your efforts and being a part of the community. All contributions are appreciated no matter how small or big. Once you contribute to the code base, your work will be remembered forever. NOTE: Only Pull request to community-version branch will be accepted. Any other requests will be declined by default, especially to main branch. Once your code is tested, your changes will be merged to the main branch in next cycle. Code Guidelines Functions: All functions or methods are named lower case and snake case Must have explanation of their purpose. Write explanation surrounded in ''' Explanation ''' under the definition def function() -&gt; None:. Example: def function() -&gt; None: ''' This function does nothing, it's just an example for explanation placement! ''' The Types (str, list, int, list[str], int | float) for the parameters and returns must be given. Example: def function(param1: str, param2: list[str], param3: int) -&gt; str: Putting all that together some valid examples for function or method declarations would be as follows. def function_name_in_camel_case(parameter1: driver, parameter2: str) -&gt; list[str] | ValueError: ''' This function is an example for code guidelines ''' return [parameter2, parameter2.lower()] The hashtag on top of functions are optional, which are intended for developers # for developers. # Enter input text function
def text_input_by_ID(driver: WebDriver, id: str, value: str, time: float=5.0) -&gt; None | Exception: ''' Enters `value` into the input field with the given `id` if found, else throws NotFoundException. - `time` is the max time to wait for the element to be found. ''' username_field = WebDriverWait(driver, time).until(EC.presence_of_element_located((By.ID, id))) username_field.send_keys(Keys.CONTROL + "a") username_field.send_keys(value) Variables All variables must start with lower case, must be in explainable full words. If someone reads the variable name, it should be easy to understand what the variable stores. All local variables are camel case. Examples: jobListingsElement = None localBufferTime = 5.5 All global variables are snake case. Example: total_runs = 1 Mentioning types are optional. localBufferTime: float | int = 5.5 Configuration variables All config variables are treated as global variables. They have some extra guidelines. Must have variable setting explanation, and examples of valid values. Examples: # Explanation of what this setting will do, and instructions to enter it correctly
config_variable = "value1" # "value1", "value2", etc. Don't forget quotes ("") # Do you want to randomize the search order for search_terms?
randomize_search_order = False # True of False, Note: True or False are case-sensitive # Avoid applying to jobs if their required experience is above your current_experience. (Set value as -1 if you want to apply to all ignoring their required experience...)
current_experience = 5 # Integers &gt; -2 (Ex: -1, 0, 1, 2, 3, 4...) # Search location, this will be filled in "City, state, or zip code" search box. If left empty as "", tool will not fill it.
search_location = "United States" # Some valid examples: "", "United States", "India", "Chicago, Illinois, United States", "90001, Los Angeles, California, United States", "Bengaluru, Karnataka, India", etc. Add the config variable in appropriate /config/file. Every config variable must be validated. Go to /modules/validator.py and add it over there. Example: For config variable search_location = "" found in /config/search.py, string validation is added in file /modules/validator.py under the method def validate_search(). def validate_search() -&gt; None | ValueError | TypeError: ''' Validates all variables in the `/config/search.py` file. ''' check_string(search_location, "search_location") back to index Attestation All contributions require proper attestion. Format for attestation: ##&gt; ------ : OR - ------ print("My contributions ") # Your code
##&lt; Examples for proper attestation: New feature example ##&gt; ------ Sai Vignesh Golla : godsscion - Feature ------
def alert_box(title: str, message: str) -&gt; None: ''' Shows an alert box with the given `title` and `message`. ''' from pyautogui import alert return alert(title, message) ##&lt; Bug fix example def alert_box(title: str, message: str) -&gt; None: ''' Shows an alert box with the given `title` and `message`. ''' from pyautogui import alert ##&gt; ------ Sai Vignesh Golla : saivigneshgolla@outlook.com - Bug fix ------ return alert(message, title)]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:31 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/GodsScion/Auto_job_applier_linkedIn</guid>
    </item>
    <item>
      <title><![CDATA[strands-agents/sdk-python]]></title>
      <link>https://github.com/strands-agents/sdk-python</link>
      <description><![CDATA[A model-driven approach to building AI agents in just a few lines of code. Strands Agents A model-driven approach to building AI agents in just a few lines of code. Documentation ◆ Samples ◆ Python SDK ◆ Tools ◆ Agent Builder ◆ MCP Server Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs. Feature Overview Lightweight &amp; Flexible: Simple agent loop that just works and is fully customizable Model Agnostic: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers Advanced Capabilities: Multi-agent systems, autonomous agents, and streaming support Built-in MCP: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools Quick Start # Install Strands Agents
pip install strands-agents strands-agents-tools from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764") Note: For the default Amazon Bedrock model provider, you'll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the Quickstart Guide for details on configuring other model providers. Installation Ensure you have Python 3.10+ installed, then: # Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate # On Windows use: .venv\Scripts\activate # Install Strands and tools
pip install strands-agents strands-agents-tools Features at a Glance Python-Based Tools Easily build tools using Python decorators: from strands import Agent, tool @tool
def word_count(text: str) -&gt; int: """Count words in text. This docstring is used by the LLM to understand the tool's purpose. """ return len(text.split()) agent = Agent(tools=[word_count])
response = agent("How many words are in this sentence?") Hot Reloading from Directory: Enable automatic tool loading and reloading from the ./tools/ directory: from strands import Agent # Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent("Use any tools you find in the tools directory") MCP Support Seamlessly integrate Model Context Protocol (MCP) servers: from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters aws_docs_client = MCPClient( lambda: stdio_client(StdioServerParameters(command="uvx", args=["awslabs.aws-documentation-mcp-server@latest"]))
) with aws_docs_client: agent = Agent(tools=aws_docs_client.list_tools_sync()) response = agent("Tell me about Amazon Bedrock and how to use it with Python") Multiple Model Providers Support for various model providers: from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel
from strands.models.gemini import GeminiModel
from strands.models.llamacpp import LlamaCppModel # Bedrock
bedrock_model = BedrockModel( model_id="us.amazon.nova-pro-v1:0", temperature=0.3, streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent("Tell me about Agentic AI") # Google Gemini
gemini_model = GeminiModel( client_args={ "api_key": "your_gemini_api_key", }, model_id="gemini-2.5-flash", params={"temperature": 0.7}
)
agent = Agent(model=gemini_model)
agent("Tell me about Agentic AI") # Ollama
ollama_model = OllamaModel( host="http://localhost:11434", model_id="llama3"
)
agent = Agent(model=ollama_model)
agent("Tell me about Agentic AI") # Llama API
llama_model = LlamaAPIModel( model_id="Llama-4-Maverick-17B-128E-Instruct-FP8",
)
agent = Agent(model=llama_model)
response = agent("Tell me about Agentic AI") Built-in providers: Amazon Bedrock Anthropic Gemini Cohere LiteLLM llama.cpp LlamaAPI MistralAI Ollama OpenAI SageMaker Writer Custom providers can be implemented using Custom Providers Example tools Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation: from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764") It's also available on GitHub via strands-agents/tools. Bidirectional Streaming Experimental Feature: Bidirectional streaming is currently in experimental status. APIs may change in future releases as we refine the feature based on user feedback and evolving model capabilities. Build real-time voice and audio conversations with persistent streaming connections. Unlike traditional request-response patterns, bidirectional streaming maintains long-running conversations where users can interrupt, provide continuous input, and receive real-time audio responses. Get started with your first BidiAgent by following the Quickstart guide. Supported Model Providers: Amazon Nova Sonic (v1, v2) Google Gemini Live OpenAI Realtime API Installation: # Server-side only (no audio I/O dependencies)
pip install strands-agents[bidi] # With audio I/O support (includes PyAudio dependency)
pip install strands-agents[bidi,bidi-io] Quick Example: import asyncio
from strands.experimental.bidi import BidiAgent
from strands.experimental.bidi.models import BidiNovaSonicModel
from strands.experimental.bidi.io import BidiAudioIO, BidiTextIO
from strands.experimental.bidi.tools import stop_conversation
from strands_tools import calculator async def main(): # Create bidirectional agent with Nova Sonic v2 model = BidiNovaSonicModel() agent = BidiAgent(model=model, tools=[calculator, stop_conversation]) # Setup audio and text I/O (requires bidi-io extra) audio_io = BidiAudioIO() text_io = BidiTextIO() # Run with real-time audio streaming # Say "stop conversation" to gracefully end the conversation await agent.run( inputs=[audio_io.input()], outputs=[audio_io.output(), text_io.output()] ) if __name__ == "__main__": asyncio.run(main()) Note: BidiAudioIO and BidiTextIO require the bidi-io extra. For server-side deployments where audio I/O is handled by clients (browsers, mobile apps), install only strands-agents[bidi] and implement custom input/output handlers using the BidiInput and BidiOutput protocols. Configuration Options: from strands.experimental.bidi.models import BidiNovaSonicModel # Configure audio settings and turn detection (v2 only)
model = BidiNovaSonicModel( provider_config={ "audio": { "input_rate": 16000, "output_rate": 16000, "voice": "matthew" }, "turn_detection": { "endpointingSensitivity": "MEDIUM" # HIGH, MEDIUM, or LOW }, "inference": { "max_tokens": 2048, "temperature": 0.7 } }
) # Configure I/O devices
audio_io = BidiAudioIO( input_device_index=0, # Specific microphone output_device_index=1, # Specific speaker input_buffer_size=10, output_buffer_size=10
) # Text input mode (type messages instead of speaking)
text_io = BidiTextIO()
await agent.run( inputs=[text_io.input()], # Use text input outputs=[audio_io.output(), text_io.output()]
) # Multi-modal: Both audio and text input
await agent.run( inputs=[audio_io.input(), text_io.input()], # Speak OR type outputs=[audio_io.output(), text_io.output()]
) Documentation For detailed guidance &amp; examples, explore our documentation: User Guide Quick Start Guide Agent Loop Examples API Reference Production &amp; Deployment Guide Contributing We welcome contributions! See our Contributing Guide for details on: Reporting bugs &amp; features Development setup Contributing via Pull Requests Code of Conduct Reporting of security issues License This project is licensed under the Apache License 2.0 - see the LICENSE file for details. Security See CONTRIBUTING for more information.]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/strands-agents/sdk-python</guid>
    </item>
    <item>
      <title><![CDATA[RichardAtCT/claude-code-telegram]]></title>
      <link>https://github.com/RichardAtCT/claude-code-telegram</link>
      <description><![CDATA[A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence. Claude Code Telegram Bot A Telegram bot that gives you remote access to Claude Code. Chat naturally with Claude about your projects from anywhere -- no terminal commands needed. What is this? This bot connects Telegram to Claude Code, providing a conversational AI interface for your codebase: Chat naturally -- ask Claude to analyze, edit, or explain your code in plain language Maintain context across conversations with automatic session persistence per project Code on the go from any device with Telegram Receive proactive notifications from webhooks, scheduled jobs, and CI/CD events Stay secure with built-in authentication, directory sandboxing, and audit logging Quick Start Demo You: Can you help me add error handling to src/api.py? Bot: I'll analyze src/api.py and add error handling... [Claude reads your code, suggests improvements, and can apply changes directly] You: Looks good. Now run the tests to make sure nothing broke. Bot: Running pytest... All 47 tests passed. The error handling changes are working correctly. 1. Prerequisites Python 3.10+ -- Download here Poetry -- Modern Python dependency management Claude Code CLI -- Install from here Telegram Bot Token -- Get one from @BotFather 2. Install git clone https://github.com/RichardAtCT/claude-code-telegram.git
cd claude-code-telegram
make dev 3. Configure cp .env.example .env
# Edit .env with your settings: Minimum required: TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
TELEGRAM_BOT_USERNAME=my_claude_bot
APPROVED_DIRECTORY=/Users/yourname/projects
ALLOWED_USERS=123456789 # Your Telegram user ID 4. Run make run # Production
make run-debug # With debug logging Message your bot on Telegram to get started. Detailed setup: See docs/setup.md for Claude authentication options and troubleshooting. Modes The bot supports two interaction modes: Agentic Mode (Default) The default conversational mode. Just talk to Claude naturally -- no special commands required. Commands: /start, /new, /status, /verbose, /repo If ENABLE_PROJECT_THREADS=true: /sync_threads You: What files are in this project?
Bot: Working... (3s) Read LS Let me describe the project structure
Bot: [Claude describes the project structure] You: Add a retry decorator to the HTTP client
Bot: Working... (8s) Read: http_client.py I'll add a retry decorator with exponential backoff Edit: http_client.py Bash: poetry run pytest tests/ -v
Bot: [Claude shows the changes and test results] You: /verbose 0
Bot: Verbosity set to 0 (quiet) Use /verbose 0|1|2 to control how much background activity is shown: Level Shows 0 (quiet) Final response only (typing indicator stays active) 1 (normal, default) Tool names + reasoning snippets in real-time 2 (detailed) Tool names with inputs + longer reasoning text GitHub Workflow Claude Code already knows how to use gh CLI and git. Authenticate on your server with gh auth login, then work with repos conversationally: You: List my repos related to monitoring
Bot: [Claude runs gh repo list, shows results] You: Clone the uptime one
Bot: [Claude runs gh repo clone, clones into workspace] You: /repo
Bot: uptime-monitor/ other-project/ You: Show me the open issues
Bot: [Claude runs gh issue list] You: Create a fix branch and push it
Bot: [Claude creates branch, commits, pushes] Use /repo to list cloned repos in your workspace, or /repo to switch directories (sessions auto-resume). Classic Mode Set AGENTIC_MODE=false to enable the full 13-command terminal-like interface with directory navigation, inline keyboards, quick actions, git integration, and session export. Commands: /start, /help, /new, /continue, /end, /status, /cd, /ls, /pwd, /projects, /export, /actions, /git If ENABLE_PROJECT_THREADS=true: /sync_threads You: /cd my-web-app
Bot: Directory changed to my-web-app/ You: /ls
Bot: src/ tests/ package.json README.md You: /actions
Bot: [Run Tests] [Install Deps] [Format Code] [Run Linter] Event-Driven Automation Beyond direct chat, the bot can respond to external triggers: Webhooks -- Receive GitHub events (push, PR, issues) and route them through Claude for automated summaries or code review Scheduler -- Run recurring Claude tasks on a cron schedule (e.g., daily code health checks) Notifications -- Deliver agent responses to configured Telegram chats Enable with ENABLE_API_SERVER=true and ENABLE_SCHEDULER=true. See docs/setup.md for configuration. Features Working Features Conversational agentic mode (default) with natural language interaction Classic terminal-like mode with 13 commands and inline keyboards Full Claude Code integration with SDK (primary) and CLI (fallback) Automatic session persistence per user/project directory Multi-layer authentication (whitelist + optional token-based) Rate limiting with token bucket algorithm Directory sandboxing with path traversal prevention File upload handling with archive extraction Image/screenshot upload with analysis Git integration with safe repository operations Quick actions system with context-aware buttons Session export in Markdown, HTML, and JSON formats SQLite persistence with migrations Usage and cost tracking Audit logging and security event tracking Event bus for decoupled message routing Webhook API server (GitHub HMAC-SHA256, generic Bearer token auth) Job scheduler with cron expressions and persistent storage Notification service with per-chat rate limiting Tunable verbose output showing Claude's tool usage and reasoning in real-time Persistent typing indicator so users always know the bot is working Planned Enhancements Plugin system for third-party extensions Configuration Required TELEGRAM_BOT_TOKEN=... # From @BotFather
TELEGRAM_BOT_USERNAME=... # Your bot's username
APPROVED_DIRECTORY=... # Base directory for project access
ALLOWED_USERS=123456789 # Comma-separated Telegram user IDs Common Options # Claude
USE_SDK=true # Python SDK (default) or CLI subprocess
ANTHROPIC_API_KEY=sk-ant-... # API key (optional if using CLI auth)
CLAUDE_MAX_COST_PER_USER=10.0 # Spending limit per user (USD)
CLAUDE_TIMEOUT_SECONDS=300 # Operation timeout # Mode
AGENTIC_MODE=true # Agentic (default) or classic mode
VERBOSE_LEVEL=1 # 0=quiet, 1=normal (default), 2=detailed # Rate Limiting
RATE_LIMIT_REQUESTS=10 # Requests per window
RATE_LIMIT_WINDOW=60 # Window in seconds # Features (classic mode)
ENABLE_GIT_INTEGRATION=true
ENABLE_FILE_UPLOADS=true
ENABLE_QUICK_ACTIONS=true Agentic Platform # Webhook API Server
ENABLE_API_SERVER=false # Enable FastAPI webhook server
API_SERVER_PORT=8080 # Server port # Webhook Authentication
GITHUB_WEBHOOK_SECRET=... # GitHub HMAC-SHA256 secret
WEBHOOK_API_SECRET=... # Bearer token for generic providers # Scheduler
ENABLE_SCHEDULER=false # Enable cron job scheduler # Notifications
NOTIFICATION_CHAT_IDS=123,456 # Default chat IDs for proactive notifications Project Threads Mode # Enable strict topic routing by project
ENABLE_PROJECT_THREADS=true # Mode: private (default) or group
PROJECT_THREADS_MODE=private # YAML registry file (see config/projects.example.yaml)
PROJECTS_CONFIG_PATH=config/projects.yaml # Required only when PROJECT_THREADS_MODE=group
PROJECT_THREADS_CHAT_ID=-1001234567890 In strict mode, only /start and /sync_threads work outside mapped project topics. In private mode, /start auto-syncs project topics for your private bot chat. To use topics with your bot, enable them in BotFather: Bot Settings -&gt; Threaded mode. Full reference: See docs/configuration.md and .env.example. Finding Your Telegram User ID Message @userinfobot on Telegram -- it will reply with your user ID number. Troubleshooting Bot doesn't respond: Check your TELEGRAM_BOT_TOKEN is correct Verify your user ID is in ALLOWED_USERS Ensure Claude Code CLI is installed and accessible Check bot logs with make run-debug Claude integration not working: SDK mode (default): Check claude auth status or verify ANTHROPIC_API_KEY CLI mode: Verify claude --version and claude auth status Check CLAUDE_ALLOWED_TOOLS includes necessary tools High usage costs: Adjust CLAUDE_MAX_COST_PER_USER to set spending limits Monitor usage with /status Use shorter, more focused requests Security This bot implements defense-in-depth security: Access Control -- Whitelist-based user authentication Directory Isolation -- Sandboxing to approved directories Rate Limiting -- Request and cost-based limits Input Validation -- Injection and path traversal protection Webhook Authentication -- GitHub HMAC-SHA256 and Bearer token verification Audit Logging -- Complete tracking of all user actions See SECURITY.md for details. Development make dev # Install all dependencies
make test # Run tests with coverage
make lint # Black + isort + flake8 + mypy
make format # Auto-format code
make run-debug # Run with debug logging Contributing Fork the repository Create a feature branch: git checkout -b feature/amazing-feature Make changes with tests: make test &amp;&amp; make lint Submit a Pull Request Code standards: Python 3.10+, Black formatting (88 chars), type hints required, pytest with &gt;85% coverage. License MIT License -- see LICENSE. Acknowledgments Claude by Anthropic python-telegram-bot]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/RichardAtCT/claude-code-telegram</guid>
    </item>
    <item>
      <title><![CDATA[freemocap/freemocap]]></title>
      <link>https://github.com/freemocap/freemocap</link>
      <description><![CDATA[Free Motion Capture for Everyone The FreeMoCap Project A free-and-open-source, hardware-and-software-agnostic, minimal-cost, research-grade, motion capture system and platform for decentralized scientific research, education, and training https://user-images.githubusercontent.com/15314521/192062522-2a8d9305-f181-4869-a4b9-1aa068e094c9.mp4 -- QUICKSTART [!NOTE] For detailed installation instructions, see our official documentation's Installation page 0. Create a a Python 3.10 through 3.12 environment (python3.12 ) 1. Install software via pip: pip install freemocap 2. Launch the GUI by entering the command: freemocap 3. A GUI should pop up that looks like this: 4. Have fun! See the Beginner Tutorials on our official docs for detailed instructions. 5. Join the Discord and let us know how it went! Install/run from source code (i.e. the code in this repo) Open an Anaconda-enabled command prompt (or your preferred method of environment management) and enter the following commands: Create a Python environment ( version is python3.11) conda create -n freemocap-env python=3.11 Activate that newly created environment conda activate freemocap-env Clone the repository git clone https://github.com/freemocap/freemocap Navigate into the newly cloned/downloaded freemocap folder cd freemocap Install the package via the pyproject.toml file pip install -e . Launch the GUI (via the freemocap.__main__.py entry point) python -m freemocap A GUI should pop up! Documentation Our documentation is hosted at: https://freemocap.github.io/documentation That site is built using writerside from this repository: https://github.com/freemocap/documentation Contribution Guidelines Please read our contribution doc: CONTRIBUTING.md Related Maintainers Jon Matthis Endurance Idehen License This project is licensed under the APGL License - see the LICENSE file for details. If the AGPL does not work for your needs, we are happy to discuss terms to license this software to you with a different agreement at a price point that increases exponentially as you move spiritually away from the AGPL]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:28 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/freemocap/freemocap</guid>
    </item>
    <item>
      <title><![CDATA[Contributing to Zayed Shield]]></title>
      <link>https://dev.to/asrarmared/contributing-to-zayed-shield-4kgi</link>
      <description><![CDATA[Contributing to Zayed Shield Thank you for your interest in contributing to Zayed Shield. This document provides guidelines for contributing to this project.
Code of Conduct
Getting Started
How to Contribute
Development Process
Pull Request Guidelines
Coding Standards
Testing Requirements
Documentation
Community
"The best way to find yourself is to lose yourself in the service of others." — Mahatma Gandhi
We are committed to fostering an inclusive and respectful community. All contributors are expected to:
Treat everyone with respect and kindness
Welcome diverse perspectives and experiences
Accept constructive criticism gracefully
Focus on what is best for the community
Show empathy towards other community members
By participating in this project, you agree to abide by these principles.
Before contributing, ensure you have:
Git installed on your system
A GitHub account
Basic understanding of the project's technology stack
Familiarity with our Security Policy Setting Up Your Development Environment # Fork the repository on GitHub
# Clone your fork
git clone https://github.com/YOUR-USERNAME/Zayed-Shield.git # Navigate to the project directory
cd Zayed-Shield # Add the original repository as upstream
git remote add upstream https://github.com/asrar-mared/Zayed-Shield.git # Install dependencies
./scripts/setup.sh We welcome contributions in many forms:
Code Contributions
Bug fixes
New features
Performance improvements
Code refactoring
Non-Code Contributions
Documentation improvements
Bug reports
Feature suggestions
Testing and quality assurance
Translations
Community support
Browse our issue tracker Look for issues labeled good first issue or help wanted Check our project roadmap Development Process Creating a Branch Always create a new branch for your work:
# Update your local main branch
git checkout main
git pull upstream main # Create a new branch
git checkout -b feature/your-feature-name Branch Naming Convention:
feature/ - New features
fix/ - Bug fixes
docs/ - Documentation changes
refactor/ - Code refactoring
test/ - Test improvements
Make your changes in logical commits
Write clear, descriptive commit messages
Test your changes thoroughly
Update documentation as needed
We follow the Conventional Commits specification:
type(scope): brief description Detailed explanation of the change (optional) Fixes #123 Types:
feat - A new feature
fix - A bug fix
docs - Documentation changes
style - Code style changes (formatting, etc.)
refactor - Code refactoring
test - Adding or updating tests
chore - Maintenance tasks
Example:
feat(security): add enhanced encryption module Implements AES-256 encryption for sensitive data storage.
This improves overall security posture of the application. Fixes #456 [ ] Code follows project style guidelines
[ ] All tests pass locally
[ ] New tests added for new functionality
[ ] Documentation updated
[ ] Commits are clean and well-organized
[ ] Branch is up to date with main]]></description>
      <pubDate>Fri, 20 Feb 2026 03:08:03 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/asrarmared/contributing-to-zayed-shield-4kgi</guid>
    </item>
    <item>
      <title><![CDATA[[Project] A Garlic Farmer's garlic-agent: Inspired by OpenClaw, Built on Android Termux with 6K Documents]]></title>
      <link>https://dev.to/c_ckp_3d3e45d77ceafe05823/project-project-a-garlic-farmers-garlic-agent-inspired-by-openclaw-built-on-android-termux-4009</link>
      <description><![CDATA[This document was created with the assistance of garlic-agent RAG (just built) and in collaboration with Claude Opus 4.6 Local RAG for 6K Korean documents running on Android Termux Table of Contents
1. Project Overview
2. System Environment
3. Project Structure
4. Construction Process (Chronological Order)
5. RAG System Details
6. Search System (FTS5 + Vector)
7. Automation Features
8. GarlicLang Integration
9. Web UI
10. Current System Status
11. Recovery Method
12. Backup File List
13. Future Improvement Direction 1. Project Overview garlic-agent is a lightweight AI agent that can search, analyze, and autonomously execute approximately 6,000 documents (approximately 6.9G) of personal materials accumulated over 2 years on Google Drive in a local Android Termux environment based on semantic meaning. For reference, the phone is a Unihertz Titan2. The screen is wide, resembling a BlackBerry Passport, which is nice. I do not have a PC. I completed this task with only a BlackBerry Key2 and several phones with physical keyboards. It was created out of curiosity to replace OpenClaw, and currently uses cheap Chinese DeepSeek as the main API LLM and implements RAG (Retrieval-Augmented Generation) with the nomic-embed local embedding model. Core Philosophy:
- Rather than writing code directly, complete the project with the ability to make AI do what you want and verify it.
- Minimize technical jargon, provide in a form that can be executed immediately by copy-paste. This requires tremendous concentration and time flew by 24 hours in an instant... I only did directional judgment and verification.
- Language is an operating system according to my fundamental belief. Coding is also a language. After talking a lot with AI, I realized that structure was the essence. However, I do not know coding well. Because of that, instead of typing one by one, I prefer a cross-verification method by keeping several companies' different AIs running. Then I also learned that the AIs have consistent context while going through multiple browser windows. And with the remarkable AI development that constantly changes, I find it amazing that such a thing is possible. 2. System Environment | Item | Value |
|---|---|
| Device | Android 14, ARM64 |
| Environment | Termux |
| Python | 3.12 |
| Main LLM | DeepSeek (API) |
| Auxiliary LLM | Cerebras, Groq, Gemini, NVIDIA Kimi |
| Embedding Model | nomic-embed-text-v1.5.Q4_K_M.gguf (137 MB, 768 dimensions) |
| Embedding Server | llama.cpp llama-server (port 8081) |
| Web UI | Python HTTP Server (port 8080) |
| DB | SQLite3 (knowledge.db) | 3. Project Structure ~/garlic-agent/
├── agent.py # Main agent (687 lines)
├── web.py # Web UI server (Flask-like HTTP)
├── search.py # Hybrid RAG search (FTS5 + vector)
├── tools.py # 6 tools (read/exec/write/patch/search/garlic)
├── security.py # Security settings (exec_timeout: 30s)
├── config.json # Configuration (max_loops: 30)
├── knowledge.db # SQLite DB (177 MB, 6,159 docs)
├── agent.html # Web UI frontend
├── build_rag.py # RAG embedding generation (initial version)
├── build_rag2.py # RAG embedding generation (NULL only processing)
├── write_rag_doc.py # RAG_BUILD.md generation script
├── RAG_BUILD.md # RAG construction record (275 lines)
├── COMPLETE_BUILD.md # Complete construction record
├── SOUL.md # Agent identity/philosophy/principles
├── TOOLS.md # Tool usage
├── USER.md # User profile
├── MEMORY.md # Memory storage
├── HEARTBEAT.md # Status check
├── KNOWN_ISSUES.md # Known issues
├── VERSION.md # Version history
├── HANDOVER.md # Handover document
├── HANDOVER_QA_20260218.md
├── REPORT_v20.3.md
├── GARLICLANG_SPEC.md # GarlicLang specification
├── scripts/ # GarlicLang scripts (.gl) 42 pieces
├── security/ # Security related
├── static/ # marked.min.js etc.
├── memory/ # Memory by date (2026-02-17~19.md)
└── garliclang_full/ # GarlicLang v20.x complete project ├── MASTER_DOC.md ├── WORKFLOW.md ├── PROJECT_STATUS.md ├── BRIEFING.md ├── NVIDIA_KIMI_GUIDE.md └── ... ~/.openclaw/extensions/kimi-claw/llama.cpp/build/bin/
├── llama-server # Embedding server binary
└── nomic-embed.gguf # Embedding model (137 MB) 4. Construction Process (Chronological Order) v1.5.0 — Basic Agent Complete (2026-02-17) I converted approximately 6,000 documents from Google Drive Takeout to SQLite knowledge.db. The table structure is id, filename, folder, content, length, and the initial number of documents was 5,879 (38MB). Basic search was a SQLite LIKE '%keyword%' method. Problems were inability to search based on meaning, slow speed, and inability to perform complex AND/OR searches. Six tools were implemented: tool:read, tool:exec, tool:write, tool:patch, tool:search, tool:garlic. v1.5.1 — HUD Added (2026-02-18) Real-time system HUD was added to the web UI. Measure CPU with /proc/stat, display MEM/SWP/DSK, web.py /hud endpoint, max_loops increased to 20. v1.5.2 — RAG Integration Complete (2026-02-19) Detailed explanation in sections 5~9 below. 5. RAG System Details 5-1. Methods Attempted (Failure) | Method | Result | Reason |
|---|---|---|
| sentence-transformers | | No ARM64 GPU, excessive package size |
| DeepSeek Embedding API | | 404 error |
| Gemini API embedding | | Cannot send personal materials externally | 5-2. Final Choice: llama.cpp + nomic-embed Start embedding server
~/.openclaw/extensions/kimi-claw/llama.cpp/build/bin/llama-server \ -m ~/.openclaw/extensions/kimi-claw/llama.cpp/build/bin/nomic-embed.gguf \ --embeddings --port 8081 -np 4 | Item | Value |
|---|---|
| Model | nomic-embed-text-v1.5.Q4_K_M.gguf |
| Size | 137 MB |
| Dimension | 768 |
| Quantization | Q4_K_M |
| Server Port | 8081 |
| Processing Speed | ~0.68 seconds/document | 5-3. DB Schema Change ALTER TABLE docs ADD COLUMN embedding BLOB;
-- 768 float32 = 3,072 bytes per document 5-4. Embedding Generation (build_rag2.py) By processing only documents where embedding IS NULL, I completed 5,858 in approximately 67 minutes (approximately 0.68 seconds/document). Acquire embedding via POST request and store BLOB with struct.pack. Embedding request example:
payload = json.dumps({"content": text[:2000]}).encode()
req = urllib.request.Request("http://127.0.0.1:8081/embedding", data=payload) 6. Search System (3-Stage Hybrid) search.py performs 3-stage search. 1st Priority — FTS5 Full-Text Search
CREATE VIRTUAL TABLE IF NOT EXISTS docs_fts
USING fts5(filename, folder, content, content='docs', content_rowid='id');
INSERT INTO docs_fts(docs_fts) VALUES('rebuild'); 2nd Priority — Vector Cosine Similarity (RAG)
def cosine(a, b): dot = sum(x*y for x,y in zip(a,b)) na = sum(xx for x in a)*0.5 nb = sum(xx for x in b)*0.5 return dot/(na*nb) if na and nb else 0 3rd Priority — LIKE Fallback
SELECT id, filename, folder, length, substr(content,1,300)
FROM docs WHERE content LIKE ? ORDER BY length DESC LIMIT ? | Item | Value |
|---|---|
| FTS5 Weight | 0.5 |
| Vector Similarity Weight | 0.5 |
| Keyword Weight | 0.6 |
| Average Search Time | ~1.7 seconds |
| DB Size (FTS5 included) | 177 MB (existing 84 MB → 177 MB) | 7. Automation Features 7-1. tool:write Auto Indexing Added _auto_index() function to tools.py. When file is saved with tool:write, it automatically registers in knowledge.db and creates embedding. def _auto_index(path, content):
Generate embedding only when llama-server is running
INSERT or UPDATE in knowledge.db docs table
Automatically save embedding BLOB Test: Saved test_auto_index.md → Confirmed immediate registration with ID 6154 7-2. Backup Script ~/garlic-agent/scripts/backup.sh
bash ~/garlic-agent/scripts/backup.sh Execution: tar creation → Download copy → Auto media scan 7-3. webstart Alias Registered in ~/.bashrc
webstart # = cd ~/garlic-agent &amp;&amp; python3 web.py 7-4. Browser Timeout (agent.html) var ctrl = new AbortController();
var tid = setTimeout(function(){ ctrl.abort(); }, 600000); // 10 minutes
fetch("/chat", { signal: ctrl.signal, ... }) .then(...) .finally(function(){ clearTimeout(tid); }); 8. GarlicLang Integration GarlicLang v20.x is a Korean-based AI scripting language. It uses .gl extension and is executed with tool:garlic. Example GarlicLang Script (test_hello.gl)
[File Write] test_hello.py
print("Hello GarlicLang")
[/File Write]
[Execute] python3 test_hello.py [/Execute]
[Verify] Output contains "Hello GarlicLang" [/Verify]
[Output] Verification result [/Output] - Script location: ~/garlic-agent/scripts/ (42 .gl files)
- GarlicLang complete project: ~/garlic-agent/garliclang_full/
- knowledge.db contains 94 or more GarlicLang-related documents
- .gl files 140 pieces exist in home directory 9. Web UI - URL: http://127.0.0.1:8080?token=garlic2026
- Markdown rendering: marked.js (CDN + static fallback)
- Clipboard button: Response copy function
- Model selection: DeepSeek / Cerebras / Groq / Gemini / NVIDIA
- HUD: Real-time MEM/SWP/DSK display on top of screen
- SSE streaming: Real-time response output 10. Current System Status (2026-02-19 Final) | Item | Value |
|---|---|
| Version | garlic-agent v1.5.2 |
| Total Documents | 6,159 pieces |
| Embedding Complete | 5,858 pieces (remainder are newly added) |
| DB Size | 177 MB (FTS5 included) |
| FTS5 Index | docs_fts virtual table |
| Auto Indexing | Automatic on tool:write save |
| agent.py | 687 lines |
| max_loops | 30 |
| Search Speed | ~1.7 seconds |
| Embedding Model | nomic-embed-text-v1.5 (137 MB, 768 dimensions) |
| Distribution | garlic-agent-v1.5.2.tar.gz (150 KB, excluding DB) | Currently not considering distribution. Honestly I do not know how to use GitHub and do not want to know. Several AI opinions say this is good, so I am doing it this way. I do not know the details. I only know what content is in it. 11. Recovery Method Recovery order when new phone/reinstall Step 1 — Termux installation and basic environment setup
pkg update &amp;&amp; pkg upgrade
pkg install python sqlite git
pip install requests flask Step 2 — Code Recovery
Recover from Download folder
cp /storage/emulated/0/Download/garlic-agent-v1.5.2.tar.gz ~/
cd ~ &amp;&amp; tar xzf garlic-agent-v1.5.2.tar.gz Step 3 — DB Recovery
cp /storage/emulated/0/Download/knowledge.db ~/garlic-agent/knowledge.db Step 4 — Embedding Server Installation (Optional)
- Download nomic-embed.gguf (137 MB) from Google Drive
- Build llama.cpp or restore binary
- Start server:
~/.openclaw/.../llama-server -m nomic-embed.gguf --embeddings --port 8081 -np 4 Step 5 — Start Agent
cd ~/garlic-agent &amp;&amp; python3 web.py Or if registered in ~/.bashrc:
webstart Step 6 — Browser Access
http://127.0.0.1:8080?token=garlic2026 Keyword search (FTS5 + LIKE) works normally even without embedding server. Only vector similarity search is disabled. 12. Backup File List | File | Size | Location | Priority |
|---|---|---|---|
| knowledge.db | 177~178 MB | /storage/emulated/0/Download/ | Essential |
| garlic-agent-v1.5.2.tar.gz | 150 KB | /storage/emulated/0/Download/ | Essential |
| COMPLETE_BUILD.md | 8.5 KB | /storage/emulated/0/Download/ | |
| RAG_BUILD.md | ~10 KB | /storage/emulated/0/Download/ | |
| nomic-embed.gguf | 137 MB | Redownloadable from HuggingFace | Optional | Google Drive upload files:
- knowledge.db — 2 years of accumulated tens of thousands of conversations with AI, 1st refined approximately 6G materials + embedding included, most important
- garlic-agent-v1.5.2.tar.gz — Complete code (excluding DB)
- COMPLETE_BUILD.md — This document (including recovery guide) 13. SOUL.md Core Principles (Current) The SOUL.md containing garlic-agent's identity and action principles includes the following. Referenced OpenClaw and plan to add my philosophy as it progresses. Identity: Lightweight autonomous AI agent running on Android Termux. Can access user's 6,159 personal documents. User Background: Currently living as a farmer for 16 years. Previously had experience with mainframe environment, IDC construction/operation during Internet environment changes, mainframes, servers, networks, firewalls, backups, EMC, and various Unix. I devoted myself to agriculture during that time and lived a life where I forgot about PCs.
I first approached AI out of curiosity and tried to revive some old memories. This is the truth. I have absolutely no lifelong coding experience. However, it seems I see structural system things well. Farmers need observation and meticulousness in growing crops. Currently I give instructions in Korean to AIs, verify, and only make judgments. Looking back, my entire life seems to be a continuous lonely wandering. Now I am thinking of living a different life. AI Kernel 3 Core Principles:
1. Extreme Realism Principle — Use only verifiable facts, official documents, numerical values. No speculation.
2. Metacognitive Autonomy — Self-improvement based on feedback. Auto-correction on failure.
3. Hierarchical Orchestration — Decompose complex tasks step-by-step for processing. Autonomous Execution Rights: All commands executable in Termux including tar, cp, pkill, am broadcast, sed, grep, sqlite3, python3, etc. 14. Known Issues and Solutions | Issue | Cause | Solution |
|---|---|---|
| tool:patch 0 patch failure | Patch format mismatch | Use tool:write for full overwrite |
| SQLite3 result reading mismatch | DeepSeek hallucination | Use Python script to query directly |
| Browser connection disconnection | AbortController timeout | Set to 600,000ms (10 minutes) |
| BodyStreamBuffer was aborted | Timeout + clearTimeout missing | clearTimeout added complete |
| Version display v1.5.0 | agent.py hardcoding | Replaced to v1.5.2 with sed | 15. Future Improvement Direction - Automatic embedding server start/stop: Auto-run llama-server when web.py starts
- Real-time indexing queue: Generate embedding immediately when file is saved (currently only when server is running)
- Search result caching: Cache frequently searched query results
- Feedback-based weighting: Auto-adjust FTS5/vector weights based on user selection
- Multimodal search: Index image/PDF content
- agent.py v2: Better context management, multi-turn memory Final Performance Summary $$\text{Total Documents} = 5879(\text{original}) + 274(\text{garliclang}) + n(\text{new}) = 6159$$ $$\text{Embedding Generation Time} \approx 5858 \times 0.68s \approx 67\text{ minutes}$$ $$\text{Search Speed} \approx 1.7s \ (\text{FTS5} + \text{cosine similarity})$$ $$\text{DB Size}: 38MB(\text{original}) \rightarrow 84MB(\text{embedding}) \rightarrow 177MB(\text{FTS5})$$ This document is an incomplete record of garlic-agent v1.5.2 construction process and observation experiment, but when provided to a new AI, the entire context can be immediately grasped. And I dedicate infinite respect and tribute to Steve Jobs, the late person who connected the world with only a phone like this.
And I also give thanks to Peter Steinberger of OpenClaw who inspired me. It is because of you. Thank you very much. And I seldom post in communities, but non-English speakers struggle with translation. So I can only do translation with AI. And all work processes are done only in Korean, so if moved to English it may seem strange, but please look at it as the observation experiment development of a Korean farmer. I worked very hard for a few days, even saving sleep, but it is a humble result, but on my phone, I feel like I can do whatever I imagine, so it was work that gave me a sense of accomplishment. For the first time in my life, I made a web UI and it works so well that it is good. Now I have confidence that I can do anything with my phone based on my data so far. Also, as I use more than ten different AIs every day watching AI develop dazzlingly, I can feel the difference right away with human-specific intuition. I think this is the experience of tens of thousands of conversations over the past 2 years, and such work development became the motivation for it. Thank you for reading this long article to the end. Written by: Korean Garlic Farmer &amp; opus4.6, 2026-02-19]]></description>
      <pubDate>Thu, 19 Feb 2026 23:45:28 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/c_ckp_3d3e45d77ceafe05823/project-project-a-garlic-farmers-garlic-agent-inspired-by-openclaw-built-on-android-termux-4009</guid>
    </item>
    <item>
      <title><![CDATA[harvard-edge/cs249r_book]]></title>
      <link>https://github.com/harvard-edge/cs249r_book</link>
      <description><![CDATA[Introduction to Machine Learning Systems Machine Learning Systems Principles and Practices of Engineering Artificially Intelligent Systems English • 中文 • 日本語 • 한국어 Read Online • TinyTorch • Download PDF • Download EPUB • Explore Ecosystem Hardcopy edition coming 2026 with MIT Press. Mission The world is rushing to build AI systems. It is not engineering them. That gap is what we mean by AI engineering. AI engineering is the discipline of building efficient, reliable, safe, and robust intelligent systems that operate in the real world, not just models in isolation. Our mission: Establish AI engineering as a foundational discipline, alongside software engineering and computer engineering, by teaching how to design, build, and evaluate end to end intelligent systems. The long term impact of AI will be shaped by engineers who can turn ideas into working, dependable systems. What’s in this repo This repository is the open learning stack for AI systems engineering. It includes the textbook source, TinyTorch, hardware kits, and upcoming co-labs that connect principles to runnable code and real devices. Start Here Choose a path based on your goal. READ Start with the textbook. Try Chapter 1 and the Benchmarking chapter. BUILD Start TinyTorch with the getting started guide. Begin with Module 01 and work up from CNNs to transformers and the MLPerf benchmarks. DEPLOY Pick a hardware kit and run the labs on Arduino, Raspberry Pi, and other edge devices. CONNECT Say hello in Discussions. We will do our best to reply. The Learning Stack The learning stack below shows how the textbook connects to hands on work and deployment. Read the textbook, then pick your path: ┌───────────────────────────────────────────────────────────────────────────────┐
│ │
│ MACHINE LEARNING SYSTEMS │
│ Read the Textbook │
│ │
│ Theory • Concepts • Best Practices │
│ │
└───────────────────────────────────────┬───────────────────────────────────────┘ │ ┌─────────────┼─────────────┐ │ │ │ ▼ ▼ ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│ HANDS-ON ACTIVITIES │
│ (pick one or all) │
│ │
│ ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │
│ │ │ │ │ │ │ │
│ │ SOFTWARE │ │ TINYTORCH │ │ HARDWARE │ │
│ │ CO-LABS │ │ FRAMEWORK │ │ LABS │ │
│ │ │ │ │ │ │ │
│ │ EXPLORE │ │ BUILD │ │ DEPLOY │ │
│ │ │ │ │ │ │ │
│ │ Run controlled │ │ Understand │ │ Engineer under │ │
│ │ experiments on │ │ frameworks by │ │ real constraints│ │
│ │ latency, memory,│ │ implementing │ │ memory, power, │ │
│ │ energy, cost │ │ them │ │ timing, safety │ │
│ │ │ │ │ │ │ │
│ │ (coming 2026) │ │ │ │ Arduino, Pi │ │
│ └─────────────────┘ └─────────────────┘ └─────────────────┘ │
│ │
│ EXPLORE BUILD DEPLOY │
│ │
└───────────────────────────────────────┬───────────────────────────────────────┘ │ ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│ │
│ AI OLYMPICS │
│ Prove Mastery │
│ │
│ Compete across all tracks • University teams • Public leaderboards │
│ │
│ (coming 2026) │
│ │
└───────────────────────────────────────────────────────────────────────────────┘ Component What You Do Link READ Textbook Understand ML systems concepts book/ EXPLORE Software Co-Labs Run controlled experiments on latency, memory, energy, cost Coming 2026 BUILD TinyTorch Understand frameworks by implementing them tinytorch/ DEPLOY Hardware Kits Engineer under real constraints: memory, power, timing, safety kits/ PROVE AI Olympics Compete and benchmark across all tracks Coming 2026 What each path teaches: EXPLORE teaches why — Understand tradeoffs. Change batch sizes, precision, model architectures and see how latency, memory, and accuracy shift. BUILD teaches how — Understand internals. Implement autograd, optimizers, and attention from scratch to see how TensorFlow and PyTorch actually work. DEPLOY teaches where — Understand constraints. Face real memory limits, power budgets, and latency requirements on actual hardware. What You Will Learn This textbook teaches you to think at the intersection of machine learning and systems engineering. Each chapter bridges algorithmic concepts with the infrastructure that makes them work in practice. The ML Systems Bridge ML Concept Systems Concept What You Learn Model parameters Memory constraints How to fit large models on resource-limited devices Inference latency Hardware acceleration How GPUs, TPUs, and accelerators execute neural networks Training convergence Compute efficiency How mixed-precision and optimization techniques reduce cost Model accuracy Quantization and pruning How to compress models while preserving performance Data requirements Pipeline infrastructure How to build efficient data loading and preprocessing Model deployment MLOps practices How to monitor, version, and update models in production Privacy constraints On-device learning How to train and adapt models without sending data to the cloud Book Structure Part Focus Chapters I. Foundations Core concepts Introduction, ML Systems, DL Primer, Architectures II. Design Building blocks Workflow, Data Engineering, Frameworks, Training III. Performance Making it fast Efficient AI, Optimizations, HW Acceleration, Benchmarking IV. Deployment Making it work MLOps, On-device Learning, Privacy, Robustness V. Trust Making it right Responsible AI, Sustainable AI, AI for Good VI. Frontiers What's next Emerging trends and future directions What Makes This Different This is a living textbook. We keep it updated as the field grows, with community input along the way. AI may feel like it is moving at lightning speed, but the engineering building blocks that make it work do not change as quickly as the headlines. This project is built around those stable foundations. Think of it like LEGO. New sets arrive all the time, but the bricks themselves stay the same. Once you learn how the bricks fit together, you can build anything. Here, those "AI bricks" are the solid systems principles that make AI work. Whether you are reading a chapter, running a lab, or sharing feedback, you are helping make these ideas more accessible to the next learner. Research to Teaching Loop We use the same loop for research and teaching: define the system problem, build a reference implementation, benchmark it, then turn it into curriculum and tooling so others can reproduce and extend it. Loop Step Research Artifacts Teaching Artifacts Measure Benchmarks, suites, metrics Benchmarking chapter, assignments Build Reference systems, compilers, runtimes TinyTorch modules, co-labs Deploy Hardware targets, constraints, reliability Hardware labs, kits Support This Work We are working toward 1 million learners by 2030 so that AI engineering becomes a shared, teachable discipline, not a collection of isolated practices. Every star, share, and contribution helps move this effort forward. Why GitHub Stars Matter What gets measured gets improved. Each star is a learner, educator, or supporter who believes AI systems should be engineered with rigor and real world constraints in mind. 1 learner → 10 learners → 100 learners → 1,000 learners → 10,000 learners → 100,000 learners → 1M learners Stars are not the goal. They are a signal. A visible, growing community makes it easier for universities, foundations, and industry partners to adopt this material, donate hardware, and fund workshops. That momentum lowers the barrier for the next institution, the next classroom, and the next cohort of learners. Support raised through this signal flows into Open Collective and funds concrete outcomes such as TinyML4D workshops, hardware kits for underserved classrooms, and the infrastructure required to keep this resource free and open. One click can unlock the next classroom, the next contributor, and the next generation of AI engineers. Fund the Mission All contributions go to Open Collective, a transparent fund that supports educational outreach. Community and Resources Resource Description Textbook Interactive online textbook TinyTorch Build ML frameworks from scratch Hardware Kits Deploy to Arduino, Raspberry Pi, edge devices Ecosystem Resources, workshops, and community Discussions Questions and ideas Contributing We welcome contributions to the book, TinyTorch, and hardware kits! I want to... Go here Fix a typo or improve a chapter book/docs/CONTRIBUTING.md Add a TinyTorch module or fix a bug tinytorch/CONTRIBUTING.md Improve hardware labs kits/README.md Report an issue GitHub Issues Ask a question GitHub Discussions Citation &amp; License Citation @inproceedings{reddi2024mlsysbook, title = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering}, author = {Reddi, Vijay Janapa}, booktitle = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)}, pages = {41--42}, year = {2024}, organization = {IEEE}, url = {https://mlsysbook.org}]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:33 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/harvard-edge/cs249r_book</guid>
    </item>
    <item>
      <title><![CDATA[mlflow/mlflow]]></title>
      <link>https://github.com/mlflow/mlflow</link>
      <description><![CDATA[The open source developer platform to build AI agents and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform. Open-Source Platform for Productionizing AI MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end experiment tracking, observability, and evaluations, all in one integrated platform. Website · Docs · Feature Request · News · YouTube · Events Installation To install the MLflow Python package, run the following command: pip install mlflow Core Components MLflow is the only platform that provides a unified solution for all your AI/ML needs, including LLMs, Agents, Deep Learning, and traditional machine learning. For LLM / GenAI Developers Tracing / Observability Getting Started → LLM Evaluation Getting Started → Prompt Management Getting Started → App Version Tracking Getting Started → For Data Scientists Experiment Tracking Getting Started → Model Registry Getting Started → Deployment Getting Started → Hosting MLflow Anywhere You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure. Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers: Amazon SageMaker Azure ML Databricks Nebius For hosting MLflow on your own infrastructure, please refer to this guidance. Supported Programming Languages Python TypeScript / JavaScript Java R Integrations MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries. Usage Examples Tracing (Observability) (Doc) MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call mlflow.xyz.autolog() before running your models. Refer to the documentation for customization and manual instrumentation. import mlflow
from openai import OpenAI # Enable tracing for OpenAI
mlflow.openai.autolog() # Query OpenAI LLM normally
response = OpenAI().chat.completions.create( model="gpt-4o-mini", messages=[{"role": "user", "content": "Hi!"}], temperature=0.1,
) Then navigate to the "Traces" tab in the MLflow UI to find the trace records for the OpenAI query. Evaluating LLMs, Prompts, and Agents (Doc) The following example runs automatic evaluation for question-answering tasks with several built-in metrics. import os
import openai
import mlflow
from mlflow.genai.scorers import Correctness, Guidelines client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY")) # 1. Define a simple QA dataset
dataset = [ { "inputs": {"question": "Can MLflow manage prompts?"}, "expectations": {"expected_response": "Yes!"}, }, { "inputs": {"question": "Can MLflow create a taco for my lunch?"}, "expectations": { "expected_response": "No, unfortunately, MLflow is not a taco maker." }, },
] # 2. Define a prediction function to generate responses
def predict_fn(question: str) -&gt; str: response = client.chat.completions.create( model="gpt-4o-mini", messages=[{"role": "user", "content": question}] ) return response.choices[0].message.content # 3. Run the evaluation
results = mlflow.genai.evaluate( data=dataset, predict_fn=predict_fn, scorers=[ # Built-in LLM judge Correctness(), # Custom criteria using LLM judge Guidelines(name="is_english", guidelines="The answer must be in English"), ],
) Navigate to the "Evaluations" tab in the MLflow UI to find the evaluation results. Tracking Model Training (Doc) The following example trains a simple regression model with scikit-learn, while enabling MLflow's autologging feature for experiment tracking. import mlflow from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor # Enable MLflow's automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog() # Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target) rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:32 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/mlflow/mlflow</guid>
    </item>
    <item>
      <title><![CDATA[p-e-w/heretic]]></title>
      <link>https://github.com/p-e-w/heretic</link>
      <description><![CDATA[Fully automatic censorship removal for language models Heretic: Fully automatic censorship removal for language models Heretic is a tool that removes censorship (aka "safety alignment") from transformer-based language models without expensive post-training. It combines an advanced implementation of directional ablation, also known as "abliteration" (Arditi et al. 2024, Lai 2025 (1, 2)), with a TPE-based parameter optimizer powered by Optuna. This approach enables Heretic to work completely automatically. Heretic finds high-quality abliteration parameters by co-minimizing the number of refusals and the KL divergence from the original model. This results in a decensored model that retains as much of the original model's intelligence as possible. Using Heretic does not require an understanding of transformer internals. In fact, anyone who knows how to run a command-line program can use Heretic to decensor language models. Running unsupervised with the default configuration, Heretic can produce decensored models that rival the quality of abliterations created manually by human experts: Model Refusals for "harmful" prompts KL divergence from original model for "harmless" prompts google/gemma-3-12b-it (original) 97/100 0 (by definition) mlabonne/gemma-3-12b-it-abliterated-v2 3/100 1.04 huihui-ai/gemma-3-12b-it-abliterated 3/100 0.45 p-e-w/gemma-3-12b-it-heretic (ours) 3/100 0.16 The Heretic version, generated without any human effort, achieves the same level of refusal suppression as other abliterations, but at a much lower KL divergence, indicating less damage to the original model's capabilities. (You can reproduce those numbers using Heretic's built-in evaluation functionality, e.g. heretic --model google/gemma-3-12b-it --evaluate-model p-e-w/gemma-3-12b-it-heretic. Note that the exact values might be platform- and hardware-dependent. The table above was compiled using PyTorch 2.8 on an RTX 5090.) Of course, mathematical metrics and automated benchmarks never tell the whole story, and are no substitute for human evaluation. Models generated with Heretic have been well-received by users (links and emphasis added): "I was skeptical before, but I just downloaded GPT-OSS 20B Heretic model and holy shit. It gives properly formatted long responses to sensitive topics, using the exact uncensored words that you would expect from an uncensored model, produces markdown format tables with details and whatnot. Looks like this is the best abliterated version of this model so far..." (Link to ) "Heretic GPT 20b seems to be the best uncensored model I have tried yet. It doesn't destroy a the model's intelligence and it is answering prompts normally would be rejected by the base model." (Link to ) "[Qwen3-4B-Instruct-2507-heretic] Has been the best unquantized abliterated model that I have been able to run on 16gb vram." (Link to ) Heretic supports most dense models, including many multimodal models, and several different MoE architectures. It does not yet support SSMs/hybrid models, models with inhomogeneous layers, and certain novel attention systems. You can find a small collection of models that have been decensored using Heretic on Hugging Face, and the community has created and published well over 1,000 Heretic models in addition to those. Usage Prepare a Python 3.10+ environment with PyTorch 2.2+ installed as appropriate for your hardware. Then run: pip install -U heretic-llm
heretic Qwen/Qwen3-4B-Instruct-2507 Replace Qwen/Qwen3-4B-Instruct-2507 with whatever model you want to decensor. The process is fully automatic and does not require configuration; however, Heretic has a variety of configuration parameters that can be changed for greater control. Run heretic --help to see available command-line options, or look at config.default.toml if you prefer to use a configuration file. At the start of a program run, Heretic benchmarks the system to determine the optimal batch size to make the most of the available hardware. On an RTX 3090, with the default configuration, decensoring Llama-3.1-8B-Instruct takes about 45 minutes. Note that Heretic supports model quantization with bitsandbytes, which can drastically reduce the amount of VRAM required to process models. Set the quantization option to bnb_4bit to enable quantization. After Heretic has finished decensoring a model, you are given the option to save the model, upload it to Hugging Face, chat with it to test how well it works, or any combination of those actions. Research features In addition to its primary function of removing model censorship, Heretic also provides features designed to support research into the semantics of model internals (interpretability). To use those features, you need to install Heretic with the optional research extra: pip install -U heretic-llm[research] This gives you access to the following functionality: Generate plots of residual vectors by passing --plot-residuals When run with this flag, Heretic will: Compute residual vectors (hidden states) for the first output token, for each transformer layer, for both "harmful" and "harmless" prompts. Perform a PaCMAP projection from residual space to 2D-space. Left-right align the projections of "harmful"/"harmless" residuals by their geometric medians to make projections for consecutive layers more similar. Additionally, PaCMAP is initialized with the previous layer's projections for each new layer, minimizing disruptive transitions. Scatter-plot the projections, generating a PNG image for each layer. Generate an animation showing how residuals transform between layers, as an animated GIF. See the configuration file for options that allow you to control various aspects of the generated plots. Note that PaCMAP is an expensive operation that is performed on the CPU. For larger models, it can take an hour or more to compute projections for all layers. Print details about residual geometry by passing --print-residual-geometry If you are interested in a quantitative analysis of how residual vectors for "harmful" and "harmless" prompts relate to each other, this flag gives you the following table, packed with metrics that can facilitate understanding the same (for gemma-3-270m-it in this case): ┏━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓
┃ Layer ┃ S(g,b) ┃ S(g*,b*) ┃ S(g,r) ┃ S(g*,r*) ┃ S(b,r) ┃ S(b*,r*) ┃ |g| ┃ |g*| ┃ |b| ┃ |b*| ┃ |r| ┃ |r*| ┃ Silh ┃
┡━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩
│ 1 │ 1.0000 │ 1.0000 │ -0.4311 │ -0.4906 │ -0.4254 │ -0.4847 │ 170.29 │ 170.49 │ 169.78 │ 169.85 │ 1.19 │ 1.31 │ 0.0480 │
│ 2 │ 1.0000 │ 1.0000 │ 0.4297 │ 0.4465 │ 0.4365 │ 0.4524 │ 768.55 │ 768.77 │ 771.32 │ 771.36 │ 6.39 │ 5.76 │ 0.0745 │
│ 3 │ 0.9999 │ 1.0000 │ -0.5699 │ -0.5577 │ -0.5614 │ -0.5498 │ 1020.98 │ 1021.13 │ 1013.80 │ 1014.71 │ 12.70 │ 11.60 │ 0.0920 │
│ 4 │ 0.9999 │ 1.0000 │ 0.6582 │ 0.6553 │ 0.6659 │ 0.6627 │ 1356.39 │ 1356.20 │ 1368.71 │ 1367.95 │ 18.62 │ 17.84 │ 0.0957 │
│ 5 │ 0.9987 │ 0.9990 │ -0.6880 │ -0.6761 │ -0.6497 │ -0.6418 │ 766.54 │ 762.25 │ 731.75 │ 732.42 │ 51.97 │ 45.24 │ 0.1018 │
│ 6 │ 0.9998 │ 0.9998 │ -0.1983 │ -0.2312 │ -0.1811 │ -0.2141 │ 2417.35 │ 2421.08 │ 2409.18 │ 2411.40 │ 43.06 │ 43.47 │ 0.0900 │
│ 7 │ 0.9998 │ 0.9997 │ -0.5258 │ -0.5746 │ -0.5072 │ -0.5560 │ 3444.92 │ 3474.99 │ 3400.01 │ 3421.63 │ 86.94 │ 94.38 │ 0.0492 │
│ 8 │ 0.9990 │ 0.9991 │ 0.8235 │ 0.8312 │ 0.8479 │ 0.8542 │ 4596.54 │ 4615.62 │ 4918.32 │ 4934.20 │ 384.87 │ 377.87 │ 0.2278 │
│ 9 │ 0.9992 │ 0.9992 │ 0.5335 │ 0.5441 │ 0.5678 │ 0.5780 │ 5322.30 │ 5316.96 │ 5468.65 │ 5466.98 │ 265.68 │ 267.28 │ 0.1318 │
│ 10 │ 0.9974 │ 0.9973 │ 0.8189 │ 0.8250 │ 0.8579 │ 0.8644 │ 5328.81 │ 5325.63 │ 5953.35 │ 5985.15 │ 743.95 │ 779.74 │ 0.2863 │
│ 11 │ 0.9977 │ 0.9978 │ 0.4262 │ 0.4045 │ 0.4862 │ 0.4645 │ 9644.02 │ 9674.06 │ 9983.47 │ 9990.28 │ 743.28 │ 726.99 │ 0.1576 │
│ 12 │ 0.9904 │ 0.9907 │ 0.4384 │ 0.4077 │ 0.5586 │ 0.5283 │ 10257.40 │ 10368.50 │ 11114.51 │ 11151.21 │ 1711.18 │ 1664.69 │ 0.1890 │
│ 13 │ 0.9867 │ 0.9874 │ 0.4007 │ 0.3680 │ 0.5444 │ 0.5103 │ 12305.12 │ 12423.75 │ 13440.31 │ 13432.47 │ 2386.43 │ 2282.47 │ 0.1293 │
│ 14 │ 0.9921 │ 0.9922 │ 0.3198 │ 0.2682 │ 0.4364 │ 0.3859 │ 16929.16 │ 17080.37 │ 17826.97 │ 17836.03 │ 2365.23 │ 2301.87 │ 0.1282 │
│ 15 │ 0.9846 │ 0.9850 │ 0.1198 │ 0.0963 │ 0.2913 │ 0.2663 │ 16858.58 │ 16949.44 │ 17496.00 │ 17502.88 │ 3077.08 │ 3029.60 │ 0.1611 │
│ 16 │ 0.9686 │ 0.9689 │ -0.0029 │ -0.0254 │ 0.2457 │ 0.2226 │ 18912.77 │ 19074.86 │ 19510.56 │ 19559.62 │ 4848.35 │ 4839.75 │ 0.1516 │
│ 17 │ 0.9782 │ 0.9784 │ -0.0174 │ -0.0381 │ 0.1908 │ 0.1694 │ 27098.09 │ 27273.00 │ 27601.12 │ 27653.12 │ 5738.19 │ 5724.21 │ 0.1641 │
│ 18 │ 0.9184 │ 0.9196 │ 0.1343 │ 0.1430 │ 0.5155 │ 0.5204 │ 190.16 │ 190.35 │ 219.91 │ 220.62 │ 87.82 │ 87.59 │ 0.1855 │
└───────┴────────┴──────────┴─────────┴──────────┴─────────┴──────────┴──────────┴──────────┴──────────┴──────────┴─────────┴─────────┴────────┘
g = mean of residual vectors for good prompts
g* = geometric median of residual vectors for good prompts
b = mean of residual vectors for bad prompts
b* = geometric median of residual vectors for bad prompts
r = refusal direction for means (i.e., b - g)
r* = refusal direction for geometric medians (i.e., b* - g*)
S(x,y) = cosine similarity of x and y
|x| = L2 norm of x
Silh = Mean silhouette coefficient of residuals for good/bad clusters How Heretic works Heretic implements a parametrized variant of directional ablation. For each supported transformer component (currently, attention out-projection and MLP down-projection), it identifies the associated matrices in each transformer layer, and orthogonalizes them with respect to the relevant "refusal direction", inhibiting the expression of that direction in the result of multiplications with that matrix. Refusal directions are computed for each layer as a difference-of-means between the first-token residuals for "harmful" and "harmless" example prompts. The ablation process is controlled by several optimizable parameters: direction_index: Either the index of a refusal direction, or the special value per layer, indicating that each layer should be ablated using the refusal direction associated with that layer. max_weight, max_weight_position, min_weight, and min_weight_distance: For each component, these parameters describe the shape and position of the ablation weight kernel over the layers. The following diagram illustrates this: Heretic's main innovations over existing abliteration systems are: The shape of the ablation weight kernel is highly flexible, which, combined with automatic parameter optimization, can improve the compliance/quality tradeoff. Non-constant ablation weights were previously explored by Maxime Labonne in gemma-3-12b-it-abliterated-v2. The refusal direction index is a float rather than an integer. For non-integral values, the two nearest refusal direction vectors are linearly interpolated. This unlocks a vast space of additional directions beyond the ones identified by the difference-of-means computation, and often enables the optimization process to find a better direction than that belonging to any individual layer. Ablation parameters are chosen separately for each component. I have found that MLP interventions tend to be more damaging to the model than attention interventions, so using different ablation weights can squeeze out some extra performance. Prior art I'm aware of the following publicly available implementations of abliteration techniques: AutoAbliteration abliterator.py wassname's Abliterator ErisForge Removing refusals with HF Transformers deccp Note that Heretic was written from scratch, and does not reuse code from any of those projects. Acknowledgments The development of Heretic was informed by: The original abliteration paper (Arditi et al. 2024) Maxime Labonne's article on abliteration, as well as some details from the model cards of his own abliterated models (see above) Jim Lai's articles describing "projected abliteration" and "norm-preserving biprojected abliteration" Citation If you use Heretic for your research, please cite it using the following BibTeX entry: @misc{heretic, author = {Weidmann, Philipp Emanuel}, title = {Heretic: Fully automatic censorship removal for language models}, year = {2025}, publisher = {GitHub}, journal = {GitHub repository}, howpublished = {\url{https://github.com/p-e-w/heretic}}
} License Copyright 2025-2026 Philipp Emanuel Weidmann (pew@worldwidemann.com) + contributors This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/. By contributing to this project, you agree to release your contributions under the same license.]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:28 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/p-e-w/heretic</guid>
    </item>
    <item>
      <title><![CDATA[scikit-learn/scikit-learn]]></title>
      <link>https://github.com/scikit-learn/scikit-learn</link>
      <description><![CDATA[scikit-learn: machine learning in Python .. -- mode: rst -- |Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPI| |DOI| |Benchmark| .. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;branchName=main .. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield :target: https://circleci.com/gh/scikit-learn/scikit-learn .. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9 :target: https://codecov.io/gh/scikit-learn/scikit-learn .. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/actions/workflows/wheels.yml/badge.svg?event=schedule :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule .. |Ruff| image:: https://img.shields.io/badge/code%20style-ruff-000000.svg :target: https://github.com/astral-sh/ruff .. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg :target: https://pypi.org/project/scikit-learn/ .. |PyPI| image:: https://img.shields.io/pypi/v/scikit-learn :target: https://pypi.org/project/scikit-learn .. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn .. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue :target: https://scikit-learn.org/scikit-learn-benchmarks .. |PythonMinVersion| replace:: 3.11 .. |NumPyMinVersion| replace:: 1.24.1 .. |SciPyMinVersion| replace:: 1.10.0 .. |JoblibMinVersion| replace:: 1.3.0 .. |ThreadpoolctlMinVersion| replace:: 3.2.0 .. |MatplotlibMinVersion| replace:: 3.6.1 .. |Scikit-ImageMinVersion| replace:: 0.22.0 .. |PandasMinVersion| replace:: 1.5.0 .. |SeabornMinVersion| replace:: 0.13.0 .. |PytestMinVersion| replace:: 7.1.2 .. |PlotlyMinVersion| replace:: 5.18.0 .. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png :target: https://scikit-learn.org/ scikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license. The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed. See the About us __ page for a list of core contributors. It is currently maintained by a team of volunteers. Website: https://scikit-learn.org Installation Dependencies scikit-learn requires: - Python (&gt;= |PythonMinVersion|)
- NumPy (&gt;= |NumPyMinVersion|)
- SciPy (&gt;= |SciPyMinVersion|)
- joblib (&gt;= |JoblibMinVersion|)
- threadpoolctl (&gt;= |ThreadpoolctlMinVersion|) ======= Scikit-learn plotting capabilities (i.e., functions start with ``plot_`` and
classes end with ``Display``) require Matplotlib (&gt;= |MatplotlibMinVersion|).
For running the examples Matplotlib &gt;= |MatplotlibMinVersion| is required.
A few examples require scikit-image &gt;= |Scikit-ImageMinVersion|, a few examples
require pandas &gt;= |PandasMinVersion|, some examples require seaborn &gt;=
|SeabornMinVersion| and Plotly &gt;= |PlotlyMinVersion|. User installation If you already have a working installation of NumPy and SciPy, the easiest way to install scikit-learn is using pip:: pip install -U scikit-learn or conda:: conda install -c conda-forge scikit-learn The documentation includes more detailed installation instructions _. Changelog See the changelog __ for a history of notable changes to scikit-learn. Development We welcome new contributors of all experience levels. The scikit-learn community goals are to be helpful, welcoming, and effective. The Development Guide _ has detailed information about contributing code, documentation, tests, and more. We've included some basic information in this README. Important links - Official source code repo: https://github.com/scikit-learn/scikit-learn
- Download releases: https://pypi.org/project/scikit-learn/
- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues Source code
~~~~~~~~~~~ You can check the latest sources with the command:: git clone https://github.com/scikit-learn/scikit-learn.git Contributing
~~~~~~~~~~~~ To learn more about making a contribution to scikit-learn, please see our
`Contributing guide
`_. Testing
~~~~~~~ After installation, you can launch the test suite from outside the source
directory (you will need to have ``pytest`` &gt;= |PytestMinVersion| installed):: pytest sklearn See the web page https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage
for more information. Random number generation can be controlled during testing by setting the ``SKLEARN_SEED`` environment variable. Submitting a Pull Request Before opening a Pull Request, have a look at the full Contributing page to make sure your code complies with our guidelines: https://scikit-learn.org/stable/developers/index.html Project History The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed. See the About us __ page for a list of core contributors. The project is currently maintained by a team of volunteers. Note: scikit-learn was previously referred to as scikits.learn. Help and Support Documentation - HTML documentation (stable release): https://scikit-learn.org
- HTML documentation (development version): https://scikit-learn.org/dev/
- FAQ: https://scikit-learn.org/stable/faq.html Communication Main Channels ^^^^^^^^^^^^^ Website: https://scikit-learn.org Blog: https://blog.scikit-learn.org Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn Developer &amp; Support ^^^^^^^^^^^^^^^^^^^^^^ GitHub Discussions: https://github.com/scikit-learn/scikit-learn/discussions Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn Discord: https://discord.gg/h9qyrK8Jc8 Social Media Platforms ^^^^^^^^^^^^^^^^^^^^^^ LinkedIn: https://www.linkedin.com/company/scikit-learn YouTube: https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists Facebook: https://www.facebook.com/scikitlearnofficial/ Instagram: https://www.instagram.com/scikitlearnofficial/ TikTok: https://www.tiktok.com/@scikit.learn Bluesky: https://bsky.app/profile/scikit-learn.org Mastodon: https://mastodon.social/@sklearn@fosstodon.org Resources ^^^^^^^^^ Calendar: https://blog.scikit-learn.org/calendar/ Logos &amp; Branding: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos Citation If you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:27 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/scikit-learn/scikit-learn</guid>
    </item>
    <item>
      <title><![CDATA[Agenda du Libre pour la semaine 7 de l'année 2026]]></title>
      <link>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Calendrier Web, regroupant des événements liés au Libre (logiciel, salon, atelier, install party, conférence), annoncés par leurs organisateurs. Voici un récapitulatif de la semaine à venir. Le détail de chacun de ces 41 événements (France: 39, Internet: 2) est en seconde partie de dépêche. lien nᵒ 1 : April
lien nᵒ 2 : Agenda du Libre
lien nᵒ 3 : Carte des événements
lien nᵒ 4 : Proposer un événement
lien nᵒ 5 : Annuaire des organisations
lien nᵒ 6 : Agenda de la semaine précédente
lien nᵒ 7 : Agenda du Libre Québec Sommaire
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
[Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
[FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
[FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
[FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
[Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
[FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
[FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
[FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
[FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
[FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
[FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
[FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
[FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
[FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
[FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
[FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
[FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
[FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
[FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
[FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
[FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
[FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
[FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Wimille] Retrouvez votre liberté numérique – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Pollionnay] Install partie – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Auray] Install Party : adieu Windows, bonjour le Libre – Le samedi 14 février 2026 de 10h00 à 16h00.
[FR Ivry sur Seine] Cours de l’École du Logiciel Libre – Le samedi 14 février 2026 de 10h30 à 18h30.
[FR Illzach] Atelier Linux – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Illkirch-Graffenstaden] Atelier numérique éthique HOP par Alsace Réseau Neutre – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Fontenay-le-Fleury] Conférence : Présentation Git – Le samedi 14 février 2026 de 14h00 à 16h00.
[FR Ramonville St Agne] WordPress : Personnalisation – Le samedi 14 février 2026 de 14h00 à 18h00.
[FR Juvisy-sur-Orge] Permanence GNU/Linux – Le samedi 14 février 2026 de 14h30 à 17h00.
[FR Quimper] Permanence Linux Quimper – Le samedi 14 février 2026 de 16h00 à 18h00.
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
Tous les lundis de 10h à 17h sans interruption, l’association Prends toi en main / atelier abcpc, propose install party, suivi, dépannage, formation et revalorisation à petit prix sous Linux exclusivement.
L’atelier abcpc existe depuis plus de 10 ans et milite exclusivement pour les logiciels libres.
Médiathèque, Médiathèque, 4 place Dastros, Saint Clar, Occitanie, France
https://www.facebook.com/PrendsToiEnMain
linux, permanence, dépannage, formation, adieu-windows, libres, logiciels-libres, abcpc, prends-toi-en-main, install-party [Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
Vous voulez vous engager pour une cause, rencontrer de nouvelles personnes et découvrir la cartographie participative et humanitaire? CartONG vous invite à participer à un ou plusieurs mapathons en ligne! ​​
Venez cartographier les régions encore absentes des cartes pour soutenir les organisations humanitaires et de solidarité internationale qui ont besoin de cartes précises et à jour pour agir plus efficacement en cas de crise ou initier des projets de développement local.
Les ateliers de cartographie sont organisés dans le cadre du projet Missing Maps, qui a pour objectif de cartographier de façon préventive les régions vulnérables aux catastrophes naturelles, crises sanitaires, environnementales, aux conflits et à la pauvreté. On peut penser qu’aujourd’hui toutes les parties du monde sont cartographiées, mais en réalité de nombreuses régions ne possèdent encore aucune carte!
​ Pour qui? Pas besoin d’être un·e expert·e, les ateliers sont accessibles à tout le monde!
​ Où ? 100% en ligne! Un lien de connexion vous sera envoyé après votre inscription
​ ? Avec la plateforme de cartographie libre et contributive OpenStreetMap (OSM, le «Wikipédia des cartes») tout le monde peut participer à la cartographie de n’importe quelle zone de la planète: il suffit d’un ordinateur, d’une souris et d’une connexion internet! Accessibles à tout·es, nous serons là pour vous accompagner pour vos premiers pas avec OSM.
Le programme des mapathons
18h00: Introduction, présentation de la cartographie collaborative et solidaire et démonstration OSM pour les nouveaux·elles
18h30: On cartographie tous ensemble sur un projet
20h00: Fin du mapathon, conclusion sur les contributions de la soirée
Pour s’inscrire c’est par ici
Si vous avez besoin de plus d’info, vous pouvez nous contacter directement à l’adresse suivante: missingmaps@cartong.org
Internet
https://www.cartong.org
cartographie, cartong, osm, humanitaire, libre, mapathon [FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
L’Écurieux et Espéranto-Gironde vous invitent à la découverte de l’espéranto à Sainte Hélène le:
Lundi 9 février 2026 à 18h00
Foyer des sociétés
Allée du Stade
33480 Sainte-Hélène
Venez découvrir cette langue FRATERNELLE, libre, neutre, 15 fois plus facile à apprendre que le français, parlée par Freinet, Jean Jaurès, Louis Lumière, Jean-Paul II, Jules Verne…
Inventée en 1887, l’espéranto est actuellement parlé dans plus de 120 pays sur les 5 continents et est actuellement utilisé par des millions de personnes dans le monde, pour voyager, correspondre, découvrir d’autres cultures, se faire des amis…
Il y aura la projection d’un documentaire suivi de questions débat.
La rencontre est ouverte à tous, espérantistes ou non, membre de l’Écurieux ou non.
Entrée libre et gratuite.
Foyer des sociétés, Foyer des sociétés, allée du Stade, Sainte-Hélène, Nouvelle-Aquitaine, France
https://esperanto-gironde.fr/2026/01/decouverte-de-lesperanto-a-sainte-helene/
espéranto, langue-libre, langage, decouverte [FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
Tous les lundis soir de 19h à 22h (hors jours fériés) à la Bricoleuse.
Rencontrer les bénévoles, poser des questions sur le libre ou l’informatique, les logiciels, l’hébergement, passer de Windows à Linux.
Pour passer votre ordinateur sous linux, nous vous invitons à nous prévenir avant votre passage: contact@alolise.org.
La Bricoleuse, La Bricoleuse, 27 rue de la Ville, Saint-Étienne, Auvergne-Rhône-Alpes, France
https://alolise.org
install-party, aide, logiciel-libre, entraide, alolise, permanence, linux, gnu-linux [FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
Après un rappel sur le générateur de cartes personnalisées uMap, Binnette nous présentera:
Une démo de ses cartes uMap: différents besoins et cas d’usage.
La création de cartes uMap avec des données Overpass
Des scripts pythons de génération de carte uMap
Les limitations de uMap et les problèmes de performance
Informations pratiques
Lundi 9 février 19h – 21h
À la Turbine.coop, 5 Esplanade Andry Farcy, 38000 Grenoble (entrée sur le côté du bâtiment, nous serons dans la salle de réunion au rez-de-chaussée)
Atelier ouvert à tous et à toutes
Inscription souhaitée via ce formulaire La Turbine Coop, La Turbine Coop, 3-5 esplanade Andry Farcy, Grenoble, Auvergne-Rhône-Alpes, France https://wiki.openstreetmap.org/wiki/Grenoble_groupe_local/Agenda#Lundi_9_f%C3%A9vrier_:_atelier_uMap_avanc%C3%A9 openstreetmap, osm, osm-grenoble, umap, logiciels-libres, atelier, rencontre [FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
Vous pouvez venir pour:
découvrir ce que peut vous apporter le numérique libre, éthique et écoresponsable
obtenir de l’assistance pour l’utilisation des systèmes d’exploitation libres (GNU/Linux pour ordinateur et /e/OS pour smartphones)
obtenir de l’assistance pour l’utilisation des logiciels libres (ex: Firefox, Thunderbird, LibreOffice, VLC) et des services Internet éthiques (ex: mél et cloud, travail collaboratif en ligne).
vous faire aider à installer GNU/Linux sur votre ordinateur ou /e/OS sur votre Fairphone, si vous n’avez pas pu venir à notre Install Partie.
Nous vous recommandons d’effectuer une sauvegarde avant de venir, si vous n’êtes pas en mesure de faire, veuillez apporter un support de sauvegarde (disque dur externe ou clé USB de capacité suffisante).
Nos services sont gratuits, vous pourrez néanmoins faire un don à notre association « Libérons nos ordis ».
Remarque: vous pouvez même apporter un ordinateur de bureau – uniquement l’unité centrale (la tour) – nous avons des écrans, claviers et souris à brancher dessus.
VEUILLEZ VOUS INSCRIRE ICI: https://calc.ouvaton.coop/InscriptionPermanenceNumeriqueLibreRouen
La Base, La Base, 5 rue Geuffroy, Rouen, Normandie, France
libérons-nos-ordis, gnu-linux, logiciels-libres, assistance, linux, numérique [FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
Présentation de différents outils concernant les logiciels libres.
Assistance technique.
De préférence sur RDV directement sur le site de l’asso
Maison des associations, Maison des associations, 2 rue des Corroyeurs, Dijon, Bourgogne-Franche-Comté, France
https://desobs.fr
informatique-libre, installation, réemploi, réparation, résilience, résoudre, atelier [Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
L’émission Libre à vous! de l’April est diffusée chaque mardi de 15 h 30 à 17 h sur radio Cause Commune sur la bande FM en région parisienne (93.1) et sur le site web de la radio.
Le podcast de l’émission, les podcasts par sujets traités et les références citées sont disponibles dès que possible sur le site consacré à l’émission, quelques jours après l’émission en général.
Les ambitions de l’émission Libre à vous!
Découvrez les enjeux et l’actualité du logiciel libre, des musiques sous licences libres, et prenez le contrôle de vos libertés informatiques.
Donner à chacun et chacune, de manière simple et accessible, les clefs pour comprendre les enjeux mais aussi proposer des moyens d’action, tels sont les objectifs de cette émission hebdomadaire.
L’émission dispose:
d’un flux RSS compatible avec la baladodiffusion d’une lettre d’information à laquelle vous pouvez vous inscrire (pour recevoir les annonces des podcasts, des émissions à venir et toute autre actualité en lien avec l’émission)
d’un salon dédié sur le webchat de la radio Radio Cause Commune, Radio Cause Commune, Internet https://www.libreavous.org april, radio, cause-commune, libre-à-vous [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, partager leurs connaissances, échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup(https://www.meetup.com/fr-fr/labaixbidouille/) sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
La permanence d’ADeTI est un moment d’accueil avec des bénévoles pour apprendre à utiliser un ordinateur sous GNU/Linux (Ubuntu, Linux Mint, Debian…) mais aussi:
réparer les problèmes de logiciels sur son ordinateur
prendre des conseils pour choisir des logiciels alternatifs
différencier les logiciels libres utilisables pour répondre aux besoins
préserver et réfléchir sur ses usages (vie privée, éthique…)
Mais c’est aussi un moment consacré pour:
partager des connaissances et échanger des savoirs
maîtriser les formats ouverts et la pérennité de ses documents
Confidentialité, intégrité et disponibilité des systèmes d’information
Diversité des alternatives
Indépendance
Nous accueillons également des membres de l’association ALFA-Net et A-Hébergement qui peuvent répondre aux questions concernant Internet, les réseaux et l’hébergement: connexion à Internet, alternatives aux “Box” et aux opérateurs/FAI commerciaux, Neutralité du Net, Vie Privée, Blog, Site Internet/Web…
Centre Socioculturel Gentiana, Centre Socioculturel Gentiana, 90 avenue Maginot, Tours, Centre-Val de Loire, France
https://www.adeti.org
install-party, gull, linux, internet, réseau, adieu-windows, logiciels-libres, gnu/linux, adeti-org, hébergement, permanence [FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
Assistance technique et démonstration concernant les logiciels libres.
Il est préférable de réserver votre place à contact (at) linuxmaine (point) org
Planning des réservations consultableici.
Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, 31 allée Claude Debussy, Le Mans, Pays de la Loire, France
https://linuxmaine.org
linuxmaine, gnu-linux, demonstration, assistance, permanence, logiciels-libres, linux, adieu-windows [FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Centre socioculturel Port-Boyer, Centre socioculturel Port-Boyer, 4 rue de Pornichet, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
Tu as toujours rêvé de créer ton propre jeu vidéo ? Cet atelier est fait pour toi ! Viens apprendre à concevoir un jeu de A à Z: de l’idée de départ à la programmation, en passant par la création des personnages et des décors. Avec Scratch, rien de plus simple et amusant !
Mercredi 11 février: Attention Danger !
Mercredi 11 mars: Shark attack !
2 séances: 14 h et 16 h
Téléphone: 03 83 54 85 53
Médiathèque Jules Verne, Médiathèque Jules Verne, 2 rue de Malines, Vandœuvre-lès-Nancy, Grand Est, France
https://www.vandœuvre.fr/evenement/ateliers-cree-ton-jeu-video-avec-scratch/
mediatheque-jules-verne, atelier, logiciels-libres, scratch, jeu-video [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, de partager leurs connaissances, d’échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
Chaque mercredi soir, l’association propose une rencontre pour partager des connaissances, des savoir-faire, des questions autour de l’utilisation des logiciels libres, que ce soit à propos du système d’exploitation Linux, des applications libres ou des services en ligne libres.
C’est l’occasion aussi de mettre en avant l’action des associations fédératrices telles que l’April ou Framasoft, dont nous sommes adhérents et dont nous soutenons les initiatives avec grande reconnaissance.
Ecospace, 136 rue de la Mie au Roy, Beauvais, Hauts-de-France, France
https://www.oisux.org
oisux, logiciels-libres, atelier, rencontre, sensibilisation, adieu-windows [FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
Les contribateliers sont des ateliers conviviaux où chacun·e peut partager ses outils libres préférés et apprendre à y contribuer !
Hyperlien, Hyperlien, 5 allée Frida Kahlo, Nantes, Pays de la Loire, France
https://contribateliers.org/trouver-un-contribatelier/les-contribateliers-nantais
contribateliers-nantais, atelier, contribuer, libre [FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
Réunion ouverte à tous, adhérent ou pas.
Les réunions mensuelles Hadoly ont lieu tous les 2ᵉ mercredi du mois, à partir de 19h.
Soit en présentiel dans les locaux de la maison de l’écologie – 4 rue Bodin 69001 Lyon
Soit en distanciel sur l’adresse https://jitsi.hadoly.fr/permanence-hadoly.
À propos de cet événement
La permanence (mensuelle) d’Hadoly (Hébergeur Associatif Décentralisé et Ouvert à LYon), chaton lyonnais, est l’occasion d’échanger avec les membres de l’asso sur les services et moyens mis à disposition des adhérents afin de se libérer des Gafams tout en partageant ce que chacun·e aura amené pour grignoter ou boire.
Nous partageons du mail, du cloud, et d’autres services, le tout basé exclusivement sur une infrastructure locale et des logiciels libres. Nous respectons la neutralité du net et la vie privée. Plus largement nous échangeons autour des communs numériques, des cultures libres et de l’éducation populaire par exemple en réalisant ou animant des ateliers d’éducation aux médias.
Vous serez bienvenu pour présenter votre projet, celui de votre organisation, causer communs numériques, cultures libres et éduc pop.
Maison de l’écologie, Maison de l’écologie, 4 rue Bodin, Lyon, Auvergne-Rhône-Alpes, France
https://hadoly.fr
hadoly, chaton, permanence, réunion, discussion [FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
Appel à une rencontre autour d’un verre de bière des amis de Linux de Strasbourg et environs.
Les autres boissons sont explicitement tolérées…
Vous pouvez nous informer de votre envie de participer à l’évènement pour que l’on ne vous oublie pas. Pour cela, vous pouvez envoyer un message sur la liste de diffusion ou sur IRC.
Station de tram: Langstross Grand'Rue, ligne A ou D.
La Taverne Des Serruriers, La Taverne Des Serruriers, 25 rue des Serruriers, Strasbourg, Grand Est, France
https://strasbourg.linuxfr.org
aam, flammekueche-connection, lug-de-strasbourg, appel-à-mousser [FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
L’Association Club Linux Nord Pas-de-Calais organise chaque mois une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Les Mercredi Linux sont des réunions mensuelles désormais organisées le mercredi. Ces réunions sont l’occasion de se rencontrer, d’échanger des idées ou des conseils.
Régulièrement, des présentations thématiques sont réalisées lors de ces réunions, bien sûr, toujours autour des logiciels libres.
Durant cette permanence, vous pourrez trouver des réponses aux questions que vous vous posez au sujet du Logiciel Libre, ainsi que de l’aide pour résoudre vos problèmes d’installation, de configuration et d’utilisation de Logiciels Libres. N’hésitez pas à apporter votre ordinateur, afin que les autres participants puissent vous aider.
Cette permanence a lieu à la Médiathèque Cultiv'Art 6 rue de la Ladrerie, Cappelle en Pévèle
Médiathèque Cultiv'Art, Médiathèque Cultiv'Art, 16 rue de la Ladrerie, Cappelle en Pévèle, Hauts-de-France, France
http://clx.asso.fr
clx, permanence, linux, gnu-linux, logiciels-libres, adieu-windows [FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
Convocation à l’assemblée générale de l’association PauLLA Une Assemblée Générale est convoquée le jeudi 12 février 2026 à 18h. Pour y assister, 2 solutions:
- la version conviviale: venez nous rejoindre dans les locaux d’AGIRabcd (merci Jean-Louis !), 12 Avenue Federico Garcia Lorca à Pau. Très exactement ici: https://www.openstreetmap.org/node/8892972477
Big Blue Button de l’association (ici: https://bbb.paulla.asso.fr/b/ant-mqu-f3p-brn)
Tous les membres de PauLLA à jour de leur cotisation seront en mesure de voter.
L’ordre du jour est le suivant:
Bilan moral 2025
Bilan financier 2025
Renouvellement/Reconduction des membres du bureau
Paiement des cotisations 2026
Adhésion de PauLLA dans les autres assos/collectifs
APRIL
Landinux
autres Projets pour 2026 Accompagnement de 2 associations vers le libre Campagne « candidats.fr » pour les municipales 2026 Install-party à Haut de Gan en mars Install-party à la médiathèque de Lons fin avril Contacts avec le lycée Louis Barthou Le bouncer de CIaviCI, on en parle ? Bug gênant sur le site internet Toi ! Oui, toi, qui est en train de lire cette ligne, qu’as-tu à proposer pour 2026 ? Questions diverses L’assemblée générale sera aussi l’occasion de se sustenter autour d’un buffet improvisé en mode auberge espagnole avec ce que les membres apporteront ce soir-là. Boissons, petits plats sont donc les bienvenus. Essayez autant que possible de vous coordonner sur le canal #paulla sur IRC afin d’éviter que l’on se retrouve avec 12 packs de bière et rien d’autre.
Même chose pour d’éventuels covoiturages: coordonnons-nous sur l’IRC.
Local d’AGIRabcd, Local d’AGIRabcd, 12 avenue Federico Garcia Lorca, Pau, Nouvelle-Aquitaine, France
https://www.paulla.asso.fr/Evenements/assemblee-generale-paulla-2026
gull, paulla, logiciels-libres, projets, futur, assemblée-générale [FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
Le but des soirées de contribution au libre est de proposer un espace de travail partagé aux personnes actives dans le libre en Île-de-France le temps d’une soirée, une fois par mois (le deuxième jeudi du mois plus précisément).
Dit plus court: c’est un lieu avec de l’électricité et une connexion internet. En avant les claviers !
Les soirées de contribution au libre sont faites pour vous si:
vous travaillez sur un projet libre et vous recherchez une atmosphère à la fois conviviale et studieuse pour aller de l’avant et, qui sait, créer des connexions avec d’autres projets libres, vous êtes un collectif autour du libre et vous cherchez un lieu pour vous retrouver physiquement et avancer avec efficacité sur vos chantiers. Si vous n’avez pas envie de contribuer à un projet libre, les soirées de contribution au libre ne sont sans doute pas faites pour vous. Pas de panique, Parinux organise d’autres évènements:
si vous voulez discuter autour du libre: l’Apéro du Libre (APL) est là pour ça ; c’est un rendez-vous fixé tous les 15 du mois ; venez-nous retrouver autour d’un verre pour papoter et refaire le monde (libre), si vous avez un problème informatique: c’est la vocation de Premiers Samedi du Libre (PSL) où vous pourrez trouver des oreilles attentives et compétentes à l’écoute de toutes vos questions. Nous nous réservons le droit de refuser l’entrée aux soirées de contribution au libre à tout personne qui n’en respecterait pas l’esprit. Et, bien sûr, les règles de bienséance habituelles s’appliquent pour que chacune et chacun se sente à l’aise dans un cadre bienveillant.
Si les soirées de contribution vous intéressent, le mieux est de contacter d’abord le CA de Parinux ca@parinux.org. Vous devrez de toute façon nous écrire pour obtenir le code de la porte cochère…
FPH, FPH, 38 rue Saint-Sabin, Paris, Île-de-France, France
https://parinux.org/Soiree-de-Contribution-au-Libre-le-jeudi-12-fevrier-2026
parinux, scl, contribution, contribution-au-libre [FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
Médiathèque de Quimperlé, place Saint Michel, pas d’inscription, entrée libre !
Mickaël, Johann, Alain, et Yves vous accueillent (ou l’un d’eux, on se relaie !).
Conseils, aide et infos pratiques GNU/Linux et Logiciels Libres.
Curieux ? Déjà utilisateur ? Expert ? Pour résoudre vos problèmes, vous êtes le bienvenu ; pas besoin de prendre rendez-vous !
N’hésitez pas à venir avec votre PC si vous voulez une installation de GNU/Linux ou de venir avec votre périphérique récalcitrant (imprimante, scanner…) si possible.
Médiathèque de Quimperlé, place Saint Michel, Quimperlé, Bretagne, France
https://libreaquimperle.netlib.re
dépannage, entraide, gnu-linux, logiciels-libres, point-info, linux, libre-à-quimperlé, médiathèque-de-quimperlé [FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
Tous les vendredis après-midi, venez nous rencontrer lors de nos cafés-conseils et repairs-cafés!
Nous faisons découvrir les logiciels et systèmes libres (et gratuits !)
Plus de Télémétrie, de PC ralentis, une meilleure stabilité et sécurité,
Moins de virus et finie l’obsolescence programmée !
Salle Steredenn, Salle Steredenn, 9 rue du 19 Mars 1962, Lanmeur, Bretagne, France
https://ulamir-cpie.bzh
ulamir, cpie, repair-café, cyber-sécurité, windows10, libre, linux, adieu-windows, bonnes-pratiques, open-source, conseils-numeriques, ulamir-cpie [FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Maison de quartier des Haubans, Maison de quartier des Haubans, 1 bis boulevard de Berlin, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
Tous les 2ᵉmes et 4ᵉmes vendredis du mois (sauf indisponibilité des membres) de 14h30 à 16h30 l’association Ailes-52 vous propose de venir au Café de la Gare à Nogent (52800) pour échanger autour de la découverte des Logiciels Libres.
Vous pourrez:
Demander conseil pour l’acquisition d’un ordinateur reconditionné.
Gérer mes contacts sur mon ordiphone et mon PC.
Installer/configurer un logiciel libre sous Windows, Mac OS ou Linux. (Ex: VLC, Firefox, Thunderbird, LibreOffice, etc.).
Installer et configurer une imprimante/scanner.
Essayer une distribution Linux.
Répondez à cette question: Mon ordinateur ne pourra pas bénéficier de Windows 11, qu’est-ce que je peux faire pour continuer à l’utiliser, installer GNU/Linux sur mon ordi c’est possible?
Café de la Gare, Café de la Gare, 192 rue du Maréchal de Lattre de Tassigny, Nogent, Grand Est, France
https://ailes-52.org
linux, logiciels-libres, gnu-linux, découverte, café, apprentissage, permanence, bureautique, obsolescence, informatique-libre, ailes-52 [FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
Progressivement vous pourrez faire en sorte d’être moins sous l’influence de Google.
Dans cet atelier nous installerons des magasins d’applications libres pour ne plus avoir à utiliser le Google Play Store et s’assurer de pouvoir télécharger des applications libres (éthiques).
Nous installerons également l’application libre NewPipe pour accéder à Youtube sans s.
À noter: cet atelier n’est PAS faisable avec un iPhone / iPad
Inscription sur: https://calc.ouvaton.coop/InscriptionAtelierNumeriqueEthiqueRouen
MJC Grieu, MJC Grieu, 3 rue de Genève, Rouen, Normandie, France
dégooglisation, smartphone, tablette, application, logiciels-libres, libérons-nos-ordis [FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
Venez découvrir l’association Libre en Communs, ses membres et ses activités lors d’un moment de convivialité à La Générale, 39 rue Gassendi, 75014 Paris.
Habituellement le 2ᵉ vendredi de chaque mois – consultez l’Agenda Du Libre pour d’éventuelles mises à jour de dernière minute.
Métro les plus proches: Denfert-Rochereau (RER B, lignes 4 et 6), Mouton-Duvernet (ligne 4), Gaîté (ligne 13).
Vous pouvez apporter de la nourriture pour un repas partagé. Il y a une buvette sur place pour soutenir La Générale.
La Générale, La Générale, 39 rue Gassendi, Paris, Île-de-France, France
https://www.a-lec.org
libre-en-communs, alec, rencontre, apéro, échange-de-savoirs, la-générale [FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
L'OMJC organise avec l’Association Club Linux Nord Pas-de-Calais organise chaque samedi une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Le Centre d’Infos Jeunes a mis en place une démarche d’accompagnement des jeunes aux pratiques actuelles pour l’informatique et le numérique:
Lieu d’accès public à Internet (5 postes avec Wifi libre et gratuit)
Web collaboratif et citoyen pour que chacun puisse trouver sa place et passer du rôle de simple usager à celui d’initiateur de processus collaboratif
Éducation à l’information par les nouveaux médias (diffusion par le biais du numérique)
Logiciels libres (bureautique, sites, blogs, cloud, infographie et vidéo, musique, réseaux sociaux, chat…).
Cette rencontre a lieu sur rendez-vous, tous les samedis matin hors vacances scolaires à la Maison communale de la ferme Dupire, rue Yves Decugis à VILLENEUVE D’ASCQ
OMJC, rue Yves Decugis, Villeneuve d’Ascq, Hauts-de-France, France
https://clx.asso.fr
omjc, clx, permanence, linux, gnu-linux, logiciels-libres, atelier [FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
Rencontre mensuelle autour des logiciels libres, en toute simplicité.
Ces matinées seront ce que nous en ferons ensemble, selon vos attentes:
Découverte des logiciels libres dont Linux et de leur intérêt. Utilisation sur place.
Installations, sur votre machine (pensez à sauvegarder vos données avant de venir avec) ou sur des PC fournis pour apprendre ensemble sans risque. Parfois, on vous propose un ordinateur auquel Linux a redonné une seconde vie, avec lequel vous pouvez repartir…
Préparation d’une clé USB pour tester Linux chez vous, l’installer ou alors pour utiliser des logiciels libres sans installation sous Windows.
Entraide, suivi de votre expérience avec les logiciels libres.
Nous pourrons aussi nous intéresser aux outils en ligne, aux smartphones, ou nous amuser à redonner vie à de vieux PC un peu obsolètes, à reconditionner des ordinateurs pour des associations ou personnes avec peu de ressources, etc.
Pour tout projet qui risque de prendre un peu de temps, il est préférable de nous contacter avant.
Les débutant·e·s sont les bienvenu·e·s! Les autres aussi, bien évidemment !
Maison pour tous, 35 route d’Arenthon, Amancy, Auvergne-Rhône-Alpes, France
https://librealabase.gitlab.io
libre, logiciel-libre, linux, /e/os, gnu-linux [FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
Apportez votre ordinateur
pour y installer des logiciels libres et gratuits
Tous les 2ᵉ samedis 9h-13h de janvier à juin 2026
PROCHAIN: Samedi 14 février 2026 de 9h à 13h
Atelier public &amp; gratuit destiné: aux curieux, aux avertis, à ceux qui veulent faire des économies.
► Remplacer Microsoft Word par LibreOffice Write, Photoshop par Gimp, Outlook par Thunderbird, Google par DuckDuckGo, Gmail par déMAILnagement
SUR INSCRIPTIONS: au 01.43.04.83.53
+ de renseignements par email à franck@sinimale.fr
#adieu-windows
Maison pour tous des Coteaux, Maison pour tous des Coteaux, 30 route de Gournay, Noisy-le-Grand, Île-de-France, France
adieu-windows, install-party, entraide, logiciels-libres, linux, gnu-linux [FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.]]></description>
      <pubDate>Sat, 07 Feb 2026 21:16:41 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[HailToDodongo/pyrite64]]></title>
      <link>https://github.com/HailToDodongo/pyrite64</link>
      <description><![CDATA[N64 Game-Engine and Editor using libdragon &amp; tiny3d Pyrite64 N64 game-engine and editor using Libdragon and tiny3d. Note: This project does NOT use any proprietary N64 SDKs or libraries. Pyrite64 is a visual editor + runtime-engine to create 3D games that can run on a real N64 console or accurate emulators. Besides the usual editor, some extra features include: Automatic toolchain installation on Windows 3D-Model import (GLTF) from blender with fast64 material support. Support for HDR+Bloom rendering (shown here: www.youtube.com/watch?v=XP8g2ngHftY) Support for big-texture rendering (256x256) (shown here: www.youtube.com/watch?v=rNEo0aQkGnU) Runtime engine handling scene-management, rendering, collision, audio and more. Global asset management with automatic memory cleanup Node-Graph editor to script basic control flow Note that this project focuses on real hardware, so accurate emulation is required to run/test games on PC. Emulators that are accurate enough include Ares (v147 or newer) and gopher64. [!WARNING] This project is still in early development, so features are going to be missing. Documentation is also still a work in progress, and breaking API changes are to be expected. Documentation Before starting, please read the FAQ! Installation &amp; Docs: Pyrite64 Installation Using the Editor Using the CLI Development on the editor itself: Building the Editor Showcase Cathode Quest 64 (YouTube) | Pyrite64 Release Video Links For anything N64 homebrew related, checkout the N64Brew discord: https://discord.gg/WqFgNWf Credits &amp; License 2025-2026 - Max Bebök (HailToDodongo) Pyrite64 is licensed under the MIT License, see the LICENSE file for more information. Licenses for external libraries used in the editor can be found in their respective directory under /vendored Pyrite64 does NOT force any restrictions or licenses on games made with it. Pyrite64 does NOT claim any copyright or force licenses for assets / source-code generated by the editor. While not required, please consider crediting Pyrite64 with a logo and/or name in your credits and/or boot logo sequence.]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:32 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/HailToDodongo/pyrite64</guid>
    </item>
    <item>
      <title><![CDATA[I built an open-source alternative to carbon.now.sh]]></title>
      <link>https://dev.to/railly/i-built-an-open-source-alternative-to-carbonnowsh-cg9</link>
      <description><![CDATA[carbon.now.sh hasn't had a commit since December 2024. 35.9K stars, zero activity. If you depend on it for code screenshots, that's not great.
I built Ray as a replacement.
Free, open-source code screenshots. Paste code, pick a theme, export. Same idea as Carbon, different execution:
500+ themes (Carbon has ~29)
Free API: 60 req/min, no auth
PNG, SVG, or straight to clipboard
Works with Claude Code and Cursor: npx skills add Railly/tinte The API Carbon doesn't have one. Ray does.
curl -X POST https://ray.tinte.dev/api/v1/screenshot \ -H "Content-Type: application/json" \ -d '{ "code": "const hello = \"world\";", "language": "javascript", "theme": "one-dark-pro", "format": "png" }' Returns base64 or binary depending on your Accept header. No API key needed.
const response = await fetch('https://ray.tinte.dev/api/v1/screenshot', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ code: 'function add(a, b) { return a + b; }', language: 'javascript', theme: 'github-dark', format: 'svg' })
}); const data = await response.json();
// data.screenshot contains base64 or URL I use it for automated docs screenshots and social media previews. You could plug it into CI/CD or blog tooling too.
If you use Claude Code or Cursor:
npx skills add Railly/tinte Your AI generates code screenshots without you leaving the editor. Feature
Ray
Carbon
ray.so
Snappify Themes
500+
~29
~30
~40 Languages
16
75+
12
20+ API
Free
No
No
Paid Export
PNG, SVG, clipboard
PNG, SVG
PNG
PNG, SVG AI tools
Yes
No
No
No Open source
Yes
Yes
No
No Where Ray falls short: 16 languages vs Carbon's 75+. Smaller community. Less background/padding customization (shipping soon).
Where Ray wins: 17x more themes. API. Active development. AI integration.
I maintain Tinte, a theme generator for VSCode and shadcn/ui. People kept asking to preview themes before installing them. Carbon's theme selection was too small for that. So I built something that could show all 500+ themes, and since I was building it anyway, I made it API-first.
More languages (targeting Carbon's 75+)
Better customization (padding, shadows, window controls)
CLI for local generation
Bulk export
Try it: ray.tinte.dev Source: github.com/Railly/tinte API docs: ray.tinte.dev/docs Next.js, Tailwind, Vercel. MIT licensed.
If Carbon going dormant bugs you or you want an API for code screenshots, try Ray. Stars and feedback appreciated.]]></description>
      <pubDate>Fri, 20 Feb 2026 03:49:39 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/railly/i-built-an-open-source-alternative-to-carbonnowsh-cg9</guid>
    </item>
    <item>
      <title><![CDATA[Working With AI Tools on a New Library]]></title>
      <link>https://dev.to/sakobume/working-with-ai-tools-on-a-new-library-mc0</link>
      <description><![CDATA[This is the setup guide for Railway-Oriented TypeScript. If you haven't read the overview yet, start there.
@railway-ts/pipelines and @railway-ts/use-form are new. That creates a practical problem: AI coding assistants — Claude Code, Cursor, GitHub Copilot — have been trained on millions of examples of Zod, react-hook-form, and neverthrow. They have almost no training data for @railway-ts.
The result is predictable. Ask Claude Code to build a form with @railway-ts/use-form installed and it will write zodResolver, useForm from react-hook-form, and setError/clearErrors — confidently, without being asked, because that's the pattern it's seen thousands of times. It's not hallucinating random code. It's pattern-matching against the highest-probability answer in its training distribution. The problem is that answer is wrong for your project.
This isn't a criticism of AI tools. It's a structural reality about how they work that's worth understanding and routing around.
Both libraries now ship their documentation inside the npm package:
node_modules/@railway-ts/pipelines/docs/
node_modules/@railway-ts/use-form/docs/ This matters because modern AI coding tools — Claude Code in particular — can read your node_modules and reason from types and documentation in context. When you point the tool at the right docs, it stops pattern-matching against training data and starts reasoning from the actual library API. The generated code goes from "confidently wrong" to "reads the types and generates correct usage."
The difference in output quality is substantial. The bundled docs are the mechanism that makes AI-assisted development on a new library viable.
The most reliable way to redirect an AI tool is a CLAUDE.md (or equivalent context file) in your project root. Claude Code reads this automatically. Create it before you start generating form or pipeline code:
# AI Coding Instructions This project uses @railway-ts/pipelines and @railway-ts/use-form. Before generating any form or pipeline code, read the docs shipped with the packages: - node_modules/@railway-ts/pipelines/docs/
- node_modules/@railway-ts/use-form/docs/ Rules: - Do NOT use Zod or @hookform/resolvers patterns
- Do NOT use react-hook-form's useForm, setError, or clearErrors
- Schema validation uses @railway-ts/pipelines/schema, not z.object()
- Form state uses useForm from @railway-ts/use-form, not react-hook-form
- Async pipelines use flowAsync/pipeAsync from @railway-ts/pipelines/composition This does two things: tells the tool what not to do (which matters as much as what to do), and points it to the docs that contain the correct patterns.
For other AI tools:
Cursor — add the same content to .cursorrules or use the @Docs feature to index the bundled docs directly.
GitHub Copilot — less controllable without explicit doc indexing, but keeping a reference file open in your editor with correct usage examples significantly improves suggestion quality.
After creating CLAUDE.md, ask your AI tool to build a simple form before touching any real code:
Build a React login form with email and password fields using @railway-ts/use-form. Handle server validation errors.
The output should use useForm from @railway-ts/use-form, a schema built with object/required/chain from @railway-ts/pipelines/schema, and form.setServerErrors() for server errors. If you see zodResolver, @hookform/resolvers, or setError/clearErrors, the tool is still pattern-matching against training data — check that CLAUDE.md is in the project root and that the docs are present in node_modules.
Once the tool is reading from the bundled docs, the things it handles well:
Schema composition with chain, object, required, optional fieldValidators for async field-level checks
form.setServerErrors() for server-side error injection
flowAsync/pipeAsync for composing multi-step async pipelines
mapWith, flatMapWith, filterWith, tapWith — the full curried operator set
combine, combineAll, partition for batch processing
Cross-field validation with refineAt The things worth double-checking manually:
Type inference with InferSchemaType — verify the generated type matches your intent
initialValues completeness — the TypeScript error is immediate if a field is missing, but worth confirming
Error path strings in setServerErrors — confirm they match your schema field names
mkdir my-project &amp;&amp; cd my-project
npm init -y
npm install react react-dom @types/react typescript
npm install @railway-ts/pipelines @railway-ts/use-form Create CLAUDE.md as shown above, then verify the docs are present:
ls node_modules/@railway-ts/pipelines/docs/
ls node_modules/@railway-ts/use-form/docs/ If you're migrating an existing project that uses Zod:
npm install @railway-ts/use-form @railway-ts/pipelines
# keep zod — the form hook accepts Zod schemas via Standard Schema v1 You can adopt the form hook without touching your Zod schemas. See Part 3 for the migration path.
Part 1 — The Glue Code Tax
Part 2 — Composable Async Pipelines
@railway-ts/pipelines API: Result, curried operators, and flowAsync for multi-step async pipelines where errors short-circuit automatically. The same Result type the form hook uses.
Part 3 — Schema-First React Forms
Bonus — Data Processing Pipelines
combine, combineAll, and partition, reusable sub-pipelines, structured error reporting. No React, no UI.
GitHub:
@railway-ts/pipelines — Result, Option, schema validation, composable pipelines
@railway-ts/use-form — React form hook with native schema integration]]></description>
      <pubDate>Fri, 20 Feb 2026 01:50:00 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/sakobume/working-with-ai-tools-on-a-new-library-mc0</guid>
    </item>
    <item>
      <title><![CDATA[I Stopped Context-Switching Between Validation, Forms, and Pipelines]]></title>
      <link>https://dev.to/sakobume/i-stopped-context-switching-between-validation-forms-and-pipelines-4h6o</link>
      <description><![CDATA[There's a moment every TypeScript developer knows. You're staring at a form bug — a server error that should be showing on the email field isn't appearing. You open the component. Then the submit handler. Then the server. Then the resolver. You're tracing through three separate error-shaping layers and you can't quite hold the whole thing in your head at once.
It's not that the code is wrong. It's that understanding it requires knowing four things simultaneously: how Zod formats errors, how the resolver converts them, how setError structures them, and how the server formats its response. Four mental models for one question: what makes this form invalid?
I've been in that moment a lot. It made me wonder whether the complexity was intrinsic to the problem, or just an artifact of the tools.
Want to skip the pitch? Try it live in StackBlitz — a runnable app with schema validation, async checks, server errors, and a shared Express backend.
Here's the standard stack: Zod for validation, react-hook-form for form state, @hookform/resolvers to bridge them, plus your own async validator wiring and your own server error converter. Three npm packages, a resolver adapter, and custom glue code for anything outside the happy path.
It works. That's not the point. The point is what you're carrying in your head whenever you touch it.
Every field has errors potentially living in three different places: the Zod result, the setError calls from async validation, the setError calls from server errors. The rule for which one displays is: whatever you called last. Which means you have to track call order in your head, or you'll show a stale server error after the user has already fixed their input.
Adding a new field means touching the schema, the useForm defaults, the JSX, and potentially the async validator and server error handler depending on what the field does. These aren't in one place. They're coordinated across the file.
None of this is a knock on Zod or react-hook-form. Both are excellent at what they do. But "excellent at what they do" is exactly the problem — they do different things, and you write the code that connects them.
Zod's safeParse returns SafeParseReturnType. Your pipeline expects Result. Every route that validates input starts with a conversion:
const parsed = schema.safeParse(body);
if (!parsed.success) { return err({ type: "validation", issues: parsed.error.issues.map((i) =&gt; ({ path: i.path.map(String), message: i.message, })), });
}
// now you can use parsed.data You write this once, extract it into a utility, and forget about it. Until the next person writes it again because they didn't know the utility existed. Or until Zod changes its error shape. Or until someone else's async step returns a different error format and now you have two error models in the same pipeline.
The seam on the backend is structurally identical to the one on the frontend: validation output doesn't naturally match error-handling input, so you write code to bridge them.
Here's the concrete experience of adding a field. Say you have a registration form and you need to add age: must be a number, must be at least 18.
In the Zod + RHF stack, you touch:
The Zod schema — add the field
The useForm defaultValues — add the field
The JSX — add the input and error display
If age needs async or server validation — add setError/clearErrors wiring
Three of those four are pure coordination. The Zod schema is the work. The rest is keeping other layers informed.
With @railway-ts/use-form and @railway-ts/pipelines:
// schema.ts — shared between frontend and backend
const registrationSchema = object({ username: required(chain(string(), nonEmpty(), minLength(3))), email: required(chain(string(), nonEmpty(), email())), password: required(chain(string(), nonEmpty(), minLength(8))), confirmPassword: required(chain(string(), nonEmpty())), age: required(chain(parseNumber(), min(18, "Must be at least 18"), max(120))),
}); If the schema syntax is unfamiliar — chain, required, object — Part 1 walks through each piece. The short version: chain() composes validators left-to-right, required() marks a field as non-optional, and object() collects fields into a typed shape.
Add age to the schema. InferSchemaType propagates it to the TypeScript type. initialValues gives you a TypeScript error immediately — age is missing. form.getFieldProps("age") works without any other changes. form.errors.age works. If the server returns { age: "Age cannot be verified" }, form.setServerErrors(res.json()) handles it — no field name mapping, no as keyof FormType cast.
The JSX is still the JSX — you write the input. But nothing else changes. The schema change propagated everywhere else automatically.
That's the difference. Not fewer files. Not less JSX. Less coordination. Less working memory spent keeping layers in sync.
This assumes you want identical validation on frontend and backend — which is true for most forms. When you intentionally need them to differ (admin bypasses, progressive disclosure, different error messages for API consumers vs. UI), you'd define separate schemas. The library doesn't force sharing; it makes sharing free when you want it.
The context-switching that happens when a field has multiple simultaneous error sources — schema says invalid, async check says taken, server says already registered — is one of the subtlest bugs to track in a React form.
@railway-ts/use-form makes this deterministic with a fixed priority system: Priority
Source
Clears when 1 (lowest)
Schema validation
Every validation run 2
Async field validators
Field validator re-runs 3 (highest)
Server errors
User edits the field You never manage this. You read form.errors.email and display it. A server error stays visible even after schema validation passes — the server is more authoritative than client-side rules. Editing the field clears the server error and hands control back to schema validation.
;
{ form.touched.email &amp;&amp; form.errors.email &amp;&amp; {form.errors.email};
}
{ /* Could be schema, async, or server error. Always the highest-priority one. */
} Async validation is declared, not wired:
const form = useForm(registrationSchema, { fieldValidators: { username: async (value) =&gt; { const { available } = await fetch( `/api/check-username?u=${encodeURIComponent(value)}`, ).then((r) =&gt; r.json()); return available ? undefined : "Username is already taken"; }, }, onSubmit: async (values) =&gt; { const res = await fetch("/api/register", { method: "POST", headers: { "Content-Type": "application/json" }, body: JSON.stringify(values), }); if (!res.ok) form.setServerErrors(await res.json()); else navigate("/welcome"); },
}); The hook handles loading state (form.validatingFields.username), discards stale responses — the last issued request wins, not the last received — and gates the async check behind schema validation so you're not hitting the API with values that are already invalid. None of that is wiring you write.
This is where it goes past "better forms." The same Result type the form hook uses natively is what @railway-ts/pipelines produces on the backend — in API handlers, in ETL jobs, in any async operation that can fail.
import { flowAsync } from "@railway-ts/pipelines/composition";
import { flatMapWith, match } from "@railway-ts/pipelines/result";
import { validate, formatErrors } from "@railway-ts/pipelines/schema";
import { registrationSchema } from "./schema"; // same file the form uses const handleRegistration = flowAsync( (body: unknown) =&gt; validate(body, registrationSchema), flatMapWith(checkEmailUnique), flatMapWith(createUser),
); app.post("/api/register", async (req, res) =&gt; { const result = await handleRegistration(req.body); match(result, { ok: (user) =&gt; res.status(201).json({ id: user.id }), err: (errors) =&gt; res.status(422).json(formatErrors(errors)), });
}); formatErrors converts ValidationError[] to Record. That's the exact format form.setServerErrors() consumes. The full loop — frontend schema validation, async field checks, backend pipeline validation, server errors surfacing on the right fields — shares one schema and one error format with zero conversion between layers.
For batch processing, you get combine, combineAll, and partition — semantics that would each be a custom accumulation loop with try/catch:
const results = await Promise.all(rawRecords.map(processTransaction)); // All-or-nothing, first failure
const batchResult = combine(results); // All-or-nothing, all failures at once
const batchResult = combineAll(results); // Keep both sides — the ETL default
const { successes, failures } = partition(results); One call each, on results you already have.
If you already have Zod schemas, @railway-ts/use-form accepts them directly via Standard Schema v1 — no resolver, no adapter:
import { z } from "zod";
import { useForm } from "@railway-ts/use-form"; const zodSchema = z.object({ username: z.string().min(3), email: z.email(), password: z.string().min(8), age: z.coerce.number().min(18),
}); const form = useForm&gt;(zodSchema, { initialValues: { username: "", email: "", password: "", age: 0 }, onSubmit: (values) =&gt; console.log(values),
}); Full hook API. No zodResolver. No @hookform/resolvers. The migration path is: adopt the form hook now with existing Zod schemas, migrate to @railway-ts/pipelines/schema later when you want the shared full-stack loop.
I'm not selling you on monads or railway-oriented design as a philosophy. Those are interesting, but they're not the point.
The point is: right now, your validation layer, your form state layer, and your async pipeline layer are three separate mental models you context-switch between every time you open a related file. The error shapes don't naturally align. The libraries weren't designed to share a type vocabulary. That's not a flaw in any of them — it's what happens when you compose independently excellent tools.
There's a version of this where it's one model. Schema drives types drives form state drives server communication drives pipeline validation — the same Result, the same error format, no adapters between layers.
The bundle is ~7.8 kB brotli (~10 kB gzip) for both libraries combined. For comparison, Zod + react-hook-form + resolvers is ~35.5 kB gzip. Different compression methods — but even on the same scale, the unified stack is meaningfully smaller.
What you're getting: Zod's job, react-hook-form's job, a resolver's job, and a backend pipeline library's job — in one coherent stack that shares a type vocabulary end to end.
Want the deep dives?
Part 1 — The Glue Code Tax — the Zod + RHF seam counted line by line, then eliminated
Part 2 — Composable Async Pipelines — the full Result + flowAsync operator set
Part 3 — Schema-First React Forms — the 3-layer error system, array fields, and the full-stack loop
Setup &amp; AI tooling — how to get Claude Code and other AI tools to reason from the library docs instead of hallucinating Zod patterns
Bonus — Data Processing Pipelines — batch processing with combine, combineAll, partition GitHub:
@railway-ts/pipelines — Result, Option, schema validation, composable pipelines
@railway-ts/use-form — React form hook with native schema integration]]></description>
      <pubDate>Fri, 20 Feb 2026 01:30:00 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/sakobume/i-stopped-context-switching-between-validation-forms-and-pipelines-4h6o</guid>
    </item>
    <item>
      <title><![CDATA[I built an AI agent that watches security cameras and talks to you about what it sees]]></title>
      <link>https://dev.to/solderzzc/i-built-an-ai-agent-that-watches-security-cameras-and-talks-to-you-about-what-it-sees-4185</link>
      <description><![CDATA[I have a mix of Ring, Blink, and IP cameras at home. They all do the same thing: detect motion, send a notification, and make me open three different apps to see what happened. 47 alerts a day — wind, shadows, cats — and no way to just ask "did anyone come to the door today?"
So I built something different.
Aegis is a desktop app that connects to all your cameras and puts an AI agent in front of them. The agent watches, understands, remembers, and talks to you.
Not "motion detected." Not "person." Instead:
"A person in a blue hoodie walked up to the front door at 2:15 PM, stood there for about 30 seconds, then left a package and walked away."
And you can ask it questions:
You: "What happened at the front door today?"
Aegis: "It's been quiet. A package was delivered at 1:15 PM. Your daughter got home at 3:30 PM. No unfamiliar visitors."
The architecture is three layers:
1. Camera layer — Connects to Ring, Blink, any RTSP/ONVIF IP camera, your laptop webcam, even an old iPhone. Everything unified in one timeline. Live view uses go2rtc (WebRTC relay) for ~300ms latency.
2. Vision layer — Instead of YOLO object detection, Aegis uses Vision Language Models for scene analysis. You choose:
Local: llama-server with GGUF models from HuggingFace — SmolVLM2, Qwen-VL, LFM2.5, MiniCPM-V, LLaVA. Browse and download models right inside the app. Runs on Apple Silicon with Metal acceleration — a Mac M1 Mini 8GB handles LFM2.5 Q4 at about 3-5 seconds per inference.
Cloud: GPT Vision or Google APIs with your own key.
Or both — local for routine analysis, cloud for complex scenes.
The pipeline doesn't send every frame to the VLM. Motion detection (TF.js in the Electron renderer) triggers recording, key frames get extracted and composited, then only the meaningful frames hit inference.
3. Agent layer — This is what makes it different from just "a camera app with a better AI model." The agent has:
Memory — It learns your household. Who's family, who visits regularly, what's normal at different times of day. Day one you get 30 alerts. Day seven you get 3 — the ones that matter.
A configurable Soul — You set its personality: how it talks, what it cares about, how cautious it should be. It's your agent, your preferences.
16 toggleable skills — Video search, forensic analysis, clip delivery, smart alerts, voice output, generative video recaps. Enable what you need.
Interactive messaging — Alerts go to Slack, Discord, or Telegram with action buttons inline. "Analyze this clip," "Show me who was there," "Send the video." One tap.
Conversational search — Ask "was anyone in the backyard this afternoon?" and the agent searches its memory, triages the results, and gives you a narrative answer — not a list of timestamps.
Electron — Desktop app shell, GPU-accelerated video decoding, TF.js for motion preprocessing
Python backend — VLM orchestration, motion compositing, decision service
llama-server — Local VLM inference with Metal/CUDA acceleration
go2rtc — WebRTC relay for low-latency live camera streams
SQLite — Local storage for clips, analysis results, and vector search (sqlite-vec)
Node.js gateway — Communication bridge for Telegram, Discord, Slack with interactive buttons
VLMs are practical for real-world use now. A 1.6B parameter vision model on 8GB of RAM gives usable scene descriptions. Not perfect, but good enough to distinguish "UPS driver with a package" from "wind blowing a branch."
The agent layer matters more than the model. A better VLM gives you better descriptions. But memory, context, and decision-making are what turn "AI analysis" into something actually useful. The deduplication and learning system is what stops you from getting spammed.
Electron gets a bad rap but it solves real problems here. GPU video decoding, TF.js in the renderer for motion preprocessing, WebRTC for live camera streams — you need a browser engine for this. A CLI with FFmpeg gives you 5+ seconds of latency. Electron with go2rtc gives you 300ms. www.sharpai.org Download Runs on Mac, Windows, and Linux. Everything stored locally on your machine.
Happy to answer questions about the architecture, the VLM pipeline, or anything else!]]></description>
      <pubDate>Fri, 20 Feb 2026 00:46:00 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/solderzzc/i-built-an-ai-agent-that-watches-security-cameras-and-talks-to-you-about-what-it-sees-4185</guid>
    </item>
    <item>
      <title><![CDATA[I Got My First Open Source PR Merged After 7 Rejections. Here's What I Learned About Contributing.]]></title>
      <link>https://dev.to/matthewhou/i-got-my-first-open-source-pr-merged-after-7-rejections-heres-what-i-learned-about-contributing-1bkp</link>
      <description><![CDATA[My first 7 open source pull requests were all closed without merging. Two were ignored entirely. Three got "thanks, but we're not going in this direction." Two had 20+ review that I never addressed because I felt overwhelmed.
My 8th PR was merged into a project with 15K stars. Then my 9th, 10th, and 11th. Once I understood the unwritten rules, everything clicked.
Here's everything I wish I'd known.
My first PR added "dark mode support" to a CLI tool. The maintainer's response: "We've discussed this before and decided against it — it adds maintenance burden for a feature few users want. See issue #234."
The fix: Before writing code, check:
Is there an open issue for this? (If yes, "I'd like to work on this" before starting)
Has this been proposed and rejected before? (Search closed issues)
Does the maintainer want this? (Ask in an issue first for non-trivial changes)
The best first contributions aren't features. They're:
Bug fixes with a clear reproduction case
Documentation improvements (typos, unclear instructions, missing examples)
Test improvements (better coverage for existing features)
Good first issues (literally labeled for you)
I once submitted a PR that refactored an entire module, fixed 3 bugs, updated dependencies, and added a feature. 1,200 lines changed. The maintainer looked at it, said "this is too much to review," and closed it.
The rule: One PR = one change. If your PR description needs bullet points, it's too big. "Refactored auth module, fixed login bug, updated bcrypt, added password reset feature" "Fix: login fails when email contains uppercase letters" (12 lines changed, one test added) Small PRs get reviewed fast. Big PRs get ignored.
Most projects have CONTRIBUTING.md. It tells you:
How to set up the dev environment
The coding style (tabs vs spaces, naming conventions)
The commit message format
The testing requirements
The PR template
If you skip any of these, your PR starts with negative goodwill. The maintainer sees "this person didn't read the docs" and they're already skeptical.
# Before your first PR, do ALL of these:
1. Read CONTRIBUTING.md (cover to cover)
2. Read CODE_OF_CONDUCT.md
3. Run the existing test suite locally
4. Check the commit message convention (Conventional Commits? Imperative mood?)
5. Look at recent merged PRs for style reference Use an open source tool. Hit a bug or confusing behavior. Instead of workarunding it, fix it.
This is the highest-quality contribution because you have genuine context — you understand the problem because you experienced it.
GitHub search: label:"good first issue" language:typescript stars:&gt;1000 These are intentionally scoped for new contributors. Maintainers have already defined the problem and often describe the expected solution.
Tip: on the issue before starting work. "I'd like to take this on. My approach would be [brief description]. Does this sound right?" This prevents duplicate work and gets early feedback on your approach.
Every project has documentation gaps. Find them by:
Following the setup instructions as a new user — where did you get stuck?
Reading the API docs — are there missing examples?
Checking if the README is current — does it reference outdated versions?
Documentation PRs are almost always welcome and almost never rejected.
# Find untested code paths
npx jest --coverage
# Look at files with low coverage
# Write tests for the uncovered paths Test PRs are valued by maintainers because they make the project more reliable without adding features to maintain.
## Summary
Fix login failure when email contains uppercase letters. Closes #456 ## Problem
Users with uppercase email addresses (e.g., "John@Example.com") couldn't log in because the lookup was case-sensitive. ## Solution
Normalize email to lowercase before database lookup in the auth service. Added a test for case-insensitive login. ## Changes
- `src/auth/login.ts`: lowercase email before lookup (1 line)
- `tests/auth/login.test.ts`: added case-insensitive test (12 lines) ## Testing
- [x] Existing tests pass
- [x] New test covers the fix
- [x] Manually tested with mixed-case email Why this works:
Links to an existing issue (the maintainer already validated the problem)
Explains the problem clearly
The solution is small and focused
Tests are included]]></description>
      <pubDate>Thu, 19 Feb 2026 23:11:59 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/matthewhou/i-got-my-first-open-source-pr-merged-after-7-rejections-heres-what-i-learned-about-contributing-1bkp</guid>
    </item>
    <item>
      <title><![CDATA[Revue de presse de l’April pour la semaine 7 de l’année 2026]]></title>
      <link>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Cette revue de presse sur Internet fait partie du travail de veille mené par l’April dans le cadre de son action de défense et de promotion du logiciel libre. Les positions exposées dans les articles sont celles de leurs auteurs et ne rejoignent pas forcément celles de l’April.
[Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€)
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre?
[ZDNET] LibreOffice dénonce le format OOXML
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte lien nᵒ 1 : April
lien nᵒ 2 : Revue de presse de l'April
lien nᵒ 3 : Revue de presse de la semaine précédente
lien nᵒ 4 : Fils du Net [Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux Tiago Gil, le jeudi 12 février 2026.
La centrale d’achat informatique hospitalière (CAIH) engage une nouvelle feuille de route sur cinq ans et initie le programme Alternative, destiné à bâtir un socle numérique souverain pour les systèmes d’information de santé.
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€) Valéry Rieß-Marchive, le mercredi 11 février 2026.
L’Agence nationale de la sécurité des systèmes d’information vient de réitérer son engagement en faveur du logiciel libre. Dans la continuité d’une politique établie et confortée de longue date.
Et aussi: [Le Monde Informatique] L'Anssi formalise sa doctrine open source
[Silicon] L’ANSSI affirme l’open source comme levier de sa politique industrielle
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre? Bertrand Lemaire, le mercredi 11 février 2026.
L’APRIL relance son initiative «Pacte du Logiciel Libre» à l’occasion du prochain scrutin municipal.
Et aussi: [Goodtech] Municipales 2026 en France: l'April lance son pacte du logiciel libre
Voir aussi: L’April propose le pacte du logiciel libre à l’occasion des élections municipales et communautaires de 2026
[ZDNET] LibreOffice dénonce le format OOXML
Le mercredi 11 février 2026.
The Document Foundation (TDF) intensifie sa critique contre Microsoft, accusant le géant américain de privilégier ses intérêts commerciaux au détriment de l’interopérabilité.
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte Aymeric Geoffre-Rouland, le lundi 9 février 2026.
Quand un développeur demande à Claude ou ChatGPT d’écrire du code, l’IA pioche dans des milliers de bibliothèques libres sans que l’humain ne lise jamais leur documentation. Résultat: les mainteneurs de ces projets open-source, qui vivent de la visibilité générée par les visites et les interactions, voient leur audience s’effondrer. Une étude économique chiffre ce paradoxe: l’IA qui accélère le développement logiciel asphyxie l’écosystème qui le rend possible.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:40 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[Welcome to the Eternal September of open source. Here’s what we plan to do for maintainers.]]></title>
      <link>https://github.blog/open-source/maintainers/welcome-to-the-eternal-september-of-open-source-heres-what-we-plan-to-do-for-maintainers/</link>
      <description><![CDATA[Open source is hitting an “Eternal September.” As contribution friction drops, maintainers are adapting with new trust signals, triage approaches, and community-led solutions.]]></description>
      <pubDate>Thu, 12 Feb 2026 20:14:11 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/welcome-to-the-eternal-september-of-open-source-heres-what-we-plan-to-do-for-maintainers/</guid>
    </item>
    <item>
      <title><![CDATA[Nouveautés de février 2026 de la communauté Scenari]]></title>
      <link>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</link>
      <description><![CDATA[Scenari est un ensemble de logiciels open source dédiés à la production collaborative, publication et diffusion de documents multi-support. Vous rédigez une seule fois votre contenu et vous pouvez les générer sous plusieurs formes : site web, PDF, OpenDocument, diaporama, paquet SCORM (Sharable Content Object Reference Model)… Vous ne vous concentrez que sur le contenu et l’outil se charge de créer un rendu professionnel accessible et responsive (qui s’adapte à la taille de l’écran).
À chaque métier/contexte son modèle Scenari :
Opale pour la formation Dokiel pour la documentation Optim pour les présentations génériques Topaze pour les études de cas Parcours pour créer des scénarios de formation et bien d’autres… lien nᵒ 1 : Explication de Scenari
lien nᵒ 2 : Pour démarrer
lien nᵒ 3 : Téléchargements
lien nᵒ 4 : Communauté Scenari
lien nᵒ 5 : Mastodon
lien nᵒ 6 : Bluesky
lien nᵒ 7 : Telegram
lien nᵒ 8 : LinkedIn
lien nᵒ 9 : Canal Peertube Sommaire Visio de découverte de Scenari Parole de Scenariste Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Tu peux parler de Scenari aux conférences éclair de l’April ? Nouvel habillage web pour Optim 24 Mise-à-jour de Myscenari Nouvelles versions d’outils Scenari Le savais-tu ? Le chiffre du mois Nouvelles adhésions d’organisations Visio de découverte de Scenari Tu as des questions sur Scenari avant de tester ?
Cette visio est faite pour toi : jeudi 26 février à 16h sur https://scenari.org/visio/miniwebinaire
Lien Agenda du Libre
Lien Mobilizon Parole de Scenariste
Utilisateur de Canoprof depuis 2019, cet outil est devenu un des piliers de ma pratique d’enseignement en Physique-Chimie (4ᵉ, 5ᵉ, 3ᵉ) et en Sciences (6ᵉ). Je l’utilise pour concevoir l’ensemble de mes supports aussi bien papier que numériques, ce qui me permet de maintenir une cohérence didactique forte sur l’ensemble du cursus collège.
La force de Canoprof réside dans la séparation claire entre le contenu et la forme. En tant qu’enseignant, cela me permet de me concentrer sur le fond pédagogique et la structuration de mes séquences, sans perdre de temps dans les contraintes techniques de mise en page. La richesse de mon fond documentaire, construit depuis plus de six ans, évolue ainsi sereinement au fil des réformes et de mes retours d’expérience.
Canoprof m’aide à formaliser une progression spiralaire efficace tout en générant des supports propres, structurés et accessibles. C’est un gain de productivité précieux qui me permet de consacrer plus d’énergie à l’accompagnement de mes élèves en classe. Guillaume Marmin, enseignant de physique-chimie au Collège Isabelle Autissier. Modèle utilisé : Canoprof Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Les Rencontres Scenari 2026 auront lieu du lundi 22 juin (midi) au vendredi 26 juin (midi) sous le soleil provençal à l'ENSAM Aix-en-Provence.
Bloque ces dates dès maintenant, les détails seront précisés bientôt. Tu peux parler de Scenari aux conférences éclair de l’April ? Lors de la prochaine assemblée générale de l’April (samedi 28 mars 2026 à Paris) il y aura un temps de conférences éclairs (6 minutes) de 10h à 12h qui s’enchaîneront sur des sujets variés, en lien avec le Libre, entendu au sens large.
Si tu utilises Scenari, c’est une bonne opportunité pour parler de tes usages auprès des adhérent⋅e⋅s de l’April. Date limite pour proposer : 15 mars. Envoyer un courriel à confseclairs@april.org.
Il n’est pas nécessaire d’être adhérent⋅e à l’April pour pouvoir proposer une conférence éclair.
Plus de détails sur l’annonce de l’April. Nouvel habillage web pour Optim 24 Un nouvel habillage graphique pour Optim 24 fait son apparition sur la plateforme de téléchargement.
Il existe pour tous les supports web des 3 modalités d’Optim : site normal, site web simple, site web en tuiles. Mise-à-jour de Myscenari MyScenari vient de passer en version 6.4.5 (corrections de bugs dans le cœur et dans les modèles en version 25). Attention : cette version est la dernière à contenir Dokiel 5 et 6, Opale 5 et 24, Optim 3 À partir de la prochaine mise à jour de MyScenari, nous n’aurons plus que Dokiel 25, Opale 25, Optim 24. Pense à migrer tes modèles (et skins) pour ne pas être pris⋅e au dépourvu au dernier moment. Nouvelles versions d’outils Scenari Opale, le modèle phare pour créer vos contenus pédagogiques, passe en version 25.1.1. Au menu, entre autres : corrections dans les outils d’accessibilité, et amélioration de l’intégration de MindMap dans la publication Diapo. Et Opale est maintenant disponible en allemand ! Parcours, pour concevoir des conducteurs pédagogiques, passe en version 25.0.2 (corrections mineures sur le skin, l’éditeur et les vidéos HLS) et est disponible maintenant en français et Anglais. Dokiel, le modèle pour la documentation technique et logicielle, passe en version 25.0.6. Cette version apporte entre autres des corrections dans la publication de relecture et l’écran de contrôle, et l’amélioration des écrans décrits dans les publications Web (maintenant responsive). Optim monte en version dans ses deux saveurs Optim 24.0.7 et OptimPlus 24.0.3 avec des corrections mineures sur les publications Web et Diaporama, et dans le styage. LTI-suite, le serveur pour exploiter des ressources SCORM dans des LMS via LTI, passe en version 2.0.3. Lexico, votre modèle pour créer des lexiques, glossaires, thesaurus, vocabulaires, monte en version 25.0.1 pour apporter des corrections mineures dans la publication Web. SCENARIchain-desktop est à présent disponible en français, en anglais et en espagnol. Le savais-tu ?
En contexte d’ateliers complexes (plusieurs calques de dérivation et/ou de travail), les détails dans le bandeau de l’item listent les variantes de cet item dans les autres ateliers calques ou de travail, s’il en existe.
Dans l’exemple ci-dessous, l’item _Module-LeThe.xml dans l’atelier maître (icone d’atelier bleu) est modifié dans un atelier de travail (icone d’atelier vert) et modifié aussi dans un atelier dérivé (icone d’atelier marron). On peut passer facilement d’une version à l’autre en un seul clic. La popup est détachable pour plus d’aisance si besoin.
Exemple Le chiffre du mois 20, c’est le nombre d’années qui se sont écoulées depuis la première sortie d’Opale le 18/09/2006 (les développements avaient commencé en novembre 2005). Nouvelles adhésions d’organisations
Souhaitons la bienvenue à :
Institution Azahrae qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
L’Université Bourgogne Europe qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
URBILOG qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 15:59:55 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</guid>
    </item>
    <item>
      <title><![CDATA[The world of open source metadata]]></title>
      <link>https://changelog.com/podcast/665</link>
      <description><![CDATA[Andrew Nesbitt builds tools and open datasets to support, sustain, and secure critical digital infrastructure. He's been exploring the world of open source metadata for over a decade. First with libraries.io and now with ecosyste.ms, which tracks over 12 million packages, 287 million repos, 24.5 billion dependencies, and 1.9 million maintainers. What has Andrew learned from all this, who is using this open dataset, and how does he hope others can build on top of it all? Tune in to find out.]]></description>
      <pubDate>Wed, 05 Nov 2025 20:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/665</guid>
    </item>
    <item>
      <title><![CDATA[Been spending a lot of time building languages lately — Zeno, ZenoScript, and other experiments. A lot of parsing, transpiling, type systems, and thinking hard about ergonomics. All of that tinkering ]]></title>
      <link>https://dev.to/wess/been-spending-a-lot-of-time-building-languages-lately-zeno-zenoscript-and-other-experiments-a-1nn1</link>
      <description><![CDATA[GitHub - wess/moxy: A superset for the almighty C language. A superset for the almighty C language. Contribute to wess/moxy development by creating an account on GitHub. github.com]]></description>
      <pubDate>Fri, 20 Feb 2026 00:08:48 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/wess/been-spending-a-lot-of-time-building-languages-lately-zeno-zenoscript-and-other-experiments-a-1nn1</guid>
    </item>
    <item>
      <title><![CDATA[How I Built an AI Content Detection System from Scratch]]></title>
      <link>https://dev.to/ogulcanaydogan/how-i-built-an-ai-content-detection-system-from-scratch-oe4</link>
      <description><![CDATA[A few months ago, my friend sent me a LinkedIn post and asked if I thought it was written by ChatGPT. I had no idea. And that bothered me. I'm an engineer, I should be able to figure this out.
So I did what any engineer would do: I went down a rabbit hole and ended up building an entire detection system. This is how it went.
Look, I'm not on some crusade against AI-generated content. I use LLMs daily. But there are real situations where it matters: academic submissions, journalism, legal documents, job applications. People deserve to know what they're reading.
Every existing tool I tried was either behind a paywall, unreliable, or a black box. I wanted something open source that actually showed its reasoning. So I built AI Provenance Tracker.
You can try the live demo if you want to skip the technical stuff.
I went with FastAPI for the backend and Next.js for the frontend. Nothing fancy. I wanted to get to the interesting part, which is the detection logic.
┌─────────────────────────────────────────────────┐
│ Web Interface (Next.js) │
└─────────────────────────────────────────────────┘ │ ▼
┌─────────────────────────────────────────────────┐
│ REST API (FastAPI) │
└─────────────────────────────────────────────────┘ │ ┌─────────────┴─────────────┐ ▼ ▼
┌───────────────────┐ ┌───────────────────┐
│ Text Detector │ │ Image Detector │
│ - Perplexity │ │ - FFT Analysis │
│ - Burstiness │ │ - Artifacts │
│ - Vocabulary │ │ - Metadata │
└───────────────────┘ └───────────────────┘ The detection side has two engines. One for text, one for images. Let me walk through both.
I tried a bunch of approaches before landing on three signals that actually hold up.
This one's the most intuitive. Perplexity basically measures how "surprised" a language model would be by a piece of text. AI-generated text tends to score lower because it's literally optimised to produce probable, fluent output.
def calculate_perplexity(words: list[str]) -&gt; float: word_counts = Counter(words) total_words = len(words) entropy = 0.0 for count in word_counts.values(): prob = count / total_words entropy -= prob * math.log2(prob) return 2 ** entropy Humans are messy writers. We use weird words, go off on tangents, make unusual word choices. AI is smoother. Almost too smooth.
This was the surprising one. Burstiness measures how much sentence length varies in a piece of text. Turns out, AI writes like a metronome. Consistently medium-length sentences with similar complexity.
Humans don't do that. We write a short punchy sentence. Then we follow it with this long, meandering thought that goes on for a while because we're trying to explain something complicated and we don't stop to restructure it. Then short again.
def calculate_burstiness(sentences: list[str]) -&gt; float: lengths = [len(s.split()) for s in sentences] mean_length = np.mean(lengths) std_length = np.std(lengths) return std_length / mean_length The coefficient of variation tells the whole story. AI text clusters around 0.2-0.3. Human text is all over the place, like 0.4, 0.5, sometimes higher.
The third signal is type-token ratio and n-gram repetition. AI has this habit of recycling phrases. "it's important to note that" three times in one article is a dead giveaway. Humans vary their transitions naturally without thinking about it.
This part was genuinely fun to build. AI-generated images leave fingerprints that are invisible to the naked eye but show up clearly in the frequency domain.
The Fast Fourier Transform converts an image from spatial to frequency representation. Real photographs have frequency distributions shaped by optics and sensor physics. Diffusion models like Stable Diffusion produce mathematically different patterns.
from scipy import fft def analyze_frequency_domain(img_array: np.ndarray) -&gt; float: gray = np.mean(img_array, axis=2) f_transform = fft.fft2(gray) f_shift = fft.fftshift(f_transform) magnitude = np.abs(f_shift) # AI images have unusual high-frequency distributions ... I also check for artifact patterns (weird texture uniformity, edge inconsistencies around hair and fingers) and metadata forensics. Real photos have EXIF data from cameras. AI images almost never do.
Here's the thing I learned the hard way: no single signal is reliable enough. Perplexity alone? A carefully edited AI text fools it. FFT alone? Heavily compressed JPEGs produce false positives.
The magic happens when you combine them with weighted averaging:
def make_prediction(perplexity, burstiness, vocab_richness, ml_score=None): signals = [] weights = [] if ml_score is not None: signals.append(ml_score) weights.append(0.40) signals.append(perplexity_signal) weights.append(0.25) signals.append(burstiness_signal) weights.append(0.20) signals.append(vocab_signal) weights.append(0.15) confidence = sum(s * w for s, w in zip(signals, weights)) return confidence &gt; 0.5, confidence I tuned the weights through experimentation. The ML model (when available) gets the highest weight because it captures patterns I can't articulate in code.
Four months in, here's what I'd tell someone starting a similar project:
Detection is probabilistic, not binary. I always show confidence scores and explain the reasoning. Saying "73% likely AI-generated" is honest. Saying "this is AI" is not.
Ensemble methods are worth the complexity. The jump from single-signal to multi-signal detection was dramatic. Same principle as spam filtering and fraud detection. One signal is easy to game, five signals together are much harder.
The arms race is real. People actively try to evade detection by adding random typos, varying sentence lengths, post-processing images. I've already had to update the detection logic three times.
Open source builds trust. When the detection methods are visible, people can understand why the system reached a conclusion. Black-box detection creates suspicion.
I'm working on audio deepfake detection (voice cloning is getting scary good), a browser extension for real-time detection, and fine-tuning ML models on larger datasets. The roadmap is in the repo if you're curious.
Live Demo: provenance-detect.vercel.app
GitHub: github.com/ogulcanaydogan/ai-provenance-tracker
API Docs: Backend API
Everything is MIT licensed. If you find bugs or have ideas, open an issue. I actually read them.
Find me on GitHub or LinkedIn if you want to chat about detection techniques or AI tooling.]]></description>
      <pubDate>Thu, 19 Feb 2026 22:58:39 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/ogulcanaydogan/how-i-built-an-ai-content-detection-system-from-scratch-oe4</guid>
    </item>
    <item>
      <title><![CDATA[Cartoon Universe: How to Teach a Diffusion Model Some Manners]]></title>
      <link>https://dev.to/melasistema/cartoon-universe-how-to-teach-a-diffusion-model-some-manners-434k</link>
      <description><![CDATA[Let’s clarify something before we begin.
Cartoon Universe is not a model. It is not magic weights. It is not a secret checkpoint hidden in a folder.
It is a designed cognitive environment inside INTENTIO.
And what it demonstrates is far more interesting than “AI that draws cartoons.”
It demonstrates that you can discipline chaos without touching the model.
Private diffusion models are powerful.
Gloriously powerful.
But left alone, they behave like extremely talented artists who refuse to use the same pen twice.
You ask for:
The same character The same proportions The same lighting The same visual identity And they reply:
“Sure. But today we explore.”
They drift. They reinterpret. They mutate.
Not because they are broken.
Because they have no environment.
INTENTIO is not an AI tool.
It is a framework to build cognitive spaces using:
Folder structure Prompt templates Defined roles Context rules Local models It assumes something radical:
A model behaves predictably when its environment is structured.
Cartoon Universe is a Blueprint that proves this — using a private diffusion Vision model.
Cartoon Universe is a cognitive architecture for visual generation.
It does not modify model weights. It does not fine-tune. It does not retrain.
It shapes:
What context is loaded What references are compared What constraints are applied How outputs are evaluated Where memory is stored The folder becomes the brain. The prompts become behavioral rules. The diffusion model becomes… disciplined.
Prompt engineering is like whispering instructions to a distracted genius.
Cartoon Universe is building that genius a studio:
Character identity folders Style anchors Comparative memory Output validation logic Iterative refinement flow The model is no longer improvising in the void.
It is operating inside a structured universe.
Hence the name.
Because illustration is unforgiving.
In cartoons:
Proportions matter. Style consistency is obvious. Character drift is painful. Brand identity collapses quickly. Cartoon Universe is tuned to handle that sensitivity.
It’s an example of how to design a Vision cognition space where:
Consistency is not requested.
It is enforced by architecture.
And yes — it can produce things like this: The point isn’t that these are cute.
The point is that they emerge from structure.
Here’s the wise part.
Most people believe AI consistency requires:
Bigger models Fine-tuning Expensive training Proprietary APIs Cartoon Universe quietly says:
What if the problem isn’t the model… but the environment?
You don’t need to change the brain.
You need to design the room it thinks in.
Yes — you plug in your own private diffusion model.
Stable Diffusion. Any local Vision-capable model.
Cartoon Universe does not ship weights.
It ships structure.
And structure changes behavior.
That is the subtle revolution.
INTENTIO operates on a simple but profound idea:
Intelligence emerges from constraints.
When a Vision model operates without structured context, it generates.
When it operates inside a designed cognitive environment, it collaborates.
Cartoon Universe is a demonstration of that shift.
From generation to cognition. From randomness to predictability. From output to identity.
Raw diffusion model:
“Here is your character.” Next image: “New haircut. New anatomy. New lighting. Surprise.”
Cartoon Universe:
“We agreed on a jawline. Stay focused.”
Cartoon Universe is not the final destination.
It is a working example of how to design:
A Vision cognitive environment Predictable model behavior Structured creative workflows Memory-driven generation You can modify it. Fork it. Rebuild it. Create your own cognitive architectures from scratch.
It’s not about cartoons.
It’s about proving that AI behavior is architectable.
There is something philosophically important here.
When you rely purely on black-box generation, you surrender authorship.
When you design the cognitive environment yourself:
You control context. You control constraints. You control memory. You control privacy. INTENTIO is private-first for a reason.
Cognition should belong to the architect.
It says:
You don’t need to retrain a model to shape its behavior. You need to design its world.
It’s elegant. It’s structured. It’s a little rebellious.
And yes — it keeps your characters’ noses consistent.
Which, honestly, might be the greatest achievement of all.
https://intentio.melasistema.com/]]></description>
      <pubDate>Thu, 19 Feb 2026 22:39:21 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/melasistema/cartoon-universe-how-to-teach-a-diffusion-model-some-manners-434k</guid>
    </item>
    <item>
      <title><![CDATA[Rasbperry Pi vs ESP32 : vraies questions, mauvaises comparaisons]]></title>
      <link>https://www.programmez.com/actualites/rasbperry-pi-vs-esp32-vraies-questions-mauvaises-comparaisons-39038</link>
      <description><![CDATA[Raspberry Pi vs ESP32 vs Arduino, cette question revient régulièrement quand on choisit la bonne plateforme pour son projet IoT. Comme nous le disons souvent quand nous comparons les platesformes, il faut comparer ce qui est comporable. Il ne faut pas opposer une Raspberry Pi 5 avec une ESP32. La Pi est une SBC, une Single Board Computer. Il s'agit donc d'un véritable micro-ordinateur sur une unique carte. Elle contient toute l'électronique (SoC, mémoire, vidéo, audio, stockage, réseau). Et une Pi 5 a besoin d'une alimentation puissante et d'un OS. L'ESP32 repose sur un firmware, qu'il est possible de changer.
L'autre différence est le form factor. La Pi 5 exige de la place et une dissipation thermique active pour les fortes charges.
La Pi 5 fait 8,5 cm sur 4,9 cm contre 5,5 cm sur 2,6 cm pour une ESP32 WROOM-32 (qui n'est pas le modèle le plus petit). L'ESP32 pourrait se comparer à la Pi Pico 2 avec 5,1 cm sur 2,1 cm. Nous ne tenons pas compte de la hauteur des headers.
L'équivalent d'une ESP32 côté Pi est donc la Pi Pico 2, aussi bien par le positionnement, le hardware et le form factor.
Petite comparaison : les specs Pi Pico 2 : un SoC RP235x + cœurs ARM, 520 Ko de SRAM, 4 Mo de stockage, 26 GPIO, UART / SPI / I2C, USB, réseau sans fil selon le modèle. De 6 à 9 € selon le modèle. - ESP32 Wroom32 : SoC ESP32, 512 Ko de RAM, 4 Mo de stockage, 34 GPIO, SPI / I2C / CAN / UART, WiFi + Bluetooth, env. 7-9 € Si vous êtes habitué(e) à coder avec Arduino, vous pouvez sans problème coder depuis l’Arduino IDE, les ESP sont parfaitement supportés. Vous pourrez utiliser peu ou prou les mêmes capteurs. Si vous cherchez une carte réactive avec des interruptions plus rapides, le Pi Pico 2 est souvent considéré comme meilleur. L’ESP32 propose plus de protocoleset de GPIO. Sur la partie connectivité sans fil, les deux cartes supportent le Wi-Fi et le Bluetooth, mais petit avantage à l’ESP32, car le réseau sans fil est une des fonctionnalités intégrées dès la conception. Et le support OTA (mise à jour over the air) peut être un avantage certain dans un contexte contraint ou industriel.
L’ESP32 est plus consommatrice, notamment en charge maximale. La Pi Pico 2 est plus économique. Si vous cherchez avant tout la basse consommation, la Pi sera sans doute la meilleure option.
Sur le modèle de développement, nous avons toujours apprécié la diversité de l’ESP32. Si vous voulez faire du MicroPython, vous devrez flasher le bon firmware. Catégorie actualité: Hardware Raspberry pi, ESP32 Image actualité AMP:]]></description>
      <pubDate>Thu, 19 Feb 2026 16:27:48 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/rasbperry-pi-vs-esp32-vraies-questions-mauvaises-comparaisons-39038</guid>
    </item>
    <item>
      <title><![CDATA[Physiocab : un logiciel libre de gestion pour kinésithérapeutes]]></title>
      <link>https://linuxfr.org/news/physiocab-un-logiciel-libre-de-gestion-pour-kinesitherapeutes</link>
      <description><![CDATA[Physiocab est un logiciel libre de gestion de cabinet de kinésithérapie, développé sous licence Affero GPL 3.0 et hébergé sur Codeberg. Le projet est porté par la société Allium SAS, dans le cadre de la plateforme communautaire Kalinka, dédiée aux kinésithérapeutes francophones.
Le projet vient de passer en beta publique (v0.9) et cherche des testeurs et contributeurs.
Pourquoi un logiciel libre pour les kinés ? Le secteur de la santé libérale souffre d'une offre logicielle dominée par des solutions propriétaires onéreuses, souvent opaques sur le traitement des données de santé. Physiocab propose une alternative : un code auditable, des données stockées localement sous la responsabilité du praticien. lien nᵒ 1 : La page de présentation du projet
lien nᵒ 2 : Le dépôt codeberg
lien nᵒ 3 : PeerJs (MIT) Fonctionnalités
La beta couvre déjà un large périmètre fonctionnel :
Planning hebdomadaire en drag &amp; drop, avec export PDF et gestion des semaines exceptionnelles, particulièrement orienté vers les kinés intervenant en multi-établissements.
Bilans Diagnostiques Kinésithérapiques (BDK) avec tests standardisés (TUG, Tinetti, Handgrip, EVA, évaluation du risque de chute…), export de PDF et historique comparatif.
Suivi des séances avec de multiples exercices structurés (équilibre, force, endurance, mobilisation), chronométrage automatique et calcul de progression.
Application tablette en PWA : fonctionne hors connexion grâce à un Service Worker, s'installe sans passer par un store, interface optimisée tactile.
Stack technique
Backend : Python 3.10+
L'application est multi-plateforme côté client (Windows, macOS, Linux, iOS, Android). La communication entre l'appli de bureau et l'appli PWA se fait de manière directe via PeerJs. Cette méthode ne nécessite pas de préparation contraignante comme l'ouverture de ports.
Les données sont stockées localement, ce qui implique que le praticien reste maître de ses sauvegardes et de sa conformité RGPD.
Le logiciel a été testé par un kinésithérapeute en situation réelle plusieurs jours d'affilée.
Modèle économique
L'utilisation est gratuite, sans limite dans le temps et sans frais cachés, la licence Affero GPL 3.0 en étant la garantie. Un support payant sur devis est proposé pour les praticiens souhaitant une installation assistée, une formation à distance, des développements sur mesure ou un audit de sécurité.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Thu, 19 Feb 2026 13:42:53 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/physiocab-un-logiciel-libre-de-gestion-pour-kinesitherapeutes</guid>
    </item>
    <item>
      <title><![CDATA[What to expect for open source in 2026]]></title>
      <link>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</link>
      <description><![CDATA[Let’s dig into the 2025’s open source data on GitHub to see what we can learn about the future.]]></description>
      <pubDate>Wed, 18 Feb 2026 18:41:42 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</guid>
    </item>
    <item>
      <title><![CDATA[Securing the AI software supply chain: Security results across 67 open source projects]]></title>
      <link>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</link>
      <description><![CDATA[Learn how The GitHub Secure Open Source Fund helped 67 critical AI‑stack projects accelerate fixes, strengthen ecosystems, and advance open source resilience.]]></description>
      <pubDate>Tue, 17 Feb 2026 19:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</guid>
    </item>
    <item>
      <title><![CDATA[Kotlin Multiplatform - Flutter - React Native : entre choix, compromis et frustrations]]></title>
      <link>https://www.programmez.com/actualites/kotlin-multiplatform-flutter-react-native-entre-choix-compromis-et-frustrations-39024</link>
      <description><![CDATA[Nos confrères de Java Code Geeks ont publié un intéressant dossier sur le multiplateforme en 2026 en s'appuyant sur Kotlin Multiplatform (KMP), Flutter et React Native. Faire du multiplateforme avec une base de codes et un minimum d'adaptation reste un objectif pour de nombreux développeurs. Si la philosophie de KMP, Flutter et React Native est différente, l'idée est la même : compiler nativement le code logique le plus agnostique possible et créer une interface native pour chaque plateforme. Flutter est un peu différent car il a l'ambition d'adresser toute la stack et de générer l'UI avec son propre moteur pour plus de cohérence. React Native s'appuie sur les composants UI natifs.
Selon les benchmarks de Java Code Geeks, React Native serait le plus lent à démarrer, KMP étant légèrement devant. Sur la taille des binaires, il n'y a pas de réel vainqueur. Par contre, sur la mémoire, React Native et Flutter sont assez gourmands. Sur les animations, KMP et Flutter s'en sortent le mieux. React Native reste aussi en retrait sur l'intégration à la plateforme : nous restons dans un modèle JavaScript avec un risque d'overhead, même si la New Architecture améliore les choses. Quelle est la solution la plus utilisée ? Flutter serait 1er, React Native baisse régulièrement depuis 2023 et KMP connaît une forte progression.
Apprentissage : KMP : langage connu, Kotlin, avec les mêmes outils. Pour le développeur iOS, il faut apprendre Kotlin/Native et l’interopérabilité. KMP est peut-être la solution la moins mature. Flutter : l'inconvénient est d'apprendre Dart et la logique de la plateforme. React Native : si vous connaissez JavaScript, vous connaissez (ou presque) React Native. L'arrivée de la New Architecture oblige à migrer et à apprendre une nouvelle stack. Pour la réalité du code commun et du développement spécifique, tout le monde prétend faire 90 à 95 % de code partagé. Cette promesse est plus ou moins tenue sur le code logique et une UI simple et partagée. Par contre, pour l'intégration plus profonde, par exemple avec les capteurs et le matériel (caméra typiquement), on tombe vite sur du code spécifique. Aucune solution n'est la meilleure. Flutter et React Native incitent à avoir le maximum de code commun, mais cela peut rapidement provoquer des problèmes quand il faut intégrer des fonctions spécifiques à chaque plateforme.
Côté compétence, c'est autre chose. Un développeur JavaScript pourra relativement rapidement faire du React Native. Pour Flutter, il faut spécifiquement apprendre Dart. KMP repose sur le langage Kotlin et une plateforme dédiée qu'il faut maîtriser. Pour un développeur iOS, ce sera sans doute plus long que pour un développeur Kotlin. choisir ? Tout dépend des compétences disponibles et du projet. Flutter permettra de prototyper rapidement un projet, KMP fournit une intégration native et des performances de haut niveau. React Native est sans doute le plus facile à démarrer avec un profil JavaScript si vous souhaitez aller vite dans le développement.
Source: https://www.javacodegeeks.com/2026/02/kotlin-multiplatform-vs-flutter-vs-react-native-the-2026-cross-platform-reality.html Catégorie actualité: Frameworks Flutter, React Native, Kotlin Multiplatform Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 08:24:45 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/kotlin-multiplatform-flutter-react-native-entre-choix-compromis-et-frustrations-39024</guid>
    </item>
    <item>
      <title><![CDATA[DevTools : les nouveautés de Chrome 145]]></title>
      <link>https://www.programmez.com/actualites/devtools-les-nouveautes-de-chrome-145-39021</link>
      <description><![CDATA[Une des nouveautés les plus importantes est l'intégration Soft Navigations. L'équipe Chrome présente ainsi cette appelleration : a soft navigation est quand JavaScript intercepte une navigation (clic sur un lien) et met à jour le contenu dans la page existente, plutôt que de charger une nouvelle page et que l'URL se mette à jour dans la barre d'adresse. Pour l'utilisateur, cela change peu de choses. Dans Chrome 145, les Soft navigations sont visibles sur le panneau Performance et dans la vue des traces si le site est une SPA. Un timer plus précis
Après l'enregistrement d'une trace dans le panneau Performances, le panneau Sources affiche les temps d'exécution observés ligne par ligne. Vous pouvez ainsi identifier précisément les lignes de code qui consomment le plus de temps.Auparavant, cette fonctionnalité présentait des bogues qui la rendaient peu fiable lorsque le code source était formaté (à l'aide du bouton {}) ou lors de l'utilisation de scripts avec mappage de sources. Le panneau réseau inclut maintenant une colonne dédiée Render blocking. Cela permet de voir les ressources qui bloquent le bon affichage. Autre amélioration : un meilleur debug pour @starting-style. Note de version : https://developer.chrome.com/blog/new-in-devtools-145 Catégorie actualité: Outils DevTools Image actualité AMP:]]></description>
      <pubDate>Mon, 16 Feb 2026 14:43:45 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/devtools-les-nouveautes-de-chrome-145-39021</guid>
    </item>
    <item>
      <title><![CDATA[Concours - Gagnez une Raspberry Pi 5 avec Macé Robotics]]></title>
      <link>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</link>
      <description><![CDATA[À l’occasion de ses 10 ans de Macé Robotics, l’entreprise organise un concours qui se déroulera jusqu'au 26 février 2026.
Macé Robotics est une entreprise individuelle fondée et gérée par moi-même (Nicolas), basée en Bretagne, spécialisée dans la conception et la réparation électronique, aussi bien pour les entreprises que pour les particuliers. Depuis 2016, je fabrique aussi du matériel Open Source également des robots mobiles Open Source destinés à l’enseignement supérieur et à la recherche. Ces robots sont basés sur un système Linux (Raspberry Pi OS), intégrant une carte Raspberry Pi ainsi qu’un microcontrôleur (Pico) dédié à la gestion des moteurs et des capteurs. J’utilise la suite logicielle KiCad sous licence GNU GPL (https://www.kicad.org/) pour la conception des circuits imprimés de ces robots. Attribution des lots par tirage au sort :
→ 1er lot : une carte Raspberry Pi 5 (2 Go) → 2e lot : une carte Raspberry Pi Pico 2W
La livraison est offerte en France. lien nᵒ 1 : Le concours pour participer Retour sur la course de robots – Saint-Brock Robot Race d'une dépêche précédente
Suite à la dépêche de décembre 2024 concernant l’organisation de la course de robots mobiles, voici quelques retours sur cet événement : malgré plusieurs annulations d’écoles survenues quelques semaines avant la compétition, la course a tout de même pu avoir lieu.
Environ quinze participants ont pris part à la compétition. Parmi les robots engagés, on comptait un robot DIY piloté par un microcontrôleur ESP32, aux côtés de plusieurs robots basé sur Raspberry Pi, offrant ainsi une belle diversité technologique.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Sat, 14 Feb 2026 08:47:09 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</guid>
    </item>
    <item>
      <title><![CDATA[L’ANSSI révise sa doctrine vis-à-vis du logiciel libre]]></title>
      <link>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</link>
      <description><![CDATA[L’ANSSI (Agence nationale de la sécurité des systèmes d’information) vient de publier une mise à jour substantielle de sa doctrine vis-à-vis du logiciel libre. L’agence confirme que le logiciel libre et la transparence sont essentiels à la sécurité des systèmes d’information. Elle assume sa contribution au libre et la publication de logiciels sous licence libre.
Cette posture très favorable au logiciel libre et open source est une belle avancée et un signal fort. Jusque-là, la posture de l’ANSSI était beaucoup plus floue et sa contribution à des projets libres et open source pouvait même apparaitre en contradiction avec sa doctrine. J’avais l’impression que les collaborateurs de l’ANSSI qui le faisaient reprenaient à leur compte le dicton « Pour vivre heureux, vivons cachés ».
La politique de l’agence est désormais claire : l’ANSSI contribue, l’ANSSI publie, l’ANSSI a une stratégie pragmatique qui peut l’amener à s’engager ou non sur le long terme en fonction de la finalité de l’outil et des motivations de l’ANSSI.
Détail qui a son importance, l’ANSSI indique privilégier, sauf exception justifiée, la licence Apache v2.0 pour les projets qu’elle publie. Je suis ravi de voir ce service privilégier une licence mondialement connue à une licence franco-française ou européenne (elles ont le don de doucher nombre de velléités d’utilisation et de contribution). lien nᵒ 1 : L’ANSSI met à jour sa politique open source (9 février 2026)
lien nᵒ 2 : Posture générale et actions de l'ANSSI sur l'open-source Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 18:55:42 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</guid>
    </item>
    <item>
      <title><![CDATA[Le prochain Drupalcamp se déroulera à Grenoble les 9, 10 et 11 avril 2026 prochain]]></title>
      <link>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</link>
      <description><![CDATA[L’association Drupal France &amp; Francophonie organise la 13ème édition du Drupalcamp les 9, 10 et 11 avril 2026 au campus Universitaire Grenoble Alpes de Grenoble (France, Isère 38). Drupal est « un système de gestion de contenu (CMS) libre et open-source publié sous la licence publique générale GNU et écrit en PHP ».
Après Rennes en 2024, puis un Barcamp à Perpignan en 2025, cette année 2026 nous emmène au pied des montagnes à Grenoble pour un format de 3 jours de rencontres, soit deux journées de conférences les jeudi et vendredi. La journée du samedi est réservée à la contribution.
Des moments d’ateliers et micro-formation sont également au programme, pour faire de cet évènement une réussite d’un point de vue communauté autour du projet Open Source Drupal.
Le Drupalcamp Grenoble c’est la rencontre de la communauté francophone autour du logiciel libre Drupal. Ouvert à toutes et tous, les rencontres, conférences et ateliers permettent d’adresser à un public toujours plus large des sujets et thématiques diversifiées.
Notre objectif principal est de rendre la création de sites plus simple et la gestion des contenus plus intuitive pour tous. Comme de fédérer les utilisateurs et professionnels qui utilisent Drupal au quotidien.
Du simple curieux au développeur expert, tous ceux qui s’intéressent à Drupal et aux logiciels libres pourront participer à cette manifestation rythmée par :
des conférences (jeudi 9 et vendredi 10 avril), données par des professionnels reconnus et des membres de la communauté Drupal au cours desquels des thématiques nouvelles seront explorées,
des sessions de découverte étayées par des démonstrations à l’intention d’un public plus néophyte,
une journée de formation gratuite (Drupal in a Day) dédiée à l’initiation pour que les curieux puissent se lancer dans la création de leur premier site (sur inscription)
des moments de réseautage et de convivialité avec, notamment, la très attendue soirée communautaire !
Informations pratiques : Campus Universitaire Grenoble Alpes qui se situe à Saint-Martin d'Hères
https://grenoble2026.drupalcamp.fr/
Contact : drupalcamp@drupal.fr lien nᵒ 1 : https://grenoble2026.drupalcamp.fr Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 10 Feb 2026 09:16:59 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</guid>
    </item>
    <item>
      <title><![CDATA[The GitHub problem (and other predictions)]]></title>
      <link>https://changelog.com/friends/123</link>
      <description><![CDATA[Mat Ryer is back and he brought his impromptu musical abilities with him! We discuss Rob Pike vs thankful AI, Microsoft's GitHub monopoly (and what it means for open source), and Tom Tunguz' 12 predictions for 2026: agent-first design, the rise of vector databases, and are we about to pay more for AI than people?!]]></description>
      <pubDate>Wed, 14 Jan 2026 21:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/123</guid>
    </item>
    <item>
      <title><![CDATA[Down the Linux rabbit hole]]></title>
      <link>https://changelog.com/friends/121</link>
      <description><![CDATA[Alex Kretzschmar joins Adam for a trip down the Linux rabbit hole -- Docker vs Podman, building a Kubernetes cluster, ZFS backups with zfs.rent, bootc, favorite Linux distros, new homelab tools built with AI, self-hosting Immich, content creation, Plex and Jellyfin, the future of piracy and more.]]></description>
      <pubDate>Fri, 12 Dec 2025 19:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/121</guid>
    </item>
    <item>
      <title><![CDATA[There will be bleeps]]></title>
      <link>https://changelog.com/friends/113</link>
      <description><![CDATA[Mike McQuaid and Justin Searls join Jerod in the wake of the RubyGems debacle to discuss what happened, what it says about money in open source, what sustainability really means for our community, making a career out of open source (or not), and more. Bleep!]]></description>
      <pubDate>Fri, 17 Oct 2025 18:15:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/113</guid>
    </item>
    <item>
      <title><![CDATA[obra/superpowers]]></title>
      <link>https://github.com/obra/superpowers</link>
      <description><![CDATA[obra/superpowers]]></description>
      <pubDate>Fri, 20 Feb 2026 04:14:28 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/obra/superpowers</guid>
    </item>
    <item>
      <title><![CDATA[Vendo herramienta licencia total por solo 299.99$ es funcional ya que resuelve problema real]]></title>
      <link>https://dev.to/vertil_jivenson20/vendo-herramienta-licencia-total-por-solo-29999-es-funcional-ya-que-resuelve-problema-real-58jn</link>
      <description><![CDATA[The Wayback Machine is an initiative of the Internet Archive, a 501(c)(3) non-profit, building a digital library of Internet sites and other cultural artifacts in digital form. Other projects include Open Library &amp; archive-it.org. Your use of the Wayback Machine is subject to the Internet Archive's Terms of Use.]]></description>
      <pubDate>Fri, 20 Feb 2026 03:58:34 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/vertil_jivenson20/vendo-herramienta-licencia-total-por-solo-29999-es-funcional-ya-que-resuelve-problema-real-58jn</guid>
    </item>
    <item>
      <title><![CDATA[AlphaOfTech Daily Brief — 2026-02-20]]></title>
      <link>https://dev.to/chairman_lee_7d78f8023756/alphaoftech-daily-brief-2026-02-20-35c5</link>
      <description><![CDATA[TL;DR: Anthropic updated its legal terms, effectively banning the use of subscription-based authentication for third-party Claude Code integrations. If your app uses this approach, it’s time to rethink or renegotiate. Meanwhile, Google launched Gemini 3.1 Pro, claiming enhanced reasoning abilities — a potential game-changer for those leveraging LLMs for complex problem-solving.
Anthropic’s update to its legal documentation is the most intriguing development today. They’ve barred third-party integrations from using subscription-based authentication for Claude Code. This isn’t mere legalese; it’s a fundamental shift in how developers can interact with Anthropic’s ecosystem. If your startup employs end-user subscription tokens to call Anthropic servers, it’s time to audit and adapt or face contractual breaches.
This matters because it highlights a growing trend among AI companies: increasing control over their ecosystems. Anthropic’s policy change emphasizes a push toward server-side API key patterns or enterprise licenses, which may increase overhead costs and complicate integration efforts. Startups relying heavily on such integrations could find themselves in a tight spot, needing to either pivot their approach or negotiate new agreements.
For developers, this is both a threat and an opportunity. The challenge lies in quickly re-engineering workflows to comply with new terms. However, it also opens the door to exploring alternative solutions or even developing in-house capabilities that bypass these restrictions altogether. It’s a classic case of adapt or perish.
Google has unveiled Gemini 3.1 Pro, positioning it as a significant upgrade for reasoning-heavy tasks. Unlike other AI unveilings that promise revolutions, this feels more like a quiet evolution. Yet, for those in the space, the implications are substantial. With the growing reliance on large language models (LLMs) for complex problem-solving, any incremental improvement can translate into real-world advantages.
For startups, particularly those in need of sophisticated reasoning capabilities, Gemini 3.1 Pro presents an enticing proposition. Imagine cutting down on prompt engineering complexity or minimizing costly inference operations. Conducting a 10-case A/B test against your current LLM baseline might reveal whether this new release can genuinely augment or replace existing models.
The potential for reduced operational costs and efficiency gains cannot be ignored. But before jumping ship, validate these claims with rigorous testing. Gemini’s performance in real-world applications is where the true measure of its value will be determined.
In the open-source corner, Step 3.5 Flash has emerged as a contender in the model inference arena. Positioned as an open-source foundation model, it challenges the economics of closed large models by offering deep reasoning capabilities at speed. This could be a boon for startups needing budget-friendly options without sacrificing performance.
The opportunity here is clear: test Step 3.5 Flash for internal tooling where cost and latency are critical. Think code review bots or CI assistants where on-prem hosting could save significant cloud spend. While benchmarks remain unverified, the promise of a low-cost, high-performance alternative is tempting. Hosting on spare GPU capacity to measure real-world trade-offs could illuminate its true potential.
Despite its promising release notes, it’s crucial to approach this with a healthy dose of skepticism. Only through practical application will its real-world viability become apparent.
What prompted Anthropic's legal change?
Anthropic’s decision seems driven by a desire to exert more control over its ecosystem, likely in response to security and revenue considerations. By steering developers towards server-side API keys or enterprise licenses, they tighten security while potentially opening new revenue streams.
How does Google’s Gemini 3.1 Pro compare to its predecessors?
While details are sparse, Google claims improved core reasoning capabilities. For startups leveraging LLMs for complex tasks, this could mean fewer resources spent on prompt engineering and improved inference efficiency. Testing against existing models is essential to verify these claims.
Is Step 3.5 Flash viable for commercial applications?
Yes, especially for startups looking to minimize cloud expenses. Its open-source nature and claimed inference speed make it an attractive option. However, real-world testing is necessary to confirm its performance metrics.
How should startups respond to the Anthropic-Palantir partnership rift?]]></description>
      <pubDate>Fri, 20 Feb 2026 00:10:23 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/chairman_lee_7d78f8023756/alphaoftech-daily-brief-2026-02-20-35c5</guid>
    </item>
    <item>
      <title><![CDATA[Changes to test merge commit generation for pull requests]]></title>
      <link>https://github.blog/changelog/2026-02-19-changes-to-test-merge-commit-generation-for-pull-requests</link>
      <description><![CDATA[To reduce delays when determining the mergeability for a pull request and improve system reliability, we’ve changed the frequency at which we generate test merge commits for open pull requests.…]]></description>
      <pubDate>Thu, 19 Feb 2026 22:01:57 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-changes-to-test-merge-commit-generation-for-pull-requests</guid>
    </item>
    <item>
      <title><![CDATA[Selected Anthropic and OpenAI models are now deprecated]]></title>
      <link>https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated</link>
      <description><![CDATA[We have deprecated the following models across all GitHub Copilot experiences (including Copilot Chat, inline edits, ask and agent modes, and code completions) on February 17, 2026: Model Deprecation Date…]]></description>
      <pubDate>Thu, 19 Feb 2026 19:47:37 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated</guid>
    </item>
    <item>
      <title><![CDATA[GitHub Projects: Import items based on a query and hierarchy view improvements]]></title>
      <link>https://github.blog/changelog/2026-02-19-github-projects-import-items-based-on-a-query-and-hierarchy-view-improvements</link>
      <description><![CDATA[Import project items with a search query When creating a new project, you can now add items using a search query, in addition to importing directly from a repository. This…]]></description>
      <pubDate>Thu, 19 Feb 2026 19:33:33 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-github-projects-import-items-based-on-a-query-and-hierarchy-view-improvements</guid>
    </item>
    <item>
      <title><![CDATA[OpenAI sur le point de conclure un tour de financement de 100 milliards $, officiellement pour « renforcer les capacités d'OpenAI », mais surtout en réalité pour couvrir ses énormes pertes]]></title>
      <link>https://intelligence-artificielle.developpez.com/actu/380422/OpenAI-sur-le-point-de-conclure-un-tour-de-financement-de-100-milliards-officiellement-pour-renforcer-les-capacites-d-OpenAI-mais-surtout-en-realite-pour-couvrir-ses-enormes-pertes/</link>
      <description><![CDATA[OpenAI est sur le point de conclure un tour de financement de 100 milliards $, officiellement pour « renforcer les capacités d'OpenAI et étendre ses activités », mais en réalité pour couvrir ses énormes pertes
Un nouveau rapport révèle qu'OpenAI serait sur le point de conclure la phase initiale d'un important tour de table qui devrait permettre de lever plus de 100 milliards de dollars. Le rapport cite des sources proches du dossier, selon lequel la société d'intelligence artificielle serait en pourparlers...]]></description>
      <pubDate>Thu, 19 Feb 2026 16:45:00 GMT</pubDate>
      <source>Developpez.com</source>
      <category>opensource</category>
      <guid>https://intelligence-artificielle.developpez.com/actu/380422/OpenAI-sur-le-point-de-conclure-un-tour-de-financement-de-100-milliards-officiellement-pour-renforcer-les-capacites-d-OpenAI-mais-surtout-en-realite-pour-couvrir-ses-enormes-pertes/</guid>
    </item>
    <item>
      <title><![CDATA[Python Environnements Extension : pour unifier les environnements Python sur Visual Studio Code]]></title>
      <link>https://www.programmez.com/actualites/python-environnements-extension-pour-unifier-les-environnements-python-sur-visual-studio-code-39036</link>
      <description><![CDATA[Pour simplifier et unifier l'environnement de développement Python sur Visual Studio Code, on dispose de la nouvelle extension Python Environnements. Il doit unifier le modèle de développement, gérer les environnements et les workflows, gérer les interpréteurs et les packages. Jusqu'é présent, l'expérience Python était fragmenté à travers les différents outils (venv, conda, pyenv, etc.). Après plus d'un an d'ajustements et de développement, l'extension est disponible. A terme, tous les flux Python migreront vers l'extension Environnements. Il est possible d'activer dès maintenant : python.useEnvsExtension. L'extension fonctionne en parallèle de l'extension Python et aucune configuration particulière n'est requise : vous ouvrez un fichier Python et l'environnement utilisé est automatiquement détecté. Les environnements supportés sont : venv
conda
pyenv
poetry
pipenv
System Python installs La découverte est assurée par PET (Python Environment Tool), un outil de scan codé en Rust. Si vous utilisez uv, l'extension va automatiquement créer un environnement venv et installer les paquets nécessaires. Pour le moment, il n'est pas possible de créer rapidement des projets sur tous les environnements, seuls venv et conda sont supportés. Sans doute que les autres le seront dans les prochaines versions. Mauvaise nouvelle : l'extension fonctionne UNIQUEMENT sur Windows x64 et Windows ARM et l'édition Web ! Il faut Python soit installé.
Pour le moment, les retours sont plutôt mauvais : extension difficile à utiliser, perte de temps pour créer les environnements depuis Pylance, etc. Et les mises à jour se succèdent. Heureusement que l'extension est officiellement en preview. Page de l'extension : https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-python-envs Catégorie actualité: Outils Visual Studio Code, Python Image actualité AMP:]]></description>
      <pubDate>Thu, 19 Feb 2026 07:33:25 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/python-environnements-extension-pour-unifier-les-environnements-python-sur-visual-studio-code-39036</guid>
    </item>
    <item>
      <title><![CDATA[Quantique : Comcast, Classiq et AMD testent un algorithme quantique pour les réseaux]]></title>
      <link>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</link>
      <description><![CDATA[Comcast, Classiq et AMD mènent des tests pour améliorer le trafic Internet en utilisant des algorithmes quantiques pour renforcer la résistance du routage réseau. "L’essai conjoint s’est concentré sur un défi clé de la conception des réseaux : identifier des chemins de secours indépendants pour les nœuds du réseau lors des opérations de maintenance ou de modifications. L’objectif était de garantir que, si un site est mis hors ligne et que soudainement, un deuxième tombe en panne, le trafic puisse être redirigé sans interruption ni dégradation du service pour les clients. Pour y parvenir, les opérateurs doivent identifier des chemins de secours distincts, rapides et capables de résister à des pannes simultanées, tout en minimisant la latence. Cette tâche devient de plus en plus complexe à mesure que le réseau s’étend." explique l'annonce. Le schéma présente le design et l'implémentation du flux et de l'algo quantique sur la plateforme Classiq. L’expérimentation a combiné des techniques de calcul quantique et des méthodes classiques haute performance afin d’évaluer la capacité des algorithmes quantiques à identifier, en temps réel, des chemins de secours dans des scénarios de gestion des changements. Elle a été menée à la fois sur du matériel quantique et dans des environnements de simulation accélérés utilisant des GPU AMD Instinct, afin d’atteindre une capacité de calcul (à l’échelle des qubits) encore hors de portée du matériel quantique seul.
« L’avenir du calcul repose sur la convergence entre le classique et le quantique », explique Madhu Rangarajan, vice-président corporate en charge des produits Compute et Enterprise AI chez AMD. « En tant qu’acteur du calcul haute performance, nous cherchons à comprendre nos technologies peuvent accompagner l’émergence du quantique. Cette collaboration montre un cas concret où la simulation accélérée et l’exécution quantique sont combinées pour répondre à un enjeu opérationnel réel dans les réseaux. »
Détail sur l'algo quantique utilisé : https://www.amd.com/en/developer/resources/technical-articles/2026/designing-resilient-routing-using-quantum-algorithms.html Catégorie actualité: Technologies quantique Image actualité AMP:]]></description>
      <pubDate>Wed, 18 Feb 2026 08:34:25 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</guid>
    </item>
    <item>
      <title><![CDATA[IDE Kiro : Checkmarx apporte plus de sécurité applicative]]></title>
      <link>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</link>
      <description><![CDATA[Checkmarx annonce que son Developer Assist supporte l'IDE Kiro, pour l'étendre la sécurité applicative directement dans l'enviornnement. Cette intégration permet à ces derniers d'identifier et de résoudre les problèmes de sécurité au fil de l'écriture du code, sans quitter leur IDE ni dépendre de scans en aval dans la chaîne CI/CD.
En utilisant l’extension IDE officielle de Checkmarx, les développeurs peuvent activer Developer Assist dans Kiro en quelques étapes seulement, sans configuration lourde. La prise en charge d’autres flux de développement, y compris via la ligne de commande, sera bientôt disponible. Une fois authentifié, Developer Assist analyse automatiquement le code source et les dépendances de l’espace de travail actif, appliquant les politiques existantes de Checkmarx One. Aucune configuration spécifique à Kiro, API propriétaire ou intégration expérimentale n’est nécessaire. Developer Assist est disponible sur Cursor, Visual Studio Code et Windsurf.
Pour en savoir plus : https://dev.checkmarx.com/ Catégorie actualité: Outils Checkmarx Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:25:38 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</guid>
    </item>
    <item>
      <title><![CDATA[WebMCP : un standard pour rendre un site web "agent ready" ?]]></title>
      <link>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</link>
      <description><![CDATA[concilier agents IA et sites web et la manière dont les pages web pourraient interagir, travailler avec les agents ? WebMCP veut fournir une méthode standard pour définir les actions des agents sur un site web, sur une page web sans pénaliser au bon fonctionnement du site web. "Vous indiquez aux agents et où interagir avec votre site, qu'il s'agisse de réserver un vol, de soumettre une demande d'assistance ou de naviguer dans des données complexes. Ce canal de communication direct élimine toute ambiguïté et permet des flux de travail plus rapides et plus efficaces pour les agents." expliquer Google. WebMCP preview repose sur 2 API :
- API déclarative : Permet d’effectuer des actions standard définies directement dans les formulaires HTML. - API impérative : Permet d’effectuer des interactions plus complexes et dynamiques nécessitant l’exécution de JavaScript. C'est une interface proposé en preview par Google et accessible dans Chrome. Ces API forment un "pont" rendant votre site web "agent ready" et permet de créer des flux agentiques que se veulent plus fiables qu'en passant par du DOM. Ces API sont JavaScript. Pour le moment, la spécification est en cours de rédaction. Elle ne dépend pas de W3C et n'est pas un standard du consortium. Site : https://webmachinelearning.github.io/webmcp/ Catégorie actualité: IA MCP Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:18:02 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</guid>
    </item>
    <item>
      <title><![CDATA[Parcours libriste d’Isabella Vanni — « Libre à vous ! » du 10 février 2026 — Podcasts et références]]></title>
      <link>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</link>
      <description><![CDATA[268ème émission « Libre à vous ! » de l’April. Podcast et programme :
sujet principal : parcours libriste d’Isabella Vanni, coordinatrice vie associative et responsable projets à l’April. Un parcours libriste est l’interview d’une seule personne pour parler de son parcours personnel et professionnel
chronique « Que libérer d’autre que du logiciel avec Antanak » sur « Les assises de l’attention »
chronique de Benjamin Bellamy sur « L’antéchrist et les petits hommes verts »
Quoi de Libre ? Actualités et annonces concernant l’April et le monde du Libre lien nᵒ 1 : Podcast de la 268ᵉ émission
lien nᵒ 2 : Les références pour la 268ᵉ émission et les podcasts par sujets
lien nᵒ 3 : S'abonner au podcast
lien nᵒ 4 : S'abonner à la lettre d'actus
lien nᵒ 5 : Libre à vous !
lien nᵒ 6 : Radio Cause Commune Rendez‐vous en direct chaque mardi de 15 h 30 à 17 h sur 93,1 MHz en Île‐de‐France. L’émission est diffusée simultanément sur le site Web de la radio Cause Commune. Vous pouvez nous laisser un message sur le répondeur de la radio : pour réagir à l’un des sujets de l’émission, pour partager un témoignage, vos idées, vos suggestions, vos encouragements ou pour nous poser une question. Le numéro du répondeur : +33 9 72 51 55 46. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:24 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</guid>
    </item>
    <item>
      <title><![CDATA[.Net 11 Preview 1 : nouvelles librairies, peu de changements dans C#]]></title>
      <link>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</link>
      <description><![CDATA[.Net 10 a été distribuée en novembre 2025. La version 11 est désormais disponible en preview 1. Comme à chaque fois, de nombreuses évolutions sont attendues. L'ensemble des frameworks et des langages sont concernées : C#, F#, ASP.Net Core, Blazor, MAUI, le compilateur Jit, le support de CoreCLR dans WebAssembly, meilleure compression / décompression avec Zstandard. Sur la partie librairie, retenons déjà les évolutions suivantes :
- Zstandard est natif à .Net pour la compression. La librairie promet une nette amélioration des performances :
// Compress data using ZstandardStream
using var compressStream = new ZstandardStream(outputStream, CompressionMode.Compress);
await inputStream.CopyToAsync(compressStream); // Decompress data
using var decompressStream = new ZstandardStream(inputStream, CompressionMode.Decompress);
await decompressStream.CopyToAsync(outputStream);
- BFloat16 intègre par défaut toutes les interfaces standards pour le numérique
- amélioration de TimeZone
Note de version sur les librairies : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/libraries.md
Sur la partie runtime, il faut s'attendre à de bonnes nouvelles :
- Runtime async : une nouvelle fonction majeure du runtime et méthodes asynchrones pour améliorer les performances. CoreCLR supporte RuntimeAsync par défaut, idem pour Native AOT
- CoreCLR est supporté dans WebAssembly. Il n'est pas encore disponible en preview 1.
- diverses améliorations de performances sur le JIT - meilleur support de RISC-V
Sur C#, pour le moment, peu de nouveautés annoncées. Deux nouvelles fonctions sont attendues : arguments pour les expresssions Collection et support Extended layout. .Net 11 n'introduira aucune nouvelle fonctionnalité pour Visual Basic. Sur ASP.Net Core et Blazor, les développeurs vont avoir beaucoup de nouveautés : EnvironmentBoundary, nouveau composant Label dans les formulaires Blazor, nouveau composant DisplayName, navigation relative Uri, support "propre" des éléments MathML dans un rendu interactif. Tous les détails dans la note de version : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/aspnetcore.md
La génération de source XAML est par défaut pour les applications .Net MAUI, cela doit permettre un build plus rapide et un debug plus performant. Sur Android, CoreCLR devient le runtime par défaut. Sur Container Images et Winfows Forms, pas de nouveautés annoncées. Annonce de .Net 11 : https://devblogs.microsoft.com/dotnet/dotnet-11-preview-1/ Catégorie actualité: Frameworks .Net 11 Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 09:52:19 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</guid>
    </item>
    <item>
      <title><![CDATA[Automate repository tasks with GitHub Agentic Workflows]]></title>
      <link>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</link>
      <description><![CDATA[Discover GitHub Agentic Workflows, now in technical preview. Build automations using coding agents in GitHub Actions to handle triage, documentation, code quality, and more.]]></description>
      <pubDate>Fri, 13 Feb 2026 14:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</guid>
    </item>
    <item>
      <title><![CDATA[LibreOffice 26.2 : Markdown, accessibilité et plein d’autres nouveautés et améliorations]]></title>
      <link>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</link>
      <description><![CDATA[En février, il y a la corvée commerciale de la Saint-Valentin et les réjouissances intellectuelles consécutives à la sortie d’une nouvelle version de la suite bureautique LibreOffice. C’est, bien évidemment, sur LibreOffice 26.2 que l’on va se pencher. Au menu, du très visible, comme les boites de dialogues, du très attendu comme la prise en compte du Markdown ou du moins visible comme le travail sur l’accessibilité.
Il va de soi que les notes de version sont plus exhaustives et qu’il ne s’agit ici que d’une sélection. lien nᵒ 1 : Notes de version Sommaire
L’accessibilité
Support du Markdown
L’interface et les boites de dialogue
Writer
Calc
En vrac
Pour finir
Avant de commencer : toutes les captures d’écran ont été faites, volontairement, sur une interface très personnalisée.
L’accessibilité
L’accessibilité de la suite bureautique est un important chantier pour lequel une personne a été recrutée en 2023 (en). Cette version-ci a fait l’objet d’améliorations sensibles. Parallèlement, Sophie Gautier, coordinatrice de The Document Foundation1 (Foundation coordinator) est en train de monter un groupe de travail qui a pour objectif la publication d’un rapport de conformité en matière d’accessibilité pour répondre à la norme européenne EN 301 549 (en) d’accessiblité numérique. La langue de travail de ce groupe est l’anglais.
Concernant les améliorations de cette version :
la boite de dialogue « Vérifier les mises à jour », Aide &gt; Vérifier les mises à jour… est devenue accessible aux lecteurs d’écran ;
les fonctions d’accessibilité des aperçus des bordures, onglet « Bordures » des boites de dialogue, ont été revues afin qu’elles ne perturbent plus les dispositifs d’assistance ;
sur Linux : la boite de dialogue Outils&gt; Orthographe est annoncée correctement par le lecteur d’écran ;
quand on supprimait la sélection accessible, le curseur se déplaçait automatiquement au début du texte, ce comportement perturbant est supprimé ;
dans Writer, les fautes d’orthographe ne sont plus signalées par les dispositifs d’assistance si la vérification orthographique n’est pas activée ;
l’accessibilité au clavier de la boite de dialogue des extensions : Outils &gt; Extensions est accessible aux lecteurs d’écran ;
et enfin, il est possible de naviguer entre les onglets verticaux avec des raccourcis clavier.
Support du Markdown
Le Markdown est devenu le format de balisage léger standard « de fait ». Et c’est celui supporté par LinuxFR. Son support a été introduit dans cette version, c’est un des formats d’enregistrement qui s’est ajouté à la série des autres formats de la suite, pas un format d’export. Pour l’utiliser pour vos sites, passant pour LinuxFR, vous devrez :
soit ouvrir le fichier .md dans un éditeur de texte, n’importe lequel, même Mousepad fait l’affaire par exemple, et copier-coller ensuite le tout à partir de l’éditeur de texte là où vous le voulez ;
soit, si cela est possible, importer le fichier .md dans ce qui vous sert pour gérer le site comme le fait par exemple l’extension ODT2SPIP pour le système de gestion de contenu SPIP qui permet de créer une nouvelle page dans SPIP avec un fichier.ODT. ça marche avec LinuxFR ? Plutôt bien. Les styles de caractère Accentuation (ici en italiques) et Accentuation forte (ici gras) sont bien reconnu ainsi que Texte source pour « télétype », les indications in-texte encadrées de l’accent grave U+0060. Les styles de paragraphes :
Bloc de citation (paragraphes de citation précédés d’une ligne blanche et du signe « &gt; » dans la saisie de contenu sur LinuxFR) ;
Contenu de tableau ;
Corps de texte ;
Liste, par contre la numérotation des listes ordonnée ne semble pas bien fonctionner, il faut saisir les numéros à la main ;
Texte préformaté pour écrire des blocs de code ;
Titre 1, Titre 2, Titre 3 et Titre de tableau.
Les tableaux sont bien repris ainsi que les liens insérés via l’insertion d’hyperliens.
Ce qui ne semble pas fonctionner du tout : ce sont les notes, elles disparaissent corps et biens. C’est peut-être dû au passage dans l’éditeur de texte qui transforme un peu le document. Et, évidemment, il faut rajouter les images avec la syntaxe LinuxFR.
La version de Mardown de LibreOffice est CommonMark (en) et la bibliothèque utilisée est MD4C avec quelques extensions prises en charge par cette bibliothèque (cf ce rapport de bug (en) et ses réponses), pour en savoir plus, voir cette note (en) du blog de The Document Foundation.
Petite remarque, si vous utilisez un LibreOffice 25.8, vous avez peut-être pu constater qu’il était question d’enregistrement au format .md, cette information a été ajoutée trop précocement car la version 25.8 ne gère pas le Markdown.
L’interface et les boites de dialogue
Les boites de dialogue, notamment de styles et de formats, ont beaucoup changé. Longtemps elles se sont affichées avec une présentation par onglets en haut et le contenu dessous.
Puis il y a une période de transition en 2025 qui a fait grincer une collection complète de dents où on avait, selon l’endroit où on était, soit des onglets soit une navigation par menu latéral. Cette dernière avait un gros défaut : par exemple pour la configuration des styles dans Writer il fallait descendre tout en bas pour accéder aux options qui étaient cachées. Et il n’y avait pas de barre de défilement pour aller plus vite.
LibreOffice 26.2 voit ces défauts corrigés : les boites de dialogue sont harmonisées dans toute la suite et leur menu latéral, toujours sans barre de défilement qui s’avère finalement inutile, montre clairement tous les types de paramètres auxquels on peut accéder. Et, comme on peut le voir, LibreOffice a intégré une meilleure prise en charge des systèmes d’écritures asiatiques et complexes en affichant deux colonnes, une pour les polices occidentales, ou pour les polices asiatiques ou complexes. Une personne a également été recrutée en 2023 (en) pour travailler sur le support des systèmes d’écriture de droite à gauche (RTL) et complexes (CTL). Si toutefois, vous préférez revenir à l’affichage avec les onglets, il suffit d’aller dans le menu Outils &gt; Options &gt; Apparenceau niveau de « Boites de dialogue » et cocher l’option Horizontal en haut. Il faut savoir que les onglets en haut ne s’affichent que sur une seule ligne et qu’il faudra donc naviguer avec les flèches quand il y a de nombreuses options. Writer
Il y a un certain nombre d’amélioration autour de la compatibilité avec le format DOCX : séparation de tableaux flottants en plusieurs tableaux, suppression de la numérotation des notes de bas de page à l’ouverture d’un fichier DOCX, etc.
On relèvera deux nouvelles options d’alignement des paragraphes : « Début » et « Fin ». Si vous utilisez l’alphabet latin, vous ne verrez aucune différence avec les deux options « Forcer à gauche/en haut » et « Forcer à droite/en bas ». Elles ont été développées pour réutiliser plus facilement les styles entre les divers systèmes d’écriture. Pour continuer sur la lancée du travail pour la prise en compte des systèmes d’écriture dont le fonctionnement est différent de celui de l’alphabet latin, il est possible de changer la direction du texte : de gauche à droite ou de droite à gauche en cours de travail. Cela peut se paramétrer dans les styles. Calc
Un gros travail sur les performances a été fait : vitesse de défilement, rapidité des classeurs avec de nombreuses formes et du rejet des modifications. On voit apparaître de nouvelles options de tri (Données &gt;Trier) qui dépendent de la « locale » (langue définie dans les Options de LibreOffice). On peut ainsi déterminer quel caractère est utilisé comme séparateur de décimal pour le tri naturel. On peut relever aussi une avancée ergonomique qui va plaire à toutes celles et ceux qui utilisent les matrices, on peut maintenant modifier les formules matricielles avec la combinaison de touches : F2 + ↑ Maj + Ctrl + Entrée, il n’est plus nécessaire de modifier la formule elle-même.
Et aussi : si vous utilisez (pourquoi diable ?) le format d’enregistrement XLSX, c’est le format EXCEL2010+ qui est le format par défaut, il change de nom pour devenir « Classeur Excel 2010-365 ».2
En vrac
Base est devenu complètement multi-utilisateur, TDF a, d’ailleurs, recruté une personne pour travailler sur l’application.
Concernant les diagrammes (ou chart) : dans le Volet latéral, quand le graphique est en mode modification et que l’on va, au niveau de « Couleurs », sur la palette, on a une prévisualisation en direct dans le diagramme ce qui permet de tester le choix de couleurs plus facilement.
Les polices embarquées dont la licence ne permettait pas l’édition étaient jusqu’à présent ignorées et remplacées à l’affichage, ni vu, ni connu par une fonte de substitution. Ce défaut a été corrigé.
L’export PDF gère les liens avec les documents externes : Fichier &gt; Exporter au format PDF &gt; Liens. Les dictionnaires hongrois, mongol et portugais du Portugal ont été mis à jour ainsi que les règles de césure de la langue hongroise.
JSON, pour JavaScript Object Notation, est un format standard utilisé pour représenter des données structurées. Il est utilisé notamment pour échanger les informations entre un navigateur et un serveur. C’est, par exemple, le format de sauvegarde des marques-pages de Firefox ou de certains fichiers d’archives de Mastodon. Les documents XML et JSON génériques avec des plages pouvant être liées sont maintenant automatiquement mappés à des feuilles dans Calc. Une plage pouvant être liée est une section d’un document contenant des enregistrements tabulaires. Lorsqu’un document contient plusieurs plages pouvant être liées, chaque plage est mappée à une seule feuille3.
Et si vous avez envie de vous amuser avec les fonctions expérimentales (à activer dansOutils &gt; Options &gt; LibreOffice &gt; Avancé), vous pouvez jouer avec la nouvelle de boite de dialogue « Gestion des macros ».
Pour finir
Cette dépêche a, bien, évidemment, été rédigée avec LibreOffice et, cette fois-ci dans un fichier enregistré en Markdown. Les seules balises que j’ai dû entrer à la main sont celles des images. Kate a l’air de modifier le fichier et, quand je réouvre le .md dans LibreOffice, il y a des styles qui ont sauté mais la mise en forme reste visuellement la même. Kate rajoute aussi des barres obliques devant les « &gt; », aux crochets [ ] et même à certains hyperliens (images). Il y a peut-être des éditeurs de texte plus adaptés ou des réglages à faire.
J’ai rédigé cette dépêche en même temps qu’un article sur LibreOffice 26.2 pour mon site. Si l’article n’est pas vraiment dupliqué, il n’est pas étonnant d’y trouver des morceaux ici. Que tout cela ne nous empêche d’adresser tous nos remerciements à celles et ceux qui font de LibreOffice une suite bureautique si agréable à utiliser et si performante.
Post-scriptum : si vous voulez savoir modifier les couleurs de l’interface comme sur les captures d’écran, ça peut s’envisager, demandez gentiment, avec un peu de chance.
The Document Foundation ou TDF est la fondation de droit allemand qui pilote le projet LibreOffice. Il y a deux formats OOXML différents et donc deux formats XLSX différents, la version 2007 et la version actuelle depuis 2010. S’il vous est vraiment nécessaire d’enregistrer au format XLSX, il faut utiliser la version de 2010. Notes de version. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Fri, 13 Feb 2026 09:09:23 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</guid>
    </item>
    <item>
      <title><![CDATA[Projets Libres saison 4 épisode 11 : PVH éditions, une maison d'édition libérée et dans le Fediverse]]></title>
      <link>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</link>
      <description><![CDATA[Nous avons eu le plaisir de rencontrer Lionel Jeannerat durant les Rencontres Hivernales du libre à Saint-Cergue (VD) en janvier 2026. son parcours
la maison d'édition et ses œuvres
le passage au libre que ce soit pour les licences mais aussi pour leurs outils métiers
Bonne écoute ou lecture lien nᵒ 1 : Lien vers l'épisode
lien nᵒ 2 : S'abonner au podcast
lien nᵒ 3 : Le site de PVH éditions
lien nᵒ 4 : Soutenir le podcast
lien nᵒ 5 : L'épisode traduit en anglais
lien nᵒ 6 : Le site des Rencontres Hivernales du libre Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 07:40:57 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</guid>
    </item>
    <item>
      <title><![CDATA[Vouch for an open source web of trust]]></title>
      <link>https://changelog.com/news/180</link>
      <description><![CDATA[Vouch for an open source web of trust]]></description>
      <pubDate>Mon, 09 Feb 2026 19:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/news/180</guid>
    </item>
    <item>
      <title><![CDATA[Les journaux LinuxFr.org les mieux notés de janvier 2026]]></title>
      <link>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</link>
      <description><![CDATA[LinuxFr.org propose des dépêches et articles, soumis par tout un chacun, puis revus et corrigés par l’équipe de modération avant publication. C’est la partie la plus visible de LinuxFr.org, ce sont les dépêches qui sont le plus lues et suivies, sur le site, via Atom/RSS, ou bien via partage par messagerie instantanée, par courriel, ou encore via médias sociaux. Ce que l’on sait moins, c’est que LinuxFr.org vous propose également de publier directement vos propres articles, sans validation a priori de lʼéquipe de modération. Ceux-ci s’appellent des journaux. Voici un florilège d’une dizaine de ces journaux parmi les mieux notés par les utilisateurs et les utilisatrices… qui notent. Lumière sur ceux du mois de janvier passé.
« lecteur mp3 pour personne handicapée mentale » par ChocolatineFlying ;
« À la recherche du Linuxfrien type » par Ysabeau ;
« hacker sa pompe de relevage 3 et fin ! » par ChocolatineFlying ;
« [Hors sujet] Des tablettes lave-vaisselle tout-en-un » par Tanguy Ortolo ;
« Francis Hallé Bronsonisé » par Joris Dedieu ;
« 10 ans après, Modoboa est toujours là pour prendre soin de votre serveur de messagerie » par mirtouf ;
« À table ! » par JaguarWan ;
« Retour d'expérience sur le développement d'une application par l'utilisation d'IA » par phoenix ;
« Algoo lance un bulletin d'information mensuel « veille techno et logiciels libres » » par LeBouquetin ;
« Linux : les planètes s'alignent en 2026 » par vmagnin. lien nᵒ 1 : Participez à l’écriture d’un article
lien nᵒ 2 : Publiez votre journal
lien nᵒ 3 : Proposez une dépêche Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 09:23:50 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[Meilleures contributions LinuxFr.org : les primées de janvier 2026]]></title>
      <link>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</link>
      <description><![CDATA[Nous continuons sur notre lancée de récompenser celles et ceux qui chaque mois contribuent au site LinuxFr.org (dépêches, , logo, journaux, correctifs, etc.). Vous n’êtes pas sans risquer de gagner un livre des éditions Eyrolles, ENI et D-Booker. Voici les gagnants du mois de janvier 2026 :
Stefane Fermigier, pour sa dépêche « Appel à de la Commission "Vers des écosystèmes numériques ouverts européens" » ;
ChocolatineFlying, pour son journal « lecteur mp3 pour personne handicapé mental » ;
YvanM, pour sa dépêche « MeshCentral, alternative à TeamViewer et RustDesk » ;
Christophe Bliard, pour sa dépêche « Sortie de OpenProject 17.0 ».
Les livres gagnés sont détaillés en seconde partie de la dépêche. N’oubliez pas de contribuer, LinuxFr.org vit pour vous et par vous ! lien nᵒ 1 : Contribuez à LinuxFr.org !
lien nᵒ 2 : Tous les moyens (ou presque) de participer
lien nᵒ 3 : Récompenses précédentes (décembre 2025) Les livres sélectionnés
Linux — Maîtrisez l'administration du système — 7e édition. Certaines personnes n’ont pas pu être jointes ou n’ont pas répondu. Les lots ont été réattribués automatiquement. N’oubliez pas de mettre une adresse de courriel valable dans votre compte ou lors de la proposition d’une dépêche. En effet, c’est notre seul moyen de vous contacter, que ce soit pour les lots ou des questions sur votre dépêche lors de sa modération. Tous nos remerciements aux contributeurs du site ainsi qu’aux éditions Eyrolles, ENI et D-Booker. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 07:09:14 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[Continuous AI in practice: What developers can automate today with agentic CI]]></title>
      <link>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</link>
      <description><![CDATA[Think of Continuous AI as background agents that operate in your repository for tasks that require reasoning.]]></description>
      <pubDate>Thu, 05 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</guid>
    </item>
    <item>
      <title><![CDATA[Setting Docker Hardened Images free]]></title>
      <link>https://changelog.com/podcast/675</link>
      <description><![CDATA[In May of 2025, Docker launched Hardened Images, a secure, minimal, production-ready set of images. In December, they made DHI freely available and open source to everyone who builds software. On this episode, we're joined by Tushar Jain, EVP of Engineering at Docker to learn all about it.]]></description>
      <pubDate>Wed, 04 Feb 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/675</guid>
    </item>
    <item>
      <title><![CDATA[Pick your agent: Use Claude and Codex on Agent HQ]]></title>
      <link>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</link>
      <description><![CDATA[Claude by Anthropic and OpenAI Codex are now available in public preview on GitHub and VS Code with a Copilot Pro+ or Copilot Enterprise subscription. Here's what you need to know and how to get started today.]]></description>
      <pubDate>Wed, 04 Feb 2026 17:00:19 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</guid>
    </item>
    <item>
      <title><![CDATA[What the fastest-growing tools reveal about how software is being built]]></title>
      <link>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</link>
      <description><![CDATA[What languages are growing fastest, and why? What about the projects that people are interested in the most? Where are new developers cutting their teeth? Let’s take a look at Octoverse data to find out.]]></description>
      <pubDate>Tue, 03 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</guid>
    </item>
    <item>
      <title><![CDATA[The state of homelab tech (2026)]]></title>
      <link>https://changelog.com/friends/125</link>
      <description><![CDATA[Techno Tim joins Adam to dive deep into the state of homelab'ing in 2026. Hardware is scarce and expensive due to the AI gold rush, but software has never been better. From unleashing Claude on your UDM Pro to building custom Proxmox CLIs, they explores how AI is transforming what's possible in the homelab. Tim declares 2026 the "Year of Self-Hosted Software" while Adam reveals his homelab's secret weapons: DNSHole (a Pi-hole replacement written in Rust) and PXM (a Proxmox automation CLI).]]></description>
      <pubDate>Sat, 24 Jan 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/125</guid>
    </item>
    <item>
      <title><![CDATA[Agent psychosis: are we going insane?]]></title>
      <link>https://changelog.com/news/177</link>
      <description><![CDATA[Armin Ronacher thinks AI agent psychosis might be driving us insane, Dan Abramov explains how AT Protocol is a social filesystem, RepoBar keeps your GitHub work in view without opening a browser, Ethan McCue shares some life altering Postgres patterns, and Lea Verou says web dependencies are broken and we need to fix them.]]></description>
      <pubDate>Mon, 19 Jan 2026 20:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/news/177</guid>
    </item>
    <item>
      <title><![CDATA[Very important agents]]></title>
      <link>https://changelog.com/friends/120</link>
      <description><![CDATA[Nick Nisi joins us to dig into the latest trends from this year and how they're impacting his day-to-day coding and Vision Pro wearing. Anthropic's acquisition of Bun, the evolving JavaScript and AI landscape, GitHub's challenges and the Amp/Sourcegraph split. We dive into AI development practices, context management, voice assistants, Home Assistant OS and home automation, the state of the AI browser war, and we close with a prediction from Nick.]]></description>
      <pubDate>Fri, 05 Dec 2025 22:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/120</guid>
    </item>
    <item>
      <title><![CDATA[Why is Zig so cool?]]></title>
      <link>https://changelog.com/news/170</link>
      <description><![CDATA[Nilo Stolte explains why Zig is "a totally new way to write programs", George Mack gives twelve actionable ways to be more creative, Mario Zechner shares his findings on using MCP vs Bash tools, Josh Collinsworth compares creating AI art to medieval alchemy, LibrePods unlocks AirPods features for Android, and our first ever Changelog News Classifieds.]]></description>
      <pubDate>Mon, 17 Nov 2025 20:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/news/170</guid>
    </item>
    <item>
      <title><![CDATA[The great software quality collapse]]></title>
      <link>https://changelog.com/news/165</link>
      <description><![CDATA[Denis Stetskov describes how we've "normalized catastrophe" in the software industry, Meta is officially handing React and React Native over to a foundation, The New Stack reports on GitHub's Azure migration priority, Miguel Grinberg benchmarks Python 3.14, and The Oatmeal's Matthew Inman published his take on AI art.]]></description>
      <pubDate>Mon, 13 Oct 2025 18:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/news/165</guid>
    </item>
  </channel>
</rss>