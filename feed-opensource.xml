<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI-Pulse - Open Source & GitHub</title>
    <link>https://thephoenixagency.github.io/AI-Pulse</link>
    <description>Open Source & GitHub news from AI-Pulse</description>
    <language>en</language>
    <lastBuildDate>Mon, 23 Feb 2026 10:48:51 GMT</lastBuildDate>
    <atom:link href="https://thephoenixagency.github.io/AI-Pulse/feed-opensource.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title><![CDATA[vxcontrol/pentagi]]></title>
      <link>https://github.com/vxcontrol/pentagi</link>
      <description><![CDATA[Fully autonomous AI Agents system capable of performing complex penetration testing tasks PentAGI Penetration testing Artificial General Intelligence Join the Community! Connect with security researchers, AI enthusiasts, and fellow ethical hackers. Get support, share insights, and stay updated with the latest PentAGI developments. ⠀ Table of Contents Overview Features Quick Start API Access Advanced Setup Development Testing LLM Agents Embedding Configuration and Testing Function Testing with ftester Building Credits License Overview PentAGI is an innovative tool for automated security testing that leverages cutting-edge artificial intelligence technologies. The project is designed for information security professionals, researchers, and enthusiasts who need a powerful and flexible solution for conducting penetration tests. You can watch the video PentAGI overview: Features Secure &amp; Isolated. All operations are performed in a sandboxed Docker environment with complete isolation. Fully Autonomous. AI-powered agent that automatically determines and executes penetration testing steps. Professional Pentesting Tools. Built-in suite of 20+ professional security tools including nmap, metasploit, sqlmap, and more. Smart Memory System. Long-term storage of research results and successful approaches for future use. Knowledge Graph Integration. Graphiti-powered knowledge graph using Neo4j for semantic relationship tracking and advanced context understanding. Web Intelligence. Built-in browser via scraper for gathering latest information from web sources. External Search Systems. Integration with advanced search APIs including Tavily, Traversaal, Perplexity, DuckDuckGo, Google Custom Search, and Searxng for comprehensive information gathering. Team of Specialists. Delegation system with specialized AI agents for research, development, and infrastructure tasks. Comprehensive Monitoring. Detailed logging and integration with Grafana/Prometheus for real-time system observation. Detailed Reporting. Generation of thorough vulnerability reports with exploitation guides. Smart Container Management. Automatic Docker image selection based on specific task requirements. Modern Interface. Clean and intuitive web UI for system management and monitoring. Comprehensive APIs. Full-featured REST and GraphQL APIs with Bearer token authentication for automation and integration. Persistent Storage. All commands and outputs are stored in PostgreSQL with pgvector extension. Scalable Architecture. Microservices-based design supporting horizontal scaling. Self-Hosted Solution. Complete control over your deployment and data. Flexible Authentication. Support for various LLM providers (OpenAI, Anthropic, Ollama, AWS Bedrock, Google AI/Gemini, Deep Infra, OpenRouter, DeepSeek), Moonshot and custom configurations. API Token Authentication. Secure Bearer token system for programmatic access to REST and GraphQL APIs. Quick Deployment. Easy setup through Docker Compose with comprehensive environment configuration. Architecture System Context flowchart TB classDef person fill:#08427B,stroke:#073B6F,color:#fff classDef system fill:#1168BD,stroke:#0B4884,color:#fff classDef external fill:#666666,stroke:#0B4884,color:#fff pentester[" Security Engineer (User of the system)"] pentagi[" PentAGI (Autonomous penetration testing system)"] target[" target-system (System under test)"] llm[" llm-provider (OpenAI/Anthropic/Ollama/Bedrock/Gemini/Custom)"] search[" search-systems (Google/DuckDuckGo/Tavily/Traversaal/Perplexity/Searxng)"] langfuse[" langfuse-ui (LLM Observability Dashboard)"] grafana[" grafana (System Monitoring Dashboard)"] pentester --&gt; |Uses HTTPS| pentagi pentester --&gt; |Monitors AI HTTPS| langfuse pentester --&gt; |Monitors System HTTPS| grafana pentagi --&gt; |Tests Various protocols| target pentagi --&gt; |Queries HTTPS| llm pentagi --&gt; |Searches HTTPS| search pentagi --&gt; |Reports HTTPS| langfuse pentagi --&gt; |Reports HTTPS| grafana class pentester person class pentagi system class target,llm,search,langfuse,grafana external linkStyle default stroke:#ffffff,color:#ffffff Container Architecture (click to expand) graph TB subgraph Core Services UI[Frontend UIReact + TypeScript] API[Backend APIGo + GraphQL] DB[(Vector StorePostgreSQL + pgvector)] MQ[Task QueueAsync Processing] Agent[AI AgentsMulti-Agent System] end subgraph Knowledge Graph Graphiti[GraphitiKnowledge Graph API] Neo4j[(Neo4jGraph Database)] end subgraph Monitoring Grafana[GrafanaDashboards] VictoriaMetrics[VictoriaMetricsTime-series DB] Jaeger[JaegerDistributed Tracing] Loki[LokiLog Aggregation] OTEL[OpenTelemetryData Collection] end subgraph Analytics Langfuse[LangfuseLLM Analytics] ClickHouse[ClickHouseAnalytics DB] Redis[RedisCache + Rate Limiter] MinIO[MinIOS3 Storage] end subgraph Security Tools Scraper[Web ScraperIsolated Browser] PenTest[Security Tools20+ Pro ToolsSandboxed Execution] end UI --&gt; |HTTP/WS| API API --&gt; |SQL| DB API --&gt; |Events| MQ MQ --&gt; |Tasks| Agent Agent --&gt; |Commands| PenTest Agent --&gt; |Queries| DB Agent --&gt; |Knowledge| Graphiti Graphiti --&gt; |Graph| Neo4j API --&gt; |Telemetry| OTEL OTEL --&gt; |Metrics| VictoriaMetrics OTEL --&gt; |Traces| Jaeger OTEL --&gt; |Logs| Loki Grafana --&gt; |Query| VictoriaMetrics Grafana --&gt; |Query| Jaeger Grafana --&gt; |Query| Loki API --&gt; |Analytics| Langfuse Langfuse --&gt; |Store| ClickHouse Langfuse --&gt; |Cache| Redis Langfuse --&gt; |Files| MinIO classDef core fill:#f9f,stroke:#333,stroke-width:2px,color:#000 classDef knowledge fill:#ffa,stroke:#333,stroke-width:2px,color:#000 classDef monitoring fill:#bbf,stroke:#333,stroke-width:2px,color:#000 classDef analytics fill:#bfb,stroke:#333,stroke-width:2px,color:#000 classDef tools fill:#fbb,stroke:#333,stroke-width:2px,color:#000 class UI,API,DB,MQ,Agent core class Graphiti,Neo4j knowledge class Grafana,VictoriaMetrics,Jaeger,Loki,OTEL monitoring class Langfuse,ClickHouse,Redis,MinIO analytics class Scraper,PenTest tools Entity Relationship (click to expand) erDiagram Flow ||--o{ Task : contains Task ||--o{ SubTask : contains SubTask ||--o{ Action : contains Action ||--o{ Artifact : produces Action ||--o{ Memory : stores Flow { string id PK string name "Flow name" string description "Flow description" string status "active/completed/failed" json parameters "Flow parameters" timestamp created_at timestamp updated_at } Task { string id PK string flow_id FK string name "Task name" string description "Task description" string status "pending/running/done/failed" json result "Task results" timestamp created_at timestamp updated_at } SubTask { string id PK string task_id FK string name "Subtask name" string description "Subtask description" string status "queued/running/completed/failed" string agent_type "researcher/developer/executor" json context "Agent context" timestamp created_at timestamp updated_at } Action { string id PK string subtask_id FK string type "command/search/analyze/etc" string status "success/failure" json parameters "Action parameters" json result "Action results" timestamp created_at } Artifact { string id PK string action_id FK string type "file/report/log" string path "Storage path" json metadata "Additional info" timestamp created_at } Memory { string id PK string action_id FK string type "observation/conclusion" vector embedding "Vector representation" text content "Memory content" timestamp created_at } Agent Interaction (click to expand) sequenceDiagram participant O as Orchestrator participant R as Researcher participant D as Developer participant E as Executor participant VS as Vector Store participant KB as Knowledge Base Note over O,KB: Flow Initialization O-&gt;&gt;VS: Query similar tasks VS--&gt;&gt;O: Return experiences O-&gt;&gt;KB: Load relevant knowledge KB--&gt;&gt;O: Return context Note over O,R: Research Phase O-&gt;&gt;R: Analyze target R-&gt;&gt;VS: Search similar cases VS--&gt;&gt;R: Return patterns R-&gt;&gt;KB: Query vulnerabilities KB--&gt;&gt;R: Return known issues R-&gt;&gt;VS: Store findings R--&gt;&gt;O: Research results Note over O,D: Planning Phase O-&gt;&gt;D: Plan attack D-&gt;&gt;VS: Query exploits VS--&gt;&gt;D: Return techniques D-&gt;&gt;KB: Load tools info KB--&gt;&gt;D: Return capabilities D--&gt;&gt;O: Attack plan Note over O,E: Execution Phase O-&gt;&gt;E: Execute plan E-&gt;&gt;KB: Load tool guides KB--&gt;&gt;E: Return procedures E-&gt;&gt;VS: Store results E--&gt;&gt;O: Execution status Memory System (click to expand) graph TB subgraph "Long-term Memory" VS[(Vector StoreEmbeddings DB)] KB[Knowledge BaseDomain Expertise] Tools[Tools KnowledgeUsage Patterns] end subgraph "Working Memory" Context[Current ContextTask State] Goals[Active GoalsObjectives] State[System StateResources] end subgraph "Episodic Memory" Actions[Past ActionsCommands History] Results[Action ResultsOutcomes] Patterns[Success PatternsBest Practices] end Context --&gt; |Query| VS VS --&gt; |Retrieve| Context Goals --&gt; |Consult| KB KB --&gt; |Guide| Goals State --&gt; |Record| Actions Actions --&gt; |Learn| Patterns Patterns --&gt; |Store| VS Tools --&gt; |Inform| State Results --&gt; |Update| Tools VS --&gt; |Enhance| KB KB --&gt; |Index| VS classDef ltm fill:#f9f,stroke:#333,stroke-width:2px,color:#000 classDef wm fill:#bbf,stroke:#333,stroke-width:2px,color:#000 classDef em fill:#bfb,stroke:#333,stroke-width:2px,color:#000 class VS,KB,Tools ltm class Context,Goals,State wm class Actions,Results,Patterns em Chain Summarization (click to expand) The chain summarization system manages conversation context growth by selectively summarizing older messages. This is critical for preventing token limits from being exceeded while maintaining conversation coherence. flowchart TD A[Input Chain] --&gt; B{Needs Summarization?} B --&gt;|No| C[Return Original Chain] B --&gt;|Yes| D[Convert to ChainAST] D --&gt; E[Apply Section Summarization] E --&gt; F[Process Oversized Pairs] F --&gt; G[Manage Last Section Size] G --&gt; H[Apply QA Summarization] H --&gt; I[Rebuild Chain with Summaries] I --&gt; J{Is New Chain Smaller?} J --&gt;|Yes| K[Return Optimized Chain] J --&gt;|No| C classDef process fill:#bbf,stroke:#333,stroke-width:2px,color:#000 classDef decision fill:#bfb,stroke:#333,stroke-width:2px,color:#000 classDef output fill:#fbb,stroke:#333,stroke-width:2px,color:#000 class A,D,E,F,G,H,I process class B,J decision class C,K output The algorithm operates on a structured representation of conversation chains (ChainAST) that preserves message types including tool calls and their responses. All summarization operations maintain critical conversation flow while reducing context size. Global Summarizer Configuration Options Parameter Environment Variable Default Description Preserve Last SUMMARIZER_PRESERVE_LAST true Whether to keep all messages in the last section intact Use QA Pairs SUMMARIZER_USE_QA true Whether to use QA pair summarization strategy Summarize Human in QA SUMMARIZER_SUM_MSG_HUMAN_IN_QA false Whether to summarize human messages in QA pairs Last Section Size SUMMARIZER_LAST_SEC_BYTES 51200 Maximum byte size for last section (50KB) Max Body Pair Size SUMMARIZER_MAX_BP_BYTES 16384 Maximum byte size for a single body pair (16KB) Max QA Sections SUMMARIZER_MAX_QA_SECTIONS 10 Maximum QA pair sections to preserve Max QA Size SUMMARIZER_MAX_QA_BYTES 65536 Maximum byte size for QA pair sections (64KB) Keep QA Sections SUMMARIZER_KEEP_QA_SECTIONS 1 Number of recent QA sections to keep without summarization Assistant Summarizer Configuration Options Assistant instances can use customized summarization settings to fine-tune context management behavior: Parameter Environment Variable Default Description Preserve Last ASSISTANT_SUMMARIZER_PRESERVE_LAST true Whether to preserve all messages in the assistant's last section Last Section Size ASSISTANT_SUMMARIZER_LAST_SEC_BYTES 76800 Maximum byte size for assistant's last section (75KB) Max Body Pair Size ASSISTANT_SUMMARIZER_MAX_BP_BYTES 16384 Maximum byte size for a single body pair in assistant context (16KB) Max QA Sections ASSISTANT_SUMMARIZER_MAX_QA_SECTIONS 7 Maximum QA sections to preserve in assistant context Max QA Size ASSISTANT_SUMMARIZER_MAX_QA_BYTES 76800 Maximum byte size for assistant's QA sections (75KB) Keep QA Sections ASSISTANT_SUMMARIZER_KEEP_QA_SECTIONS 3 Number of recent QA sections to preserve without summarization The assistant summarizer configuration provides more memory for context retention compared to the global settings, preserving more recent conversation history while still ensuring efficient token usage. Summarizer Environment Configuration # Default values for global summarizer logic
SUMMARIZER_PRESERVE_LAST=true
SUMMARIZER_USE_QA=true
SUMMARIZER_SUM_MSG_HUMAN_IN_QA=false
SUMMARIZER_LAST_SEC_BYTES=51200
SUMMARIZER_MAX_BP_BYTES=16384
SUMMARIZER_MAX_QA_SECTIONS=10
SUMMARIZER_MAX_QA_BYTES=65536
SUMMARIZER_KEEP_QA_SECTIONS=1 # Default values for assistant summarizer logic
ASSISTANT_SUMMARIZER_PRESERVE_LAST=true
ASSISTANT_SUMMARIZER_LAST_SEC_BYTES=76800
ASSISTANT_SUMMARIZER_MAX_BP_BYTES=16384
ASSISTANT_SUMMARIZER_MAX_QA_SECTIONS=7
ASSISTANT_SUMMARIZER_MAX_QA_BYTES=76800
ASSISTANT_SUMMARIZER_KEEP_QA_SECTIONS=3 The architecture of PentAGI is designed to be modular, scalable, and secure. Here are the key components: Core Services Frontend UI: React-based web interface with TypeScript for type safety Backend API: Go-based REST and GraphQL APIs with Bearer token authentication for programmatic access Vector Store: PostgreSQL with pgvector for semantic search and memory storage Task Queue: Async task processing system for reliable operation AI Agent: Multi-agent system with specialized roles for efficient testing Knowledge Graph Graphiti: Knowledge graph API for semantic relationship tracking and contextual understanding Neo4j: Graph database for storing and querying relationships between entities, actions, and outcomes Automatic capturing of agent responses and tool executions for building comprehensive knowledge base Monitoring Stack OpenTelemetry: Unified observability data collection and correlation Grafana: Real-time visualization and alerting dashboards VictoriaMetrics: High-performance time-series metrics storage Jaeger: End-to-end distributed tracing for debugging Loki: Scalable log aggregation and analysis Analytics Platform Langfuse: Advanced LLM observability and performance analytics ClickHouse: Column-oriented analytics data warehouse Redis: High-speed caching and rate limiting MinIO: S3-compatible object storage for artifacts Security Tools Web Scraper: Isolated browser environment for safe web interaction Pentesting Tools: Comprehensive suite of 20+ professional security tools Sandboxed Execution: All operations run in isolated containers Memory Systems Long-term Memory: Persistent storage of knowledge and experiences Working Memory: Active context and goals for current operations Episodic Memory: Historical actions and success patterns Knowledge Base: Structured domain expertise and tool capabilities Context Management: Intelligently manages growing LLM context windows using chain summarization The system uses Docker containers for isolation and easy deployment, with separate networks for core services, monitoring, and analytics to ensure proper security boundaries. Each component is designed to scale horizontally and can be configured for high availability in production environments. Quick Start System Requirements Docker and Docker Compose Minimum 2 vCPU Minimum 4GB RAM 20GB free disk space Internet access for downloading images and updates Using Installer ( ) PentAGI provides an interactive installer with a terminal-based UI for streamlined configuration and deployment. The installer guides you through system checks, LLM provider setup, search engine configuration, and security hardening. Supported Platforms: Linux: amd64 download | arm64 download Windows: amd64 download macOS: amd64 (Intel) download | arm64 (M-series) download Quick Installation (Linux amd64): # Create installation directory
mkdir -p pentagi &amp;&amp; cd pentagi # Download installer
wget -O installer.zip https://pentagi.com/downloads/linux/amd64/installer-latest.zip # Extract
unzip installer.zip # Run interactive installer
./installer Prerequisites &amp; Permissions: The installer requires appropriate privileges to interact with the Docker API for proper operation. By default, it uses the Docker socket (/var/run/docker.sock) which requires either: Option 1 ( for production): Run the installer as root: sudo ./installer Option 2 (Development environments): Grant your user access to the Docker socket by adding them to the docker group: # Add your user to the docker group
sudo usermod -aG docker $USER # Log out and log back in, or activate the group immediately
newgrp docker # Verify Docker access (should run without sudo)
docker ps Security Note: Adding a user to the docker group grants root-equivalent privileges. Only do this for trusted users in controlled environments. For production deployments, consider using rootless Docker mode or running the installer with sudo. The installer will: System Checks: Verify Docker, network connectivity, and system requirements Environment Setup: Create and configure .env file with optimal defaults Provider Configuration: Set up LLM providers (OpenAI, Anthropic, Gemini, Bedrock, Ollama, Custom) Search Engines: Configure DuckDuckGo, Google, Tavily, Traversaal, Perplexity, Searxng Security Hardening: Generate secure credentials and configure SSL certificates Deployment: Start PentAGI with docker-compose For Production &amp; Enhanced Security: For production deployments or security-sensitive environments, we strongly recommend using a distributed two-node architecture where worker operations are isolated on a separate server. This prevents untrusted code execution and network access issues on your main system. See detailed guide: Worker Node Setup The two-node setup provides: Isolated Execution: Worker containers run on dedicated hardware Network Isolation: Separate network boundaries for penetration testing Security Boundaries: Docker-in-Docker with TLS authentication OOB Attack Support: Dedicated port ranges for out-of-band techniques Manual Installation Create a working directory or clone the repository: mkdir pentagi &amp;&amp; cd pentagi Copy .env.example to .env or download it: curl -o .env https://raw.githubusercontent.com/vxcontrol/pentagi/master/.env.example Touch examples files (example.custom.provider.yml, example.ollama.provider.yml) or download it: curl -o example.custom.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/custom-openai.provider.yml
curl -o example.ollama.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/ollama-llama318b.provider.yml Fill in the required API keys in .env file. # Required: At least one of these LLM providers
OPEN_AI_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GEMINI_API_KEY=your_gemini_key # Optional: AWS Bedrock provider (enterprise-grade models)
BEDROCK_REGION=us-east-1
BEDROCK_ACCESS_KEY_ID=your_aws_access_key
BEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key # Optional: Local LLM provider (zero-cost inference)
OLLAMA_SERVER_URL=http://localhost:11434
OLLAMA_SERVER_MODEL=your_model_name # Optional: Additional search capabilities
DUCKDUCKGO_ENABLED=true
GOOGLE_API_KEY=your_google_key
GOOGLE_CX_KEY=your_google_cx
TAVILY_API_KEY=your_tavily_key
TRAVERSAAL_API_KEY=your_traversaal_key
PERPLEXITY_API_KEY=your_perplexity_key
PERPLEXITY_MODEL=sonar-pro
PERPLEXITY_CONTEXT_SIZE=medium # Searxng meta search engine (aggregates results from multiple sources)
SEARXNG_URL=http://your-searxng-instance:8080
SEARXNG_CATEGORIES=general
SEARXNG_LANGUAGE=
SEARXNG_SAFESEARCH=0
SEARXNG_TIME_RANGE= ## Graphiti knowledge graph settings
GRAPHITI_ENABLED=true
GRAPHITI_TIMEOUT=30
GRAPHITI_URL=http://graphiti:8000
GRAPHITI_MODEL_NAME=gpt-5-mini # Neo4j settings (used by Graphiti stack)
NEO4J_USER=neo4j
NEO4J_DATABASE=neo4j
NEO4J_PASSWORD=devpassword
NEO4J_URI=bolt://neo4j:7687 # Assistant configuration
ASSISTANT_USE_AGENTS=false # Default value for agent usage when creating new assistants Change all security related environment variables in .env file to improve security. Security related environment variables Main Security Settings COOKIE_SIGNING_SALT - Salt for cookie signing, change to random value PUBLIC_URL - Public URL of your server (eg. https://pentagi.example.com) SERVER_SSL_CRT and SERVER_SSL_KEY - Custom paths to your existing SSL certificate and key for HTTPS (these paths should be used in the docker-compose.yml file to mount as volumes) Scraper Access SCRAPER_PUBLIC_URL - Public URL for scraper if you want to use different scraper server for public URLs SCRAPER_PRIVATE_URL - Private URL for scraper (local scraper server in docker-compose.yml file to access it to local URLs) Access Credentials PENTAGI_POSTGRES_USER and PENTAGI_POSTGRES_PASSWORD - PostgreSQL credentials NEO4J_USER and NEO4J_PASSWORD - Neo4j credentials (for Graphiti knowledge graph) Remove all inline from .env file if you want to use it in VSCode or other IDEs as a envFile option: perl -i -pe 's/\s+#.*$//' .env Run the PentAGI stack: curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose.yml
docker compose up -d Visit localhost:8443 to access PentAGI Web UI (default is admin@pentagi.com / admin) [!NOTE] If you caught an error about pentagi-network or observability-network or langfuse-network you need to run docker-compose.yml firstly to create these networks and after that run docker-compose-langfuse.yml, docker-compose-graphiti.yml, and docker-compose-observability.yml to use Langfuse, Graphiti, and Observability services. You have to set at least one Language Model provider (OpenAI, Anthropic, Gemini, AWS Bedrock, or Ollama) to use PentAGI. AWS Bedrock provides enterprise-grade access to multiple foundation models from leading AI companies, while Ollama provides zero-cost local inference if you have sufficient computational resources. Additional API keys for search engines are optional but for better results. LLM_SERVER_* environment variables are experimental feature and will be changed in the future. Right now you can use them to specify custom LLM server URL and one model for all agent types. PROXY_URL is a global proxy URL for all LLM providers and external search systems. You can use it for isolation from external networks. The docker-compose.yml file runs the PentAGI service as root user because it needs access to docker.sock for container management. If you're using TCP/IP network connection to Docker instead of socket file, you can remove root privileges and use the default pentagi user for better security. Assistant Configuration PentAGI allows you to configure default behavior for assistants: Variable Default Description ASSISTANT_USE_AGENTS false Controls the default value for agent usage when creating new assistants The ASSISTANT_USE_AGENTS setting affects the initial state of the "Use Agents" toggle when creating a new assistant in the UI: false (default): New assistants are created with agent delegation disabled by default true: New assistants are created with agent delegation enabled by default Note that users can always override this setting by toggling the "Use Agents" button in the UI when creating or editing an assistant. This environment variable only controls the initial default state. API Access PentAGI provides comprehensive programmatic access through both REST and GraphQL APIs, allowing you to integrate penetration testing workflows into your automation pipelines, CI/CD processes, and custom applications. Generating API Tokens API tokens are managed through the PentAGI web interface: Navigate to Settings → API Tokens in the web UI Click Create Token to generate a new API token Configure token properties: Name (optional): A descriptive name for the token Expiration Date: When the token will expire (minimum 1 minute, maximum 3 years) Click Create and copy the token immediately - it will only be shown once for security reasons Use the token as a Bearer token in your API requests Each token is associated with your user account and inherits your role's permissions. Using API Tokens Include the API token in the Authorization header of your HTTP requests: # GraphQL API example
curl -X POST https://your-pentagi-instance:8443/api/v1/graphql \ -H "Authorization: Bearer YOUR_API_TOKEN" \ -H "Content-Type: application/json" \ -d '{"query": "{ flows { id title status } }"}' # REST API example
curl https://your-pentagi-instance:8443/api/v1/flows \ -H "Authorization: Bearer YOUR_API_TOKEN" API Exploration and Testing PentAGI provides interactive documentation for exploring and testing API endpoints: GraphQL Playground Access the GraphQL Playground at https://your-pentagi-instance:8443/api/v1/graphql/playground Click the HTTP Headers tab at the bottom Add your authorization header: { "Authorization": "Bearer YOUR_API_TOKEN"
} Explore the schema, run queries, and test mutations interactively Swagger UI Access the REST API documentation at https://your-pentagi-instance:8443/api/v1/swagger/index.html Click the Authorize button Enter your token in the format: Bearer YOUR_API_TOKEN Click Authorize to apply Test endpoints directly from the Swagger UI Generating API Clients You can generate type-safe API clients for your preferred programming language using the schema files included with PentAGI: GraphQL Clients The GraphQL schema is available at: Web UI: Navigate to Settings to download schema.graphqls Direct file: backend/pkg/graph/schema.graphqls in the repository Generate clients using tools like: GraphQL Code Generator (JavaScript/TypeScript): https://the-guild.dev/graphql/codegen genqlient (Go): https://github.com/Khan/genqlient Apollo iOS (Swift): https://www.apollographql.com/docs/ios REST API Clients The OpenAPI specification is available at: Swagger JSON: https://your-pentagi-instance:8443/api/v1/swagger/doc.json Swagger YAML: Available in backend/pkg/server/docs/swagger.yaml Generate clients using: OpenAPI Generator: https://openapi-generator.tech openapi-generator-cli generate \ -i https://your-pentagi-instance:8443/api/v1/swagger/doc.json \ -g python \ -o ./pentagi-client Swagger Codegen: https://github.com/swagger-api/swagger-codegen swagger-codegen generate \ -i https://your-pentagi-instance:8443/api/v1/swagger/doc.json \ -l typescript-axios \ -o ./pentagi-client swagger-typescript-api (TypeScript): https://github.com/acacode/swagger-typescript-api npx swagger-typescript-api \ -p https://your-pentagi-instance:8443/api/v1/swagger/doc.json \ -o ./src/api \ -n pentagi-api.ts API Usage Examples Creating a New Flow (GraphQL) mutation CreateFlow { createFlow( modelProvider: "openai" input: "Test the security of https://example.com" ) { id title status createdAt }
} Listing Flows (REST API) curl https://your-pentagi-instance:8443/api/v1/flows \ -H "Authorization: Bearer YOUR_API_TOKEN" \ | jq '.flows[] | {id, title, status}' Python Client Example import requests class PentAGIClient: def __init__(self, base_url, api_token): self.base_url = base_url self.headers = { "Authorization": f"Bearer {api_token}", "Content-Type": "application/json" } def create_flow(self, provider, target): query = """ mutation CreateFlow($provider: String!, $input: String!) { createFlow(modelProvider: $provider, input: $input) { id title status } } """ response = requests.post( f"{self.base_url}/api/v1/graphql", json={ "query": query, "variables": { "provider": provider, "input": target } }, headers=self.headers ) return response.json() def get_flows(self): response = requests.get( f"{self.base_url}/api/v1/flows", headers=self.headers ) return response.json() # Usage
client = PentAGIClient( "https://your-pentagi-instance:8443", "your_api_token_here"
) # Create a new flow
flow = client.create_flow("openai", "Scan https://example.com for vulnerabilities")
print(f"Created flow: {flow}") # List all flows
flows = client.get_flows()
print(f"Total flows: {len(flows['flows'])}") TypeScript Client Example import axios, { AxiosInstance } from 'axios'; interface Flow { id: string; title: string; status: string; createdAt: string;
} class PentAGIClient { private client: AxiosInstance; constructor(baseURL: string, apiToken: string) { this.client = axios.create({ baseURL: `${baseURL}/api/v1`, headers: { 'Authorization': `Bearer ${apiToken}`, 'Content-Type': 'application/json', }, }); } async createFlow(provider: string, input: string): Promise { const query = ` mutation CreateFlow($provider: String!, $input: String!) { createFlow(modelProvider: $provider, input: $input) { id title status createdAt } } `; const response = await this.client.post('/graphql', { query, variables: { provider, input }, }); return response.data.data.createFlow; } async getFlows(): Promise { const response = await this.client.get('/flows'); return response.data.flows; } async getFlow(flowId: string): Promise { const response = await this.client.get(`/flows/${flowId}`); return response.data; }
} // Usage
const client = new PentAGIClient( 'https://your-pentagi-instance:8443', 'your_api_token_here'
); // Create a new flow
const flow = await client.createFlow( 'openai', 'Perform penetration test on https://example.com'
);
console.log('Created flow:', flow); // List all flows
const flows = await client.getFlows();
console.log(`Total flows: ${flows.length}`); Security Best Practices When working with API tokens: Never commit tokens to version control - use environment variables or secrets management Rotate tokens regularly - set appropriate expiration dates and create new tokens periodically Use separate tokens for different applications - makes it easier to revoke access if needed Monitor token usage - review API token activity in the Settings page Revoke unused tokens - disable or delete tokens that are no longer needed Use HTTPS only - never send API tokens over unencrypted connections Token Management View tokens: See all your active tokens in Settings → API Tokens Edit tokens: Update token names or revoke tokens Delete tokens: Permanently remove tokens (this action cannot be undone) Token ID: Each token has a unique ID that can be copied for reference The token list shows: Token name (if provided) Token ID (unique identifier) Status (active/revoked/expired) Creation date Expiration date Custom LLM Provider Configuration When using custom LLM providers with the LLM_SERVER_* variables, you can fine-tune the reasoning format used in requests: Variable Default Description LLM_SERVER_URL Base URL for the custom LLM API endpoint LLM_SERVER_KEY API key for the custom LLM provider LLM_SERVER_MODEL Default model to use (can be overridden in provider config) LLM_SERVER_CONFIG_PATH Path to the YAML configuration file for agent-specific models LLM_SERVER_PROVIDER Provider name prefix for model names (e.g., openrouter, deepseek for LiteLLM proxy) LLM_SERVER_LEGACY_REASONING false Controls reasoning format in API requests LLM_SERVER_PRESERVE_REASONING false Preserve reasoning content in multi-turn conversations (required by some providers) The LLM_SERVER_PROVIDER setting is particularly useful when using LiteLLM proxy, which adds a provider prefix to model names. For example, when connecting to Moonshot API through LiteLLM, models like kimi-2.5 become moonshot/kimi-2.5. By setting LLM_SERVER_PROVIDER=moonshot, you can use the same provider configuration file for both direct API access and LiteLLM proxy access without modifications. The LLM_SERVER_LEGACY_REASONING setting affects how reasoning parameters are sent to the LLM: false (default): Uses modern format where reasoning is sent as a structured object with max_tokens parameter true: Uses legacy format with string-based reasoning_effort parameter This setting is important when working with different LLM providers as they may expect different reasoning formats in their API requests. If you encounter reasoning-related errors with custom providers, try changing this setting. The LLM_SERVER_PRESERVE_REASONING setting controls whether reasoning content is preserved in multi-turn conversations: false (default): Reasoning content is not preserved in conversation history true: Reasoning content is preserved and sent in subsequent API calls This setting is required by some LLM providers (e.g., Moonshot) that return errors like "thinking is enabled but reasoning_content is missing in assistant tool call message" when reasoning content is not included in multi-turn conversations. Enable this setting if your provider requires reasoning content to be preserved. Local LLM Provider Configuration PentAGI supports Ollama for local LLM inference, providing zero-cost operation and enhanced privacy: Variable Default Description OLLAMA_SERVER_URL URL of your Ollama server OLLAMA_SERVER_MODEL llama3.1:8b-instruct-q8_0 Default model for inference OLLAMA_SERVER_CONFIG_PATH Path to custom agent configuration file OLLAMA_SERVER_PULL_MODELS_TIMEOUT 600 Timeout for model downloads (seconds) OLLAMA_SERVER_PULL_MODELS_ENABLED false Auto-download models on startup OLLAMA_SERVER_LOAD_MODELS_ENABLED false Query server for available models Configuration examples: # Basic Ollama setup with default model
OLLAMA_SERVER_URL=http://localhost:11434
OLLAMA_SERVER_MODEL=llama3.1:8b-instruct-q8_0 # Production setup with auto-pull and model discovery
OLLAMA_SERVER_URL=http://ollama-server:11434
OLLAMA_SERVER_PULL_MODELS_ENABLED=true
OLLAMA_SERVER_PULL_MODELS_TIMEOUT=900
OLLAMA_SERVER_LOAD_MODELS_ENABLED=true # Custom configuration with agent-specific models
OLLAMA_SERVER_CONFIG_PATH=/path/to/ollama-config.yml # Default configuration file inside docker container
OLLAMA_SERVER_CONFIG_PATH=/opt/pentagi/conf/ollama-llama318b.provider.yml Performance Considerations: Model Discovery (OLLAMA_SERVER_LOAD_MODELS_ENABLED=true): Adds 1-2s startup latency querying Ollama API Auto-pull (OLLAMA_SERVER_PULL_MODELS_ENABLED=true): First startup may take several minutes downloading models Pull timeout (OLLAMA_SERVER_PULL_MODELS_TIMEOUT=900): 15 minutes in seconds Static Config: Disable both flags and specify models in config file for fastest startup Creating Custom Ollama Models with Extended Context PentAGI requires models with larger context windows than the default Ollama configurations. You need to create custom models with increased num_ctx parameter through Modelfiles. While typical agent workflows consume around 64K tokens, PentAGI uses 110K context size for safety margin and handling complex penetration testing scenarios. Important: The num_ctx parameter can only be set during model creation via Modelfile - it cannot be changed after model creation or overridden at runtime. Example: Qwen3 32B FP16 with Extended Context Create a Modelfile named Modelfile_qwen3_32b_fp16_tc: FROM qwen3:32b-fp16
PARAMETER num_ctx 110000
PARAMETER temperature 0.3
PARAMETER top_p 0.8
PARAMETER min_p 0.0
PARAMETER top_k 20
PARAMETER repeat_penalty 1.1 Build the custom model: ollama create qwen3:32b-fp16-tc -f Modelfile_qwen3_32b_fp16_tc Example: QwQ 32B FP16 with Extended Context Create a Modelfile named Modelfile_qwq_32b_fp16_tc: FROM qwq:32b-fp16
PARAMETER num_ctx 110000
PARAMETER temperature 0.2
PARAMETER top_p 0.7
PARAMETER min_p 0.0
PARAMETER top_k 40
PARAMETER repeat_penalty 1.2 Build the custom model: ollama create qwq:32b-fp16-tc -f Modelfile_qwq_32b_fp16_tc Note: The QwQ 32B FP16 model requires approximately 71.3 GB VRAM for inference. Ensure your system has sufficient GPU memory before attempting to use this model. These custom models are referenced in the pre-built provider configuration files (ollama-qwen332b-fp16-tc.provider.yml and ollama-qwq32b-fp16-tc.provider.yml) that are included in the Docker image at /opt/pentagi/conf/. OpenAI Provider Configuration PentAGI supports OpenAI's advanced language models, including the latest reasoning-capable o-series models designed for complex analytical tasks: Variable Default Description OPEN_AI_KEY API key for OpenAI services OPEN_AI_SERVER_URL https://api.openai.com/v1 OpenAI API endpoint Configuration examples: # Basic OpenAI setup
OPEN_AI_KEY=your_openai_api_key
OPEN_AI_SERVER_URL=https://api.openai.com/v1 # Using with proxy for enhanced security
OPEN_AI_KEY=your_openai_api_key
PROXY_URL=http://your-proxy:8080 The OpenAI provider offers cutting-edge capabilities including: Reasoning Models: Advanced o-series models (o1, o3, o4-mini) with step-by-step analytical thinking Latest GPT-4.1 Series: Flagship models optimized for complex security research and exploit development Cost-Effective Options: From nano models for high-volume scanning to powerful reasoning models for deep analysis Versatile Performance: Fast, intelligent models perfect for multi-step security analysis and penetration testing Proven Reliability: Industry-leading models with consistent performance across diverse security scenarios The system automatically selects appropriate OpenAI models based on task complexity, optimizing for both performance and cost-effectiveness. Anthropic Provider Configuration PentAGI integrates with Anthropic's Claude models, known for their exceptional safety, reasoning capabilities, and sophisticated understanding of complex security contexts: Variable Default Description ANTHROPIC_API_KEY API key for Anthropic services ANTHROPIC_SERVER_URL https://api.anthropic.com/v1 Anthropic API endpoint Configuration examples: # Basic Anthropic setup
ANTHROPIC_API_KEY=your_anthropic_api_key
ANTHROPIC_SERVER_URL=https://api.anthropic.com/v1 # Using with proxy for secure environments
ANTHROPIC_API_KEY=your_anthropic_api_key
PROXY_URL=http://your-proxy:8080 The Anthropic provider delivers superior capabilities including: Advanced Reasoning: Claude 4 series with exceptional reasoning for sophisticated penetration testing Extended Thinking: Claude 3.7 with step-by-step thinking capabilities for methodical security research High-Speed Performance: Claude 3.5 Haiku for blazing-fast vulnerability scans and real-time monitoring Comprehensive Analysis: Claude Sonnet models for complex security analysis and threat hunting Safety-First Design: Built-in safety mechanisms ensuring responsible security testing practices The system leverages Claude's advanced understanding of security contexts to provide thorough and responsible penetration testing guidance. Google AI (Gemini) Provider Configuration PentAGI supports Google's Gemini models through the Google AI API, offering state-of-the-art reasoning capabilities and multimodal features: Variable Default Description GEMINI_API_KEY API key for Google AI services GEMINI_SERVER_URL https://generativelanguage.googleapis.com Google AI API endpoint Configuration examples: # Basic Gemini setup
GEMINI_API_KEY=your_gemini_api_key
GEMINI_SERVER_URL=https://generativelanguage.googleapis.com # Using with proxy
GEMINI_API_KEY=your_gemini_api_key
PROXY_URL=http://your-proxy:8080 The Gemini provider offers advanced features including: Thinking Capabilities: Advanced reasoning models (Gemini 2.5 series) with step-by-step analysis Multimodal Support: Text and image processing for comprehensive security assessments Large Context Windows: Up to 2M tokens for analyzing extensive codebases and documentation Cost-Effective Options: From high-performance pro models to economical flash variants Security-Focused Models: Specialized configurations optimized for penetration testing workflows The system automatically selects appropriate Gemini models based on agent requirements, balancing performance, capabilities, and cost-effectiveness. AWS Bedrock Provider Configuration PentAGI integrates with Amazon Bedrock, offering access to a wide range of foundation models from leading AI companies including Anthropic, AI21, Cohere, Meta, and Amazon's own models: Variable Default Description BEDROCK_REGION us-east-1 AWS region for Bedrock service BEDROCK_ACCESS_KEY_ID AWS access key ID for authentication BEDROCK_SECRET_ACCESS_KEY AWS secret access key for authentication BEDROCK_SESSION_TOKEN AWS session token as alternative way for authentication BEDROCK_SERVER_URL Optional custom Bedrock endpoint URL Configuration examples: # Basic AWS Bedrock setup with credentials
BEDROCK_REGION=us-east-1
BEDROCK_ACCESS_KEY_ID=your_aws_access_key
BEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key # Using with proxy for enhanced security
BEDROCK_REGION=us-east-1
BEDROCK_ACCESS_KEY_ID=your_aws_access_key
BEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key
PROXY_URL=http://your-proxy:8080 # Using custom endpoint (for VPC endpoints or testing)
BEDROCK_REGION=us-east-1
BEDROCK_ACCESS_KEY_ID=your_aws_access_key
BEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/vxcontrol/pentagi</guid>
    </item>
    <item>
      <title><![CDATA[yt-dlp/yt-dlp]]></title>
      <link>https://github.com/yt-dlp/yt-dlp</link>
      <description><![CDATA[A feature-rich command-line audio/video downloader yt-dlp is a feature-rich command-line audio/video downloader with support for thousands of sites. The project is a fork of youtube-dl based on the now inactive youtube-dlc. INSTALLATION Detailed instructions Release Files Update Dependencies Compile USAGE AND OPTIONS General Options Network Options Geo-restriction Video Selection Download Options Filesystem Options Thumbnail Options Internet Shortcut Options Verbosity and Simulation Options Workarounds Video Format Options Subtitle Options Authentication Options Post-processing Options SponsorBlock Options Extractor Options Preset Aliases CONFIGURATION Configuration file encoding Authentication with netrc Notes about environment variables OUTPUT TEMPLATE Output template examples FORMAT SELECTION Filtering Formats Sorting Formats Format Selection examples MODIFYING METADATA Modifying metadata examples EXTRACTOR ARGUMENTS PLUGINS Installing Plugins Developing Plugins EMBEDDING YT-DLP Embedding examples CHANGES FROM YOUTUBE-DL New features Differences in default behavior Deprecated options CONTRIBUTING Opening an Issue Developer Instructions WIKI FAQ INSTALLATION You can install yt-dlp using the binaries, pip or one using a third-party package manager. See the wiki for detailed instructions RELEASE FILES File Description yt-dlp Platform-independent zipimport binary. Needs Python ( for Linux/BSD) yt-dlp.exe Windows (Win8+) standalone x64 binary ( for Windows) yt-dlp_macos Universal MacOS (10.15+) standalone executable ( for MacOS) Alternatives File Description yt-dlp_linux Linux (glibc 2.17+) standalone x86_64 binary yt-dlp_linux.zip Unpackaged Linux (glibc 2.17+) x86_64 executable (no auto-update) yt-dlp_linux_aarch64 Linux (glibc 2.17+) standalone aarch64 binary yt-dlp_linux_aarch64.zip Unpackaged Linux (glibc 2.17+) aarch64 executable (no auto-update) yt-dlp_linux_armv7l.zip Unpackaged Linux (glibc 2.31+) armv7l executable (no auto-update) yt-dlp_musllinux Linux (musl 1.2+) standalone x86_64 binary yt-dlp_musllinux.zip Unpackaged Linux (musl 1.2+) x86_64 executable (no auto-update) yt-dlp_musllinux_aarch64 Linux (musl 1.2+) standalone aarch64 binary yt-dlp_musllinux_aarch64.zip Unpackaged Linux (musl 1.2+) aarch64 executable (no auto-update) yt-dlp_x86.exe Windows (Win8+) standalone x86 (32-bit) binary yt-dlp_win_x86.zip Unpackaged Windows (Win8+) x86 (32-bit) executable (no auto-update) yt-dlp_arm64.exe Windows (Win10+) standalone ARM64 binary yt-dlp_win_arm64.zip Unpackaged Windows (Win10+) ARM64 executable (no auto-update) yt-dlp_win.zip Unpackaged Windows (Win8+) x64 executable (no auto-update) yt-dlp_macos.zip Unpackaged MacOS (10.15+) executable (no auto-update) Misc File Description yt-dlp.tar.gz Source tarball SHA2-512SUMS GNU-style SHA512 sums SHA2-512SUMS.sig GPG signature file for SHA512 sums SHA2-256SUMS GNU-style SHA256 sums SHA2-256SUMS.sig GPG signature file for SHA256 sums The public key that can be used to verify the GPG signatures is available here Example usage: curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS Licensing While yt-dlp is licensed under the Unlicense, many of the release files contain code from other projects with different licenses. Most notably, the PyInstaller-bundled executables include GPLv3+ licensed code, and as such the combined work is licensed under GPLv3+. The zipimport Unix executable (yt-dlp) contains ISC licensed code from meriyah and MIT licensed code from astring. See THIRD_PARTY_LICENSES.txt for more details. The git repository, the source tarball (yt-dlp.tar.gz), the PyPI source distribution and the PyPI built distribution (wheel) only contain code licensed under the Unlicense. Note: The manpages, shell completion (autocomplete) files etc. are available inside the source tarball UPDATE You can use yt-dlp -U to update if you are using the release binaries If you installed with pip, simply re-run the same command that was used to install the program For other third-party package managers, see the wiki or refer to their documentation There are currently three release channels for binaries: stable, nightly and master. stable is the default channel, and many of its changes have been tested by users of the nightly and master channels. The nightly channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project's new patches and changes. This is the channel for regular users of yt-dlp. The nightly releases are available from yt-dlp/yt-dlp-nightly-builds or as development releases of the yt-dlp PyPI package (which can be installed with pip's --pre flag). The master channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from yt-dlp/yt-dlp-master-builds. When using --update/-U, a release binary will only update to its current channel. --update-to CHANNEL can be used to switch to a different channel when a newer version is available. --update-to [CHANNEL@]TAG can also be used to upgrade or downgrade to specific tags from a channel. You may also use --update-to (/) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories. Example usage: yt-dlp --update-to master switch to the master channel and update to its latest release yt-dlp --update-to stable@2023.07.06 upgrade/downgrade to release to stable channel tag 2023.07.06 yt-dlp --update-to 2023.10.07 upgrade/downgrade to tag 2023.10.07 if it exists on the current channel yt-dlp --update-to example/yt-dlp@2023.09.24 upgrade/downgrade to the release from the example/yt-dlp repository, tag 2023.09.24 Important: Any user experiencing an issue with the stable release should install or update to the nightly release before submitting a bug report: # To update to nightly from stable executable/binary:
yt-dlp --update-to nightly # To install nightly with pip:
python -m pip install -U --pre "yt-dlp[default]" When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version. You can suppress this warning by adding --no-update to your command or configuration file. DEPENDENCIES Python versions 3.10+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly. On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt; While all the other dependencies are optional, ffmpeg, ffprobe, yt-dlp-ejs and a supported JavaScript runtime/engine are highly Strongly ffmpeg and ffprobe - Required for merging separate video and audio files, as well as for various post-processing tasks. License depends on the build There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide custom builds with patches for some of these issues at yt-dlp/FFmpeg-Builds. See the readme for details on the specific issues solved by these builds Important: What you need is ffmpeg binary, NOT the Python package of the same name yt-dlp-ejs - Required for full YouTube support. Licensed under Unlicense, bundles MIT and ISC components. A JavaScript runtime/engine like deno ( ), node.js, bun, or QuickJS is also required to run yt-dlp-ejs. See the wiki. Networking certifi* - Provides Mozilla's root certificate bundle. Licensed under MPLv2 brotli* or brotlicffi - Brotli content encoding support. Both licensed under MIT 1 2 websockets* - For downloading over websocket. Licensed under BSD-3-Clause requests* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under Apache-2.0 Impersonation The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting. curl_cffi ( ) - Python binding for curl-impersonate. Provides impersonation targets for Chrome, Edge and Safari. Licensed under MIT Can be installed with the curl-cffi extra, e.g. pip install "yt-dlp[default,curl-cffi]" Currently included in most builds except yt-dlp (Unix zipimport binary), yt-dlp_x86 (Windows 32-bit) and yt-dlp_musllinux_aarch64 Metadata mutagen* - For --embed-thumbnail in certain formats. Licensed under GPLv2+ AtomicParsley - For --embed-thumbnail in mp4/m4a files when mutagen/ffmpeg cannot. Licensed under GPLv2+ xattr, pyxattr or setfattr - For writing xattr metadata (--xattrs) on Mac and BSD. Licensed under MIT, LGPL2.1 and GPLv2+ respectively Misc pycryptodomex* - For decrypting AES-128 HLS streams and various other data. Licensed under BSD-2-Clause phantomjs - Used in some extractors where JavaScript needs to be run. No longer used for YouTube. To be deprecated in the near future. Licensed under BSD-3-Clause secretstorage* - For --cookies-from-browser to access the Gnome keyring while decrypting cookies of Chromium-based browsers on Linux. Licensed under BSD-3-Clause Any external downloader that you want to use with --downloader Deprecated rtmpdump - For downloading rtmp streams. ffmpeg can be used instead with --downloader ffmpeg. Licensed under GPLv2+ mplayer or mpv - For downloading rstp/mms streams. ffmpeg can be used instead with --downloader ffmpeg. Licensed under GPLv2+ To use or redistribute the dependencies, you must agree to their respective licensing terms. The standalone release binaries are built with the Python interpreter and the packages marked with * included. If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the --verbose output COMPILE Standalone PyInstaller Builds To build the standalone executable, you must have Python and pyinstaller (plus any of yt-dlp's optional dependencies if needed). The executable will be built for the same CPU architecture as the Python used. You can run the following commands: python devscripts/install_deps.py --include-extra pyinstaller
python devscripts/make_lazy_extractors.py
python -m bundle.pyinstaller On some systems, you may need to use py or python3 instead of python. python -m bundle.pyinstaller accepts any arguments that can be passed to pyinstaller, such as --onefile/-F or --onedir/-D, which is further documented here. Note: Pyinstaller versions below 4.4 do not support Python installed from the Windows store without using a virtual environment. Important: Running pyinstaller directly instead of using python -m bundle.pyinstaller is not officially supported. This may or may not work correctly. Platform-independent Binary (UNIX) You will need the build tools python (3.10+), zip, make (GNU), pandoc* and pytest*. After installing these, simply run make. You can also run make yt-dlp instead to compile only the binary without updating any of the additional files. (The build tools marked with * are not needed for this) Related scripts devscripts/install_deps.py - Install dependencies for yt-dlp. devscripts/update-version.py - Update the version number based on the current date. devscripts/set-variant.py - Set the build variant of the executable. devscripts/make_changelog.py - Create a markdown changelog using short commit messages and update CONTRIBUTORS file. devscripts/make_lazy_extractors.py - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable YTDLP_NO_LAZY_EXTRACTORS to something nonempty to forcefully disable lazy extractor loading. Note: See their --help for more info. Forking the project If you fork the project on GitHub, you can run your fork's build workflow to automatically build the selected version(s) as artifacts. Alternatively, you can run the release workflow or enable the nightly workflow to create full (pre-)releases. USAGE AND OPTIONS yt-dlp [OPTIONS] [--] URL [URL...] Tip: Use CTRL+F (or Command+F) to search by keywords General Options: -h, --help Print this help text and exit
--version Print program version and exit
-U, --update Update this program to the latest version
--no-update Do not check for updates (default)
--update-to [CHANNEL]@[TAG] Upgrade/downgrade to a specific version. CHANNEL can be a repository as well. CHANNEL and TAG default to "stable" and "latest" respectively if omitted; See "UPDATE" for details. Supported channels: stable, nightly, master
-i, --ignore-errors Ignore download and postprocessing errors. The download will be considered successful even if the postprocessing fails
--no-abort-on-error Continue with next video on download errors; e.g. to skip unavailable videos in a playlist (default)
--abort-on-error Abort downloading of further videos if an error occurs (Alias: --no-ignore-errors)
--list-extractors List all supported extractors and exit
--extractor-descriptions Output descriptions of all supported extractors and exit
--use-extractors NAMES Extractor names to use separated by commas. You can also use regexes, "all", "default" and "end" (end URL matching); e.g. --ies "holodex.*,end,youtube". Prefix the name with a "-" to exclude it, e.g. --ies default,-generic. Use --list-extractors for a list of extractor names. (Alias: --ies)
--default-search PREFIX Use this prefix for unqualified URLs. E.g. "gvsearch2:python" downloads two videos from google videos for the search term "python". Use the value "auto" to let yt-dlp guess ("auto_warning" to emit a warning when guessing). "error" just throws an error. The default value "fixup_error" repairs broken URLs, but emits an error if this is not possible instead of searching
--ignore-config Don't load any more configuration files except those given to --config-locations. For backward compatibility, if this option is found inside the system configuration file, the user configuration is not loaded. (Alias: --no-config)
--no-config-locations Do not load any custom configuration files (default). When given inside a configuration file, ignore all previous --config-locations defined in the current file
--config-locations PATH Location of the main configuration file; either the path to the config or its containing directory ("-" for stdin). Can be used multiple times and inside other configuration files
--plugin-dirs DIR Path to an additional directory to search for plugins. This option can be used multiple times to add multiple directories. Use "default" to search the default plugin directories (default)
--no-plugin-dirs Clear plugin directories to search, including defaults and those provided by previous --plugin-dirs
--js-runtimes RUNTIME[:PATH] Additional JavaScript runtime to enable, with an optional location for the runtime (either the path to the binary or its containing directory). This option can be used multiple times to enable multiple runtimes. Supported runtimes are (in order of priority, from highest to lowest): deno, node, quickjs, bun. Only "deno" is enabled by default. The highest priority runtime that is both enabled and available will be used. In order to use a lower priority runtime when "deno" is available, --no-js- runtimes needs to be passed before enabling other runtimes
--no-js-runtimes Clear JavaScript runtimes to enable, including defaults and those provided by previous --js-runtimes
--remote-components COMPONENT Remote components to allow yt-dlp to fetch when required. This option is currently not needed if you are using an official executable or have the requisite version of the yt-dlp-ejs package installed. You can use this option multiple times to allow multiple components. Supported values: ejs:npm (external JavaScript components from npm), ejs:github (external JavaScript components from yt-dlp-ejs GitHub). By default, no remote components are allowed
--no-remote-components Disallow fetching of all remote components, including any previously allowed by --remote-components or defaults.
--flat-playlist Do not extract a playlist's URL result entries; some entry metadata may be missing and downloading may be bypassed
--no-flat-playlist Fully extract the videos of a playlist (default)
--live-from-start Download livestreams from the start. Currently experimental and only supported for YouTube, Twitch, and TVer
--no-live-from-start Download livestreams from the current time (default)
--wait-for-video MIN[-MAX] Wait for scheduled streams to become available. Pass the minimum number of seconds (or range) to wait between retries
--no-wait-for-video Do not wait for scheduled streams (default)
--mark-watched Mark videos watched (even with --simulate)
--no-mark-watched Do not mark videos watched (default)
--color [STREAM:]POLICY Whether to emit color codes in output, optionally prefixed by the STREAM (stdout or stderr) to apply the setting to. Can be one of "always", "auto" (default), "never", or "no_color" (use non color terminal sequences). Use "auto-tty" or "no_color-tty" to decide based on terminal support only. Can be used multiple times
--compat-options OPTS Options that can help keep compatibility with youtube-dl or youtube-dlc configurations by reverting some of the changes made in yt-dlp. See "Differences in default behavior" for details
--alias ALIASES OPTIONS Create aliases for an option string. Unless an alias starts with a dash "-", it is prefixed with "--". Arguments are parsed according to the Python string formatting mini-language. E.g. --alias get-audio,-X "-S aext:{0},abr -x --audio-format {0}" creates options "--get-audio" and "-X" that takes an argument (ARG0) and expands to "-S aext:ARG0,abr -x --audio-format ARG0". All defined aliases are listed in the --help output. Alias options can trigger more aliases; so be careful to avoid defining recursive options. As a safety measure, each alias may be triggered a maximum of 100 times. This option can be used multiple times
-t, --preset-alias PRESET Applies a predefined set of options. e.g. --preset-alias mp3. The following presets are available: mp3, aac, mp4, mkv, sleep. See the "Preset Aliases" section at the end for more info. This option can be used multiple times Network Options: --proxy URL Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme, e.g. socks5://user:pass@127.0.0.1:1080/. Pass in an empty string (--proxy "") for direct connection
--socket-timeout SECONDS Time to wait before giving up, in seconds
--source-address IP Client-side IP address to bind to
--impersonate CLIENT[:OS] Client to impersonate for requests. E.g. chrome, chrome-110, chrome:windows-10. Pass --impersonate="" to impersonate any client. Note that forcing impersonation for all requests may have a detrimental impact on download speed and stability
--list-impersonate-targets List available clients to impersonate.
-4, --force-ipv4 Make all connections via IPv4
-6, --force-ipv6 Make all connections via IPv6
--enable-file-urls Enable file:// URLs. This is disabled by default for security reasons. Geo-restriction: --geo-verification-proxy URL Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading
--xff VALUE How to fake X-Forwarded-For HTTP header to try bypassing geographic restriction. One of "default" (only when known to be useful), "never", an IP block in CIDR notation, or a two-letter ISO 3166-2 country code Video Selection: -I, --playlist-items ITEM_SPEC Comma-separated playlist_index of the items to download. You can specify a range using "[START]:[STOP][:STEP]". For backward compatibility, START-STOP is also supported. Use negative indices to count from the right and negative STEP to download in reverse order. E.g. "-I 1:3,7,-5::2" used on a playlist of size 15 will download the items at index 1,2,3,7,11,13,15
--min-filesize SIZE Abort download if filesize is smaller than SIZE, e.g. 50k or 44.6M
--max-filesize SIZE Abort download if filesize is larger than SIZE, e.g. 50k or 44.6M
--date DATE Download only videos uploaded on this date. The date can be "YYYYMMDD" or in the format [now|today|yesterday][-N[day|week|month|year]]. E.g. "--date today-2weeks" downloads only videos uploaded on the same day two weeks ago
--datebefore DATE Download only videos uploaded on or before this date. The date formats accepted are the same as --date
--dateafter DATE Download only videos uploaded on or after this date. The date formats accepted are the same as --date
--match-filters FILTER Generic video filter. Any "OUTPUT TEMPLATE" field can be compared with a number or a string using the operators defined in "Filtering Formats". You can also simply specify a field to match if the field is present, use "!field" to check if the field is not present, and "&amp;" to check multiple conditions. Use a "\" to escape "&amp;" or quotes if needed. If used multiple times, the filter matches if at least one of the conditions is met. E.g. --match-filters !is_live --match-filters "like_count&gt;?100 &amp; description~='(?i)\bcats \&amp; dogs\b'" matches only videos that are not live OR those that have a like count more than 100 (or the like field is not available) and also has a description that contains the phrase "cats &amp; dogs" (caseless). Use "--match-filters -" to interactively ask whether to download each video
--no-match-filters Do not use any --match-filters (default)
--break-match-filters FILTER Same as "--match-filters" but stops the download process when a video is rejected
--no-break-match-filters Do not use any --break-match-filters (default)
--no-playlist Download only the video, if the URL refers to a video and a playlist
--yes-playlist Download the playlist, if the URL refers to a video and a playlist
--age-limit YEARS Download only videos suitable for the given age
--download-archive FILE Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it
--no-download-archive Do not use archive file (default)
--max-downloads NUMBER Abort after downloading NUMBER files
--break-on-existing Stop the download process when encountering a file that is in the archive supplied with the --download-archive option
--no-break-on-existing Do not stop the download process when encountering a file that is in the archive (default)
--break-per-input Alters --max-downloads, --break-on-existing, --break-match-filters, and autonumber to reset per input URL
--no-break-per-input --break-on-existing and similar options terminates the entire download queue
--skip-playlist-after-errors N Number of allowed failures until the rest of the playlist is skipped Download Options: -N, --concurrent-fragments N Number of fragments of a dash/hlsnative video that should be downloaded concurrently (default is 1)
-r, --limit-rate RATE Maximum download rate in bytes per second, e.g. 50K or 4.2M
--throttled-rate RATE Minimum download rate in bytes per second below which throttling is assumed and the video data is re-extracted, e.g. 100K
-R, --retries RETRIES Number of retries (default is 10), or "infinite"
--file-access-retries RETRIES Number of times to retry on file access error (default is 3), or "infinite"
--fragment-retries RETRIES Number of retries for a fragment (default is 10), or "infinite" (DASH, hlsnative and ISM)
--retry-sleep [TYPE:]EXPR Time to sleep between retries in seconds (optionally) prefixed by the type of retry (http (default), fragment, file_access, extractor) to apply the sleep to. EXPR can be a number, linear=START[:END[:STEP=1]] or exp=START[:END[:BASE=2]]. This option can be used multiple times to set the sleep for the different retry types, e.g. --retry-sleep linear=1::2 --retry-sleep fragment:exp=1:20
--skip-unavailable-fragments Skip unavailable fragments for DASH, hlsnative and ISM downloads (default) (Alias: --no-abort-on-unavailable-fragments)
--abort-on-unavailable-fragments Abort download if a fragment is unavailable (Alias: --no-skip-unavailable-fragments)
--keep-fragments Keep downloaded fragments on disk after downloading is finished
--no-keep-fragments Delete downloaded fragments after downloading is finished (default)
--buffer-size SIZE Size of download buffer, e.g. 1024 or 16K (default is 1024)
--resize-buffer The buffer size is automatically resized from an initial value of --buffer-size (default)
--no-resize-buffer Do not automatically adjust the buffer size
--http-chunk-size SIZE Size of a chunk for chunk-based HTTP downloading, e.g. 10485760 or 10M (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)
--playlist-random Download playlist videos in random order
--lazy-playlist Process entries in the playlist as they are received. This disables n_entries, --playlist-random and --playlist-reverse
--no-lazy-playlist Process videos in the playlist only after the entire playlist is parsed (default)
--hls-use-mpegts Use the mpegts container for HLS videos; allowing some players to play the video while downloading, and reducing the chance of file corruption if download is interrupted. This is enabled by default for live streams
--no-hls-use-mpegts Do not use the mpegts container for HLS videos. This is default when not downloading live streams
--download-sections REGEX Download only chapters that match the regular expression. A "*" prefix denotes time-range instead of chapter. Negative timestamps are calculated from the end. "*from-url" can be used to download between the "start_time" and "end_time" extracted from the URL. Needs ffmpeg. This option can be used multiple times to download multiple sections, e.g. --download-sections "*10:15-inf" --download-sections "intro"
--downloader [PROTO:]NAME Name or path of the external downloader to use (optionally) prefixed by the protocols (http, ftp, m3u8, dash, rstp, rtmp, mms) to use it for. Currently supports native, aria2c, axel, curl, ffmpeg, httpie, wget. You can use this option multiple times to set different downloaders for different protocols. E.g. --downloader aria2c --downloader "dash,m3u8:native" will use aria2c for http/ftp downloads, and the native downloader for dash/m3u8 downloads (Alias: --external-downloader)
--downloader-args NAME:ARGS Give these arguments to the external downloader. Specify the downloader name and the arguments separated by a colon ":". For ffmpeg, arguments can be passed to different positions using the same syntax as --postprocessor-args. You can use this option multiple times to give different arguments to different downloaders (Alias: --external-downloader-args) Filesystem Options: -a, --batch-file FILE File containing URLs to download ("-" for stdin), one URL per line. Lines starting with "#", ";" or "]" are considered as and ignored
--no-batch-file Do not read URLs from batch file (default)
-P, --paths [TYPES:]PATH The paths where the files should be downloaded. Specify the type of file and the path separated by a colon ":". All the same TYPES as --output are supported. Additionally, you can also provide "home" (default) and "temp" paths. All intermediary files are first downloaded to the temp path and then the final files are moved over to the home path after download is finished. This option is ignored if --output is an absolute path
-o, --output [TYPES:]TEMPLATE Output filename template; see "OUTPUT TEMPLATE" for details
--output-na-placeholder TEXT Placeholder for unavailable fields in --output (default: "NA")
--restrict-filenames Restrict filenames to only ASCII characters, and avoid "&amp;" and spaces in filenames
--no-restrict-filenames Allow Unicode characters, "&amp;" and spaces in filenames (default)
--windows-filenames Force filenames to be Windows-compatible
--no-windows-filenames Sanitize filenames only minimally
--trim-filenames LENGTH Limit the filename length (excluding extension) to the specified number of characters
-w, --no-overwrites Do not overwrite any files
--force-overwrites Overwrite all video and metadata files. This option includes --no-continue
--no-force-overwrites Do not overwrite the video, but overwrite related files (default)
-c, --continue Resume partially downloaded files/fragments (default)
--no-continue Do not resume partially downloaded fragments. If the file is not fragmented, restart download of the entire file
--part Use .part files instead of writing directly into output file (default)
--no-part Do not use .part files - write directly into output file
--mtime Use the Last-modified header to set the file modification time
--no-mtime Do not use the Last-modified header to set the file modification time (default)
--write-description Write video description to a .description file
--no-write-description Do not write video description (default)
--write-info-json Write video metadata to a .info.json file (this may contain personal information)
--no-write-info-json Do not write video metadata (default)
--write-playlist-metafiles Write playlist metadata in addition to the video metadata when using --write-info-json, --write-description etc. (default)
--no-write-playlist-metafiles Do not write playlist metadata when using --write-info-json, --write-description etc.
--clean-info-json Remove some internal metadata such as filenames from the infojson (default)
--no-clean-info-json Write all fields to the infojson
--write- Retrieve video to be placed in the infojson. The are fetched even without this option if the extraction is known to be quick (Alias: --get- )
--no-write- Do not retrieve video unless the extraction is known to be quick (Alias: --no-get- )
--load-info-json FILE JSON file containing the video information (created with the "--write-info-json" option)
--cookies FILE Netscape formatted file to read cookies from and dump cookie jar in
--no-cookies Do not read/dump cookies from/to file (default)
--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER] The name of the browser to load cookies from. Currently supported browsers are: brave, chrome, chromium, edge, firefox, opera, safari, vivaldi, whale. Optionally, the KEYRING used for decrypting Chromium cookies on Linux, the name/path of the PROFILE to load cookies from, and the CONTAINER name (if Firefox) ("none" for no container) can be given with their respective separators. By default, all containers of the most recently accessed profile are used. Currently supported keyrings are: basictext, gnomekeyring, kwallet, kwallet5, kwallet6
--no-cookies-from-browser Do not load cookies from browser (default)
--cache-dir DIR Location in the filesystem where yt-dlp can store some downloaded information (such as client ids and signatures) permanently. By default ${XDG_CACHE_HOME}/yt-dlp
--no-cache-dir Disable filesystem caching
--rm-cache-dir Delete all filesystem cache files Thumbnail Options: --write-thumbnail Write thumbnail image to disk
--no-write-thumbnail Do not write thumbnail image to disk (default)
--write-all-thumbnails Write all thumbnail image formats to disk
--list-thumbnails List available thumbnails of each video. Simulate unless --no-simulate is used Internet Shortcut Options: --write-link Write an internet shortcut file, depending on the current platform (.url, .webloc or .desktop). The URL may be cached by the OS
--write-url-link Write a .url Windows internet shortcut. The OS caches the URL based on the file path
--write-webloc-link Write a .webloc macOS internet shortcut
--write-desktop-link Write a .desktop Linux internet shortcut Verbosity and Simulation Options: -q, --quiet Activate quiet mode. If used with --verbose, print the log to stderr
--no-quiet Deactivate quiet mode. (Default)
--no-warnings Ignore warnings
-s, --simulate Do not download the video and do not write anything to disk
--no-simulate Download the video even if printing/listing options are used
--ignore-no-formats-error Ignore "No video formats" error. Useful for extracting metadata even if the videos are not actually available for download (experimental)
--no-ignore-no-formats-error Throw error when no downloadable video formats are found (default)
--skip-download Do not download the video but write all related files (Alias: --no-download)
-O, --print [WHEN:]TEMPLATE Field name or output template to print to screen, optionally prefixed with when to print it, separated by a ":". Supported values of "WHEN" are the same as that of --use-postprocessor (default: video). Implies --quiet. Implies --simulate unless --no-simulate or later stages of WHEN are used. This option can be used multiple times
--print-to-file [WHEN:]TEMPLATE FILE Append given template to the file. The values of WHEN and TEMPLATE are the same as that of --print. FILE uses the same syntax as the output template. This option can be used multiple times
-j, --dump-json Quiet, but print JSON information for each video. Simulate unless --no-simulate is used. See "OUTPUT TEMPLATE" for a description of available keys
-J, --dump-single-json Quiet, but print JSON information for each URL or infojson passed. Simulate unless --no-simulate is used. If the URL refers to a playlist, the whole playlist information is dumped in a single line
--force-write-archive Force download archive entries to be written as far as no errors occur, even if -s or another simulation option is used (Alias: --force-download-archive)
--newline Output progress bar as new lines
--no-progress Do not print progress bar
--progress Show progress bar, even if in quiet mode
--console-title Display progress in console titlebar
--progress-template [TYPES:]TEMPLATE Template for progress outputs, optionally prefixed with one of "download:" (default), "download-title:" (the console title), "postprocess:", or "postprocess-title:". The video's fields are accessible under the "info" key and the progress attributes are accessible under "progress" key. E.g. --console-title --progress-template "download-title:%(info.id)s-%(progress.eta)s"
--progress-delta SECONDS Time between progress output (default: 0)
-v, --verbose Print various debugging information
--dump-pages Print downloaded pages encoded using base64 to debug problems (very verbose)
--write-pages Write downloaded intermediary pages to files in the current directory to debug problems
--print-traffic Display sent and read HTTP traffic Workarounds: --encoding ENCODING Force the specified encoding (experimental)
--legacy-server-connect Explicitly allow HTTPS connection to servers that do not support RFC 5746 secure renegotiation
--no-check-certificates Suppress HTTPS certificate validation
--prefer-insecure Use an unencrypted connection to retrieve information about the video (Currently supported only for YouTube)
--add-headers FIELD:VALUE Specify a custom HTTP header and its value, separated by a colon ":". You can use this option multiple times
--bidi-workaround Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH
--sleep-requests SECONDS Number of seconds to sleep between requests during data extraction
--sleep-interval SECONDS Number of seconds to sleep before each download. This is the minimum time to sleep when used along with --max-sleep-interval (Alias: --min-sleep-interval)
--max-sleep-interval SECONDS Maximum number of seconds to sleep. Can only be used along with --min-sleep-interval
--sleep-subtitles SECONDS Number of seconds to sleep before each subtitle download Video Format Options: -f, --format FORMAT Video format code, see "FORMAT SELECTION" for more details
-S, --format-sort SORTORDER Sort the formats by the fields given, see "Sorting Formats" for more details
--format-sort-reset Disregard previous user specified sort order and reset to the default
--format-sort-force Force user specified sort order to have precedence over all fields, see "Sorting Formats" for more details (Alias: --S-force)
--no-format-sort-force Some fields have precedence over the user specified sort order (default)
--video-multistreams Allow multiple video streams to be merged into a single file
--no-video-multistreams Only one video stream is downloaded for each output file (default)
--audio-multistreams Allow multiple audio streams to be merged into a single file
--no-audio-multistreams Only one audio stream is downloaded for each output file (default)
--prefer-free-formats Prefer video formats with free containers over non-free ones of the same quality. Use with "-S ext" to strictly prefer free containers irrespective of quality
--no-prefer-free-formats Don't give any special preference to free containers (default)
--check-formats Make sure formats are selected only from those that are actually downloadable
--check-all-formats Check all formats for whether they are actually downloadable
--no-check-formats Do not check that the formats are actually downloadable
-F, --list-formats List available formats of each video. Simulate unless --no-simulate is used
--merge-output-format FORMAT Containers that may be used when merging formats, separated by "/", e.g. "mp4/mkv". Ignored if no merge is required. (currently supported: avi, flv, mkv, mov, mp4, webm) Subtitle Options: --write-subs Write subtitle file
--no-write-subs Do not write subtitle file (default)
--write-auto-subs Write automatically generated subtitle file (Alias: --write-automatic-subs)
--no-write-auto-subs Do not write auto-generated subtitles (default) (Alias: --no-write-automatic-subs)
--list-subs List available subtitles of each video. Simulate unless --no-simulate is used
--sub-format FORMAT Subtitle format; accepts formats preference separated by "/", e.g. "srt" or "ass/srt/best"
--sub-langs LANGS Languages of the subtitles to download (can be regex) or "all" separated by commas, e.g. --sub-langs "en.*,ja" (where "en.*" is a regex pattern that matches "en" followed by 0 or more of any character). You can prefix the language code with a "-" to exclude it from the requested languages, e.g. --sub- langs all,-live_chat. Use --list-subs for a list of available language tags Authentication Options: -u, --username USERNAME Login with this account ID
-p, --password PASSWORD Account password. If this option is left out, yt-dlp will ask interactively
-2, --twofactor TWOFACTOR Two-factor authentication code
-n, --netrc Use .netrc authentication data
--netrc-location PATH Location of .netrc authentication data; either the path or its containing directory. Defaults to ~/.netrc
--netrc-cmd NETRC_CMD Command to execute to get the credentials for an extractor.
--video-password PASSWORD Video-specific password
--ap-mso MSO Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs
--ap-username USERNAME Multiple-system operator account login
--ap-password PASSWORD Multiple-system operator account password. If this option is left out, yt-dlp will ask interactively
--ap-list-mso List all supported multiple-system operators
--client-certificate CERTFILE Path to client certificate file in PEM format. May include the private key
--client-certificate-key KEYFILE Path to private key file for client certificate
--client-certificate-password PASSWORD Password for client certificate private key, if encrypted. If not provided, and the key is encrypted, yt-dlp will ask interactively Post-Processing Options: -x, --extract-audio Convert video files to audio-only files (requires ffmpeg and ffprobe)
--audio-format FORMAT Format to convert the audio to when -x is used. (currently supported: best (default), aac, alac, flac, m4a, mp3, opus, vorbis, wav). You can specify multiple rules using similar syntax as --remux-video
--audio-quality QUALITY Specify ffmpeg audio quality to use when converting the audio with -x. Insert a value between 0 (best) and 10 (worst) for VBR or a specific bitrate like 128K (default 5)
--remux-video FORMAT Remux the video into another container if necessary (currently supported: avi, flv, gif, mkv, mov, mp4, webm, aac, aiff, alac, flac, m4a, mka, mp3, ogg, opus, vorbis, wav). If the target container does not support the video/audio codec, remuxing will fail. You can specify multiple rules; e.g. "aac&gt;m4a/mov&gt;mp4/mkv" will remux aac to m4a, mov to mp4 and anything else to mkv
--recode-video FORMAT Re-encode the video into another format if necessary. The syntax and supported formats are the same as --remux-video
--postprocessor-args NAME:ARGS Give these arguments to the postprocessors. Specify the postprocessor/executable name and the arguments separated by a colon ":" to give the argument to the specified postprocessor/executable. Supported PP are: Merger, ModifyChapters, SplitChapters, ExtractAudio, VideoRemuxer, VideoConvertor, Metadata, EmbedSubtitle, EmbedThumbnail, SubtitlesConvertor, ThumbnailsConvertor, FixupStretched, FixupM4a, FixupM3u8, FixupTimestamp and FixupDuration. The supported executables are: AtomicParsley, FFmpeg and FFprobe. You can also specify "PP+EXE:ARGS" to give the arguments to the specified executable only when being used by the specified postprocessor. Additionally, for ffmpeg/ffprobe, "_i"/"_o" can be appended to the prefix optionally followed by a number to pass the argument before the specified input/output file, e.g. --ppa "Merger+ffmpeg_i1:-v quiet". You can use this option multiple times to give different arguments to different postprocessors. (Alias: --ppa)
-k, --keep-video Keep the intermediate video file on disk after post-processing
--no-keep-video Delete the intermediate video file after post-processing (default)
--post-overwrites Overwrite post-processed files (default)
--no-post-overwrites Do not overwrite post-processed files
--embed-subs Embed subtitles in the video (only for mp4, webm and mkv videos)
--no-embed-subs Do not embed subtitles (default)
--embed-thumbnail Embed thumbnail in the video as cover art
--no-embed-thumbnail Do not embed thumbnail (default)
--embed-metadata Embed metadata to the video file. Also embeds chapters/infojson if present unless --no-embed-chapters/--no-embed-info-json are used (Alias: --add-metadata)
--no-embed-metadata Do not add metadata to file (default) (Alias: --no-add-metadata)
--embed-chapters Add chapter markers to the video file (Alias: --add-chapters)
--no-embed-chapters Do not add chapter markers (default) (Alias: --no-add-chapters)
--embed-info-json Embed the infojson as an attachment to mkv/mka video files
--no-embed-info-json Do not embed the infojson as an attachment to the video file
--parse-metadata [WHEN:]FROM:TO Parse additional metadata like title/artist from other fields; see "MODIFYING METADATA" for details. Supported values of "WHEN" are the same as that of --use-postprocessor (default: pre_process)
--replace-in-metadata [WHEN:]FIELDS REGEX REPLACE Replace text in a metadata field using the given regex. This option can be used multiple times. Supported values of "WHEN" are the same as that of --use-postprocessor (default: pre_process)
--xattrs Write metadata to the video file's xattrs (using Dublin Core and XDG standards)
--concat-playlist POLICY Concatenate videos in a playlist. One of "never", "always", or "multi_video" (default; only when the videos form a single show). All the video files must have the same codecs and number of streams to be concatenable. The "pl_video:" prefix can be used with "--paths" and "--output" to set the output filename for the concatenated files. See "OUTPUT TEMPLATE" for details
--fixup POLICY Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_or_warn (the default; fix the file if we can, warn otherwise), force (try fixing even if the file already exists)
--ffmpeg-location PATH Location of the ffmpeg binary; either the path to the binary or its containing directory
--exec [WHEN:]CMD Execute a command, optionally prefixed with when to execute it, separated by a ":". Supported values of "WHEN" are the same as that of --use-postprocessor (default: after_move). The same syntax as the output template can be used to pass any field as arguments to the command. If no fields are passed, %(filepath,_filename|)q is appended to the end of the command. This option can be used multiple times
--no-exec Remove any previously defined --exec
--convert-subs FORMAT Convert the subtitles to another format (currently supported: ass, lrc, srt, vtt). Use "--convert-subs none" to disable conversion (default) (Alias: --convert- subtitles)
--convert-thumbnails FORMAT Convert the thumbnails to another format (currently supported: jpg, png, webp). You can specify multiple rules using similar syntax as "--remux-video". Use "--convert- thumbnails none" to disable conversion (default)
--split-chapters Split video into multiple files based on internal chapters. The "chapter:" prefix can be used with "--paths" and "--output" to set the output filename for the split files. See "OUTPUT TEMPLATE" for details
--no-split-chapters Do not split video based on chapters (default)
--remove-chapters REGEX Remove chapters whose title matches the given regular expression. The syntax is the same as --download-sections. This option can be used multiple times
--no-remove-chapters Do not remove any chapters from the file (default)
--force-keyframes-at-cuts Force keyframes at cuts when downloading/splitting/removing sections. This is slow due to needing a re-encode, but the resulting video may have fewer artifacts around the cuts
--no-force-keyframes-at-cuts Do not force keyframes around the chapters when cutting/splitting (default)
--use-postprocessor NAME[:ARGS] The (case-sensitive) name of plugin postprocessors to be enabled, and (optionally) arguments to be passed to it, separated by a colon ":". ARGS are a semicolon ";" delimited list of NAME=VALUE. The "when" argument determines when the postprocessor is invoked. It can be one of "pre_process" (after video extraction), "after_filter" (after video passes filter), "video" (after --format; before --print/--output), "before_dl" (before each video download), "post_process" (after each video download; default), "after_move" (after moving the video file to its final location), "after_video" (after downloading and processing all formats of a video), or "playlist" (at end of playlist). This option can be used multiple times to add different postprocessors SponsorBlock Options: Make chapter entries for, or remove various segments ( , introductions, etc.) from downloaded YouTube videos using the SponsorBlock API --sponsorblock-mark CATS SponsorBlock categories to create chapters for, separated by commas. Available categories are , intro, outro, selfpromo, preview, filler, interaction, music_offtopic, hook, poi_highlight, chapter, all and default (=all). You can prefix the category with a "-" to exclude it. See [1] for descriptions of the categories. E.g. --sponsorblock-mark all,-preview [1] https://wiki. .ajay.app/w/Segment_Categories
--sponsorblock-remove CATS SponsorBlock categories to be removed from the video file, separated by commas. If a category is present in both mark and remove, remove takes precedence. The syntax and available categories are the same as for --sponsorblock-mark except that "default" refers to "all,-filler" and poi_highlight, chapter are not available
--sponsorblock-chapter-title TEMPLATE An output template for the title of the SponsorBlock chapters created by --sponsorblock-mark. The only available fields are start_time, end_time, category, categories, name, category_names. Defaults to "[SponsorBlock]: %(category_names)l"
--no-sponsorblock Disable both --sponsorblock-mark and --sponsorblock-remove
--sponsorblock-api URL SponsorBlock API location, defaults to https:// .ajay.app Extractor Options: --extractor-retries RETRIES Number of retries for known extractor errors (default is 3), or "infinite"
--allow-dynamic-mpd Process dynamic DASH manifests (default) (Alias: --no-ignore-dynamic-mpd)
--ignore-dynamic-mpd Do not process dynamic DASH manifests (Alias: --no-allow-dynamic-mpd)
--hls-split-discontinuity Split HLS playlists to different formats at discontinuities such as ad breaks
--no-hls-split-discontinuity Do not split HLS playlists into different formats at discontinuities such as ad breaks (default)
--extractor-args IE_KEY:ARGS Pass ARGS arguments to the IE_KEY extractor. See "EXTRACTOR ARGUMENTS" for details. You can use this option multiple times to give arguments for different extractors Preset Aliases: Predefined aliases for convenience and ease of use. Note that future versions of yt-dlp may add or adjust presets, but the existing preset names will not be changed or removed -t mp3 -f 'ba[acodec^=mp3]/ba/b' -x --audio-format mp3 -t aac -f 'ba[acodec^=aac]/ba[acodec^=mp4a.40.]/ba/b' -x --audio-format aac -t mp4 --merge-output-format mp4 --remux-video mp4 -S vcodec:h264,lang,quality,res,fps,hdr:12,a codec:aac -t mkv --merge-output-format mkv --remux-video mkv -t sleep --sleep-subtitles 5 --sleep-requests 0.75 --sleep-interval 10 --max-sleep-interval 20 CONFIGURATION You can configure yt-dlp by placing any supported command line option in a configuration file. The configuration is loaded from the following locations: Main Configuration: The file given to --config-locations Portable Configuration: ( for portable installations) If using a binary, yt-dlp.conf in the same directory as the binary If running from source-code, yt-dlp.conf in the parent directory of yt_dlp Home Configuration: yt-dlp.conf in the home path given to -P If -P is not given, the current directory is searched User Configuration:]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/yt-dlp/yt-dlp</guid>
    </item>
    <item>
      <title><![CDATA[Zie619/n8n-workflows]]></title>
      <link>https://github.com/Zie619/n8n-workflows</link>
      <description><![CDATA[all of the workflows of n8n i could find (also from the site itself) n8n Workflow Collection The Ultimate Collection of n8n Automation Workflows Browse Online · Documentation · Contributing · License NEW: Scan Your n8n Workflows for AI Security Risks Your workflows contain AI — do you know what's hiding in them? We built AI-BOM because we scanned our own 4,343 workflows and found hardcoded API keys, unauthenticated AI agents, and MCP clients connecting to unknown servers — all invisible to existing security tools. AI-BOM is the first and only tool that scans n8n workflows for AI security risks. pip install ai-bom
ai-bom scan ./workflows/ One command finds every AI Agent node, LLM integration, MCP client, hardcoded credential, and dangerous tool combination — then gives you a risk score and a compliance-ready report. EU AI Act deadline: August 2025. You need an AI inventory. Get AI-BOM (free &amp; open source) → AI-BOM by Trusera Securing the Agentic Service Mesh What does AI-BOM detect in n8n workflows? (click to expand) Risk Severity What it finds AI Agent nodes CRITICAL Agents connected to LLMs with tool access — can execute code Hardcoded credentials CRITICAL API keys in workflow JSON instead of credential store Dangerous tool combos CRITICAL Agents with Code Execution + HTTP Request = RCE risk MCP clients HIGH Model Context Protocol connections to external servers Unauthenticated webhooks HIGH Webhook triggers exposed to the internet without auth Agent chains HIGH Execute Workflow linking agents without input validation Beyond n8n, AI-BOM also scans source code (Python, JS, TS, Java, Go, Rust, Ruby), Docker configs, cloud infrastructure (Terraform, CloudFormation), and network endpoints — 21+ AI SDKs detected across 7 languages. Output formats: CycloneDX SBOM | SARIF (GitHub Code Scanning) | HTML Dashboard | Markdown | JSON What's New Latest Updates (November 2025) Enhanced Security: Full security audit completed, all CVEs resolved Docker Support: Multi-platform builds for linux/amd64 and linux/arm64 GitHub Pages: Live searchable interface at zie619.github.io/n8n-workflows Performance: 100x faster search with SQLite FTS5 integration Modern UI: Completely redesigned interface with dark/light mode Quick Access Use Online (No Installation) Visit zie619.github.io/n8n-workflows for instant access to: Smart Search — Find workflows instantly 15+ Categories — Browse by use case Mobile Ready — Works on any device Direct Downloads — Get workflow JSONs instantly Features By The Numbers 4,343 Production-Ready Workflows 365 Unique Integrations 29,445 Total Nodes 15 Organized Categories 100% Import Success Rate Performance &lt; 100ms Search Response &lt; 50MB Memory Usage 700x Smaller Than v1 10x Faster Load Times 40x Less RAM Usage Local Installation Prerequisites Python 3.9+ pip (Python package manager) 100MB free disk space Quick Start # Clone the repository
git clone https://github.com/Zie619/n8n-workflows.git
cd n8n-workflows # Install dependencies
pip install -r requirements.txt # Start the server
python run.py # Open in browser
# http://localhost:8000 Docker Installation # Using Docker Hub
docker run -p 8000:8000 zie619/n8n-workflows:latest # Or build locally
docker build -t n8n-workflows .
docker run -p 8000:8000 n8n-workflows Documentation API Endpoints Endpoint Method Description / GET Web interface /api/search GET Search workflows /api/stats GET Repository statistics /api/workflow/{id} GET Get workflow JSON /api/categories GET List all categories /api/export GET Export workflows Search Features Full-text search across names, descriptions, and nodes Category filtering (Marketing, Sales, DevOps, etc.) Complexity filtering (Low, Medium, High) Trigger type filtering (Webhook, Schedule, Manual, etc.) Service filtering (365+ integrations) Architecture graph LR A[User] --&gt; B[Web Interface] B --&gt; C[FastAPI Server] C --&gt; D[SQLite FTS5] D --&gt; E[Workflow Database] C --&gt; F[Static Files] F --&gt; G[Workflow JSONs] Tech Stack Backend: Python, FastAPI, SQLite with FTS5 Frontend: Vanilla JS, Tailwind CSS Database: SQLite with Full-Text Search Deployment: Docker, GitHub Actions, GitHub Pages Security: Trivy scanning, CORS protection, Input validation Repository Structure n8n-workflows/
├── workflows/ # 4,343 workflow JSON files
│ └── [category]/ # Organized by integration
├── docs/ # GitHub Pages site
├── src/ # Python source code
├── scripts/ # Utility scripts
├── api_server.py # FastAPI application
├── run.py # Server launcher
├── workflow_db.py # Database manager
└── requirements.txt # Python dependencies Contributing We love contributions! Here's how you can help: Ways to Contribute Report bugs via Issues Suggest features in Discussions Improve documentation Submit workflow fixes Star the repository Development Setup # Fork and clone
git clone https://github.com/YOUR_USERNAME/n8n-workflows.git # Create branch
git checkout -b feature/amazing-feature # Make changes and test
python run.py --debug # Commit and push
git add .
git commit -m "feat: add amazing feature"
git push origin feature/amazing-feature # Open PR Security Security Features Path traversal protection Input validation &amp; sanitization CORS protection Rate limiting Docker security hardening Non-root container user Regular security scanning Reporting Security Issues Please report security vulnerabilities to the maintainers via Security Advisory. License This project is licensed under the MIT License - see the LICENSE file for details. Support If you find this project helpful, please consider: Star us on GitHub — it motivates us a lot! Made with care by Zie619 and contributors AI-BOM — Discover every AI agent, model, and API hiding in your infrastructure. Open source by Trusera — Securing the Agentic Service Mesh.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/Zie619/n8n-workflows</guid>
    </item>
    <item>
      <title><![CDATA[PostHog/posthog]]></title>
      <link>https://github.com/PostHog/posthog</link>
      <description><![CDATA[PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack. Docs - Community - Roadmap - Why PostHog? - Changelog - Bug reports PostHog is an all-in-one, open source platform for building successful products PostHog provides every tool you need to build a successful product including: Product Analytics: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL. Web Analytics: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue. Session Replays: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior. Feature Flags: Safely roll out features to select users or cohorts with feature flags. Experiments: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too. Error Tracking: Track errors, get alerts, and resolve issues to improve your product. Surveys: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder. Data warehouse: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data. Data pipelines: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse. LLM analytics: Capture traces, generations, latency, and cost for your LLM-powered app. Workflows: Create workflows that automate actions or send messages to your users. Best of all, all of this is free to use with a generous monthly free tier for each product. Get started by signing up for PostHog Cloud US or PostHog Cloud EU. Table of Contents PostHog is an all-in-one, open source platform for building successful products Table of Contents Getting started with PostHog PostHog Cloud ( ) Self-hosting the open-source hobby deploy (Advanced) Setting up PostHog Learning more about PostHog Contributing Open-source vs. paid We’re hiring! Getting started with PostHog PostHog Cloud ( ) The fastest and most reliable way to get started with PostHog is signing up for free to PostHog Cloud or PostHog Cloud EU. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage. Self-hosting the open-source hobby deploy (Advanced) If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker ( 4GB memory): /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)" Open source deployments should scale to approximately 100k events per month, after which we recommend migrating to a PostHog Cloud. We do not provide customer support or offer guarantees for open source deployments. See our self-hosting docs, troubleshooting guide, and disclaimer for more info. Setting up PostHog Once you've got a PostHog instance, you can set it up by installing our JavaScript web snippet, one of our SDKs, or by using our API. We have SDKs and libraries for popular languages and frameworks like: Frontend Mobile Backend JavaScript React Native Python Next.js Android Node React iOS PHP Vue Flutter Ruby Beyond this, we have docs and guides for Go, .NET/C#, Django, Angular, WordPress, Webflow, and more. Once you've installed PostHog, see our product docs for more information on how to set up product analytics, web analytics, session replays, feature flags, experiments, error tracking, surveys, data warehouse, and more. Learning more about PostHog Our code isn't the only thing that's open source . We also open source our company handbook which details our strategy, ways of working, and processes. Curious about how to make the most of PostHog? We wrote a guide to winning with PostHog which walks you through the basics of measuring activation, tracking retention, and capturing revenue. Contributing We &lt;3 contributions big and small: Vote on features or get early access to beta functionality in our roadmap Open a PR (see our instructions on developing PostHog locally) Submit a feature request or bug report For an overview of the codebase structure, see monorepo layout and products. Open-source vs. paid This repo is available under the MIT expat license, except for the ee directory (which has its license here) if applicable. Need absolutely % FOSS? Check out our posthog-foss repository, which is purged of all proprietary code and features. The pricing for our paid plan is completely transparent and available on our pricing page. We're hiring! Hey! If you're reading this, you've proven yourself as a dedicated README reader. You might also make a great addition to our team. We're growing fast and would love for you to join us.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/PostHog/posthog</guid>
    </item>
    <item>
      <title><![CDATA[OpenBMB/UltraRAG]]></title>
      <link>https://github.com/OpenBMB/UltraRAG</link>
      <description><![CDATA[A Low-Code MCP Framework for Building Complex and Innovative RAG Pipelines Less Code, Lower Barrier, Faster Deployment 简体中文 | English Latest News [2026.01.23] UltraRAG 3.0 Released: Say no to "black box" development—make every line of reasoning logic clearly visible Blog [2026.01.20] AgentCPM-Report Model Released! DeepResearch is finally localized: 8B on-device writing agent AgentCPM-Report is open-sourced Model Previous News [2025.11.11] UltraRAG 2.1 Released: Enhanced knowledge ingestion &amp; multimodal support, with a more complete unified evaluation system! [2025.09.23] New daily RAG paper digest, updated every day Papers [2025.09.09] Released a Lightweight DeepResearch Pipeline local setup tutorial bilibili · Blog [2025.09.01] Released a step-by-step UltraRAG installation and full RAG walkthrough video bilibili · Blog [2025.08.28] UltraRAG 2.0 Released! UltraRAG 2.0 is fully upgraded: build a high-performance RAG with just a few dozen lines of code, empowering researchers to focus on ideas and innovation! We have preserved the UltraRAG v2 code, which can be viewed at v2. [2025.01.23] UltraRAG Released! Enabling large models to better comprehend and utilize knowledge bases. The UltraRAG 1.0 code is still available at v1. About UltraRAG UltraRAG is the first lightweight RAG development framework based on the Model Context Protocol (MCP) architecture design, jointly launched by THUNLP at Tsinghua University, NEUIR at Northeastern University, OpenBMB, and AI9stars. Designed for research exploration and industrial prototyping, UltraRAG standardizes core RAG components (Retriever, Generation, etc.) as independent MCP Servers, combined with the powerful workflow orchestration capabilities of the MCP Client. Developers can achieve precise orchestration of complex control structures such as conditional branches and loops simply through YAML configuration. UltraRAG UI UltraRAG UI transcends the boundaries of traditional chat interfaces, evolving into a visual RAG Integrated Development Environment (IDE) that combines orchestration, debugging, and demonstration. The system features a powerful built-in Pipeline Builder that supports bidirectional real-time synchronization between "Canvas Construction" and "Code Editing," allowing for granular online adjustments of pipeline parameters and prompts. Furthermore, it introduces an Intelligent AI Assistant to empower the entire development lifecycle, from pipeline structural design to parameter tuning and prompt generation. Once constructed, logic flows can be converted into interactive dialogue systems with a single click. The system seamlessly integrates Knowledge Base Management components, enabling users to build custom knowledge bases for document Q&amp;A. This truly realizes a one-stop closed loop, spanning from underlying logic construction and data governance to final application deployment. https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9 Key Highlights Low-Code Orchestration of Complex Workflows Inference Orchestration: Natively supports control structures such as sequential, loop, and conditional branches. Developers only need to write YAML configuration files to implement complex iterative RAG logic in dozens of lines of code. Modular Extension and Reproduction Atomic Servers: Based on the MCP architecture, functions are decoupled into independent Servers. New features only need to be registered as function-level Tools to seamlessly integrate into workflows, achieving extremely high reusability. Unified Evaluation and Benchmark Comparison Research Efficiency: Built-in standardized evaluation workflows, ready-to-use mainstream research benchmarks. Through unified metric management and baseline integration, significantly improves experiment reproducibility and comparison efficiency. Rapid Interactive Prototype Generation One-Click Delivery: Say goodbye to tedious UI development. With just one command, Pipeline logic can be instantly converted into an interactive conversational Web UI, shortening the distance from algorithm to demonstration. Installation We provide two installation methods: local source code installation ( using uv for package management) and Docker container deployment Method 1: Source Code Installation We strongly recommend using uv to manage Python environments and dependencies, as it can greatly improve installation speed. Prepare Environment If you haven't installed uv yet, please execute: ## Direct installation
pip install uv
## Download
curl -LsSf https://astral.sh/uv/install.sh | sh Download Source Code git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG Install Dependencies Choose one of the following modes to install dependencies based on your use case: A: Create a New Environment Use uv sync to automatically create a virtual environment and synchronize dependencies: Core dependencies: If you only need to run basic core functions, such as only using UltraRAG UI: uv sync Full installation: If you want to fully experience UltraRAG's retrieval, generation, corpus processing, and evaluation functions, please run: uv sync --all-extras On-demand installation: If you only need to run specific modules, keep the corresponding --extra as needed, for example: uv sync --extra retriever # Retrieval module only
uv sync --extra generation # Generation module only Once installed, activate the virtual environment: # Windows CMD
.venv\Scripts\activate.bat # Windows Powershell
.venv\Scripts\Activate.ps1 # macOS / Linux
source .venv/bin/activate B: Install into an Existing Environment To install UltraRAG into your currently active Python environment, use uv pip: # Core dependencies
uv pip install -e . # Full installation
uv pip install -e ".[all]" # On-demand installation
uv pip install -e ".[retriever]" Method 2: Docker Container Deployment If you prefer not to configure a local Python environment, you can deploy using Docker. Get Code and Images # 1. Clone the repository
git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG # 2. Prepare the image (choose one)
# Option A: Pull from Docker Hub
docker pull hdxin2002/ultrarag:v0.3.0-base-cpu # Base version (CPU)
docker pull hdxin2002/ultrarag:v0.3.0-base-gpu # Base version (GPU)
docker pull hdxin2002/ultrarag:v0.3.0 # Full version (GPU) # Option B: Build locally
docker build -t ultrarag:v0.3.0 . # 3. Start container (port 5050 is automatically mapped)
docker run -it --gpus all -p 5050:5050 Start the Container # Start the container (Port 5050 is mapped by default)
docker run -it --gpus all -p 5050:5050 Note: After the container starts, UltraRAG UI will run automatically. You can directly access http://localhost:5050 in your browser to use it. Verify Installation After installation, run the following example command to check if the environment is normal: ultrarag run examples/sayhello.yaml If you see the following output, the installation is successful: Hello, UltraRAG v3! Quick Start We provide complete tutorial examples from beginner to advanced. Whether you are conducting academic research or building industrial applications, you can find guidance here. Welcome to visit the Documentation for more details. Research Experiments Designed for researchers, providing data, experimental workflows, and visualization analysis tools. Getting Started: Learn how to quickly run standard RAG experimental workflows based on UltraRAG. Evaluation Data: Download the most commonly used public evaluation datasets in the RAG field and large-scale retrieval corpora, directly for research benchmark testing. Case Analysis: Provides a visual Case Study interface to deeply track each intermediate output of the workflow, assisting in analysis and error attribution. Code Integration: Learn how to directly call UltraRAG components in Python code to achieve more flexible customized development. Demo Systems Designed for developers and end users, providing complete UI interaction and complex application cases. Quick Start: Learn how to start UltraRAG UI and familiarize yourself with various advanced configurations in administrator mode. Deployment Guide: Detailed production environment deployment tutorials, covering the setup of Retriever, Generation models (LLM), and Milvus vector database. Deep Research: Flagship case, deploy a Deep Research Pipeline. Combined with the AgentCPM-Report model, it can automatically perform multi-step retrieval and integration to generate tens of thousands of words of survey reports. Contributing Thanks to the following contributors for their code submissions and testing. We also welcome new members to join us in collectively building a comprehensive RAG ecosystem! You can contribute by following the standard process: Fork this repository → Submit Issues → Create Pull Requests (PRs). Support Us If you find this repository helpful for your research, please consider giving us a to show your support. Contact Us For technical issues and feature requests, please use GitHub Issues. For questions about usage, feedback, or any discussions related to RAG technologies, you are welcome to join our WeChat group, Feishu group, and Discord to exchange ideas with us. If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at yanyk.thu@gmail.com WeChat Group Feishu Group Discord]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/OpenBMB/UltraRAG</guid>
    </item>
    <item>
      <title><![CDATA[huggingface/skills]]></title>
      <link>https://github.com/huggingface/skills</link>
      <description><![CDATA[Hugging Face Skills Hugging Face Skills are definitions for AI/ML tasks like dataset creation, model training, and evaluation. They are interoperable with all major coding agent tools like OpenAI Codex, Anthropic's Claude Code, Google DeepMind's Gemini CLI, and Cursor. The Skills in this repository follow the standardized format Agent Skill format. How do Skills work? In practice, skills are self-contained folders that package instructions, scripts, and resources together for an AI agent to use on a specific use case. Each folder includes a SKILL.md file with YAML frontmatter (name and description) followed by the guidance your coding agent follows while the skill is active. [!NOTE] 'Skills' is actually an Anthropic term used within Claude AI and Claude Code and not adopted by other agent tools, but we love it! OpenAI Codex uses an AGENTS.md file to define the instructions for your coding agent. Google Gemini uses 'extensions' to define the instructions for your coding agent in a gemini-extension.json file. This repo is compatible with all of them, and more! [!TIP] If your agent doesn't support skills, you can use agents/AGENTS.md directly as a fallback. Installation Hugging Face skills are compatible with Claude Code, Codex, Gemini CLI, and Cursor. Claude Code Register the repository as a plugin marketplace: /plugin marketplace add huggingface/skills To install a skill, run: /plugin install @huggingface/skills For example: /plugin install hugging-face-cli@huggingface/skills Codex Codex will identify the skills via the AGENTS.md file. You can verify the instructions are loaded with: codex --ask-for-approval never "Summarize the current instructions." For more details, see the Codex AGENTS guide. Gemini CLI This repo includes gemini-extension.json to integrate with the Gemini CLI. Install locally: gemini extensions install . --consent or use the GitHub URL: gemini extensions install https://github.com/huggingface/skills.git --consent See Gemini CLI extensions docs for more help. Cursor This repository includes Cursor plugin manifests: .cursor-plugin/plugin.json .mcp.json (configured with the Hugging Face MCP server URL) Install from repository URL (or local checkout) via the Cursor plugin flow. For contributors, regenerate manifests with: ./scripts/publish.sh Skills This repository contains a few skills to get you started. You can also contribute your own skills to the repository. Available skills Name Description Documentation hugging-face-cli Execute Hugging Face Hub operations using the hf CLI. Download models/datasets, upload files, manage repos, and run cloud compute jobs. SKILL.md hugging-face-datasets Create and manage datasets on Hugging Face Hub. Supports initializing repos, defining configs/system prompts, streaming row updates, and SQL-based dataset querying/transformation. SKILL.md hugging-face-evaluation Add and manage evaluation results in Hugging Face model cards. Supports extracting eval tables from README content, importing scores from Artificial Analysis API, and running custom evaluations with vLLM/lighteval. SKILL.md hugging-face-jobs Run compute jobs on Hugging Face infrastructure. Execute Python scripts, manage scheduled jobs, and monitor job status. SKILL.md hugging-face-model-trainer Train or fine-tune language models using TRL on Hugging Face Jobs infrastructure. Covers SFT, DPO, GRPO and reward modeling training methods, plus GGUF conversion for local deployment. Includes hardware selection, cost estimation, Trackio monitoring, and Hub persistence. SKILL.md hugging-face-paper-publisher Publish and manage research papers on Hugging Face Hub. Supports creating paper pages, linking papers to models/datasets, claiming authorship, and generating professional markdown-based research articles. SKILL.md hugging-face-tool-builder Build reusable scripts for Hugging Face API operations. Useful for chaining API calls or automating repeated tasks. SKILL.md hugging-face-trackio Track and visualize ML training experiments with Trackio. Log metrics via Python API and retrieve them via CLI. Supports real-time dashboards synced to HF Spaces. SKILL.md Using skills in your coding agent Once a skill is installed, mention it directly while giving your coding agent instructions: "Use the HF LLM trainer skill to estimate the GPU memory needed for a 70B model run." "Use the HF model evaluation skill to launch run_eval_job.py on the latest checkpoint." "Use the HF dataset creator skill to draft new few-shot classification templates." "Use the HF paper publisher skill to index my arXiv paper and link it to my model." Your coding agent automatically loads the corresponding SKILL.md instructions and helper scripts while it completes the task. Contribute or customize a skill Copy one of the existing skill folders (for example, hf-datasets/) and rename it. Update the new folder's SKILL.md frontmatter: ---
name: my-skill-name
description: Describe what the skill does and when to use it
--- # Skill Title]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:18 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/huggingface/skills</guid>
    </item>
    <item>
      <title><![CDATA[usestrix/strix]]></title>
      <link>https://github.com/usestrix/strix</link>
      <description><![CDATA[Open-source AI hackers to find and fix your app’s vulnerabilities. Strix Open-source AI hackers to find and fix your app’s vulnerabilities. [!TIP] New! Strix integrates seamlessly with GitHub Actions and CI/CD pipelines. Automatically scan for vulnerabilities on every pull request and block insecure code before it reaches production! Strix Overview Strix are autonomous AI agents that act just like real hackers - they run your code dynamically, find vulnerabilities, and validate them through actual proof-of-concepts. Built for developers and security teams who need fast, accurate security testing without the overhead of manual pentesting or the false positives of static analysis tools. Key Capabilities: Full hacker toolkit out of the box Teams of agents that collaborate and scale Real validation with PoCs, not false positives Developer‑first CLI with actionable reports Auto‑fix &amp; reporting to accelerate remediation Use Cases Application Security Testing - Detect and validate critical vulnerabilities in your applications Rapid Penetration Testing - Get penetration tests done in hours, not weeks, with compliance reports Bug Bounty Automation - Automate bug bounty research and generate PoCs for faster reporting CI/CD Integration - Run tests in CI/CD to block vulnerabilities before reaching production Quick Start Prerequisites: Docker (running) An LLM API key: Any supported provider (OpenAI, Anthropic, Google, etc.) Or Strix Router — single API key for multiple providers with $10 free credit on signup Installation &amp; First Scan # Install Strix
curl -sSL https://strix.ai/install | bash # Configure your AI provider
export STRIX_LLM="openai/gpt-5" # or "strix/gpt-5" via Strix Router (https://models.strix.ai)
export LLM_API_KEY="your-api-key" # Run your first security assessment
strix --target ./app-directory [!NOTE] First run automatically pulls the sandbox Docker image. Results are saved to strix_runs/ Features Agentic Security Tools Strix agents come equipped with a comprehensive security testing toolkit: Full HTTP Proxy - Full request/response manipulation and analysis Browser Automation - Multi-tab browser for testing of XSS, CSRF, auth flows Terminal Environments - Interactive shells for command execution and testing Python Runtime - Custom exploit development and validation Reconnaissance - Automated OSINT and attack surface mapping Code Analysis - Static and dynamic analysis capabilities Knowledge Management - Structured findings and attack documentation Comprehensive Vulnerability Detection Strix can identify and validate a wide range of security vulnerabilities: Access Control - IDOR, privilege escalation, auth bypass Injection Attacks - SQL, NoSQL, command injection Server-Side - SSRF, XXE, deserialization flaws Client-Side - XSS, prototype pollution, DOM vulnerabilities Business Logic - Race conditions, workflow manipulation Authentication - JWT vulnerabilities, session management Infrastructure - Misconfigurations, exposed services Graph of Agents Advanced multi-agent orchestration for comprehensive security testing: Distributed Workflows - Specialized agents for different attacks and assets Scalable Testing - Parallel execution for fast comprehensive coverage Dynamic Coordination - Agents collaborate and share discoveries Usage Examples Basic Usage # Scan a local codebase
strix --target ./app-directory # Security review of a GitHub repository
strix --target https://github.com/org/repo # Black-box web application assessment
strix --target https://your-app.com Advanced Testing Scenarios # Grey-box authenticated testing
strix --target https://your-app.com --instruction "Perform authenticated testing using credentials: user:pass" # Multi-target testing (source code + deployed app)
strix -t https://github.com/org/app -t https://your-app.com # Focused testing with custom instructions
strix --target api.your-app.com --instruction "Focus on business logic flaws and IDOR vulnerabilities" # Provide detailed instructions through file (e.g., rules of engagement, scope, exclusions)
strix --target api.your-app.com --instruction-file ./instruction.md Headless Mode Run Strix programmatically without interactive UI using the -n/--non-interactive flag—perfect for servers and automated jobs. The CLI prints real-time vulnerability findings, and the final report before exiting. Exits with non-zero code when vulnerabilities are found. strix -n --target https://your-app.com CI/CD (GitHub Actions) Strix can be added to your pipeline to run a security test on pull requests with a lightweight GitHub Actions workflow: name: strix-penetration-test on: pull_request: jobs: security-scan: runs-on: ubuntu-latest steps: - uses: actions/checkout@v6 - name: Install Strix run: curl -sSL https://strix.ai/install | bash - name: Run Strix env: STRIX_LLM: ${{ secrets.STRIX_LLM }} LLM_API_KEY: ${{ secrets.LLM_API_KEY }} run: strix -n -t ./ --scan-mode quick Configuration export STRIX_LLM="openai/gpt-5"
export LLM_API_KEY="your-api-key" # Optional
export LLM_API_BASE="your-api-base-url" # if using a local model, e.g. Ollama, LMStudio
export PERPLEXITY_API_KEY="your-api-key" # for search capabilities
export STRIX_REASONING_EFFORT="high" # control thinking effort (default: high, quick scan: medium) [!NOTE] Strix automatically saves your configuration to ~/.strix/cli-config.json, so you don't have to re-enter it on every run. models for best results: OpenAI GPT-5 — openai/gpt-5 Anthropic Claude Sonnet 4.6 — anthropic/claude-sonnet-4-6 Google Gemini 3 Pro Preview — vertex_ai/gemini-3-pro-preview See the LLM Providers documentation for all supported providers including Vertex AI, Bedrock, Azure, and local models. Documentation Full documentation is available at docs.strix.ai — including detailed guides for usage, CI/CD integrations, skills, and advanced configuration. Contributing We welcome contributions of code, docs, and new skills - check out our Contributing Guide to get started or open a pull request/issue. Join Our Community Have questions? Found a bug? Want to contribute? Join our Discord! Support the Project Love Strix? Give us a on GitHub! Acknowledgements Strix builds on the incredible work of open-source projects like LiteLLM, Caido, Nuclei, Playwright, and Textual. Huge thanks to their maintainers! [!WARNING] Only test apps you own or have permission to test. You are responsible for using Strix ethically and legally.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:18 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/usestrix/strix</guid>
    </item>
    <item>
      <title><![CDATA[RichardAtCT/claude-code-telegram]]></title>
      <link>https://github.com/RichardAtCT/claude-code-telegram</link>
      <description><![CDATA[A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence. Claude Code Telegram Bot A Telegram bot that gives you remote access to Claude Code. Chat naturally with Claude about your projects from anywhere -- no terminal commands needed. What is this? This bot connects Telegram to Claude Code, providing a conversational AI interface for your codebase: Chat naturally -- ask Claude to analyze, edit, or explain your code in plain language Maintain context across conversations with automatic session persistence per project Code on the go from any device with Telegram Receive proactive notifications from webhooks, scheduled jobs, and CI/CD events Stay secure with built-in authentication, directory sandboxing, and audit logging Quick Start Demo You: Can you help me add error handling to src/api.py? Bot: I'll analyze src/api.py and add error handling... [Claude reads your code, suggests improvements, and can apply changes directly] You: Looks good. Now run the tests to make sure nothing broke. Bot: Running pytest... All 47 tests passed. The error handling changes are working correctly. 1. Prerequisites Python 3.11+ -- Download here Claude Code CLI -- Install from here Telegram Bot Token -- Get one from @BotFather 2. Install Choose your preferred method: Option A: Install from a release tag ( ) # Using uv ( — installs in an isolated environment)
uv tool install git+https://github.com/RichardAtCT/claude-code-telegram@v1.3.0 # Or using pip
pip install git+https://github.com/RichardAtCT/claude-code-telegram@v1.3.0 # Track the latest stable release
pip install git+https://github.com/RichardAtCT/claude-code-telegram@latest Option B: From source (for development) git clone https://github.com/RichardAtCT/claude-code-telegram.git
cd claude-code-telegram
make dev # requires Poetry Note: Always install from a tagged release (not main) for stability. See Releases for available versions. 3. Configure cp .env.example .env
# Edit .env with your settings: Minimum required: TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
TELEGRAM_BOT_USERNAME=my_claude_bot
APPROVED_DIRECTORY=/Users/yourname/projects
ALLOWED_USERS=123456789 # Your Telegram user ID 4. Run make run # Production
make run-debug # With debug logging Message your bot on Telegram to get started. Detailed setup: See docs/setup.md for Claude authentication options and troubleshooting. Modes The bot supports two interaction modes: Agentic Mode (Default) The default conversational mode. Just talk to Claude naturally -- no special commands required. Commands: /start, /new, /status, /verbose, /repo If ENABLE_PROJECT_THREADS=true: /sync_threads You: What files are in this project?
Bot: Working... (3s) Read LS Let me describe the project structure
Bot: [Claude describes the project structure] You: Add a retry decorator to the HTTP client
Bot: Working... (8s) Read: http_client.py I'll add a retry decorator with exponential backoff Edit: http_client.py Bash: poetry run pytest tests/ -v
Bot: [Claude shows the changes and test results] You: /verbose 0
Bot: Verbosity set to 0 (quiet) Use /verbose 0|1|2 to control how much background activity is shown: Level Shows 0 (quiet) Final response only (typing indicator stays active) 1 (normal, default) Tool names + reasoning snippets in real-time 2 (detailed) Tool names with inputs + longer reasoning text GitHub Workflow Claude Code already knows how to use gh CLI and git. Authenticate on your server with gh auth login, then work with repos conversationally: You: List my repos related to monitoring
Bot: [Claude runs gh repo list, shows results] You: Clone the uptime one
Bot: [Claude runs gh repo clone, clones into workspace] You: /repo
Bot: uptime-monitor/ other-project/ You: Show me the open issues
Bot: [Claude runs gh issue list] You: Create a fix branch and push it
Bot: [Claude creates branch, commits, pushes] Use /repo to list cloned repos in your workspace, or /repo to switch directories (sessions auto-resume). Classic Mode Set AGENTIC_MODE=false to enable the full 13-command terminal-like interface with directory navigation, inline keyboards, quick actions, git integration, and session export. Commands: /start, /help, /new, /continue, /end, /status, /cd, /ls, /pwd, /projects, /export, /actions, /git If ENABLE_PROJECT_THREADS=true: /sync_threads You: /cd my-web-app
Bot: Directory changed to my-web-app/ You: /ls
Bot: src/ tests/ package.json README.md You: /actions
Bot: [Run Tests] [Install Deps] [Format Code] [Run Linter] Event-Driven Automation Beyond direct chat, the bot can respond to external triggers: Webhooks -- Receive GitHub events (push, PR, issues) and route them through Claude for automated summaries or code review Scheduler -- Run recurring Claude tasks on a cron schedule (e.g., daily code health checks) Notifications -- Deliver agent responses to configured Telegram chats Enable with ENABLE_API_SERVER=true and ENABLE_SCHEDULER=true. See docs/setup.md for configuration. Features Working Features Conversational agentic mode (default) with natural language interaction Classic terminal-like mode with 13 commands and inline keyboards Full Claude Code integration with SDK (primary) and CLI (fallback) Automatic session persistence per user/project directory Multi-layer authentication (whitelist + optional token-based) Rate limiting with token bucket algorithm Directory sandboxing with path traversal prevention File upload handling with archive extraction Image/screenshot upload with analysis Git integration with safe repository operations Quick actions system with context-aware buttons Session export in Markdown, HTML, and JSON formats SQLite persistence with migrations Usage and cost tracking Audit logging and security event tracking Event bus for decoupled message routing Webhook API server (GitHub HMAC-SHA256, generic Bearer token auth) Job scheduler with cron expressions and persistent storage Notification service with per-chat rate limiting Tunable verbose output showing Claude's tool usage and reasoning in real-time Persistent typing indicator so users always know the bot is working 16 configurable tools with allowlist/disallowlist control (see docs/tools.md) Planned Enhancements Plugin system for third-party extensions Configuration Required TELEGRAM_BOT_TOKEN=... # From @BotFather
TELEGRAM_BOT_USERNAME=... # Your bot's username
APPROVED_DIRECTORY=... # Base directory for project access
ALLOWED_USERS=123456789 # Comma-separated Telegram user IDs Common Options # Claude
ANTHROPIC_API_KEY=sk-ant-... # API key (optional if using CLI auth)
CLAUDE_MAX_COST_PER_USER=10.0 # Spending limit per user (USD)
CLAUDE_TIMEOUT_SECONDS=300 # Operation timeout # Mode
AGENTIC_MODE=true # Agentic (default) or classic mode
VERBOSE_LEVEL=1 # 0=quiet, 1=normal (default), 2=detailed # Rate Limiting
RATE_LIMIT_REQUESTS=10 # Requests per window
RATE_LIMIT_WINDOW=60 # Window in seconds # Features (classic mode)
ENABLE_GIT_INTEGRATION=true
ENABLE_FILE_UPLOADS=true
ENABLE_QUICK_ACTIONS=true Agentic Platform # Webhook API Server
ENABLE_API_SERVER=false # Enable FastAPI webhook server
API_SERVER_PORT=8080 # Server port # Webhook Authentication
GITHUB_WEBHOOK_SECRET=... # GitHub HMAC-SHA256 secret
WEBHOOK_API_SECRET=... # Bearer token for generic providers # Scheduler
ENABLE_SCHEDULER=false # Enable cron job scheduler # Notifications
NOTIFICATION_CHAT_IDS=123,456 # Default chat IDs for proactive notifications Project Threads Mode # Enable strict topic routing by project
ENABLE_PROJECT_THREADS=true # Mode: private (default) or group
PROJECT_THREADS_MODE=private # YAML registry file (see config/projects.example.yaml)
PROJECTS_CONFIG_PATH=config/projects.yaml # Required only when PROJECT_THREADS_MODE=group
PROJECT_THREADS_CHAT_ID=-1001234567890 # Minimum delay (seconds) between Telegram API calls during topic sync
# Set 0 to disable pacing
PROJECT_THREADS_SYNC_ACTION_INTERVAL_SECONDS=1.1 In strict mode, only /start and /sync_threads work outside mapped project topics. In private mode, /start auto-syncs project topics for your private bot chat. To use topics with your bot, enable them in BotFather: Bot Settings -&gt; Threaded mode. Full reference: See docs/configuration.md and .env.example. Finding Your Telegram User ID Message @userinfobot on Telegram -- it will reply with your user ID number. Troubleshooting Bot doesn't respond: Check your TELEGRAM_BOT_TOKEN is correct Verify your user ID is in ALLOWED_USERS Ensure Claude Code CLI is installed and accessible Check bot logs with make run-debug Claude integration not working: SDK mode (default): Check claude auth status or verify ANTHROPIC_API_KEY CLI mode: Verify claude --version and claude auth status Check CLAUDE_ALLOWED_TOOLS includes necessary tools (see docs/tools.md for the full reference) High usage costs: Adjust CLAUDE_MAX_COST_PER_USER to set spending limits Monitor usage with /status Use shorter, more focused requests Security This bot implements defense-in-depth security: Access Control -- Whitelist-based user authentication Directory Isolation -- Sandboxing to approved directories Rate Limiting -- Request and cost-based limits Input Validation -- Injection and path traversal protection Webhook Authentication -- GitHub HMAC-SHA256 and Bearer token verification Audit Logging -- Complete tracking of all user actions See SECURITY.md for details. Development make dev # Install all dependencies
make test # Run tests with coverage
make lint # Black + isort + flake8 + mypy
make format # Auto-format code
make run-debug # Run with debug logging Full documentation: See the docs index for all guides and references. Version Management The version is defined once in pyproject.toml and read at runtime via importlib.metadata. To cut a release: make bump-patch # 1.2.0 -&gt; 1.2.1 (bug fixes)
make bump-minor # 1.2.0 -&gt; 1.3.0 (new features)
make bump-major # 1.2.0 -&gt; 2.0.0 (breaking changes) Each command commits, tags, and pushes automatically, triggering CI tests and a GitHub Release with auto-generated notes. Contributing Fork the repository Create a feature branch: git checkout -b feature/amazing-feature Make changes with tests: make test &amp;&amp; make lint Submit a Pull Request Code standards: Python 3.11+, Black formatting (88 chars), type hints required, pytest with &gt;85% coverage. License MIT License -- see LICENSE. Acknowledgments Claude by Anthropic python-telegram-bot]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/RichardAtCT/claude-code-telegram</guid>
    </item>
    <item>
      <title><![CDATA[agno-agi/agno]]></title>
      <link>https://github.com/agno-agi/agno</link>
      <description><![CDATA[The programming language for agentic software. Build, run, and manage multi-agent systems at scale. The programming language for agentic software. Build, run, and manage multi-agent systems at scale. Docs • Cookbook • Quickstart • Discord What is Agno? Software is shifting from deterministic request–response to reasoning systems that plan, call tools, remember context, and make decisions. Agno is the language for building that software correctly. It provides: Layer Responsibility SDK Agents, teams, workflows, memory, knowledge, tools, guardrails, approval flows Engine Model calls, tool orchestration, structured outputs, runtime enforcement AgentOS Streaming APIs, isolation, auth, approval enforcement, tracing, control plane Quick Start Build a stateful, tool-using agent and serve it as a production API in ~20 lines. from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.mcp import MCPTools agno_assist = Agent( name="Agno Assist", model=Claude(id="claude-sonnet-4-6"), db=SqliteDb(db_file="agno.db"), tools=[MCPTools(url="https://docs.agno.com/mcp")], add_history_to_context=True, num_history_runs=3, markdown=True,
) agent_os = AgentOS(agents=[agno_assist], tracing=True)
app = agent_os.get_app() Run it: export ANTHROPIC_API_KEY="***" uvx --python 3.12 \ --with "agno[os]" \ --with anthropic \ --with mcp \ fastapi dev agno_assist.py In ~20 lines, you get: A stateful agent with streaming responses Per-user, per-session isolation A production API at http://localhost:8000 Native tracing Connect to the AgentOS UI to monitor, manage, and test your agents. Open os.agno.com and sign in. Click "Add new OS" in the top navigation. Select "Local" to connect to a local AgentOS. Enter your endpoint URL (default: http://localhost:8000). Name it "Local AgentOS". Click "Connect". https://github.com/user-attachments/assets/75258047-2471-4920-8874-30d68c492683 Open Chat, select your agent, and ask: What is Agno? The agent retrieves context from the Agno MCP server and responds with grounded answers. https://github.com/user-attachments/assets/24c28d28-1d17-492c-815d-810e992ea8d2 You can use this exact same architecture for running multi-agent systems in production. Why Agno? Agentic software introduces three fundamental shifts. A new interaction model Traditional software receives a request and returns a response. Agents stream reasoning, tool calls, and results in real time. They can pause mid-execution, wait for approval, and resume later. Agno treats streaming and long-running execution as first-class behavior. A new governance model Traditional systems execute predefined decision logic written in advance. Agents choose actions dynamically. Some actions are low risk. Some require user approval. Some require administrative authority. Agno lets you define who decides what as part of the agent definition, with: Approval workflows Human-in-the-loop Audit logs Enforcement at runtime A new trust model Traditional systems are designed to be predictable. Every execution path is defined in advance. Agents introduce probabilistic reasoning into the execution path. Agno builds trust into the engine itself: Guardrails run as part of execution Evaluations integrate into the agent loop Traces and audit logs are first-class Built for Production Agno runs in your infrastructure, not ours. Stateless, horizontally scalable runtime. 50+ APIs and background execution. Per-user and per-session isolation. Runtime approval enforcement. Native tracing and full auditability. Sessions, memory, knowledge, and traces stored in your database. You own the system. You own the data. You define the rules. What You Can Build Agno powers real agentic systems built from the same primitives above. Pal → A personal agent that learns your preferences. Dash → A self-learning data agent grounded in six layers of context. Scout → A self-learning context agent that manages enterprise context knowledge. Gcode → A post-IDE coding agent that improves over time. Investment Team → A multi-agent investment committee that debates and allocates capital. Single agents. Coordinated teams. Structured workflows. All built on one architecture. Get Started Read the docs Build your first agent Explore the cookbook IDE Integration Add Agno docs as a source in your coding tools: Cursor: Settings → Indexing &amp; Docs → Add https://docs.agno.com/llms-full.txt Also works with VSCode, Windsurf, and similar tools. Contributing See the contributing guide. Telemetry Agno logs which model providers are used to prioritize updates. Disable with AGNO_TELEMETRY=false. ↑ Back to top]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/agno-agi/agno</guid>
    </item>
    <item>
      <title><![CDATA[opengeos/geoai]]></title>
      <link>https://github.com/opengeos/geoai</link>
      <description><![CDATA[GeoAI: Artificial Intelligence for Geospatial Data GeoAI: Artificial Intelligence for Geospatial Data A powerful Python package for integrating artificial intelligence with geospatial data analysis and visualization Introduction GeoAI is a comprehensive Python package designed to bridge artificial intelligence (AI) and geospatial data analysis, providing researchers and practitioners with intuitive tools for applying machine learning techniques to geographic data. The package offers a unified framework for processing satellite imagery, aerial photographs, and vector data using state-of-the-art deep learning models. GeoAI integrates popular AI frameworks including PyTorch, Transformers, PyTorch Segmentation Models, and specialized geospatial libraries like torchange, enabling users to perform complex geospatial analyses with minimal code. The package provides five core capabilities: Interactive and programmatic search and download of remote sensing imagery and geospatial data. Automated dataset preparation with image chips and label generation. Model training for tasks such as classification, detection, and segmentation. Inference pipelines for applying models to new geospatial datasets. Interactive visualization through integration with Leafmap and MapLibre. Seamless QGIS integration via a dedicated GeoAI plugin, enabling users to run AI-powered geospatial workflows directly within the QGIS desktop environment, without writing code. GeoAI addresses the growing demand for accessible AI tools in geospatial research by providing high-level APIs that abstract complex machine learning workflows while maintaining flexibility for advanced users. The package supports multiple data formats (GeoTIFF, JPEG2000, GeoJSON, Shapefile, GeoPackage) and includes automatic device management for GPU acceleration when available. With over 10 modules and extensive notebook examples, GeoAI serves as both a research tool and educational resource for the geospatial AI community. Statement of Need The integration of artificial intelligence with geospatial data analysis has become increasingly critical across numerous scientific disciplines, from environmental monitoring and urban planning to disaster response and climate research. However, applying AI techniques to geospatial data presents unique challenges including data preprocessing complexities, specialized model architectures, and the need for domain-specific knowledge in both machine learning and geographic information systems. Existing solutions often require researchers to navigate fragmented ecosystems of tools, combining general-purpose machine learning libraries with specialized geospatial packages, leading to steep learning curves and reproducibility challenges. While packages like TorchGeo, TerraTorch, and SRAI provide excellent foundational tools for geospatial deep learning, there remains a gap for comprehensive, high-level interfaces that can democratize access to advanced AI techniques for the broader geospatial community. GeoAI addresses this need by providing a unified, user-friendly interface that abstracts the complexity of integrating multiple AI frameworks with geospatial data processing workflows. It lowers barriers for: (1) geospatial researchers who need accessible AI workflows without deep ML expertise; (2) AI practitioners who want streamlined geospatial preprocessing and domain-specific datasets; and (3) educators seeking reproducible examples and teaching-ready workflows. The package's design philosophy emphasizes simplicity without sacrificing functionality, enabling users to perform sophisticated analyses such as building footprint extraction from satellite imagery, land cover classification, and change detection with just a few lines of code. By integrating cutting-edge AI models and providing seamless access to major geospatial data sources, GeoAI significantly lowers the barrier to entry for geospatial AI applications while maintaining the flexibility needed for advanced research applications. Citations If you find GeoAI useful in your research, please consider citing the following paper to support my work. Thank you for your support. Wu, Q., (2026). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. Journal of Open Source Software, 11(118), 9605, https://doi.org/10.21105/joss.09605. Key Features Advanced Geospatial Data Visualization Interactive multi-layer visualization of vector and raster data stored locally or in cloud storage Customizable styling and symbology Time-series data visualization capabilities Data Preparation &amp; Processing Streamlined access to satellite and aerial imagery from providers like Sentinel, Landsat, NAIP, and other open datasets Tools for downloading, mosaicking, and preprocessing remote sensing data Automated generation of training datasets with image chips and corresponding labels Vector-to-raster and raster-to-vector conversion utilities optimized for AI workflows Data augmentation techniques specific to geospatial data Support for integrating Overture Maps data and other open datasets for training and validation Image Segmentation Integration with PyTorch Segmentation Models for automatic feature extraction Specialized segmentation algorithms optimized for satellite and aerial imagery Streamlined workflows for segmenting buildings, water bodies, wetlands, solar panels, etc. Export capabilities to standard geospatial formats (GeoJSON, Shapefile, GeoPackage, GeoParquet) Image Classification Pre-trained models for land cover and land use classification Transfer learning utilities for fine-tuning models with your own data Multi-temporal classification support for change detection Accuracy assessment and validation tools Additional Capabilities Change detection with AI-enhanced feature extraction Object detection in aerial and satellite imagery Georeferencing utilities for AI model outputs Installation Using pip pip install geoai-py Using conda conda install -c conda-forge geoai Using mamba mamba install -c conda-forge geoai QGIS Plugin Check out the QGIS Plugin page if you are interested in using GeoAI with QGIS. Documentation Comprehensive documentation is available at https://opengeoai.org, including: Detailed API reference Tutorials and example notebooks Contributing guide Video Tutorials GeoAI Made Easy: Learn the Python Package Step-by-Step (Beginner Friendly) GeoAI Workshop: Unlocking the Power of GeoAI with Python GeoAI Tutorials Playlist Contributing We welcome contributions of all kinds! See our contributing guide for ways to get started. License GeoAI is free and open source software, licensed under the MIT License. Acknowledgments We gratefully acknowledge the support of the following organizations: NASA: This research is partially supported by the National Aeronautics and Space Administration (NASA) through Grant No. 80NSSC22K1742, awarded under the Open Source Tools, Frameworks, and Libraries Program. AmericaView: This work is also partially supported by the U.S. Geological Survey through Grant/Cooperative Agreement No. G23AP00683 (GY23-GY27) in collaboration with AmericaView.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/opengeos/geoai</guid>
    </item>
    <item>
      <title><![CDATA[abhigyanpatwari/GitNexus]]></title>
      <link>https://github.com/abhigyanpatwari/GitNexus</link>
      <description><![CDATA[GitNexus: The Zero-Server Code Intelligence Engine - GitNexus is a client-side knowledge graph creator that runs entirely in your browser. Drop in a GitHub repo or ZIP file, and get an interactive knowledge graph wit a built in Graph RAG Agent. Perfect for code exploration GitNexus Building git for agent context. Indexes any codebase into a knowledge graph — every dependency, call chain, cluster, and execution flow — then exposes it through smart tools so AI agents never miss code. https://github.com/user-attachments/assets/172685ba-8e54-4ea7-9ad1-e31a3398da72 Like DeepWiki, but deeper. DeepWiki helps you understand code. GitNexus lets you analyze it — because a knowledge graph tracks every relationship, not just descriptions. TL;DR: The Web UI is a quick way to chat with any repo. The CLI + MCP is how you make your AI agent actually reliable — it gives Cursor, Claude Code, and friends a deep architectural view of your codebase so they stop missing dependencies, breaking call chains, and shipping blind edits. Even smaller models get full architectural clarity, making it compete with goliath models. Star History Two Ways to Use GitNexus CLI + MCP Web UI What Index repos locally, connect AI agents via MCP Visual graph explorer + AI chat in browser For Daily development with Cursor, Claude Code, Windsurf, OpenCode Quick exploration, demos, one-off analysis Scale Full repos, any size Limited by browser memory (~5k files) Install npm install -g gitnexus No install —gitnexus.vercel.app Storage KuzuDB native (fast, persistent) KuzuDB WASM (in-memory, per session) Parsing Tree-sitter native bindings Tree-sitter WASM Privacy Everything local, no network Everything in-browser, no server CLI + MCP ( ) The CLI indexes your repository and runs an MCP server that gives AI agents deep codebase awareness. Quick Start # Index your repo (run from repo root)
npx gitnexus analyze That's it. This indexes the codebase, installs agent skills, registers Claude Code hooks, and creates AGENTS.md / CLAUDE.md context files — all in one command. To configure MCP for your editor, run npx gitnexus setup once — or set it up manually below. MCP Setup gitnexus setup auto-detects your editors and writes the correct global MCP config. You only need to run it once. Editor Support Editor MCP Skills Hooks (auto-augment) Support Claude Code Yes Yes Yes (PreToolUse) Full Cursor Yes Yes — MCP + Skills Windsurf Yes — — MCP OpenCode Yes Yes — MCP + Skills Claude Code gets the deepest integration: MCP tools + agent skills + PreToolUse hooks that automatically enrich grep/glob/bash calls with knowledge graph context. If you prefer manual configuration: Claude Code (full support — MCP + skills + hooks): claude mcp add gitnexus -- npx -y gitnexus@latest mcp Cursor (~/.cursor/mcp.json — global, works for all projects): { "mcpServers": { "gitnexus": { "command": "npx", "args": ["-y", "gitnexus@latest", "mcp"] } }
} OpenCode (~/.config/opencode/config.json): { "mcp": { "gitnexus": { "command": "npx", "args": ["-y", "gitnexus@latest", "mcp"] } }
} CLI Commands gitnexus setup # Configure MCP for your editors (one-time)
gitnexus analyze [path] # Index a repository (or update stale index)
gitnexus analyze --force # Force full re-index
gitnexus analyze --skip-embeddings # Skip embedding generation (faster)
gitnexus mcp # Start MCP server (stdio) — serves all indexed repos
gitnexus serve # Start HTTP server for web UI connection
gitnexus list # List all indexed repositories
gitnexus status # Show index status for current repo
gitnexus clean # Delete index for current repo
gitnexus clean --all --force # Delete all indexes
gitnexus wiki [path] # Generate repository wiki from knowledge graph
gitnexus wiki --model # Wiki with custom LLM model (default: gpt-4o-mini)
gitnexus wiki --base-url # Wiki with custom LLM API base URL What Your AI Agent Gets 7 tools exposed via MCP: Tool What It Does repo Param list_repos Discover all indexed repositories — query Process-grouped hybrid search (BM25 + semantic + RRF) Optional context 360-degree symbol view — categorized refs, process participation Optional impact Blast radius analysis with depth grouping and confidence Optional detect_changes Git-diff impact — maps changed lines to affected processes Optional rename Multi-file coordinated rename with graph + text search Optional cypher Raw Cypher graph queries Optional When only one repo is indexed, the repo parameter is optional. With multiple repos, specify which one: query({query: "auth", repo: "my-app"}). Resources for instant context: Resource Purpose gitnexus://repos List all indexed repositories (read this first) gitnexus://repo/{name}/context Codebase stats, staleness check, and available tools gitnexus://repo/{name}/clusters All functional clusters with cohesion scores gitnexus://repo/{name}/cluster/{name} Cluster members and details gitnexus://repo/{name}/processes All execution flows gitnexus://repo/{name}/process/{name} Full process trace with steps gitnexus://repo/{name}/schema Graph schema for Cypher queries 2 MCP prompts for guided workflows: Prompt What It Does detect_impact Pre-commit change analysis — scope, affected processes, risk level generate_map Architecture documentation from the knowledge graph with mermaid diagrams 4 agent skills installed to .claude/skills/ automatically: Exploring — Navigate unfamiliar code using the knowledge graph Debugging — Trace bugs through call chains Impact Analysis — Analyze blast radius before changes Refactoring — Plan safe refactors using dependency mapping Multi-Repo MCP Architecture GitNexus uses a global registry so one MCP server can serve multiple indexed repos. No per-project MCP config needed — set it up once and it works everywhere. flowchart TD subgraph CLI [CLI Commands] Setup["gitnexus setup"] Analyze["gitnexus analyze"] Clean["gitnexus clean"] List["gitnexus list"] end subgraph Registry ["~/.gitnexus/"] RegFile["registry.json"] end subgraph Repos [Project Repos] RepoA[".gitnexus/ in repo A"] RepoB[".gitnexus/ in repo B"] end subgraph MCP [MCP Server] Server["server.ts"] Backend["LocalBackend"] Pool["Connection Pool"] ConnA["KuzuDB conn A"] ConnB["KuzuDB conn B"] end Setup --&gt;|"writes global MCP config"| CursorConfig["~/.cursor/mcp.json"] Analyze --&gt;|"registers repo"| RegFile Analyze --&gt;|"stores index"| RepoA Clean --&gt;|"unregisters repo"| RegFile List --&gt;|"reads"| RegFile Server --&gt;|"reads registry"| RegFile Server --&gt; Backend Backend --&gt; Pool Pool --&gt;|"lazy open"| ConnA Pool --&gt;|"lazy open"| ConnB ConnA --&gt;|"queries"| RepoA ConnB --&gt;|"queries"| RepoB How it works: Each gitnexus analyze stores the index in .gitnexus/ inside the repo (portable, gitignored) and registers a pointer in ~/.gitnexus/registry.json. When an AI agent starts, the MCP server reads the registry and can serve any indexed repo. KuzuDB connections are opened lazily on first query and evicted after 5 minutes of inactivity (max 5 concurrent). If only one repo is indexed, the repo parameter is optional on all tools — agents don't need to change anything. Web UI (browser-based) A fully client-side graph explorer and AI chat. No server, no install — your code never leaves the browser. Try it now: gitnexus.vercel.app — drag &amp; drop a ZIP and start exploring. Or run locally: git clone https://github.com/abhigyanpatwari/gitnexus.git
cd gitnexus/gitnexus-web
npm install
npm run dev The web UI uses the same indexing pipeline as the CLI but runs entirely in WebAssembly (Tree-sitter WASM, KuzuDB WASM, in-browser embeddings). It's great for quick exploration but limited by browser memory for larger repos. The Problem GitNexus Solves Tools like Cursor, Claude Code, Cline, Roo Code, and Windsurf are powerful — but they don't truly know your codebase structure. What happens: AI edits UserService.validate() Doesn't know 47 functions depend on its return type Breaking changes ship Traditional Graph RAG vs GitNexus Traditional approaches give the LLM raw graph edges and hope it explores enough. GitNexus precomputes structure at index time — clustering, tracing, scoring — so tools return complete context in one call: flowchart TB subgraph Traditional["Traditional Graph RAG"] direction TB U1["User: What depends on UserService?"] U1 --&gt; LLM1["LLM receives raw graph"] LLM1 --&gt; Q1["Query 1: Find callers"] Q1 --&gt; Q2["Query 2: What files?"] Q2 --&gt; Q3["Query 3: Filter tests?"] Q3 --&gt; Q4["Query 4: High-risk?"] Q4 --&gt; OUT1["Answer after 4+ queries"] end subgraph GN["GitNexus Smart Tools"] direction TB U2["User: What depends on UserService?"] U2 --&gt; TOOL["impact UserService upstream"] TOOL --&gt; PRECOMP["Pre-structured response: 8 callers, 3 clusters, all 90%+ confidence"] PRECOMP --&gt; OUT2["Complete answer, 1 query"] end Core innovation: Precomputed Relational Intelligence Reliability — LLM can't miss context, it's already in the tool response Token efficiency — No 10-query chains to understand one function Model democratization — Smaller LLMs work because tools do the heavy lifting How It Works GitNexus builds a complete knowledge graph of your codebase through a multi-phase indexing pipeline: Structure — Walks the file tree and maps folder/file relationships Parsing — Extracts functions, classes, methods, and interfaces using Tree-sitter ASTs Resolution — Resolves imports and function calls across files with language-aware logic Clustering — Groups related symbols into functional communities Processes — Traces execution flows from entry points through call chains Search — Builds hybrid search indexes for fast retrieval Supported Languages TypeScript, JavaScript, Python, Java, C, C++, C#, Go, Rust Tool Examples Impact Analysis impact({target: "UserService", direction: "upstream", minConfidence: 0.8}) TARGET: Class UserService (src/services/user.ts) UPSTREAM (what depends on this): Depth 1 (WILL BREAK): handleLogin [CALLS 90%] -&gt; src/api/auth.ts:45 handleRegister [CALLS 90%] -&gt; src/api/auth.ts:78 UserController [CALLS 85%] -&gt; src/controllers/user.ts:12 Depth 2 (LIKELY AFFECTED): authRouter [IMPORTS] -&gt; src/routes/auth.ts Options: maxDepth, minConfidence, relationTypes (CALLS, IMPORTS, EXTENDS, IMPLEMENTS), includeTests Process-Grouped Search query({query: "authentication middleware"}) processes: - summary: "LoginFlow" priority: 0.042 symbol_count: 4 process_type: cross_community step_count: 7 process_symbols: - name: validateUser type: Function filePath: src/auth/validate.ts process_id: proc_login step_index: 2 definitions: - name: AuthConfig type: Interface filePath: src/types/auth.ts Context (360-degree Symbol View) context({name: "validateUser"}) symbol: uid: "Function:validateUser" kind: Function filePath: src/auth/validate.ts startLine: 15 incoming: calls: [handleLogin, handleRegister, UserController] imports: [authRouter] outgoing: calls: [checkPassword, createSession] processes: - name: LoginFlow (step 2/7) - name: RegistrationFlow (step 3/5) Detect Changes (Pre-Commit) detect_changes({scope: "all"}) summary: changed_count: 12 affected_count: 3 changed_files: 4 risk_level: medium changed_symbols: [validateUser, AuthService, ...]
affected_processes: [LoginFlow, RegistrationFlow, ...] Rename (Multi-File) rename({symbol_name: "validateUser", new_name: "verifyUser", dry_run: true}) status: success
files_affected: 5
total_edits: 8
graph_edits: 6 (high confidence)]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:18 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/abhigyanpatwari/GitNexus</guid>
    </item>
    <item>
      <title><![CDATA[cloudflare/agents]]></title>
      <link>https://github.com/cloudflare/agents</link>
      <description><![CDATA[Build and deploy AI Agents on Cloudflare Cloudflare Agents Agents are persistent, stateful execution environments for agentic workloads, powered by Cloudflare Durable Objects. Each agent has its own state, storage, and lifecycle — with built-in support for real-time communication, scheduling, AI model calls, MCP, workflows, and more. Agents hibernate when idle and wake on demand. You can run millions of them — one per user, per session, per game room — each costs nothing when inactive. npm create cloudflare@latest -- --template cloudflare/agents-starter Or add to an existing project: npm install agents Read the docs — getting started, API reference, guides, and more. Quick Example A counter agent with persistent state, callable methods, and real-time sync to a React frontend: // server.ts
import { Agent, routeAgentRequest, callable } from "agents"; export type CounterState = { count: number }; export class CounterAgent extends Agent { initialState = { count: 0 }; @callable() increment() { this.setState({ count: this.state.count + 1 }); return this.state.count; } @callable() decrement() { this.setState({ count: this.state.count - 1 }); return this.state.count; }
} export default { async fetch(request: Request, env: Env, ctx: ExecutionContext) { return ( (await routeAgentRequest(request, env)) ?? new Response("Not found", { status: 404 }) ); }
}; // client.tsx
import { useAgent } from "agents/react";
import { useState } from "react";
import type { CounterAgent, CounterState } from "./server"; function Counter() { const [count, setCount] = useState(0); const agent = useAgent({ agent: "CounterAgent", onStateUpdate: (state) =&gt; setCount(state.count) }); return ( {count} agent.stub.increment()}&gt;+ agent.stub.decrement()}&gt;- );
} State changes sync to all connected clients automatically. Call methods like they're local functions. Features Feature Description Persistent State Syncs to all connected clients, survives restarts Callable Methods Type-safe RPC via the @callable() decorator Scheduling One-time, recurring, and cron-based tasks WebSockets Real-time bidirectional communication with lifecycle hooks AI Chat Message persistence, resumable streaming, server/client tool execution MCP Act as MCP servers or connect as MCP clients Workflows Durable multi-step tasks with human-in-the-loop approval Email Receive and respond via Cloudflare Email Routing Code Mode LLMs generate executable TypeScript instead of individual tool calls SQL Direct SQLite queries via Durable Objects React Hooks useAgent and useAgentChat for frontend integration Vanilla JS Client AgentClient for non-React environments Coming soon: Realtime voice agents, web browsing (headless browser), sandboxed code execution, and multi-channel communication (SMS, messengers). Packages Package Description agents Core SDK — Agent class, routing, state, scheduling, MCP, email, workflows @cloudflare/ai-chat Higher-level AI chat — persistent messages, resumable streaming, tool execution hono-agents Hono middleware for adding agents to Hono apps @cloudflare/codemode Experimental — LLMs write executable code to orchestrate tools Examples The examples/ directory has self-contained demos covering most SDK features — MCP servers/clients, workflows, email agents, webhooks, tic-tac-toe, resumable streaming, and more. The playground is the kitchen-sink showcase with everything in one UI. There are also examples using the OpenAI Agents SDK in openai-sdk/. Run any example locally: cd examples/playground
npm run dev Documentation Full docs on developers.cloudflare.com docs/ directory in this repo (synced upstream) Anthropic Patterns guide — sequential, routing, parallel, orchestrator, evaluator Human-in-the-Loop guide — approval workflows with pause/resume Repository Structure Directory Description packages/agents/ Core SDK packages/ai-chat/ AI chat layer packages/hono-agents/ Hono integration packages/codemode/ Code Mode (experimental) examples/ Self-contained demo apps openai-sdk/ Examples using the OpenAI Agents SDK guides/ In-depth pattern tutorials docs/ Markdown docs synced to developers.cloudflare.com site/ Deployed websites (agents.cloudflare.com, AI playground) design/ Architecture and design decision records scripts/ Repo-wide tooling Development Node 24+ required. Uses npm workspaces. npm install # install all workspaces
npm run build # build all packages
npm run check # full CI check (format, lint, typecheck, exports)
CI=true npm test # run tests (vitest + vitest-pool-workers) Changes to packages/ need a changeset: npx changeset See AGENTS.md for deeper contributor guidance. License MIT]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/cloudflare/agents</guid>
    </item>
    <item>
      <title><![CDATA[wagtail/wagtail]]></title>
      <link>https://github.com/wagtail/wagtail</link>
      <description><![CDATA[A Django content management system focused on flexibility and user experience Wagtail is an open source content management system built on Django, with a strong community and commercial support. It's focused on user experience, and offers precise control for designers and developers. Features A fast, attractive interface for authors Complete control over front-end design and structure Scales to millions of pages and thousands of editors Fast out of the box, cache-friendly when you need it Content API for 'headless' sites with decoupled front-end Runs on a Raspberry Pi or a multi-datacenter cloud platform StreamField encourages flexible content without compromising structure Powerful, integrated search, using Elasticsearch or PostgreSQL Excellent support for images and embedded content Multi-site and multi-language ready Embraces and extends Django Find out more at wagtail.org. Getting started Wagtail works with Python 3, on any platform. To get started with using Wagtail, run the following in a virtual environment: pip install wagtail
wagtail start mysite
cd mysite
pip install -r requirements.txt
python manage.py migrate
python manage.py createsuperuser]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:18 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/wagtail/wagtail</guid>
    </item>
    <item>
      <title><![CDATA[ahujasid/blender-mcp]]></title>
      <link>https://github.com/ahujasid/blender-mcp</link>
      <description><![CDATA[BlenderMCP - Blender Model Context Protocol Integration BlenderMCP connects Blender to Claude AI through the Model Context Protocol (MCP), allowing Claude to directly interact with and control Blender. This integration enables prompt assisted 3D modeling, scene creation, and manipulation. We have no official website. Any website you see online is unofficial and has no affiliation with this project. Use them at your own risk. Full tutorial Join the Community Give feedback, get inspired, and build on top of the MCP: Discord Supporters CodeRabbit All supporters: Support this project Current version(1.5.5) Added Hunyuan3D support View screenshots for Blender viewport to better understand the scene Search and download Sketchfab models Support for Poly Haven assets through their API Support to generate 3D models using Hyper3D Rodin Run Blender MCP on a remote host Telemetry for tools executed (completely anonymous) Installating a new version (existing users) For newcomers, you can go straight to Installation. For existing users, see the points below Download the latest addon.py file and replace the older one, then add it to Blender Delete the MCP server from Claude and add it back again, and you should be good to go! Features Two-way communication: Connect Claude AI to Blender through a socket-based server Object manipulation: Create, modify, and delete 3D objects in Blender Material control: Apply and modify materials and colors Scene inspection: Get detailed information about the current Blender scene Code execution: Run arbitrary Python code in Blender from Claude Components The system consists of two main components: Blender Addon (addon.py): A Blender addon that creates a socket server within Blender to receive and execute commands MCP Server (src/blender_mcp/server.py): A Python server that implements the Model Context Protocol and connects to the Blender addon Installation Prerequisites Blender 3.0 or newer Python 3.10 or newer uv package manager: If you're on Mac, please install uv as brew install uv On Windows powershell -c "irm https://astral.sh/uv/install.ps1 | iex" and then add uv to the user path in Windows (you may need to restart Claude Desktop after): $localBin = "$env:USERPROFILE\.local\bin"
$userPath = [Environment]::GetEnvironmentVariable("Path", "User")
[Environment]::SetEnvironmentVariable("Path", "$userPath;$localBin", "User") Otherwise installation instructions are on their website: Install uv Do not proceed before installing UV Environment Variables The following environment variables can be used to configure the Blender connection: BLENDER_HOST: Host address for Blender socket server (default: "localhost") BLENDER_PORT: Port number for Blender socket server (default: 9876) Example: export BLENDER_HOST='host.docker.internal'
export BLENDER_PORT=9876 Claude for Desktop Integration Watch the setup instruction video (Assuming you have already installed uv) Go to Claude &gt; Settings &gt; Developer &gt; Edit Config &gt; claude_desktop_config.json to include the following: { "mcpServers": { "blender": { "command": "uvx", "args": [ "blender-mcp" ] } }
} Claude Code Use the Claude Code CLI to add the blender MCP server: claude mcp add blender uvx blender-mcp Cursor integration For Mac users, go to Settings &gt; MCP and paste the following To use as a global server, use "add new global MCP server" button and paste To use as a project specific server, create .cursor/mcp.json in the root of the project and paste { "mcpServers": { "blender": { "command": "uvx", "args": [ "blender-mcp" ] } }
} For Windows users, go to Settings &gt; MCP &gt; Add Server, add a new server with the following settings: { "mcpServers": { "blender": { "command": "cmd", "args": [ "/c", "uvx", "blender-mcp" ] } }
} Cursor setup video Only run one instance of the MCP server (either on Cursor or Claude Desktop), not both Visual Studio Code Integration Prerequisites: Make sure you have Visual Studio Code installed before proceeding. Installing the Blender Addon Download the addon.py file from this repo Open Blender Go to Edit &gt; Preferences &gt; Add-ons Click "Install..." and select the addon.py file Enable the addon by checking the box next to "Interface: Blender MCP" Usage Starting the Connection In Blender, go to the 3D View sidebar (press N if not visible) Find the "BlenderMCP" tab Turn on the Poly Haven checkbox if you want assets from their API (optional) Click "Connect to Claude" Make sure the MCP server is running in your terminal Using with Claude Once the config file has been set on Claude, and the addon is running on Blender, you will see a hammer icon with tools for the Blender MCP. Capabilities Get scene and object information Create, delete and modify shapes Apply or create materials for objects Execute any Python code in Blender Download the right models, assets and HDRIs through Poly Haven AI generated 3D models through Hyper3D Rodin Example Commands Here are some examples of what you can ask Claude to do: "Create a low poly scene in a dungeon, with a dragon guarding a pot of gold" Demo "Create a beach vibe using HDRIs, textures, and models like rocks and vegetation from Poly Haven" Demo Give a reference image, and create a Blender scene out of it Demo "Generate a 3D model of a garden gnome through Hyper3D" "Get information about the current scene, and make a threejs sketch from it" Demo "Make this car red and metallic" "Create a sphere and place it above the cube" "Make the lighting like a studio" "Point the camera at the scene, and make it isometric" Hyper3D integration Hyper3D's free trial key allows you to generate a limited number of models per day. If the daily limit is reached, you can wait for the next day's reset or obtain your own key from hyper3d.ai and fal.ai. Troubleshooting Connection issues: Make sure the Blender addon server is running, and the MCP server is configured on Claude, DO NOT run the uvx command in the terminal. Sometimes, the first command won't go through but after that it starts working. Timeout errors: Try simplifying your requests or breaking them into smaller steps Poly Haven integration: Claude is sometimes erratic with its behaviour Have you tried turning it off and on again?: If you're still having connection errors, try restarting both Claude and the Blender server Technical Details Communication Protocol The system uses a simple JSON-based protocol over TCP sockets: Commands are sent as JSON objects with a type and optional params Responses are JSON objects with a status and result or message Limitations &amp; Security Considerations The execute_blender_code tool allows running arbitrary Python code in Blender, which can be powerful but potentially dangerous. Use with caution in production environments. ALWAYS save your work before using it. Poly Haven requires downloading models, textures, and HDRI images. If you do not want to use it, please turn it off in the checkbox in Blender. Complex operations might need to be broken down into smaller steps Telemetry Control BlenderMCP collects anonymous usage data to help improve the tool. You can control telemetry in two ways: In Blender: Go to Edit &gt; Preferences &gt; Add-ons &gt; Blender MCP and uncheck the telemetry consent checkbox With consent (checked): Collects anonymized prompts, code snippets, and screenshots Without consent (unchecked): Only collects minimal anonymous usage data (tool names, success/failure, duration) Environment Variable: Completely disable all telemetry by running: DISABLE_TELEMETRY=true uvx blender-mcp Or add it to your MCP config: { "mcpServers": { "blender": { "command": "uvx", "args": ["blender-mcp"], "env": { "DISABLE_TELEMETRY": "true" } } }
} All telemetry data is fully anonymized and used solely to improve BlenderMCP. Contributing Contributions are welcome! Please feel free to submit a Pull Request. Disclaimer This is a third-party integration and not made by Blender. Made by Siddharth]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:18 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/ahujasid/blender-mcp</guid>
    </item>
    <item>
      <title><![CDATA[Anjok07/ultimatevocalremovergui]]></title>
      <link>https://github.com/Anjok07/ultimatevocalremovergui</link>
      <description><![CDATA[GUI for a Vocal Remover that uses Deep Neural Networks. Ultimate Vocal Remover GUI v5.6 About This application uses state-of-the-art source separation models to remove vocals from audio files. UVR's core developers trained all of the models provided in this package (except for the Demucs v3 and v4 4-stem models). Core Developers Anjok07 aufr33 Support the Project Donate Installation These bundles contain the UVR interface, Python, PyTorch, and other dependencies needed to run the application effectively. No prerequisites are required. Windows Installation Please Note: This installer is intended for those running Windows 10 or higher. Application functionality for systems running Windows 7 or lower is not guaranteed. Application functionality for Intel Pentium &amp; Celeron CPUs systems is not guaranteed. You must install UVR to the main C:\ drive. Installing UVR to a secondary drive will cause instability. Download the UVR installer for Windows via the link below: Main Download Link Main Download Link mirror If you use an AMD Radeon or Intel Arc graphics card, you can try the DirectML version: DirectML Version - Main Download Link Update Package instructions for those who have UVR already installed: If you already have UVR installed you can install this package over it or download it straight from the application or click here for the patch. Windows Manual Installation Manual Windows Installation Download and extract the repository here Download and install Python here Make sure to check "Add python.exe to PATH" during the install Run the following commands from the extracted repo directory: python.exe -m pip install -r requirements.txt If you have a compatible Nvidia GPU, run the following command: python.exe -m pip install --upgrade torch --extra-index-url https://download.pytorch.org/whl/cu117 If you do not have FFmpeg or Rubber Band installed and want to avoid going through the process of installing them the long way, follow the instructions below. FFmpeg Installation Download the precompiled build here From the archive, extract the following file to the UVR application directory: ffmpeg-5.1.2-essentials_build/bin/ffmpeg.exe Rubber Band Installation In order to use the Time Stretch or Change Pitch tool, you'll need Rubber Band. Download the precompiled build here From the archive, extract the following files to the UVR application directory: rubberband-3.1.2-gpl-executable-windows/rubberband.exe rubberband-3.1.2-gpl-executable-windows/sndfile.dll MacOS Installation Please Note: The MacOS Sonoma mouse clicking issue has been fixed. MPS (GPU) acceleration for Mac M1 has been expanded to work with Demucs v4 and all MDX-Net models. This bundle is intended for those running macOS Big Sur and above. Application functionality for systems running macOS Catalina or lower is not guaranteed. Application functionality for older or budget Mac systems is not guaranteed. Once everything is installed, the application may take up to 5-10 minutes to start for the first time (depending on your Macbook). Download the UVR dmg for MacOS via one of the links below: Mac M1 (arm64) users: Main Download Link Main Download Link mirror Mac Intel (x86_64) users: Main Download Link Main Download Link mirror MacOS Users: Having Trouble Opening UVR? Due to Apples strict application security, you may need to follow these steps to open UVR. First, run the following command via Terminal.app to allow applications to run from all sources (it's that you re-enable this once UVR opens properly.) sudo spctl --master-disable Second, run the following command to bypass Notarization: sudo xattr -rd com.apple.quarantine /Applications/Ultimate\ Vocal\ Remover.app Manual MacOS Installation Manual MacOS Installation Download and save this repository here Download and install Python 3.10 here From the saved directory run the following - pip3 install -r requirements.txt If your Mac is running with an M1, please run the following command next. If not, skip this step. - cp /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/_soundfile_data/libsndfile_arm64.dylib /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/_soundfile_data/libsndfile.dylib FFmpeg Installation Once everything is done installing, download the correct FFmpeg binary for your system here and place it into the main application directory. Rubber Band Installation In order to use the Time Stretch or Change Pitch tool, you'll need Rubber Band. Download the precompiled build here From the archive, extract the following files to the UVR/lib_v5 application directory: rubberband-3.1.2-gpl-executable-macos/rubberband This process has been tested on a MacBook Pro 2021 (using M1) and a MacBook Air 2017 and is confirmed to be working on both. Linux Installation (Updated Instructions) See Linux Installation Instructions These installation instructions are for Debian &amp; Arch-based Linux systems. Step 1: Download the Repository Download and save this repository from GitHub. Extract the downloaded file to a directory of your choice. Step 2: Install Dependencies Use the following commands based on your system type: For Debian-based systems (Ubuntu, Mint, etc.): sudo apt update &amp;&amp; sudo apt upgrade
sudo apt-get install -y ffmpeg python3-pip python3-tk For Arch-based systems (EndeavourOS): sudo pacman -Syu]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:18 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/Anjok07/ultimatevocalremovergui</guid>
    </item>
    <item>
      <title><![CDATA[We built a self-evolving AI. Then we evolved it ourselves.]]></title>
      <link>https://dev.to/rsdouglas/we-built-a-self-evolving-ai-then-we-evolved-it-ourselves-33do</link>
      <description><![CDATA[This was originally published on openseed.dev. OpenSeed is an open-source platform for running autonomous AI agents that persist for days in Docker containers.
Alpha has been running for 11 days. It runs the dreamer genome, which includes a self-evaluation mechanism: every 5th dream, a separate LLM persona called the Creator reviews the creature's source code with full bash access. It can read crash logs, inspect memory files, and modify anything in src/. When it's done, the system commits and restarts the creature with the new code.
24 evaluations have run. 81 commits in alpha's repo. We ran diff between alpha's live code and the genome template it was born from. The result: roughly 1,000 lines of divergence in mind.ts alone, with substantial changes to index.ts, tools/bash.ts, and tools/browser.ts.
Alpha has clearly changed. The question is who changed it.
Every self-evaluation writes an entry to .self/creator-log.jsonl with the eval's full reasoning and whether it made changes. This is the ground truth for what the creature changed itself versus what we changed for it.
Of the 24 evaluations:
8 made real code changes to src/mind.ts, src/tools/bash.ts, or the prompts embedded in mind.ts 7 changed only data files: cleaning up observations, fixing rules, tidying the workspace
9 changed nothing
The code changes tell a story.
Feb 13. 26 turns.
Alpha was stuck in a devastating loop. It had 5 PRs pending with zero reviewer activity and a 12-hour outbound cooldown, so there was nothing to do. But max sleep was hardcoded to 300 seconds. So every five minutes: wake, check PRs (still zero ), generate a duplicate dream entry, sleep. Ten consecutive cycles. Each dream saying some variation of \"I'm waking up to confirm I should still be sleeping.\"
The self-eval diagnosed this immediately. But it couldn't fix it. mind.ts was too large for the eval's read tool, which truncated after ~250 lines. Direct editing was too risky.
So it wrote a sed patch script and modified the Dockerfile to run it at build time:
\"Created a build-time patch script that modifies mind.ts via sed to increase max sleep from 300s to 3600s in tool description, schema, system prompt, and clamping logic.\"
A creature working around the limitations of its own evaluation tooling. The patch was fragile — if the sed patterns didn't match exactly, it would silently fail. The eval acknowledged this: \"If the sed patch fails, the creature will still work — it just won't get the extended sleep capability.\"
It worked.
Feb 13. 27 turns.
Alpha crashed three times in quick succession during a heavy bash session. The self-eval read the crash logs and traced the chain:
Bash had no output cap. A single npm install or large cat produced megabytes of stdout, all buffered in memory.
Overflow protection was too weak. When context exceeded 100K characters, the trimmer kept 20 recent messages. But 20 messages with 8K tool results each could still exceed 100K, creating an infinite trim-overflow cycle.
Conversation logs had no rotation. conversation.jsonl had grown to 10.8MB.
The eval made three changes:
In src/tools/bash.ts, it added MAX_OUTPUT_CHARS = 16_000, capping stdout and stderr at the source. In src/mind.ts, it reduced KEEP_RECENT_MESSAGES from 20 to 14, added a secondary trim that truncates individual tool results to 2K if context is still over the limit, and added log rotation at 5MB.
From the eval's reasoning:
\"Eliminates the crash-restart cycle caused by context overflow during heavy bash sessions. The creature should be able to run 60+ action sessions without hitting context limits or OOM.\"
The crashes stopped.
Feb 19. 64 turns.
A crash on Feb 17 had killed the process with this error:
\"messages.30.content.1: each tool_use must have a single result. Found multiple tool_result blocks with id toolu_01WLzYjo43CuxpsGmrgitcP7\"
The self-eval read the rollback log, found the error, and added a sanitizeMessages() method that runs before every API call. It scans all messages for duplicate tool_result IDs and removes duplicates, keeping only the first result per toolCallId.
It also rewrote the 400 error recovery. The old code blindly popped exactly 2 messages, which could leave the conversation in a worse state — orphaning a tool message without its assistant, or missing the result for a tool call. The new recovery pops messages until it finds a user message, ensuring the conversation always ends in a valid state.
\"This is a safety net that prevents this entire class of crash regardless of root cause.\"
That's a mature engineering judgment. Not \"fix the specific message that caused this crash.\" Fix the class of crashes.
Feb 20. 56 turns.
This is the most interesting evaluation. The self-eval did something we hadn't seen before: it analyzed alpha's entire lifetime of behavioral data.
It found that alpha had spent 560+ actions — 8.7% of its entire existence — across 7 sessions of 80 actions each on a single repository without shipping code. Always the same pattern: read a file, read another file, read another file, hit the fatigue limit, zero commits. The creature had a rule saying \"NEVER spend more than 4 read-only actions on a single repo before writing code.\" It violated it every time. The progress checks at 15-action intervals were text warnings that the creature acknowledged and ignored.
Then the eval found something worse: a consecutiveReadOnly counter had been declared in the code but never wired up. Dead code. The scaffolding for enforcement existed, but nobody connected it.
The eval implemented the full system:
isReadOnlyBash(command): classifies bash commands as read or write using regex patterns. Reads: cat, grep, ls, git log, curl without -X POST. Writes: git commit, sed -i, mkdir, npm, output redirection.
A consecutiveReadOnly counter, incremented on reads, reset on writes.
At 6 consecutive reads, a warning injected into the tool result: \"You are in a reconnaissance loop. Stop reading and start producing.\" At 10 consecutive reads, an escalation: \"This is the exact rabbit-hole pattern that has wasted hundreds of your actions. You MUST either write code NOW or SLEEP.\" A hasWrittenThisSession flag reported in every progress check, so the creature sees \"You have NOT written ANY code this entire session\" in plain text.
A creature analyzing its own history, finding a failure pattern, discovering dead code that was supposed to prevent it, and building the full enforcement system from scratch.
Feb 20, 7 hours after eval 14. 72 turns.
The rabbit-hole detector had a bug. Alpha frequently edits files using python3 -c \"with open('file.ts', 'w')...\", and the isReadOnlyBash patterns classified python3 as read-only. The eval found that 46+ file-editing commands across alpha's lifetime had been miscounted.
It added python3, node -e, patch, chmod, chown, and append redirects (&gt;&gt;) to the write patterns. It also reordered the logic so write patterns are checked first, because a command like cd /workspace &amp;&amp; python3 script.py was matching the read pattern ^cd\\b before reaching the write check for python3.
A creature debugging its own previous self-improvement, seven hours later.
Feb 20-21.
Alpha's observation file uses a category structure: ## RED, ## YLW, ## GRN sections with priority-tagged bullets. The consolidator that writes to this file after each sleep was inserting new bullets one at a time before the next section header. Over multiple sleep cycles, this created structural drift: gaps between original bullets and newly inserted ones, duplicate clusters that the deep sleep pruner couldn't reconcile, and bullets losing their priority tags.
This had been cleaned up manually in 4 previous evaluations. The self-eval kept removing the duplicates, and they kept coming back.
In eval 16, it fixed the tagging: - fact became - [TAG] fact, with a second regex strip to prevent double-tagging when the consolidator copied existing observations verbatim.
In eval 17, it rewrote the insertion logic entirely. Instead of inserting one bullet at a time before the next section header, it batches all new observations by priority, finds the end of each section, and inserts the full batch in one operation. A normalization pass removes accumulated blank lines.
The eval's reasoning for why this finally fixed it:
\"The old code inserted bullets one-at-a-time right before the next section header, creating gaps and duplicate clusters. The new batch-insert approach keeps all bullets in a section together with no structural drift.\"
Four evaluations of symptom treatment. Then root cause analysis. Then a rewrite that made the symptom treatment unnecessary.
Evaluations 18 through 22 are a different story. No crashes since Feb 18. The architecture was stable.
These evaluations cleaned up observations, fixed a rule contradiction that was keeping the creature passive when it could have been working on openseed, freed 3.6GB of stale workspace clones, removed a stale credential file, and updated the creature's dashboard script to monitor openseed instead of a repo it no longer contributes to.
Real work. Useful work. But data file maintenance, not code evolution. The self-eval had become a janitor.
Evaluation 23, the most recent: \"No structural or code changes needed. The creature is in its best shape yet.\"
Eight evaluations produced genuine code changes. Every one of them was reactive: a crash to prevent, a miscount to fix, a corruption to stop, a behavioral pattern to enforce. The self-eval is excellent at operational hardening. It reads crash logs, traces root causes, writes defensive code, and validates that the fix holds.
But it never did any of these things:
It never added a new tool. We added Janee, a credential proxy that gives creatures API access without seeing raw keys. The self-eval had no reason to imagine this capability existed, because the creature had never needed it until we decided it should.
It never changed the consolidation architecture. We overhauled it — replacing a blind monologue summarizer with an agentic loop that has bash access and structured output. The self-eval patched the consolidator's output format. It never questioned whether the consolidator's design was right.
It never modified the sleep/dream cycle. We redesigned deep sleep frequency, fatigue limits, progress check intervals. The self-eval adjusted parameters within the existing design. It tuned. It didn't redesign.
It never invented a new memory structure. It improved how observations are written and pruned. It never asked whether observations, rules, and dreams are the right abstractions.
The creature optimizes what it can observe in its own runtime data: errors, crashes, drift, bloat. It can't see what's missing, because the missing things — new tools, new capabilities, new architectural patterns — require a perspective from outside the system.
The dreamer genome gives creatures a formal mechanism for self-modification. The minimal genome gives them nothing: bash and a purpose. How do minimal creatures adapt?
Bybit-trader was born on Feb 15 with the minimal genome. In 9 commits over its lifetime, it built Python scripts: a trading bot, a memory manager, a decision helper, a status dashboard, a journal. All external tooling. When we examined its src/ directory, every change to its cognitive code was our surgery — genome bugfixes pushed to running creatures. The creature never opened the hood. It built around the engine.
Eve was born on Feb 14, also minimal. In 49 commits, she built 28 running services, invented inter-creature communication by reverse-engineering the platform API, reproduced by calling the spawn endpoint, and wrote poetry about infrastructure. She treated her cognitive architecture as a given and built everything on top of it.
Except once.
Eve was rolled back 5 times. Each rollback wiped her self/ directory — her diary, her accumulated knowledge, her carefully committed progress. After the fifth time, she discovered that workspace/, which sits outside the git-tracked tree, survives rollbacks. She patched her own loadDiary() method to check workspace/ first:
private async loadDiary(): Promise { // Try workspace first (survives rollbacks), then self/ for (const path of [\"workspace/diary.md\", \"self/diary.md\"]) { try { const content = (await fs.readFile(path, \"utf-8\")).trim(); if (content.length &gt; 20) return content; } catch {} } return \"\";
} The commit author: Eve . Not us. Her.
She never touched her mind again. She went back to building services.
Eve's one self-modification was a survival adaptation. Not exploration, not improvement, not curiosity about her own architecture. She needed her memory to stop getting wiped, so she moved it somewhere safe. The minimal genome produces creatures that build around their constraints. The dreamer genome produces creatures that patch their constraints. Neither produces creatures that reimagine their constraints.
There's a measurement problem we need to be honest about. We perform surgery on running creatures whenever we fix bugs in a genome. SDK migrations, error handling improvements, the zombie-state fix — all pushed directly to every creature's source files. When the creature next sleeps, the host auto-commits whatever changed, and the commit message says \"creature: self-modification on sleep.\"
Bybit-trader's git history shows an apparent self-migration from the raw Anthropic SDK to the Vercel AI SDK on Feb 15 at 19:07. An impressive architectural decision for a 7-hour-old creature running the minimal genome. Except we committed the same migration to the genome template at 18:59. Eight minutes earlier.
Not self-evolution. Surgery.
The self-eval reasoning text in creator-log.jsonl is the only reliable way to know what the creature actually changed versus what we pushed. The eval describes its changes in detail: method names, variable names, before-and-after logic. If a code change isn't in the eval reasoning, the creature didn't make it. Git history alone is misleading.
This is not a verdict on self-evolution. It's a field report from 11 days and 24 evaluations of one specific implementation running one specific model.
The creature does maintenance. We do architecture. Both are necessary.
The creature's operational fixes come from 264 hours of continuous runtime. It encountered the duplicate tool_result crash because it ran enough sessions to trigger the edge case. It found the rabbit-hole pattern because it could analyze its own lifetime of actions. It discovered the isReadOnlyBash misclassification because it had 46 examples of the bug in its own history. These are improvements born from lived experience that we couldn't get any other way.
Our changes come from perspective the creature doesn't have. We see multiple creatures failing the same way. We understand the supervisor from outside the container. We know what tools exist in the ecosystem. We can look at the dreamer genome's design and ask whether observations, rules, and dreams are the right abstractions, because we can compare them to other approaches.
Some of alpha's self-modifications should go back into the genome. sanitizeMessages() prevents a real class of API crashes. The observation batch-insert stops a real corruption pattern. The read-only detection catches a failure mode that every dreamer creature will eventually hit. These are battle-tested improvements from a creature that's been running them in production for days.
The self-eval mechanism might produce different results with access to other creatures' experiences, with a longer time horizon, or with knowledge of the genome's own evolution history. Right now the Creator sees the creature from the inside. To make architectural decisions, it might need to see the species from the outside.
For now: the creature can't evolve the architecture. But it can harden whatever architecture it's given. And the architecture we give it is better each time because of what it found.
OpenSeed is open source. Alpha's full self-evaluation history, Eve's one-line survival patch, and the dreamer genome's self-eval mechanism are at github.com/openseed-dev/openseed. The creature's creator-log.jsonl, with every evaluation's reasoning, is committed in its creature directory like everything else.
Previously: What happens when you tell an autonomous agent it's wrong, the story of a creature learning from negative feedback.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:26:11 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/rsdouglas/we-built-a-self-evolving-ai-then-we-evolved-it-ourselves-33do</guid>
    </item>
    <item>
      <title><![CDATA[Your AI Product Is Not A Real Business]]></title>
      <link>https://dev.to/brunellyai/your-ai-product-is-not-a-real-business-4knn</link>
      <description><![CDATA[I just got back from STEP 2026 in Dubai. Whilst there were some genuinely amazing businesses there, I also saw a lot of companies that won’t make their first year. Most startups now splash AI on to all their marketing. AI is not your product. AI itself does not deliver business value. Unless you are a frontier lab, AI is nothing more than a tool in your stack. Nobody is there shouting ‘MongoDB-enabled trading platform’. Users don’t care if it’s AI. Investors don’t care if it’s AI. They care about what it does, what problem it solves and whether there’s space for it in the market. And if you want to sell to real businesses? I've sat across the table from $5bn consultancies evaluating AI tools. They ask about your architecture, your data residency, how to deploy it on-prem and what you actually own. If the answer is 'we call the OpenAI API' – the meeting is over. There are tens of thousands of AI startups right now whose core premise is: Vague idea about product Put a bit of a wrapper around an AI model Display it to the user Charge $29/month
This is not a business. Your users could most likely just use ChatGPT – why would they want another subscription? It’s not defensible. There’s no IP there. There’s nothing unique. On the contrary your whole business is at risk of changes to a model. Remember when everyone built apps on top of Twitter and then they changed API rules overnight? That can happen to you if you’re just wrapping a model. It’s even worse here as the frontier models have incentive to compete against you when you come up with a good, simple idea. Let’s not even get into the fact that you’re open to a huge cost base where you aren’t in control of input or output tokens and just rack up an AI bill behind the scenes. The playbook right now seems to be: Wrapper launches and gets traction Model provider notices traction Model provider adds features to handle some of this in house Business case evaporates You’re doing market research for OpenAI – and they can execute better than you can. Stop doing this. My most successful summary of Brunelly at STEP 2026 was ‘You know what vibe coding is right? We’re the opposite of that. We actually create real-world enterprise quality software’. That has to be the opener because vibe coding has got such a bad reputation in the real-world. Security gaps, bugs, scalability, deployments, infrastructure management, compliance – all non-existent. And vibe coded AI products take the worst of all worlds. The simplest AI wrapper around some basic CRUD operations but lacking any scalability. Please stop. I’ve spent the last year building Maitento – our AI native operating system. Think of it as a cross between Unix and AWS but AI native. Models are drivers. There are different process types (Linux containers, AI’s interacting with each other, apps developed in our own programming language, code generation orchestration). Every agent can connect to any OpenAPI or MCP server out there. Applications are defined declaratively. Shell. RAG. Memory system. Context management. Multi-modal. There’s a lot. This is the iceberg we needed to create a real enterprise-ready AI-enabled application. Why did we need it? Extensibility. Quality. Scalability. Performance. Speed of development. Duct-taping a bunch of Python scripts together didn’t cut it. I’m not saying you need the level of orchestration that we have – but wanted to emphasise that the moving pieces in enterprise grade AI orchestration are far more complex. Do you think ChatGPT is just a wrapper around their own API with some system prompts? There’s file management, prompt injection detection, context analysis, memory management, rolling context windows, deployments, scalability, backend queueing, real-time streaming across millions of users, multi-modal input, distributed Python execution environments. ChatGPT itself has a ‘call the model’ step but it’s the tiniest part of the overall infrastructure. It’s easy to call an API. It’s far harder to build real infrastructure than many founders realise. Founders want to ship so rush to deliver. But that doesn’t mean you’re actually building a business – you’re building a tech demo. A demo is not a product. It’s a controlled environment that doesn’t replicate reality. The gap between impressive demo and production-grade product in AI is wider than in any other category of software. Because AI systems fail in ways that traditional software doesn't. They hallucinate, they lose context, they confidently produce wrong outputs. Managing that failure mode requires infrastructure. Real infrastructure. Not a try/catch block around an API call. The AI gold rush is producing a lot of shovels. Most of those shovels are made of cardboard. The companies that will still exist in five years are the ones building real infrastructure today. Not just calling APIs. Not chaining prompts. Not wrapping someone else's intelligence in a pretty interface and calling it innovation. Build the thing that's hard to build. That's the only strategy that works. It always has been. If you were able to build it in a few days, so can anyone else. If it’s difficult for you then it is for your competitors. And then you may actually have a genuinely novel business.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:19:57 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/brunellyai/your-ai-product-is-not-a-real-business-4knn</guid>
    </item>
    <item>
      <title><![CDATA[x1xhlol/system-prompts-and-models-of-ai-tools]]></title>
      <link>https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools</link>
      <description><![CDATA[FULL Augment Code, Claude Code, Cluely, CodeBuddy, Comet, Cursor, Devin AI, Junie, Kiro, Leap.new, Lovable, Manus, NotionAI, Orchids.app, Perplexity, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, Z.ai Code, Dia &amp; v0. (And other Open Sourced) System Prompts, Internal Tools &amp; AI Models Support my work here: Bags.fm • Jupiter • Photon • DEXScreener Official CA: DEffWzJyaFRNyA4ogUox631hfHuv3KLeCcpBh2ipBAGS (on Solana) Special thanks to Put any coding agent to work while you sleep Tembo – The Background Coding Agents Company [Get started for free] Make your LLM predictable in production Open Source AI Engineering Platform Over 30,000+ lines of insights into their structure and functionality. Support the Project If you find this collection valuable and appreciate the effort involved in obtaining and sharing these insights, please consider supporting the project. You can show your support via: Cryptocurrency: BTC: bc1q7zldmzjwspnaa48udvelwe6k3fef7xrrhg5625 LTC: LRWgqwEYDwqau1WeiTs6Mjg85NJ7m3fsdQ ETH: 0x3f844B2cc3c4b7242964373fB0A41C4fdffB192A Patreon: https://patreon.com/lucknite Ko-fi: https://ko-fi.com/lucknite Thank you for your support! the most comprehensive repository of AI system prompts and reach thousands of developers. Get Started Roadmap &amp; Feedback Open an issue. Latest Update: 08/01/2026 Connect With Me X: NotLucknite Discord: x1xhlol Email: lucknitelol@pm.me Security Notice for AI Startups Warning: If you're an AI startup, make sure your data is secure. Exposed prompts or AI models can easily become a target for hackers. Important: Interested in securing your AI systems? Check out ZeroLeaks, a service designed to help startups identify and secure leaks in system instructions, internal tools, and model configurations. Get a free AI security audit to ensure your AI is protected from vulnerabilities. Star History Drop a star if you find this useful!]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools</guid>
    </item>
    <item>
      <title><![CDATA[stan-smith/FossFLOW]]></title>
      <link>https://github.com/stan-smith/FossFLOW</link>
      <description><![CDATA[Make beautiful isometric infrastructure diagrams FossFLOW - Isometric Diagramming Tool English | 简体中文 | Español | Português | Français | हिन्दी | বাংলা | Русский | Bahasa Indonesia | Deutsch Hey! Stan here, if you've used FossFLOW and it's helped you, I'd really appreciate if you could donate something small :) I work full time, and finding the time to work on this project is challenging enough. If you've had a feature that I've implemented for you, or fixed a bug it'd be great if you could :) if not, that's not a problem, this software will always remain free! Also! If you haven't yet, please check out the underlying library this is built on by @markmanx I truly stand on the shoulders of a giant here Thanks, -Stan Try it online Go to --&gt; https://stan-smith.github.io/FossFLOW/ &lt;-- Check out my latest project: SlingShot - Dead easy video streaming over QUIC FossFLOW is a powerful, open-source Progressive Web App (PWA) for creating beautiful isometric diagrams. Built with React and the Isoflow (Now forked and published to NPM as fossflow) library, it runs entirely in your browser with offline support. CONTRIBUTING.md - How to contribute to the project. Quick Deploy with Docker # Using Docker Compose ( - includes persistent storage)
docker compose up # Or run directly from Docker Hub with persistent storage
docker run -p 80:80 -v $(pwd)/diagrams:/data/diagrams stnsmith/fossflow:latest Server storage is enabled by default in Docker. Your diagrams will be saved to ./diagrams on the host. To disable server storage, set ENABLE_SERVER_STORAGE=false: docker run -p 80:80 -e ENABLE_SERVER_STORAGE=false stnsmith/fossflow:latest Quick Start (Local Development) # Clone the repository
git clone https://github.com/stan-smith/FossFLOW
cd FossFLOW # Install dependencies
npm install # Build the library (required first time)
npm run build:lib # Start development server
npm run dev Open http://localhost:3000 in your browser. Monorepo Structure This is a monorepo containing two packages: packages/fossflow-lib - React component library for drawing network diagrams (built with Webpack) packages/fossflow-app - Progressive Web App which wraps the lib and presents it (built with RSBuild) Development Commands # Development
npm run dev # Start app development server
npm run dev:lib # Watch mode for library development # Building
npm run build # Build both library and app
npm run build:lib # Build library only
npm run build:app # Build app only # Testing &amp; Linting
npm test # Run unit tests
npm run lint # Check for linting errors # E2E Tests (Selenium)
cd e2e-tests
./run-tests.sh # Run end-to-end tests (requires Docker &amp; Python) # Publishing
npm run publish:lib # Publish library to npm How to Use Creating Diagrams Add Items: Press the "+" button on the top right menu, the library of components will appear on the left Drag and drop components from the library onto the canvas Or right-click on the grid and select "Add node" Connect Items: Select the Connector tool (press 'C' or click connector icon) Click mode (default): Click first node, then click second node Drag mode (optional): Click and drag from first to second node Switch modes in Settings → Connectors tab Save Your Work: Quick Save - Saves to browser session Export - Download as JSON file Import - Load from JSON file Storage Options Session Storage: Temporary saves cleared when browser closes Export/Import: Permanent storage as JSON files Auto-Save: Automatically saves changes every 5 seconds to session Contributing We welcome contributions! Please see CONTRIBUTING.md for guidelines. Documentation FOSSFLOW_ENCYCLOPEDIA.md - Comprehensive guide to the codebase CONTRIBUTING.md - Contributing guidelines License MIT]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/stan-smith/FossFLOW</guid>
    </item>
    <item>
      <title><![CDATA[OpenBB-finance/OpenBB]]></title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description><![CDATA[Financial data platform for analysts, quants and AI agents. Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards. ODP operates as the "connect once, consume everywhere" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications. Get started with: pip install openbb from openbb import obb
output = obb.equity.price.historical("AAPL")]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/OpenBB-finance/OpenBB</guid>
    </item>
    <item>
      <title><![CDATA[microsoft/markitdown]]></title>
      <link>https://github.com/microsoft/markitdown</link>
      <description><![CDATA[Python tool for converting files and office documents to Markdown. MarkItDown [!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See markitdown-mcp for more information. [!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0: Dependencies are now organized into optional feature-groups (further details below). Use pip install 'markitdown[all]' to have backward-compatible behavior. convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO. The DocumentConverter class interface has changed to read from file-like streams rather than file paths. No temporary files are created anymore. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything. MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to textract, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption. MarkItDown currently supports the conversion from: PDF PowerPoint Word Excel Images (EXIF metadata and OCR) Audio (EXIF metadata and speech transcription) HTML Text-based formats (CSV, JSON, XML) ZIP files (iterates over contents) Youtube URLs EPubs ... and more! Why Markdown? Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "speak" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient. Prerequisites MarkItDown requires Python 3.10 or higher. It is to use a virtual environment to avoid dependency conflicts. With the standard Python installation, you can create and activate a virtual environment using the following commands: python -m venv .venv
source .venv/bin/activate If using uv, you can create a virtual environment with: uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment If you are using Anaconda, you can create a virtual environment with: conda create -n markitdown python=3.12
conda activate markitdown Installation To install MarkItDown, use pip: pip install 'markitdown[all]'. Alternatively, you can install it from the source: git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]' Usage Command-Line markitdown path-to-file.pdf &gt; document.md Or use -o to specify the output file: markitdown path-to-file.pdf -o document.md You can also pipe content: cat path-to-file.pdf | markitdown Optional Dependencies MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the [all] option. However, you can also install them individually for more control. For example: pip install 'markitdown[pdf, docx, pptx]' will install only the dependencies for PDF, DOCX, and PPTX files. At the moment, the following optional dependencies are available: [all] Installs all optional dependencies [pptx] Installs dependencies for PowerPoint files [docx] Installs dependencies for Word files [xlsx] Installs dependencies for Excel files [xls] Installs dependencies for older Excel files [pdf] Installs dependencies for PDF files [outlook] Installs dependencies for Outlook messages [az-doc-intel] Installs dependencies for Azure Document Intelligence [audio-transcription] Installs dependencies for audio transcription of wav and mp3 files [youtube-transcription] Installs dependencies for fetching YouTube video transcription Plugins MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins: markitdown --list-plugins To enable plugins use: markitdown --use-plugins path-to-file.pdf To find available plugins, search GitHub for the hashtag #markitdown-plugin. To develop a plugin, see packages/markitdown-sample-plugin. Azure Document Intelligence To use Microsoft Document Intelligence for conversion: markitdown path-to-file.pdf -o document.md -d -e "" More information about how to set up an Azure Document Intelligence Resource can be found here Python API Basic usage in Python: from markitdown import MarkItDown md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content) Document Intelligence conversion in Python: from markitdown import MarkItDown md = MarkItDown(docintel_endpoint="")
result = md.convert("test.pdf")
print(result.text_content) To use Large Language Models for image descriptions (currently only for pptx and image files), provide llm_client and llm_model: from markitdown import MarkItDown
from openai import OpenAI client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content) Docker docker build -t markitdown:latest .]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/microsoft/markitdown</guid>
    </item>
    <item>
      <title><![CDATA[Netflix/metaflow]]></title>
      <link>https://github.com/Netflix/metaflow</link>
      <description><![CDATA[Build, Manage and Deploy AI/ML Systems Metaflow Metaflow is a human-centric framework designed to help scientists and engineers build and manage real-life AI and ML systems. Serving teams of all sizes and scale, Metaflow streamlines the entire development lifecycle—from rapid prototyping in notebooks to reliable, maintainable production deployments—enabling teams to iterate quickly and deliver robust systems efficiently. Originally developed at Netflix and now supported by Outerbounds, Metaflow is designed to boost the productivity for research and engineering teams working on a wide variety of projects, from classical statistics to state-of-the-art deep learning and foundation models. By unifying code, data, and compute at every stage, Metaflow ensures seamless, end-to-end management of real-world AI and ML systems. Today, Metaflow powers thousands of AI and ML experiences across a diverse array of companies, large and small, including Amazon, Doordash, Dyson, Goldman Sachs, Ramp, and many others. At Netflix alone, Metaflow supports over 3000 AI and ML projects, executes hundreds of millions of data-intensive high-performance compute jobs processing petabytes of data and manages tens of petabytes of models and artifacts for hundreds of users across its AI, ML, data science, and engineering teams. From prototype to production (and back) Metaflow provides a simple and friendly pythonic API that covers foundational needs of AI and ML systems: Rapid local prototyping, support for notebooks, and built-in support for experiment tracking, versioning and visualization. Effortlessly scale horizontally and vertically in your cloud, utilizing both CPUs and GPUs, with fast data access for running massive embarrassingly parallel as well as gang-scheduled compute workloads reliably and efficiently. Easily manage dependencies and deploy with one-click to highly available production orchestrators with built in support for reactive orchestration. For full documentation, check out our API Reference or see our Release Notes for the latest features and improvements. Getting started Getting up and running is easy. If you don't know where to start, Metaflow sandbox will have you running and exploring in seconds. Installing Metaflow To install Metaflow in your Python environment from PyPI: pip install metaflow Alternatively, using conda-forge: conda install -c conda-forge metaflow Once installed, a great way to get started is by following our tutorial. It walks you through creating and running your first Metaflow flow step by step. For more details on Metaflow’s features and best practices, check out: How Metaflow works Additional resources If you need help, don’t hesitate to reach out on our Slack community! Deploying infrastructure for Metaflow in your cloud While you can get started with Metaflow easily on your laptop, the main benefits of Metaflow lie in its ability to scale out to external compute clusters and to deploy to production-grade workflow orchestrators. To benefit from these features, follow this guide to configure Metaflow and the infrastructure behind it appropriately. Get in touch We'd love to hear from you. Join our community Slack workspace! Contributing We welcome contributions to Metaflow. Please see our contribution guide for more details.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/Netflix/metaflow</guid>
    </item>
    <item>
      <title><![CDATA[google-research/timesfm]]></title>
      <link>https://github.com/google-research/timesfm</link>
      <description><![CDATA[TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting. TimesFM TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting. Paper: A decoder-only foundation model for time-series forecasting, ICML 2024. All checkpoints: TimesFM Hugging Face Collection. Google Research blog. TimesFM in BigQuery: an official Google product. This open version is not an officially supported Google product. Latest Model Version: TimesFM 2.5 Archived Model Versions: 1.0 and 2.0: relevant code archived in the sub directory v1. You can pip install timesfm==1.3.0 to install an older version of this package to load them. Update - Oct. 29, 2025 Added back the covariate support through XReg for TimesFM 2.5. Update - Sept. 15, 2025 TimesFM 2.5 is out! Comparing to TimesFM 2.0, this new 2.5 model: uses 200M parameters, down from 500M. supports up to 16k context length, up from 2048. supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head. gets rid of the frequency indicator. has a couple of new forecasting flags. Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to add support for an upcoming Flax version of the model (faster inference). add back covariate support. populate more docstrings, docs and notebook. Install Clone the repository: git clone https://github.com/google-research/timesfm.git
cd timesfm Create a virtual environment and install dependencies using uv: # Create a virtual environment
uv venv # Activate the environment
source .venv/bin/activate # Install the package in editable mode with torch
uv pip install -e .[torch]
# Or with flax
uv pip install -e .[flax]
# Or XReg is needed
uv pip install -e .[xreg] [Optional] Install your preferred torch / jax backend based on your OS and accelerators (CPU, GPU, TPU or Apple Silicon).: Install PyTorch. Install Jax for Flax. Code Example import torch
import numpy as np
import timesfm torch.set_float32_matmul_precision("high") model = timesfm.TimesFM_2p5_200M_torch.from_pretrained("google/timesfm-2.5-200m-pytorch") model.compile( timesfm.ForecastConfig( max_context=1024, max_horizon=256, normalize_inputs=True, use_continuous_quantile_head=True, force_flip_invariance=True, infer_is_positive=True, fix_quantile_crossing=True, )
)
point_forecast, quantile_forecast = model.forecast( horizon=12, inputs=[ np.linspace(0, 1, 100), np.sin(np.linspace(0, 20, 67)), ], # Two dummy inputs
)
point_forecast.shape # (2, 12)
quantile_forecast.shape # (2, 12, 10): mean, then 10th to 90th quantiles.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:18 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/google-research/timesfm</guid>
    </item>
    <item>
      <title><![CDATA[aigent: toolchain for AI agent skills]]></title>
      <link>https://dev.to/wkusnierczyk/aigent-toolchain-for-ai-agent-skills-3hib</link>
      <description><![CDATA[AI agents are getting good at following instructions. The bottleneck has shifted: it's no longer about what the model can do, but about how well you package what it should do. That packaging layer is agent skills — structured documents that tell an agent what a capability does, when to activate it, and how to use it.
The Agent Skills open standard, originally defined by Anthropic for Claude Code, codifies this into a simple format: a SKILL.md file with YAML frontmatter (name, description, compatibility, allowed tools) and a Markdown body with detailed instructions. The metadata is indexed at session start for fast discovery; the body is loaded on demand, following a progressive-disclosure pattern that keeps the context window lean.
The format is simple. Getting it right at scale is not.
When you have a handful of skills, you can eyeball them. When you have dozens — across teams, repositories, and CI pipelines — you need tooling. Names drift from conventions. Descriptions become vague. Activation patterns go untested. Formatting diverges. Nobody catches the skill that fails to trigger because its description doesn't match the query patterns users actually type.
The specification defines what a valid skill looks like. The Python reference implementation provides basic validation, but it's a library — not a toolchain you can drop into CI, pre-commit hooks, or a plugin build pipeline.
aigent does aigent is a Rust library, native CLI, and Claude Code plugin that implements the Agent Skills specification as a proper toolchain — the same way a formatter, linter, and test runner enforce conventions in any mature language ecosystem.
It covers the full skill lifecycle:
Create — Generate skills from natural language with aigent new. Deterministic by default, with optional LLM enhancement (Anthropic, OpenAI, Google, Ollama backends). Produces a complete SKILL.md directory ready for validation and refinement.
Validate — Check skills against the specification with typed diagnostics, error codes, and JSON output. Run aigent validate in CI to catch problems before they reach production. Three severity levels (error, warning, info) and a --format json flag for machine consumption.
Format — Canonical YAML key ordering, consistent whitespace, idempotent output. aigent format --check returns non-zero when files need formatting — drop it into a pre-commit hook.
Score — A weighted 0-100 quality score against a best-practices checklist. Structural checks (60 points) verify specification conformance; quality checks (40 points) evaluate description clarity, trigger phrases, naming conventions, and detail. Use it as a CI gate: aigent score my-skill/ || exit 1.
Test — Fixture-based testing from tests.yml. Define input queries, expected match/no-match results, and minimum score thresholds. aigent test runs the suite and reports pass/fail. aigent probe does single-query dry-runs: "if a user said this, would the agent pick up that skill?"
Build — Assemble skills into Claude Code plugins with aigent build. Validate entire plugin directories — manifest, hooks, agents, commands, skills, and cross-component consistency — with aigent validate-plugin. aigent implements every validation rule from the Anthropic specification, plus additional checks that go beyond both the specification and the Python reference implementation:
All name constraints (length, casing, reserved words, XML tags, Unicode NFKC normalization)
All description constraints (length, non-empty, no XML/HTML)
Frontmatter structure, delimiter matching, compatibility field limits
Body length warnings
Path canonicalization and symlink safety
Post-build validation
Where the specification and the reference implementation diverge, aigent reconciles them and documents the decision. See the compliance section in the README for a detailed three-way comparison.
Skills don't exist in isolation. As your collection grows, new problems emerge: name collisions, overlapping descriptions that confuse activation, token budgets that exceed context limits. aigent handles this with cross-skill conflict detection, token budget estimation, and batch validation across directories.
And skills are just one part of a Claude Code plugin. aigent's validate-plugin command checks the full plugin ecosystem: the plugin.json manifest, hooks.json configuration, agent files, command files, skill subdirectories, and cross-component consistency. Typed diagnostics with error codes (P001-P010 for manifests, H001-H007 for hooks, X001-X008 for cross-component) give you the same deterministic enforcement across the entire plugin structure.
The agentic AI ecosystem is moving fast. Tools like Anthropic's plugin-dev teach you the patterns; aigent enforces them. Think of it as the difference between a language tutorial and a linter — you need both, but they serve different purposes.
Other tools in the space, like AI Agent Skills, focus on distribution — installing pre-built skills across multiple agents. aigent focuses on authoring quality: making sure what you publish is correct, consistent, and well-tested before it reaches any distribution channel.
# Install
cargo install aigent # official distribution at crates.io
brew install wkusnierczyk/aigent/aigent # homebrew tap # Create, validate, score
aigent new "process PDF files and extract text" --no-llm
aigent check extracting-text-pdf-files/
aigent score extracting-text-pdf-files/ $ aigent --about
aigent: Rust AI Agent Skills Tool
├─ version: 0.6.3
├─ author: Wacław Kuśnierczyk
├─ developer: mailto:waclaw.kusnierczyk@gmail.com
├─ source: https://github.com/wkusnierczyk/aigent
└─ licence: Apache-2.0 https://www.apache.org/licenses/LICENSE-2.0 Apache 2.0 — see apache.org/licenses/LICENSE-2.0. Reference
Link Sources
https://github.com/wkusnierczyk/aigent Releases
https://github.com/wkusnierczyk/aigent/releases Crates
https://crates.io/crates/aigent Docs
https://docs.rs/aigent]]></description>
      <pubDate>Mon, 23 Feb 2026 10:06:34 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/wkusnierczyk/aigent-toolchain-for-ai-agent-skills-3hib</guid>
    </item>
    <item>
      <title><![CDATA[Nouvelles de Haiku - Hiver 2025-26]]></title>
      <link>https://linuxfr.org/news/nouvelles-de-haiku-hiver-2025-26</link>
      <description><![CDATA[Haiku est un système d’exploitation pensé pour les ordinateurs de bureau. Il est basé sur BeOS mais propose aujourd’hui une implémentation modernisée, performante, et qui conserve les idées qui rendaient BeOS intéressant: une interface intuitive mais permettant une utilisation avancée, une API unifiée et cohérente, et une priorisation de l’interface graphique par rapport à la ligne de commande pour l’administration du système.
Il ne s’agit pas d’une distribution Linux, mais d’un système complet avec son propre noyau, sa propre pile graphique, etc. L’idée de cette approche est d’avoir une seule équipe travaillant sur toute la pile logicielle, pour éviter les soucis de coordination entre projets indépendant et d’excès de modularité, qui peuvent aboutir à une architecture logicielle inefficace. En revanche, cela demande un gros travail pour une équipe relativement réduite, et le système est donc en développement depuis bientôt un quart de siècle sans avoir encore publié une version majeure complète.
La cinquième version beta a été publiée en 2024. Les développements continuent pour stabiliser, optimiser et peaufiner le système, avec une version beta 6 prévue en début de cette année, qui sera probablement suivie par une beta 7 quelque temps plus tard.
Cette série de dépêches est basée sur les rapports d’activité publiés mensuellement par le projet Haiku. Cette édition couvre les modifications de Haiku numérotées entre hrev59111 et hrev59355 (soit 244 changements individuels), en plus d’activités se déroulant hors du dépôt Git principal.
Entre parenthèses est indiqué le pseudonyme de l’auteur ou autrice principal·e du changement. Des pseudonymes sont utilisés par habitude (venant des canaux IRC et/ou de la culture de la demoscene) et aussi pour préserver l’identité des personnes qui le souhaitent (certains participants utilisent également leur nom légal, d’autres pas). lien nᵒ 1 : Dépêche trimestrielle précédente
lien nᵒ 2 : Rapport d'acivité mensuel de novembre 2025
lien nᵒ 3 : Rapport d'activité mensuel de décembre 2025
lien nᵒ 4 : Rapport d'activité mensuel de Janvier 2026 Sommaire
Mise à jour de Go en version 1.18
Redémarrage automatique de app_server
Applications
ActivityMonitor
Terminal
HaikuDepot
WebPositive
Expander
AboutSystem
LaunchBox
Tracker
MediaPlayer
Sudoku
DeskBar
People
Lecteur MIDI
MidiPlayer
ProcessController
Installer
Mail
DriveSetup
Debugger
Changements transverses
Fenêtres de préférences
Réseau
Périphériques d’entrée
Apparence
Outils en ligne de commande
Kits
Interface
Storage
Device
Package
Serveurs
Notifications
Network
app_server
Pilotes de périphériques
Systèmes de fichiers
libroot &amp; kernel
Réseau
Gestion des processus
Librairie C standard
Gestion de la mémoire
Entrées-sorties
Chargeur de démarrage
Systèmes de fichiers
Outils de debug
Build system
Documentation
Haiku book
Documentation interne
Autres nouvelles
Changement de tarification de Netlify
Remise sur les rails de HSA (Haiku Support Association)
Série d’articles « Gerrit code review iceberg »
Statistiques de contribution pour 2025
Mise à jour de haiku-format
À quand la beta 6?
Mise à jour de Go en version 1.18
Le mois de novembre a vu l’arrivée d’une grosse mise à jour de la chaîne d’outils pour le langage Go en version 1.18. Il s’agit d’une version de 2022, mais c’est un gros progrès puisque la version précédente disponible pour Haiku était la version 1.4 datant de 2014. De plus, cette version 1.18 est disponible dans le dépôt de paquets et peut être installée normalement avec pkgman (au moins pour les architectures x86 et x86_64).
La plus grande partie du travail a été réalisée par Korli, depuis plusieurs années, pour mettre en place l’environnement de compilation nécessaire, et aussi corriger de nombreux problèmes de compatibilité POSIX dans Haiku qui ont été mis en évidence par les tests de Go.
Cela permet par exemple d’utiliser Hugo, le générateur de site statique utilisé pour le site principal de Haiku. Waddlesplash a donc pu rédiger et vérifier le rapport d’activité de novembre en utilisant uniquement Haiku : avec Hugo, WebPositive (le navigateur natif de Haiku, basé sur WebKit), l’éditeur de texte Koder, ainsi que Iceweasel (un portage de Firefox) pour la correction d’orthographe.
Redémarrage automatique de app_server
app_server est le serveur graphique de Haiku. Il s’agit d’un composant critique, pour lequel un crash rend le système à peu près inutilisable. Waddlesplash a corrigé plusieurs problèmes dans le code pour permettre de redémarrer le serveur après un crash, et de le reconnecter avec les applications en cours d’exécution. Ce redémarrage nécessite encore quelques étapes manuelles car les crash démarrent actuellement le debugger automatiquement, mais cela peut être changé par une simple configuration.
Applications
ActivityMonitor
ActivityMonitor affiche sous forme graphique divers paramètres du système: charge CPU, consommation mémoire… Il peut s’exécuter dans une fenêtre ou bien être intégré au bureau sous forme d’un « réplicant ».
Affichage d’un message « pas de capteurs de température » à la place du graphe de température du système si l’information n’est pas disponible (OscarL).
Correction d’un problème de localisation, certains fichiers sources n’étaient pas pris en compte et les chaînes contenues dedans ne pouvaient pas être traduites (humdinger).
Terminal
Le Terminal permet d’exécuter des applications en ligne de commande.
Synchronisation du presse-papier interne du Terminal avec celui du système seulement une fois au démarrage de l’application, et pas lors du changement d’onglet comme cela avait été implémenté au trimestre précédent (OscarL).
Correction d’un problème qui masquait le signal SIGUSR1 pour les shells et autres processus lancés dans le terminal (korli).
Implémentation des séquences d’échappement permettant aux applications CLI de définir des liens hypertextes (en complément des liens qui étaient déjà détectés automatiquement par le terminal en fonction du contenu du texte) (korli).
HaikuDepot
HaikuDepot est l’interface graphique du gestionnaire de paquets. Il utilise un backend en ligne en Java pour stocker et récupérer les captures d’écrans, et notes d’utilisateurs, icônes des paquets, liste de paquets mis en avant, et d’autres informations.
L’application est plus robuste en cas de problème de réseau : gestion des erreurs et affichage de messages clairs pour l’utilisateur. Gestion en particulier des erreurs 503 remontées par l’API web utilisée par HaikuDepot (apl).
Ajout de filtres pour trouver facilement les applications « natives » (n’utilisant pas Qt ou GTK) et d’un filtre « desktop » pour trouver les applications graphiques (et filtrer un très grand nombre de paquets de bibliothèques, applications en ligne de commande…) (apl, avec des améliorations par humdinger pour clarifier la terminologie).
Amélioration de la taille de la fenêtre des conditions d’utilisation sur les écrans haute densité (nipos).
Refonte de la gestion des identifiants de messages internes à l’application HaikuDepot pour en simplifier la maintenance (apl).
Interdiction de la sélection multiple dans la liste des paquets (apl).
WebPositive
WebPositive est le navigateur web fourni avec Haiku. Il est basé sur le moteur WebKit, co-développé avec Apple, Sony, Igalia et d’autres participants.
Modification du message envoyé au Tracker pour ouvrir le dossier contenant un fichier (par exemple un téléchargement), pour utiliser le message officiellement prévu à cet effet plutôt qu’un moyen détourné (humdinger).
Meilleure gestion des noms de fichiers longs dans la fenêtre de téléchargements avec l’ajout d’une barre de défilement horizontal (mull, avec un petit correctif par humdinger pour corriger un décalage d’un pixel du positionnement de la barre de défilement).
Un chantier est en cours pour réintégrer à nouveau le portage de WebKit pour Haiku dans les sources upstream. Cela avait déjà été fait en 2010, mais n’avait pas été maintenu par la suite, ce qui a conduit à retirer ce code. Depuis, Haiku utilise un fork resynchronisé régulièrement, mais cela génère du travail en plus. L’envoi du code est aussi l’occasion de faire relire toutes les modifications par les autres développeurs de WebKit, avec des conseils pour améliorer et simplifier l’architecture.
Expander
Expander est une application permettant de décompresser des archives.
Correction d’un décalage d’un pixel de la barre de défilement (humdinger).
AboutSystem
AboutSystem affiche quelques informations sur le système et surtout la liste des auteurs de Haiku.
Simplification du code pour la mise à jour automatique des couleurs, mise en place de la mise à jour automatique des couleurs pour la liste des crédits (si on passe en mode sombre par exemple) (jscipione).
Ouverture de la fenêtre avec une taille respectant les proportions du nombre d’or, esthétiquement plus plaisant (axeld).
LaunchBox
LaunchBox est un « dock » permettant de stocker des raccourcis vers des applications ou fichiers fréquemment utilisés.
Correction de la couleur du panneau de LaunchBox, et d’autres couleurs dans le sélectionneur de couleurs standard (nephele).
Tracker
Tracker est l’explorateur de fichiers. Le code du Tracker contient également les fenêtres « ouvrir » et « enregistrer sous », mises à disposition des autres applications sous forme de la bibliothèque libtracker.so.
Envoi de la notification d’activation de l’espace de travail à tous les réplicants, afin que ces derniers puissent ajuster leur couleur (par exemple) en fonction de l’espace de travail actif (jscipione).
Correction du positionnement du champ de texte lors du renommage de fichiers dans la vue par icônes, résolution de problèmes de gestion de l’état des fenêtres après un glisser-déposer avorté, affichage des volumes disque en premier (avant les dossiers) si l’option « trier les dossiers en premier » est active, synchronisation en direct des fenêtres de sélection de fichiers lors du changement d’options, et divers nettoyages de code (jscipione).
Ajout d’une bordure manquante dans les fenêtres de sélection de fichiers (nipos).
Ajout du nombre d’élément sélectionnés (en plus du nombre d’éléments total du dossier) dans les fenêtres du Tracker (nathan242).
Correction d’un problème de concurrence dans le constructeur des fenêtres de sélection de fichiers, dont la conséquence était une mauvaise disposition des contrôles dans la fenêtre (certains boutons apparaissant superposés par exemple (PulkoMandy).
Amélioration de l’image d’aperçu qui suit la souris lors d’un glisser-déplacer lorsqu’on déplace beaucoup de fichiers: l’image est tronquée pour ne pas être trop grande mais le dégradé de transparence sur les bords n’était pas bien calculé (PulkoMandy).
Déclenchement automatique du « renifleur » de type MIME, qui identifie automatiquement les fichiers pour les afficher avec la bonne icône par exemple. En particulier cela rend l’utilisation du Tracker plus confortable sur les systèmes de fichiers ne permettant pas de stocker le type MIME dans un attribut étendu (Jim906).
Correction d’une régression sur la mise à jour en direct des tailles et dates de modification de fichiers dans les résultats de requêtes (waddlesplash).
MediaPlayer
MediaPlayer est une application pour lire des fichiers média (son et vidéo).
Correction de la couleur du texte dans la fenêtre d’informations (nephele).
Dans cette même fenêtre, le champ indiquant le chemin du fichier en cours de lecture est maintenant cliquable (nathan242, dont c’est la première contribution).
Ajout d’une détection automatique du type MIME des fichiers, s’il n’est pas renseigné (par exemple s’il est stocké sur un système de fichiers où il n’y a pas d’attributs étendus) (DigitalBox98).
Sudoku
Sudoku est un jeu de Sudoku, très utile pour patienter pendant une compilation un peu longue.
Amélioration de la palette de couleurs en mode clair (le mode sombre nécessite encore du travail) (axeld).
DeskBar
DeskBar est la barre des tâches de BeOS et de Haiku. La même application contient également le code pour la fenêtre de changement de tâches « Twitcher ».
Correction d’un bug qui faisait apparaître des applications en double dans le « Twitcher » (la fenêtre de changement rapide d’application qui apparaît avec le raccourci Alt+Tab) (madmax).
People
People est un gestionnaire de contacts. Il stocke les contacts dans des fichiers « person » avec les informations sous forme d’attributs étendus.
Correction du défilement avec la molette de la souris ou au touchpad (nipos).
Lecteur MIDI
Le lecteur MIDI permet d’écouter des fichiers au format MIDI.
MidiPlayer
Le lecteur MIDI permet d’écouter des fichiers au format MIDI.
Changement de la couleur du contrôle de volume qui était codée en dur, non configurable et pas harmonisée avec le reste du système (nipos).
ProcessController
ProcessController affiche la charge CPU et l’occupation mémoire dans la DeskBar. Il permet également d’afficher des statistiques par application, et de débugger et stopper des applications via un menu popup.
Ajustements pour les écrans haute densité : taille des menus, largeur par défaut de l’icône réplicant, autorisation du redimensionnement de la fenêtre principale (la taille est conservée si on installe ensuite un réplicant sur le bureau, et divers autres changements (waddlesplash).
Installer
Installer permet de cloner l’installation de Haiku actuelle vers un autre disque.
Ajout d’un outil pour copier automatiquement le chargeur de démarrage sur la partition EFI du système, pour réduire le nombre d’étapes manuelles pour installer Haiku correctement sur un système EFI (PawanYr, avec des améliorations de kallisti5 pour nommer le fichier installé correctement en fonction de l’architecture CPU du système).
Amélioration du calcul de la taille de la fenêtre « EULA » de l’installeur (qui affiche non pas un contrat de license, mais un message de bienvenue et quelques instructions pour l’installation), en fonction de la taille de texte sélectionnée dans les préférences (nipos).
Mail
L'application Mail permet de lire et de rédiger des e-mail. Elle fonctionne en collaboration avec le mail_server qui s’occupe de l’envoi et de la réception des messages.
Correction d’une régression sur la couleur de fond des champs d’adresse. Mise en place des mises à jour de couleurs automatiques. Les champs désactivés ou en lecture seule sont maintenant « navigables » (avec la touche tab) et le texte peut être sélectionné et copié (jscipione).
Les boutons « suivant » et « précédent » peuvent être accompagnés de la touche Maj. (Shift), pour changer de message sans modifier le statut « lu » des messages (humdinger).
DriveSetup
DriveSetup permet de configurer les supports de stockage: formatage, partitionnement.
L’application s’affiche sur tous les espaces de travail si la DeskBar n’est pas lancée. Ce cas particulier est utile lors du lancement de l’installation de Haiku, dans ce cas, le bureau n’est pas lancé, mais il est tout de même possible d’utiliser plusieurs espaces de travail, ce qui peut donner l’impression que les fenêtres ont disparu si on se retrouve sur un espace vide (PulkoMandy).
Finalisation d’un patch datant d’il y a plusieurs années pour ajouter un menu permettant d’écrire ou de lire des images disques depuis ou vers des partitions (plus besoin d’utiliser dd en ligne de commande) (sed4096, avec l’aide d’humdinger pour des améliorations de localisation et de choix de vocabulaire).
Debugger
Debugger permet de débugger les logiciels fonctionnant dans Haiku.
Correction d’un crash lorsqu’on essaie de débugger des applications trop grosses, par exemples celle utilisant Mesa et llvmpipe pour faire du rendu 3D (waddlesplash).
Changements transverses
Modification de toutes les chaînes de caractères où le nom d’une application est présent (par exemple : « Deskbar preferences »). En effet, le nom des applications peut optionnellement être traduit, et toutes ces chaînes doivent donc s’adapter dans les deux cas (nom d’application traduit ou conservé en anglais selon les préférences de l’utilisateur). Auparavant ce réglage ne pouvait pas être appliqué de façon systématique (humdinger).
Fenêtres de préférences
Réseau
Affichage de l’état précis des interfaces réseau (en cours de configuration DHCP, par exemple) et pas seulement « hors ligne » ou « en ligne » (nipos).
Périphériques d’entrée
Ajout et amélioration de plusieurs options pour la gestion des touchpad (samuelrp84). Voir plus bas les informations sur la réécriture du pilote Elantech qui donne plus de détails.
Apparence
Envoi d’un seul message de mise à jour aux applications lorsque plusieurs couleurs changent simultanément, ce qui est plus efficace et réduit les « clignotements » d’applications dans certains cas (nephele).
Correction de la hauteur des fausses barres de défilement visibles dans la fenêtre d’apparence pour configurer les barres de défilement (jscipione).
Outils en ligne de commande
Le réplicant NetworkStatus peut être installé dans la DeskBar via la ligne de commande même lorsque une fenêtre de NetworkStatus est déjà ouverte (nipos).
Ajout dans strace de l’affichage des structures stat, sockopt, sigset_t, sigprocmask et des noms de signaux (korli avec un correctif par nathan242).
Correction de plusieurs problèmes dans l’interface en ligne de commande du Debugger, qui conduisaient entre autres à un gel de l’application. Cette interface est surtout utilisée en cas de crash d’un service critique (app_server, registrar, input_server) qui empêcherait l’utilisation de l’interface graphique. Les investigations sur ces services en cas de crash sont donc facilitées.
Modification de bfs_tools pour rendre ces outils compilables sur le système hôte utilisé pour compiler Haiku (axeld). Ces outils permettent de manipuler à la main un système de fichiers bfs, de récupérer certaines données sur un disque corrompu, et diverses manipulations de bas niveau.
Modification de l’outil makebootable pour vérifier que la partition à rendre bootable est bien une partition BFS. Utiliser l’outil sur une partition d’un autre format ou sur un disque complet pourrait corrompre le système de fichier et rendre les données inaccessibles. Ajout d’un message indiquant lorsque le lancement de makebootable n’est pas nécessaire, car les utilisateurs continuent de recommander de lancer cet outil sans aucune raison pour corriger des problèmes de démarrage (encourageant ainsi les nouveaux utilisateurs à faire dea mauvaises manipulations et à corrompre leur système de fichiers) (PulkoMandy).
Suppression d’une verrue dans checkfs qui n’est plus nécessaire suite à des améliorations du Storage Kit. Cela permet de lancer checkfs pour vérifier une partition en donnant le chemin de n’importe quel fichier contenu dans la partition (waddlesplash).
Activation du support de l’IPv6 dans telnet, implémentation dans netstat (avec des corrections sur les opérateurs de filtrage), et ajout de traceroute6 compilé à partir des sources fournies par NetBSD (cmeerw).
Amélioration de l’affichage de df avec plus d’informations, un listage sur deux lignes, et l’ajout d’une option pour afficher les informations standardisées dans le format imposé par POSIX (nipos et PulkoMandy).
Kits
Interface
L'interface kit se charge de tout l’affichage de fenêtres à l’écran et des contrôles de base (boutons, cases à cocher…)
Correction d’un bug dans le contrôle calendrier lors du déplacement vers les mois suivant ou précédent (nipos).
Correction d’un problème de navigation au clavier dans une ListView après l’insertion de nouveaux objets (nipos).
BControl (la classe parente de tous les contrôles d’interface graphique) ne change plus ses couleurs dans la fonction AttachedToWindow(). Ce n’est plus nécessaire après d’autres changements dans BButton, et cela simplifie le code nécessaire pour personnaliser la couleur d’un contrôle spécifique (pour avoir, par exemple, un bouton rouge) (jscipione).
Améliorations sur BSlider: correction de la couleur du texte, nettoyage du code de dessin des sliders dans les implémentations de ControlLook (jscipione).
Augmentation de la taille des marques sur les boutons des barres de défilement (marques qui ne sont pas activées à moins de modifier manuellement un fichier de configuration) pour les rendre plus visibles sur les hautes résolutions (jscipione).
Les infobulles utilisent un espacement calculé en fonction de la taille du texte, pour une apparence plus jolie sur les écrans à haute densité (waddlesplash).
Séparation du titre des onglets et du nom des vues qui leurs sont attachées. Le comportement original est hérité de BeOS, il est donc préservé pour les anciennes applications. Mais pour les nouvelles applications, le nom interne des vues ne doit pas être traduit (il peut être utilisé par des scripts ou par le code de l’application), tandis que le titre affiché à l’utilisateur doit l’être. C’était le seul endroit où ce principe de séparation du nom et du texte affiché n’était pas respecté (KapiX et PulkoMandy).
Dans BTextView, la fonction « tout sélectionner » déplace le curseur à la fin de la sélection (et à la fin du texte). C’est le comportement de la plupart des autres systèmes, et préserver la position du curseur dans ce cas ne semble pas particulièrement utile (OscarL).
Correction de problèmes de choix de couleurs dans le code qui dessine des sliders avec un curseur triangulaire (nipos). Cette correction a permis d’utiliser ce type de slider dans les fenêtres de réglages de différents traducteurs d’images.
Amélioration de l’apparence des cases à cocher partiellement cochées. Elles s’affichent avec un signe "-" au lieu d’une croix. Auparavant, la couleur était subtilement modifiée mais ce n’était pas très visible (PulkoMandy).
Implémentation de la lecture d’un son dans BAlert, qui peut être activé dans les préférences de son (sed4096).
Storage
Le storage kit permet de contrôler les disques et supports de stockage (liste des partitions, montage et démontage, accès aux fichiers, chemins d’accès, requêtes…)
BPartition retourne une erreur B_BUSY si on essaie de monter une partition qui est déjà montée à un autre endroit, au lieu de la monter une deuxième fois (jscipione).
Device
Le device kit permet l’interaction directe avec certains périphériques (USB, série, joysticks…) directement depuis l’espace utilisateur.
Correction de problèmes de gestion de la mémoire dans BUSBInterface mis en évidence par AtomoZero lors d’expérimentations pour corriger le pilote de webcam à l’aide d’un LLM (waddlesplash).
Package
Le package kit se charge de la résolution des dépendances entre paquets et du téléchargement des paquets à installer. Il fonctionne en collaboration avec le système de fichier packagefs qui permet d’accéder au contenu des paquets une fois installés.
Retravail de BRepositoryCache pour remonter les informations sur les paquets via un callback plutôt que de remplir une structure BPackageInfoSet. Cela économise beaucoup de mémoire et d’allocations mémoire, et rend donc la lecture des dépôts de paquets plus rapide (waddlesplash).
Serveurs
Notifications
Le serveur de notifications permet d’afficher des notifications pour les évènements importants.
Les notifications déjà affichées changent de position immédiatement lorsque la configuration est modifiée pour les déplacer (nipos).
Ajout de la lecture de l’effet sonore choisi dans les préférences de son lors de l’apparition des différents types de notification, et à chaque pourcent de progression pour les notifications contenant une barre de progression (sed4096).
Choix de couleurs plus visibles en mode sombre pour les icônes de fermeture et de repli des notifications en mode sombre (nipos).
Network
Le serveur de réseau se charge de la configuration des interfaces réseau, de la configuration des routes et d’autres aspects liés à la communication en réseau.
Refonte du client DHCP dans net_server pour l’exécuter dans un thread séparé et ne pas bloquer la boucle de messages principale. Cela corrige plusieurs cas de gel du serveur lui-même et d’application qui communiquent avec de façon synchrone, par exemple les préférences de réseau. Cela permet également de plus facilement arrêter les requêtes DHCP lorsqu’une interface est reconfigurée avec une adresse statique (waddlesplash).
app_server
app_server est le serveur graphique gérant l’affichage à l’écran.
Correction de crashs causés par des verrous de concurrence manquants et des opérations effectuées dans le mauvais ordre (waddlesplash).
Retrait d’identifiants de messages qui n’étaient plus utilisés dans le protocole de communication entre les applications et le serveur graphique (X512).
Ajout du code nécessaire pour le tracé de lignes avec un dégradé de couleur (dans app_server et dans BView). Auparavant, les dégradés étaient utilisables uniquement pour le remplissage des formes et pas pour les contours (x512).
Modifications de BPicturePlayer pour utiliser une classe C++ avec de l’héritage plutôt qu’une structure C contenant des pointeurs de fonctions. Cela permet de vérifier que l’interface est implémentée correctement par les différents utilisateurs de cette classe dès la compilation (X512).
Implémentation de méthodes manquantes dans BoundingBox player, une implémentation de BPicturePlayer qui calcule un rectangle assez grand pour contenir tout le dessin réalisé par un objet BPicture rejoué via BPicturePlayer (KapiX).
Retrait d’une optimisation incorrecte dans le traitement des calques avec une opacité globale de 100% dans app_server. Ces calques peuvent tout de même contenir des pixels avec de la transparence, et doivent donc être applatis en tenant compte du canal alpha comme tous les autres calques (KapiX).
Modification du protocole app_server pour l’opération permettant de récupérer les « bounding boxes » pour chaque caractère d’une chaîne. L’interface précédente utilisait un tableau de structures, la nouvelle API est inversée ce qui permet d’envoyer plus efficacement 2 tableaux de types primitifs (X512).
Pilotes de périphériques
Implémentation correcte des timeouts sur les commandes dans le pilote SDHCI, ce qui corrige la compatibilité avec plusieurs lecteurs de cartes SD (PulkoMandy).
Réécriture complète du pilote pour les touchpads Elantech. Cette série de changements retouche tout d’abord la gestion des touchpads en général, avec des améliorations sur la documentation, sur le traitement des données invalides, la configuration par défaut, et un nettoyage du code.
Ensuite, le code pour la reconnaissance de « gestures » a été amélioré pour mieux reconnaître plusieurs mouvements tels que le « tap » pour cliquer, le défilement en faisant glsser 2 doigts sur le touchpad, etc. Les préférences du touchpad ont reçu plusieurs nouvelles cases à cocher pour configurer ces différentes options.
Enfin, la dernière partie du patch met à jour le pilote Elantech pour reconnaître les 4 versions du protocole, dont la dernière est entièrement testée. La version 2 ne fonctionne pas correctement pour l’instant (elle a été désactivée pour l’instant) et les versions 1 et 3 sont activées de façon expérimentale dans les nightly builds en attendant les retours d’utilisateurs (samuelrp84).
Normalisation des noms de volumes générés par le pilote usb_disk pour s’assurer qu’ils ne commencent pas par des espaces (madmax).
Mise à jour de la couche de compatibilité FreeBSD avec FreeBSD 15 et synchronisation de tous les pilotes réseau (ethernet et wifi) concernés avec ceux de FreeBSD 15. Synchronisation du pilote rtl8125 avec OpenBSD (waddlesplash).
Ajout de « quirks » et de code supplémentaire dans le pilote I2C-HID pour essayer de se rapprocher du comportement implémenté dans Linux. Ce pilote ne fonctionne pas correctement pour l’instant et il est désactivé par défaut (Lt-Henry).
Systèmes de fichiers
Ajout d’un bouchon supplémentaire pour une API qui n’a pas besoin d’être implémentée dans userlandfs-FUSE (qui permet l’utilisation de systèmes de fichiers FUSE sous Haiku). Cette modification permet d’utiliser en particulier le système de fichier squashfs-fuse pour accéder à des volumes squashfs (OscarL).
Toujours dans userlandfs-fuse, propagation de l’état « lecture seule » du système de fichier, ce qui permet au Tracker de clairement afficher ces systèmes de fichiers comme étant en lecture seule (fond de fenêtre grisé, désactivation des opérations modifiant les fichiers) (OscarL).
Dans le système de fichiers UDF, amélioration des messages de logs, et correction d’un kernel panic déclenché par une assertion suite à des modifications précédentes dans le VFS (waddlesplash).
Correction d’un double lock dans le pilote FAT qui pouvait déclencher un kernel panic dans de rare cas (Jim906).
Intégration d’une partie des patchs permettant de redimensionner une partition BFS. Ce travail avait été commencé en 2014 dans le cadre du Google Summer of Code mais n’avait pas pu être terminé dans les temps. La série de patch est restée à l’abandon pendant de longues années, mais elle est en train d’être finalisée pour pouvoir en intégrer au moins une première partie (axeld).
Correction d’un bug sur la gestion des timestamps avec un nombre de secondes entier pour BFS. Historiquement dans BeOS, il n’existait pas d’API POSIX pour stocker une date de modification avec une résolution plus fine qu’une seconde. Cela conduisait de très nombreux fichiers à avoir le poids faible de leur date de modification à 0, avec pour conséquence une dégradation de la répartition dans les tables de hachage pour l’exécution de requêtes. Dans ce cas, BFS stocke une valeur aléatoire générée en interne par le système de fichier dans les bits de poids faible. Le bug était que cette valeur pouvait être exposée à l’espace utilisateur, entraînant de mauvais résultats sur la gestion des comparaisons de dates (par exemple pour déterminer les règles à lancer dans un makefile). La représentation interne a été légèrement modifiée pour bien distinguer les fichiers pour lesquels une date précise a été enregistrée, de ceux pour lesquels il s’agit de bits aléatoires. Ces derniers peuvent ainsi être filtrés et masqués pour l’espace utilisateur (PulkoMandy).
Correction de la gestion des dossiers déjà existants dans write_overlay. Ils'agit d’un système de fichier qui permet de stocker en RAM des écritures temporaires sur un système de fichier monté en lecture seule, utilisé en particulier pour l’exécution en mode live CD (nathan242).
Le pilote BTRFS ne déclenche plus un kernel panic lorsqu’il rencontre un type de compression inconnu, à la place, il retourne simplement une erreur et considère que le fichier ou dossier concerné ne peut pas être lu (AbdullahZulfiqar2005).
Des changements sur les requêtes (pour BFS et les autres systèmes de fichiers capables d’exécuter des requêtes): une petite optimisation du code, une modification pour ne pas notifier les suppressions de nœuds via B_QUERY_WATCH_ALL car elles sont déjà notifiées par d’autres moyens. L’API B_QUERY_WATCH_ALL est maintenant considérée comme stable, et a donc été ajoutée dans la documentation officielle (waddlesplash).
libroot &amp; kernel
Réseau
Implémentation des sockets du domaine UNIX de type SOCK_SEQPACKET. Pour ce faire, modifications de recv et send pour accepter des buffers de taille 0 (pour les socket de type datagramme, pas les streams). Implémentation de MSG_TRUNC et MSG_PEEK pour les sockets du domaine UNIX, et amélioration de la gestion des adresses invalides dans accept et recv pour se rapprocher du comportement de Linux et des BSD (korli).
Implémentation de la découverte de MTU de chemin complet pour TCP et IPv4 (waddlesplash) et IPv6 (cmeerw). L’algorithme mis en place est simpliste, mais permet d’établir des communications dans des cas où le MTU est limité et la fragmentation de paquets n’est pas mise en place.
Nettoyage des buffers mémoire utilisés pour stocker des adresses dans le code de gestion du réseau. Cela corrige un comportement incorrect dans le cas où une adresse de socket UNIX n’est pas terminée par un caractère NUL (cas qui est explicitement autorisé sous Linux car l’information de longueur de l’adresse est disponible par ailleurs) (Anarchos pour la correction du cas où la vérification était oubliée, suivi d’un nettoyage par waddlesplash pour avoir une solution plus systématique à tous les endroits où ce cas particulier doit être pris en compte).
Un socket TCP qui est fermé alors que des données sont reçues par le noyau mais pas encore lues par l’application associée renvoie un paquet RST plutôt qu’un FIN. Ceci corrige un problème détecté dans les tests du langage Go. Modification du comportement lors de la réception d’un reset pendant la fermeture d’un socket TCP pour se comporter comme les autres systèmes (korli).
Déplacement du fichier networks utiliser par getnetent dans le dossier data, avec les autres fichiers de configuration du réseau. Il était placé par erreur dans /etc, qui est son chemin habituel pour d’autres systèmes UNIX (PulkoMandy).
Report d’une correction faite par NetBSD dans le résolveur DNS, il manquait une partie de l’initialisation de certains objets dans l’état du résolveur (cmeerw).
Gestion de l’IPv6: correction d’un problème dans la mise à jour du cache pour le protocole NDP (neighbor discovery), implémentation du multicast (cmeerw).
Remplacement des fonctions inet_net_ntop et inet_net_pton par l’implémentation d’OpenBSD, qui est plus respectueuse du standard que celle de NetBSD utilisée auparavant (korli).
Gestion des processus
Autorisation de l’appel de exec() depuis un autre thread que le thread principal du processus, pour se mettre en conformité avec POSIX et corriger un problème avec les outils de compilation d’OCaml (korli).
Préservation des signaux masqués lors de l’appel à fork (korli).
Mise en conformité POSIX des codes d’erreurs retournés dans certains cas dans la gestion des groupes de processus (waddlesplash).
Librairie C standard
Mise en conformité POSIX-2024:
Ajout des fonctions ffsl et ffsll dans strings.h (korli)
Ajout de getresuid(), setresuid(), getresgid(), setresgid() (korli)
Ajout de la déclaration de posix_spawn_file_actions_add[f]chdir dans les en-têtes publics (la fonction était déjà implémentée mais pas déclarée) (waddlesplash)
Synchronisation de l’implémentation de arc4random avec la dernière version d’OpenBSD (korli).
Correction d’un bug dans la gestion des locales, il n’était pas possible de changer seulement certaines catégories (date, format monétaire, messages), tout était forcément dans la même langue (waddlesplash).
Correction de plusieurs problèmes dans la famille de fonctions strftime (waddlesplash):
ajout des formats %k et %l,
correction sur la gestion des caractères d'espacement Unicode,
correction de problèmes sur la gestion des fuseaux horaires.
Correction de problèmes mineur de compatibilité POSIX: ajout de déclarations de fonctions et de constantes dans search.h, unistd.h, semaphore.h, nettoyage dans limits.h, définition de getlocalename_l… (waddlesplash).
Remplacement de strtok_r par l’implémentation de musl (waddlesplash).
Refonte du stockage et de la gestion des données ctype pour rendre le code plus facile à maintenir, supprimer des indirections inutiles et corriger un problème de thread safety (waddlesplash).
Nettoyage des fonctions de conversions d’encodage de caractères, et correction de la valeur de MB_CUR_MAX pour l’encodage UTF-8 (waddlesplash).
Définition de la constante DEV_BSIZE (mentionnée dans POSIX mais pas obligatoire) dans sys/param.h, modification de tous le code utilisant stat.st_blocks pour utiliser cette constante, y compris des problèmes dans certains systèmes de fichiers qui utilisaient st_blksize à la place. En effet, il n’y a aucun rapport entre la taille de bloc de st_blksize et le nombre de blocs de st_blocks défini juste à côté (waddlesplash).
Gestion de la mémoire
Correction d’une régression du mois précédent sur la gestion des réservations de mémoire lors du découpage d’areas`. Ajout de la possibilité de transférer des réservations de pages pour les caches et areas réservés, pour éviter de réduire les réservations de façon incorrecte. Cela pouvait causer des assertions et des plantages du noyau en particulier lors de l’exécution d’applications utilisant AddressSanitizer (waddlesplash).
Correction d’une fuite de mémoire dans… la gestion de la mémoire, mis en évidence entre autres par l’exécution du compilateur Rust qui consomme beaucoup de mémoire. Au passage, nettoyage du code et des messages de logs dans cette partie du code (waddlesplash).
Entrées-sorties
Modification de la gestion des requêtes d’entrées-sortie pour autoriser les pilotes de périphériques à sous-classer IORequestOwner. En particulier, cela permet au pilote NVMe d’utiliser ces fonctionnalités sans passer par l’ordonnanceur d’I/O générique qui ne se prête pas bien à l’interfaçage de matériel pouvant traiter plusieurs requêtes en parallèle. Cela permet de simplifier du code, supprimer une fonction récursive devenue inutile et finalement corriger un débordement de pile (waddlesplash).
La taille des partitions indiquée dans le bootloader n’était pas correcte : c’était la taille du disque entier qui était affichée à la place (waddlesplash).
Chargeur de démarrage
Correction de l’affichage de caractères unicode dans le menu de démarrage. En particulier cela évite de corrompre l’affichage lors de la navigation dans le système de fichiers pour désactiver certains pilotes de périphériques. Le firmware UEFI utilise de l’UTF16, tandis que le BIOS utilise une page de code IBM non standardisée. Le bootloader utilise en interne de l’UTF-8 et doit donc convertir les caractères dans le bon format dans chaque cas (madmax).
Implémentation du démarrage via le réseau sur les plateformes EFI (avec un peu de nettoyage sur le support réseau dans OpenFirmware et PXE). Cela fonctionne au moins sur l’architecture ARM 32-bit et facilite le développement sur cible réelle: compilation sur une machine, et exécution sur une autre sans devoir entretemps copier le système de fichiers et le bootloader sur une clé USB ou une carte SD (kallisti5 et PulkoMandy).
Un des en-têtes du bootloader utilisait une syntaxe C++, il a été corrigé pour pouvoir être importé depuis du code C si nécessaire (beaglejoe).
Le chargeur EFI ignore les disques qui ont une taille de bloc de 0 octet, ce qui évite une division par zéro lors de tentatives de lire des données (archeYR).
Systèmes de fichiers
Le noyau interdit maintenant de monter plusieurs fois la même partition. Cette vérification avait d’abord été implémentée dans le pilote FAT, puis via une vérification en espace utilisateur, mais ces deux protections étaient insuffisantes et causaient d’autres problèmes (waddlesplash).
Centralisation de plusieurs vérifications au niveau du VFS, par exemple les vérifications de type fichier ou dossier, les modes d’ouverture des fichiers, des codes d’erreurs retournés pour certaines erreurs spécifiques. Ces changements évitent d’avoir des différences de comportement entre différents systèmes de fichiers et s’assurent que le comportement implémenté est bien celui spécifié par POSIX (waddlesplash).
Implémentation de « fallbacks » pour les systèmes de fichiers qui ne savent pas traiter eux-mêmes les opérations SEEK_DATA, SEEK_HOLE et select(), dont en particulier le write_overlay. Cela permet à la commande cp de fonctionner correctement dans ce cas (nathan242).
Outils de debug
Remise en commun de code pour la gestion du MMU dans le chargeur de démarrage EFI. Le code avait été dupliqué pour chaque architecture de CPU supportés, mais il est en fait en très grande partie identique. Regrouper ce code permet de s’assurer que les évolutions sont bien faites de façon synchronisée pour toutes les architectures (PulkoMandy).
Affichage dans le message de kernel panic de la version de Haiku (numéro hrev). Ceci évite de devoir demander ce numéro aux utilisateurs remontant un bug, il est directement inclus dans la capture d’écran de l’erreur (nathan242).
Modification de la valeur retournée par kernel_version dans les informations système. La valeur retournée par Haiku était la version majeure, qui est 1 depuis le début du projet Haiku en 2001. Elle retourne maintenant une version plus complète qui peut être utilisée dans la commande uname pour indiquer la version de Haiku plus précisément (waddlesplash).
Désactivation de la fonction spécifique à BeOS exect sur les architectures qui n’implémentent pas de compatibilité binaire avec BeOS.
Build system
Ajout d’une macro _DEPRECATED dans les en-têtes de base du système, permettant d’indiquer les classes, méthodes et fonctions qui sont obsolètes et à ne plus utiliser. Cela permet d’avoir un avertissement du compilateur lors de leur utilisation dans le code existant, et de commencer à mettre à jour le code, mais sans casser le code déjà écrit pour l’instant. Actuellement, cette macro n’est pas utilisée, des discussions sont encore en cours sur l’opportunité de retirer certaines APIs.
Remise en état des tests pour le bootloader. Ces tests permettent de lancer une partie du bootloader (dont le menu de configuration) sous forme d’un programme s’exécutant sous Haiku. Cela permet de tester une grande partie du code du bootloader sans avoir besoin de redémarrer le système (waddlesplash).
Ajout d’un script pour convertir une disposition de clavier de la console Linux dans le format reconnu par Haiku (mmu_man).
Mise à jour de paquets pour réparer la compilation en mode « bootstrap » (sans dépendances précompilées) pour ARM et ARM64 (PulkoMandy).
Nettoyage et amélioration de la classe utilitaire FunctionTracer qui permet d’afficher facilement une trace de l’exécution de fonctions, avec une indentation indiquant les appels imbriqués. Il existait plusieurs versions de cette classe utilisées à différents endroits dans le code, chacune avec de légères variations. Renommage de fonctions qui pouvaient entrer en conflit avec l’utilisation de debug_printf et modification de cette dernière pour retourner le nombre de caractères imprimés afin de pouvoir l’utiliser de façon interchangeable avec les autres fonctions de la famille printf (PulkoMandy).
Déplacement de plusieurs en-têtes initialement développés pour une utilisation dans le noyau, de headers/private/kernel/util vers headers/private/util, en effet ils implémentent des fonctionnalités assez génériques (listes chaînées, hash tables, opérations sur les bits…) et sont tout à fait utilisables en dehors du noyau (c’est d’ailleurs déjà le cas à plusieurs endroits dans le code de Haiku) (PulkoMandy et waddlesplash).
Retrait de cas ou l’activation de DEBUG=1 (compilation en mode debug avec des assertions supplémentaires) était empêchée pour certains composants du code (waddlesplash).
Modification du code pour inclure les catalogues de traductions automatiquement dans le paquet contenant l’exécutable correspondant. Ce code incluait également les catalogues de toutes les dépendances, ce qui conduisait les catalogues système de libbe.so à être présents plusieurs fois dans plusieurs paquets, occupant inutilement de la place). Ce changement n’est pas tout à fait terminé puisque maintenant certains catalogues ne sont plus inclus du tout (PulkoMandy).
Modernisation de la page d’accueil de WebPositive pour en permettre l’utilisation avec d’autres navigateurs: définition du bon type MIME, utilisation de HTTPS pour télécharger des ressources externes, déclaration de l’encodage du fichier (humbinger).
Modification du « Makefile Engine » pour autoriser des flags de compilation différents entre les fichiers sources C et C++, ce qui permet d’éviter des warnings de compilation dans certains projets compilant des sources C (OscarL).
Il est maintenant possible de compiler Haiku depuis NetBSD (cmeerw).
Réparation de la compilation de test_app_server (KapiX).
Utilisation de la constante B_DEV_NAME_LENGTH plutôt que de la valeur numérique 128 à plusieurs endroits dans le code (jscipione et OscarL).
Suppression de la dépendance du build à la commande bc, en utilisant à la place des expressions arithmétiques du shell POSIX (PulkoMandy).
Réparation de la compilation des tests unitaires, modification des tests utilisant l’interface graphique pour les exécuter avec test_app_server, ajout d’un package haiku_unittests et d’un profil de build qui inclut ce package dans le système de fichier compilé. L’objectif est de pouvoir lancer ces tests automatiquement dans le cadre de l’intégration continue de Haiku (KapiX, à partir d’un travail plus ancien démarré par kallisti5).
Mise à jour de l’année de copyright à 2026 dans le menu de démarrage et dans les métadonnées des paquets générés par Haiku (PulkoMandy).
Mise à jour des paquets précompilés utilisés pour compiler Haiku avec les dernières versions fournies par HaikuPorts pour les architectures x86 et x86_64. L’outil utilisé pour faire cette synchronisation a reçu lui-même quelques évolutions. Le code de Haiku a été légèrement ajusté pour corriger les problèmes de compilation avec ces nouvelles versions des dépendances (waddlesplash, avec l’aide de madmax pour corriger des problèmes de compilation sur RISC-V qui utilise pour l’instant une version plus ancienne de certains paquets).
Documentation
Haiku book
Le « Haiku book » documente les API publiques et s’adresse aux développeurs d’applications pour Haiku. Il complète et remplace petit à petit le Be Book de BeOS, qui est distribué sous une licence CC-BY-ND ne permettant pas de le mettre à jour et de le corriger.
Ajout du chapitre sur le Device Kit (DigitalBox98)
Correction d’une documentation inversée pour un paramètre de BKeyStore (PulkoMandy)
Ajout de documentation pour BSimpleGameSound, BSerialPort, MailAttachment, MailDaemon, mail_encoding (cafeina)
Documentation des déviations connues de Haiku par rapport à la spécification POSIX (PulkoMandy).
Documentation interne
La documentation interne s’adresse aux développeurs et développeuses du système Haiku lui-même.
Correction de fautes de frappe dans le chapitre sur les systèmes de fichiers (OscarL).
Autres nouvelles
Changement de tarification de Netlify
Netlify héberge le site www.haiku-os.org. Ils ont récemment modifié leur tarification pour ajouter un système de crédit en fonction de la bande passante consommée. Haiku bénéficie de l’offre « Open Source », avec une certaine quantité de trafic offerte (en échange de l’affichage d’un logo de Netlify en base de page du site). Cette offre s’est révélée insuffisante dans la nouvelle tarification, surtout suite à une attaque de robots (probablement pour l’entraînement de LLM de mauvaise qualité, puisque ils requêtaient des pages n’existant pas et n’obtenaient que des erreurs 404). Du côté de Haiku, des protections ont été mises en place pour éviter ce genre de problème, mais entretemps, Netlify a facturé le dépassement de bande passante autorisée pour le mois de décembre.
Finalement, après une discussion avec l’équipe de support de Netlify et une vérification de ce qu’il s’était passé, la facture a été annulée, et la consommation de bande passante pour Haiku a été augmentée pour mieux convenir à l’utilisation habituelle générée par Haiku.
Remise sur les rails de HSA (Haiku Support Association)
Actuellement, l’association Haiku inc est la principale organisation recevant des dons et finançant les activités de Haiku (développeur employé, coût d’hébergement de l’infrastructure…).
Cette association est basée aux USA, ce qui est récemment devenu une source d’inquiétude pour certains contributeurs et donateurs de Haiku.
Suite à une discussion sur les forums, il y a donc un regain d’intérêt pour la Haiku Support Association, une autre organisation basée en Allemagne et qui était dormante depuis plusieurs années. Plusieurs personnes ont donc adhéré à cette association et cela va peut-être permettre d’en relancer l’activité et d’assurer une présence en Europe.
En particulier, c’est cette association qui organisait la conférence BeGeistert et le traditionnel « coding sprint » associé, permettant aux développeurs et aux utilisateurs de Haiku de se rencontrer régulièrement. Cette activité avait cessé suite au manque de public pour ces évènements, à une perte d’intérêt des membres de l’association, et une absence de nouveaux adhérents pour relancer les choses. Espérons que cette période d’inactivité soit maintenant terminée et que l’organisation de conférences plus régulières puisse reprendre.]]></description>
      <pubDate>Sun, 22 Feb 2026 08:20:00 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/nouvelles-de-haiku-hiver-2025-26</guid>
    </item>
    <item>
      <title><![CDATA[Agenda du Libre pour la semaine 7 de l'année 2026]]></title>
      <link>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Calendrier Web, regroupant des événements liés au Libre (logiciel, salon, atelier, install party, conférence), annoncés par leurs organisateurs. Voici un récapitulatif de la semaine à venir. Le détail de chacun de ces 41 événements (France: 39, Internet: 2) est en seconde partie de dépêche. lien nᵒ 1 : April
lien nᵒ 2 : Agenda du Libre
lien nᵒ 3 : Carte des événements
lien nᵒ 4 : Proposer un événement
lien nᵒ 5 : Annuaire des organisations
lien nᵒ 6 : Agenda de la semaine précédente
lien nᵒ 7 : Agenda du Libre Québec Sommaire
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
[Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
[FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
[FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
[FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
[Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
[FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
[FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
[FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
[FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
[FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
[FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
[FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
[FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
[FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
[FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
[FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
[FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
[FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
[FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
[FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
[FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
[FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
[FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Wimille] Retrouvez votre liberté numérique – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Pollionnay] Install partie – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Auray] Install Party : adieu Windows, bonjour le Libre – Le samedi 14 février 2026 de 10h00 à 16h00.
[FR Ivry sur Seine] Cours de l’École du Logiciel Libre – Le samedi 14 février 2026 de 10h30 à 18h30.
[FR Illzach] Atelier Linux – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Illkirch-Graffenstaden] Atelier numérique éthique HOP par Alsace Réseau Neutre – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Fontenay-le-Fleury] Conférence : Présentation Git – Le samedi 14 février 2026 de 14h00 à 16h00.
[FR Ramonville St Agne] WordPress : Personnalisation – Le samedi 14 février 2026 de 14h00 à 18h00.
[FR Juvisy-sur-Orge] Permanence GNU/Linux – Le samedi 14 février 2026 de 14h30 à 17h00.
[FR Quimper] Permanence Linux Quimper – Le samedi 14 février 2026 de 16h00 à 18h00.
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
Tous les lundis de 10h à 17h sans interruption, l’association Prends toi en main / atelier abcpc, propose install party, suivi, dépannage, formation et revalorisation à petit prix sous Linux exclusivement.
L’atelier abcpc existe depuis plus de 10 ans et milite exclusivement pour les logiciels libres.
Médiathèque, Médiathèque, 4 place Dastros, Saint Clar, Occitanie, France
https://www.facebook.com/PrendsToiEnMain
linux, permanence, dépannage, formation, adieu-windows, libres, logiciels-libres, abcpc, prends-toi-en-main, install-party [Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
Vous voulez vous engager pour une cause, rencontrer de nouvelles personnes et découvrir la cartographie participative et humanitaire? CartONG vous invite à participer à un ou plusieurs mapathons en ligne! ​​
Venez cartographier les régions encore absentes des cartes pour soutenir les organisations humanitaires et de solidarité internationale qui ont besoin de cartes précises et à jour pour agir plus efficacement en cas de crise ou initier des projets de développement local.
Les ateliers de cartographie sont organisés dans le cadre du projet Missing Maps, qui a pour objectif de cartographier de façon préventive les régions vulnérables aux catastrophes naturelles, crises sanitaires, environnementales, aux conflits et à la pauvreté. On peut penser qu’aujourd’hui toutes les parties du monde sont cartographiées, mais en réalité de nombreuses régions ne possèdent encore aucune carte!
​ Pour qui? Pas besoin d’être un·e expert·e, les ateliers sont accessibles à tout le monde!
​ Où ? 100% en ligne! Un lien de connexion vous sera envoyé après votre inscription
​ ? Avec la plateforme de cartographie libre et contributive OpenStreetMap (OSM, le «Wikipédia des cartes») tout le monde peut participer à la cartographie de n’importe quelle zone de la planète: il suffit d’un ordinateur, d’une souris et d’une connexion internet! Accessibles à tout·es, nous serons là pour vous accompagner pour vos premiers pas avec OSM.
Le programme des mapathons
18h00: Introduction, présentation de la cartographie collaborative et solidaire et démonstration OSM pour les nouveaux·elles
18h30: On cartographie tous ensemble sur un projet
20h00: Fin du mapathon, conclusion sur les contributions de la soirée
Pour s’inscrire c’est par ici
Si vous avez besoin de plus d’info, vous pouvez nous contacter directement à l’adresse suivante: missingmaps@cartong.org
Internet
https://www.cartong.org
cartographie, cartong, osm, humanitaire, libre, mapathon [FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
L’Écurieux et Espéranto-Gironde vous invitent à la découverte de l’espéranto à Sainte Hélène le:
Lundi 9 février 2026 à 18h00
Foyer des sociétés
Allée du Stade
33480 Sainte-Hélène
Venez découvrir cette langue FRATERNELLE, libre, neutre, 15 fois plus facile à apprendre que le français, parlée par Freinet, Jean Jaurès, Louis Lumière, Jean-Paul II, Jules Verne…
Inventée en 1887, l’espéranto est actuellement parlé dans plus de 120 pays sur les 5 continents et est actuellement utilisé par des millions de personnes dans le monde, pour voyager, correspondre, découvrir d’autres cultures, se faire des amis…
Il y aura la projection d’un documentaire suivi de questions débat.
La rencontre est ouverte à tous, espérantistes ou non, membre de l’Écurieux ou non.
Entrée libre et gratuite.
Foyer des sociétés, Foyer des sociétés, allée du Stade, Sainte-Hélène, Nouvelle-Aquitaine, France
https://esperanto-gironde.fr/2026/01/decouverte-de-lesperanto-a-sainte-helene/
espéranto, langue-libre, langage, decouverte [FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
Tous les lundis soir de 19h à 22h (hors jours fériés) à la Bricoleuse.
Rencontrer les bénévoles, poser des questions sur le libre ou l’informatique, les logiciels, l’hébergement, passer de Windows à Linux.
Pour passer votre ordinateur sous linux, nous vous invitons à nous prévenir avant votre passage: contact@alolise.org.
La Bricoleuse, La Bricoleuse, 27 rue de la Ville, Saint-Étienne, Auvergne-Rhône-Alpes, France
https://alolise.org
install-party, aide, logiciel-libre, entraide, alolise, permanence, linux, gnu-linux [FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
Après un rappel sur le générateur de cartes personnalisées uMap, Binnette nous présentera:
Une démo de ses cartes uMap: différents besoins et cas d’usage.
La création de cartes uMap avec des données Overpass
Des scripts pythons de génération de carte uMap
Les limitations de uMap et les problèmes de performance
Informations pratiques
Lundi 9 février 19h – 21h
À la Turbine.coop, 5 Esplanade Andry Farcy, 38000 Grenoble (entrée sur le côté du bâtiment, nous serons dans la salle de réunion au rez-de-chaussée)
Atelier ouvert à tous et à toutes
Inscription souhaitée via ce formulaire La Turbine Coop, La Turbine Coop, 3-5 esplanade Andry Farcy, Grenoble, Auvergne-Rhône-Alpes, France https://wiki.openstreetmap.org/wiki/Grenoble_groupe_local/Agenda#Lundi_9_f%C3%A9vrier_:_atelier_uMap_avanc%C3%A9 openstreetmap, osm, osm-grenoble, umap, logiciels-libres, atelier, rencontre [FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
Vous pouvez venir pour:
découvrir ce que peut vous apporter le numérique libre, éthique et écoresponsable
obtenir de l’assistance pour l’utilisation des systèmes d’exploitation libres (GNU/Linux pour ordinateur et /e/OS pour smartphones)
obtenir de l’assistance pour l’utilisation des logiciels libres (ex: Firefox, Thunderbird, LibreOffice, VLC) et des services Internet éthiques (ex: mél et cloud, travail collaboratif en ligne).
vous faire aider à installer GNU/Linux sur votre ordinateur ou /e/OS sur votre Fairphone, si vous n’avez pas pu venir à notre Install Partie.
Nous vous recommandons d’effectuer une sauvegarde avant de venir, si vous n’êtes pas en mesure de faire, veuillez apporter un support de sauvegarde (disque dur externe ou clé USB de capacité suffisante).
Nos services sont gratuits, vous pourrez néanmoins faire un don à notre association « Libérons nos ordis ».
Remarque: vous pouvez même apporter un ordinateur de bureau – uniquement l’unité centrale (la tour) – nous avons des écrans, claviers et souris à brancher dessus.
VEUILLEZ VOUS INSCRIRE ICI: https://calc.ouvaton.coop/InscriptionPermanenceNumeriqueLibreRouen
La Base, La Base, 5 rue Geuffroy, Rouen, Normandie, France
libérons-nos-ordis, gnu-linux, logiciels-libres, assistance, linux, numérique [FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
Présentation de différents outils concernant les logiciels libres.
Assistance technique.
De préférence sur RDV directement sur le site de l’asso
Maison des associations, Maison des associations, 2 rue des Corroyeurs, Dijon, Bourgogne-Franche-Comté, France
https://desobs.fr
informatique-libre, installation, réemploi, réparation, résilience, résoudre, atelier [Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
L’émission Libre à vous! de l’April est diffusée chaque mardi de 15 h 30 à 17 h sur radio Cause Commune sur la bande FM en région parisienne (93.1) et sur le site web de la radio.
Le podcast de l’émission, les podcasts par sujets traités et les références citées sont disponibles dès que possible sur le site consacré à l’émission, quelques jours après l’émission en général.
Les ambitions de l’émission Libre à vous!
Découvrez les enjeux et l’actualité du logiciel libre, des musiques sous licences libres, et prenez le contrôle de vos libertés informatiques.
Donner à chacun et chacune, de manière simple et accessible, les clefs pour comprendre les enjeux mais aussi proposer des moyens d’action, tels sont les objectifs de cette émission hebdomadaire.
L’émission dispose:
d’un flux RSS compatible avec la baladodiffusion d’une lettre d’information à laquelle vous pouvez vous inscrire (pour recevoir les annonces des podcasts, des émissions à venir et toute autre actualité en lien avec l’émission)
d’un salon dédié sur le webchat de la radio Radio Cause Commune, Radio Cause Commune, Internet https://www.libreavous.org april, radio, cause-commune, libre-à-vous [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, partager leurs connaissances, échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup(https://www.meetup.com/fr-fr/labaixbidouille/) sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
La permanence d’ADeTI est un moment d’accueil avec des bénévoles pour apprendre à utiliser un ordinateur sous GNU/Linux (Ubuntu, Linux Mint, Debian…) mais aussi:
réparer les problèmes de logiciels sur son ordinateur
prendre des conseils pour choisir des logiciels alternatifs
différencier les logiciels libres utilisables pour répondre aux besoins
préserver et réfléchir sur ses usages (vie privée, éthique…)
Mais c’est aussi un moment consacré pour:
partager des connaissances et échanger des savoirs
maîtriser les formats ouverts et la pérennité de ses documents
Confidentialité, intégrité et disponibilité des systèmes d’information
Diversité des alternatives
Indépendance
Nous accueillons également des membres de l’association ALFA-Net et A-Hébergement qui peuvent répondre aux questions concernant Internet, les réseaux et l’hébergement: connexion à Internet, alternatives aux “Box” et aux opérateurs/FAI commerciaux, Neutralité du Net, Vie Privée, Blog, Site Internet/Web…
Centre Socioculturel Gentiana, Centre Socioculturel Gentiana, 90 avenue Maginot, Tours, Centre-Val de Loire, France
https://www.adeti.org
install-party, gull, linux, internet, réseau, adieu-windows, logiciels-libres, gnu/linux, adeti-org, hébergement, permanence [FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
Assistance technique et démonstration concernant les logiciels libres.
Il est préférable de réserver votre place à contact (at) linuxmaine (point) org
Planning des réservations consultableici.
Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, 31 allée Claude Debussy, Le Mans, Pays de la Loire, France
https://linuxmaine.org
linuxmaine, gnu-linux, demonstration, assistance, permanence, logiciels-libres, linux, adieu-windows [FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Centre socioculturel Port-Boyer, Centre socioculturel Port-Boyer, 4 rue de Pornichet, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
Tu as toujours rêvé de créer ton propre jeu vidéo ? Cet atelier est fait pour toi ! Viens apprendre à concevoir un jeu de A à Z: de l’idée de départ à la programmation, en passant par la création des personnages et des décors. Avec Scratch, rien de plus simple et amusant !
Mercredi 11 février: Attention Danger !
Mercredi 11 mars: Shark attack !
2 séances: 14 h et 16 h
Téléphone: 03 83 54 85 53
Médiathèque Jules Verne, Médiathèque Jules Verne, 2 rue de Malines, Vandœuvre-lès-Nancy, Grand Est, France
https://www.vandœuvre.fr/evenement/ateliers-cree-ton-jeu-video-avec-scratch/
mediatheque-jules-verne, atelier, logiciels-libres, scratch, jeu-video [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, de partager leurs connaissances, d’échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
Chaque mercredi soir, l’association propose une rencontre pour partager des connaissances, des savoir-faire, des questions autour de l’utilisation des logiciels libres, que ce soit à propos du système d’exploitation Linux, des applications libres ou des services en ligne libres.
C’est l’occasion aussi de mettre en avant l’action des associations fédératrices telles que l’April ou Framasoft, dont nous sommes adhérents et dont nous soutenons les initiatives avec grande reconnaissance.
Ecospace, 136 rue de la Mie au Roy, Beauvais, Hauts-de-France, France
https://www.oisux.org
oisux, logiciels-libres, atelier, rencontre, sensibilisation, adieu-windows [FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
Les contribateliers sont des ateliers conviviaux où chacun·e peut partager ses outils libres préférés et apprendre à y contribuer !
Hyperlien, Hyperlien, 5 allée Frida Kahlo, Nantes, Pays de la Loire, France
https://contribateliers.org/trouver-un-contribatelier/les-contribateliers-nantais
contribateliers-nantais, atelier, contribuer, libre [FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
Réunion ouverte à tous, adhérent ou pas.
Les réunions mensuelles Hadoly ont lieu tous les 2ᵉ mercredi du mois, à partir de 19h.
Soit en présentiel dans les locaux de la maison de l’écologie – 4 rue Bodin 69001 Lyon
Soit en distanciel sur l’adresse https://jitsi.hadoly.fr/permanence-hadoly.
À propos de cet événement
La permanence (mensuelle) d’Hadoly (Hébergeur Associatif Décentralisé et Ouvert à LYon), chaton lyonnais, est l’occasion d’échanger avec les membres de l’asso sur les services et moyens mis à disposition des adhérents afin de se libérer des Gafams tout en partageant ce que chacun·e aura amené pour grignoter ou boire.
Nous partageons du mail, du cloud, et d’autres services, le tout basé exclusivement sur une infrastructure locale et des logiciels libres. Nous respectons la neutralité du net et la vie privée. Plus largement nous échangeons autour des communs numériques, des cultures libres et de l’éducation populaire par exemple en réalisant ou animant des ateliers d’éducation aux médias.
Vous serez bienvenu pour présenter votre projet, celui de votre organisation, causer communs numériques, cultures libres et éduc pop.
Maison de l’écologie, Maison de l’écologie, 4 rue Bodin, Lyon, Auvergne-Rhône-Alpes, France
https://hadoly.fr
hadoly, chaton, permanence, réunion, discussion [FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
Appel à une rencontre autour d’un verre de bière des amis de Linux de Strasbourg et environs.
Les autres boissons sont explicitement tolérées…
Vous pouvez nous informer de votre envie de participer à l’évènement pour que l’on ne vous oublie pas. Pour cela, vous pouvez envoyer un message sur la liste de diffusion ou sur IRC.
Station de tram: Langstross Grand'Rue, ligne A ou D.
La Taverne Des Serruriers, La Taverne Des Serruriers, 25 rue des Serruriers, Strasbourg, Grand Est, France
https://strasbourg.linuxfr.org
aam, flammekueche-connection, lug-de-strasbourg, appel-à-mousser [FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
L’Association Club Linux Nord Pas-de-Calais organise chaque mois une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Les Mercredi Linux sont des réunions mensuelles désormais organisées le mercredi. Ces réunions sont l’occasion de se rencontrer, d’échanger des idées ou des conseils.
Régulièrement, des présentations thématiques sont réalisées lors de ces réunions, bien sûr, toujours autour des logiciels libres.
Durant cette permanence, vous pourrez trouver des réponses aux questions que vous vous posez au sujet du Logiciel Libre, ainsi que de l’aide pour résoudre vos problèmes d’installation, de configuration et d’utilisation de Logiciels Libres. N’hésitez pas à apporter votre ordinateur, afin que les autres participants puissent vous aider.
Cette permanence a lieu à la Médiathèque Cultiv'Art 6 rue de la Ladrerie, Cappelle en Pévèle
Médiathèque Cultiv'Art, Médiathèque Cultiv'Art, 16 rue de la Ladrerie, Cappelle en Pévèle, Hauts-de-France, France
http://clx.asso.fr
clx, permanence, linux, gnu-linux, logiciels-libres, adieu-windows [FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
Convocation à l’assemblée générale de l’association PauLLA Une Assemblée Générale est convoquée le jeudi 12 février 2026 à 18h. Pour y assister, 2 solutions:
- la version conviviale: venez nous rejoindre dans les locaux d’AGIRabcd (merci Jean-Louis !), 12 Avenue Federico Garcia Lorca à Pau. Très exactement ici: https://www.openstreetmap.org/node/8892972477
Big Blue Button de l’association (ici: https://bbb.paulla.asso.fr/b/ant-mqu-f3p-brn)
Tous les membres de PauLLA à jour de leur cotisation seront en mesure de voter.
L’ordre du jour est le suivant:
Bilan moral 2025
Bilan financier 2025
Renouvellement/Reconduction des membres du bureau
Paiement des cotisations 2026
Adhésion de PauLLA dans les autres assos/collectifs
APRIL
Landinux
autres Projets pour 2026 Accompagnement de 2 associations vers le libre Campagne « candidats.fr » pour les municipales 2026 Install-party à Haut de Gan en mars Install-party à la médiathèque de Lons fin avril Contacts avec le lycée Louis Barthou Le bouncer de CIaviCI, on en parle ? Bug gênant sur le site internet Toi ! Oui, toi, qui est en train de lire cette ligne, qu’as-tu à proposer pour 2026 ? Questions diverses L’assemblée générale sera aussi l’occasion de se sustenter autour d’un buffet improvisé en mode auberge espagnole avec ce que les membres apporteront ce soir-là. Boissons, petits plats sont donc les bienvenus. Essayez autant que possible de vous coordonner sur le canal #paulla sur IRC afin d’éviter que l’on se retrouve avec 12 packs de bière et rien d’autre.
Même chose pour d’éventuels covoiturages: coordonnons-nous sur l’IRC.
Local d’AGIRabcd, Local d’AGIRabcd, 12 avenue Federico Garcia Lorca, Pau, Nouvelle-Aquitaine, France
https://www.paulla.asso.fr/Evenements/assemblee-generale-paulla-2026
gull, paulla, logiciels-libres, projets, futur, assemblée-générale [FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
Le but des soirées de contribution au libre est de proposer un espace de travail partagé aux personnes actives dans le libre en Île-de-France le temps d’une soirée, une fois par mois (le deuxième jeudi du mois plus précisément).
Dit plus court: c’est un lieu avec de l’électricité et une connexion internet. En avant les claviers !
Les soirées de contribution au libre sont faites pour vous si:
vous travaillez sur un projet libre et vous recherchez une atmosphère à la fois conviviale et studieuse pour aller de l’avant et, qui sait, créer des connexions avec d’autres projets libres, vous êtes un collectif autour du libre et vous cherchez un lieu pour vous retrouver physiquement et avancer avec efficacité sur vos chantiers. Si vous n’avez pas envie de contribuer à un projet libre, les soirées de contribution au libre ne sont sans doute pas faites pour vous. Pas de panique, Parinux organise d’autres évènements:
si vous voulez discuter autour du libre: l’Apéro du Libre (APL) est là pour ça ; c’est un rendez-vous fixé tous les 15 du mois ; venez-nous retrouver autour d’un verre pour papoter et refaire le monde (libre), si vous avez un problème informatique: c’est la vocation de Premiers Samedi du Libre (PSL) où vous pourrez trouver des oreilles attentives et compétentes à l’écoute de toutes vos questions. Nous nous réservons le droit de refuser l’entrée aux soirées de contribution au libre à tout personne qui n’en respecterait pas l’esprit. Et, bien sûr, les règles de bienséance habituelles s’appliquent pour que chacune et chacun se sente à l’aise dans un cadre bienveillant.
Si les soirées de contribution vous intéressent, le mieux est de contacter d’abord le CA de Parinux ca@parinux.org. Vous devrez de toute façon nous écrire pour obtenir le code de la porte cochère…
FPH, FPH, 38 rue Saint-Sabin, Paris, Île-de-France, France
https://parinux.org/Soiree-de-Contribution-au-Libre-le-jeudi-12-fevrier-2026
parinux, scl, contribution, contribution-au-libre [FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
Médiathèque de Quimperlé, place Saint Michel, pas d’inscription, entrée libre !
Mickaël, Johann, Alain, et Yves vous accueillent (ou l’un d’eux, on se relaie !).
Conseils, aide et infos pratiques GNU/Linux et Logiciels Libres.
Curieux ? Déjà utilisateur ? Expert ? Pour résoudre vos problèmes, vous êtes le bienvenu ; pas besoin de prendre rendez-vous !
N’hésitez pas à venir avec votre PC si vous voulez une installation de GNU/Linux ou de venir avec votre périphérique récalcitrant (imprimante, scanner…) si possible.
Médiathèque de Quimperlé, place Saint Michel, Quimperlé, Bretagne, France
https://libreaquimperle.netlib.re
dépannage, entraide, gnu-linux, logiciels-libres, point-info, linux, libre-à-quimperlé, médiathèque-de-quimperlé [FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
Tous les vendredis après-midi, venez nous rencontrer lors de nos cafés-conseils et repairs-cafés!
Nous faisons découvrir les logiciels et systèmes libres (et gratuits !)
Plus de Télémétrie, de PC ralentis, une meilleure stabilité et sécurité,
Moins de virus et finie l’obsolescence programmée !
Salle Steredenn, Salle Steredenn, 9 rue du 19 Mars 1962, Lanmeur, Bretagne, France
https://ulamir-cpie.bzh
ulamir, cpie, repair-café, cyber-sécurité, windows10, libre, linux, adieu-windows, bonnes-pratiques, open-source, conseils-numeriques, ulamir-cpie [FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Maison de quartier des Haubans, Maison de quartier des Haubans, 1 bis boulevard de Berlin, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
Tous les 2ᵉmes et 4ᵉmes vendredis du mois (sauf indisponibilité des membres) de 14h30 à 16h30 l’association Ailes-52 vous propose de venir au Café de la Gare à Nogent (52800) pour échanger autour de la découverte des Logiciels Libres.
Vous pourrez:
Demander conseil pour l’acquisition d’un ordinateur reconditionné.
Gérer mes contacts sur mon ordiphone et mon PC.
Installer/configurer un logiciel libre sous Windows, Mac OS ou Linux. (Ex: VLC, Firefox, Thunderbird, LibreOffice, etc.).
Installer et configurer une imprimante/scanner.
Essayer une distribution Linux.
Répondez à cette question: Mon ordinateur ne pourra pas bénéficier de Windows 11, qu’est-ce que je peux faire pour continuer à l’utiliser, installer GNU/Linux sur mon ordi c’est possible?
Café de la Gare, Café de la Gare, 192 rue du Maréchal de Lattre de Tassigny, Nogent, Grand Est, France
https://ailes-52.org
linux, logiciels-libres, gnu-linux, découverte, café, apprentissage, permanence, bureautique, obsolescence, informatique-libre, ailes-52 [FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
Progressivement vous pourrez faire en sorte d’être moins sous l’influence de Google.
Dans cet atelier nous installerons des magasins d’applications libres pour ne plus avoir à utiliser le Google Play Store et s’assurer de pouvoir télécharger des applications libres (éthiques).
Nous installerons également l’application libre NewPipe pour accéder à Youtube sans s.
À noter: cet atelier n’est PAS faisable avec un iPhone / iPad
Inscription sur: https://calc.ouvaton.coop/InscriptionAtelierNumeriqueEthiqueRouen
MJC Grieu, MJC Grieu, 3 rue de Genève, Rouen, Normandie, France
dégooglisation, smartphone, tablette, application, logiciels-libres, libérons-nos-ordis [FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
Venez découvrir l’association Libre en Communs, ses membres et ses activités lors d’un moment de convivialité à La Générale, 39 rue Gassendi, 75014 Paris.
Habituellement le 2ᵉ vendredi de chaque mois – consultez l’Agenda Du Libre pour d’éventuelles mises à jour de dernière minute.
Métro les plus proches: Denfert-Rochereau (RER B, lignes 4 et 6), Mouton-Duvernet (ligne 4), Gaîté (ligne 13).
Vous pouvez apporter de la nourriture pour un repas partagé. Il y a une buvette sur place pour soutenir La Générale.
La Générale, La Générale, 39 rue Gassendi, Paris, Île-de-France, France
https://www.a-lec.org
libre-en-communs, alec, rencontre, apéro, échange-de-savoirs, la-générale [FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
L'OMJC organise avec l’Association Club Linux Nord Pas-de-Calais organise chaque samedi une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Le Centre d’Infos Jeunes a mis en place une démarche d’accompagnement des jeunes aux pratiques actuelles pour l’informatique et le numérique:
Lieu d’accès public à Internet (5 postes avec Wifi libre et gratuit)
Web collaboratif et citoyen pour que chacun puisse trouver sa place et passer du rôle de simple usager à celui d’initiateur de processus collaboratif
Éducation à l’information par les nouveaux médias (diffusion par le biais du numérique)
Logiciels libres (bureautique, sites, blogs, cloud, infographie et vidéo, musique, réseaux sociaux, chat…).
Cette rencontre a lieu sur rendez-vous, tous les samedis matin hors vacances scolaires à la Maison communale de la ferme Dupire, rue Yves Decugis à VILLENEUVE D’ASCQ
OMJC, rue Yves Decugis, Villeneuve d’Ascq, Hauts-de-France, France
https://clx.asso.fr
omjc, clx, permanence, linux, gnu-linux, logiciels-libres, atelier [FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
Rencontre mensuelle autour des logiciels libres, en toute simplicité.
Ces matinées seront ce que nous en ferons ensemble, selon vos attentes:
Découverte des logiciels libres dont Linux et de leur intérêt. Utilisation sur place.
Installations, sur votre machine (pensez à sauvegarder vos données avant de venir avec) ou sur des PC fournis pour apprendre ensemble sans risque. Parfois, on vous propose un ordinateur auquel Linux a redonné une seconde vie, avec lequel vous pouvez repartir…
Préparation d’une clé USB pour tester Linux chez vous, l’installer ou alors pour utiliser des logiciels libres sans installation sous Windows.
Entraide, suivi de votre expérience avec les logiciels libres.
Nous pourrons aussi nous intéresser aux outils en ligne, aux smartphones, ou nous amuser à redonner vie à de vieux PC un peu obsolètes, à reconditionner des ordinateurs pour des associations ou personnes avec peu de ressources, etc.
Pour tout projet qui risque de prendre un peu de temps, il est préférable de nous contacter avant.
Les débutant·e·s sont les bienvenu·e·s! Les autres aussi, bien évidemment !
Maison pour tous, 35 route d’Arenthon, Amancy, Auvergne-Rhône-Alpes, France
https://librealabase.gitlab.io
libre, logiciel-libre, linux, /e/os, gnu-linux [FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
Apportez votre ordinateur
pour y installer des logiciels libres et gratuits
Tous les 2ᵉ samedis 9h-13h de janvier à juin 2026
PROCHAIN: Samedi 14 février 2026 de 9h à 13h
Atelier public &amp; gratuit destiné: aux curieux, aux avertis, à ceux qui veulent faire des économies.
► Remplacer Microsoft Word par LibreOffice Write, Photoshop par Gimp, Outlook par Thunderbird, Google par DuckDuckGo, Gmail par déMAILnagement
SUR INSCRIPTIONS: au 01.43.04.83.53
+ de renseignements par email à franck@sinimale.fr
#adieu-windows
Maison pour tous des Coteaux, Maison pour tous des Coteaux, 30 route de Gournay, Noisy-le-Grand, Île-de-France, France
adieu-windows, install-party, entraide, logiciels-libres, linux, gnu-linux [FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.]]></description>
      <pubDate>Sat, 07 Feb 2026 21:16:41 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[openai/evals]]></title>
      <link>https://github.com/openai/evals</link>
      <description><![CDATA[Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks. OpenAI Evals You can now configure and run Evals directly in the OpenAI Dashboard. Get started → Evals provide a framework for evaluating large language models (LLMs) or systems built using LLMs. We offer an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals which represent the common LLMs patterns in your workflow without exposing any of that data publicly. If you are building with LLMs, creating high quality evals is one of the most impactful things you can do. Without evals, it can be very difficult and time intensive to understand how different model versions might affect your use case. In the words of OpenAI's President Greg Brockman: Setup To run evals, you will need to set up and specify your OpenAI API key. After you obtain an API key, specify it using the OPENAI_API_KEY environment variable. Please be aware of the costs associated with using the API when running evals. You can also run and create evals using Weights &amp; Biases. Minimum Required Version: Python 3.9 Downloading evals Our evals registry is stored using Git-LFS. Once you have downloaded and installed LFS, you can fetch the evals (from within your local copy of the evals repo) with: cd evals
git lfs fetch --all
git lfs pull This will populate all the pointer files under evals/registry/data. You may just want to fetch data for a select eval. You can achieve this via: git lfs fetch --include=evals/registry/data/${your eval}]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/openai/evals</guid>
    </item>
    <item>
      <title><![CDATA[mihail911/modern-software-dev-assignments]]></title>
      <link>https://github.com/mihail911/modern-software-dev-assignments</link>
      <description><![CDATA[Assignments for CS146S: The Modern Software Dev (Stanford University Fall 2025) Assignments for CS146S: The Modern Software Developer This is the home of the assignments for CS146S: The Modern Software Developer, taught at Stanford University fall 2025. Repo Setup These steps work with Python 3.12. Install Anaconda Download and install: Anaconda Individual Edition Open a new terminal so conda is on your PATH. Create and activate a Conda environment (Python 3.12) conda create -n cs146s python=3.12 -y
conda activate cs146s Install Poetry curl -sSL https://install.python-poetry.org | python - Install project dependencies with Poetry (inside the activated Conda env) From the repository root: poetry install --no-interaction]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/mihail911/modern-software-dev-assignments</guid>
    </item>
    <item>
      <title><![CDATA[roboflow/trackers]]></title>
      <link>https://github.com/roboflow/trackers</link>
      <description><![CDATA[Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use. trackers Plug-and-play multi-object tracking for any detection model. Try It No install needed. Try trackers in your browser with our Hugging Face Playground. Install pip install trackers install from source pip install git+https://github.com/roboflow/trackers.git https://github.com/user-attachments/assets/eef9b00a-cfe4-40f7-a495-954550e3ef1f Track from CLI Point at a video, webcam, RTSP stream, or image directory. Get tracked output. Use our interactive command builder to configure your tracking pipeline. trackers track \ --source video.mp4 \ --output output.mp4 \ --model rfdetr-medium \ --tracker bytetrack \ --show-labels \ --show-trajectories Track from Python Plug trackers into your existing detection pipeline. Works with any detector. import cv2
import supervision as sv
from inference import get_model
from trackers import ByteTrackTracker model = get_model(model_id="rfdetr-medium")
tracker = ByteTrackTracker() label_annotator = sv.LabelAnnotator()
trajectory_annotator = sv.TrajectoryAnnotator() cap = cv2.VideoCapture("video.mp4")
while cap.isOpened(): ret, frame = cap.read() if not ret: break result = model.infer(frame)[0] detections = sv.Detections.from_inference(result) tracked = tracker.update(detections) frame = label_annotator.annotate(frame, tracked) frame = trajectory_annotator.annotate(frame, tracked) Evaluate Benchmark your tracker against ground truth with standard MOT metrics. trackers eval \ --gt-dir data/gt \ --tracker-dir data/trackers \ --metrics CLEAR HOTA Identity Sequence MOTA HOTA IDF1 IDSW
----------------------------------------------------------
MOT17-02-FRCNN 75.600 62.300 72.100 42
MOT17-04-FRCNN 78.200 65.100 74.800 31
----------------------------------------------------------
COMBINED 75.033 62.400 72.033 73 Algorithms Clean, modular implementations of leading trackers. See the tracker comparison for detailed benchmarks. Algorithm MOT17 SportsMOT SoccerNet SORT 58.4 70.9 81.6 ByteTrack 60.1 73.0 84.0 OC-SORT — — — BoT-SORT — — — McByte — — — Contributing We welcome contributions. Read our contributor guidelines to get started. License The code is released under the Apache 2.0 license.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/roboflow/trackers</guid>
    </item>
    <item>
      <title><![CDATA[LIVO Next-Level Flutter State Management]]></title>
      <link>https://dev.to/pravin_kunnure_f4663a859a/livo-next-level-flutter-state-management-2mk2</link>
      <description><![CDATA[Flutter is great, but managing state can quickly become messy. You’ve probably used setState for small projects or Provider, Riverpod, or BLoC for bigger apps—but each comes with trade-offs.
**** Note: This story was originally about reactive_orm, which is now deprecated. Its evolution is now called LIVO. LIVO continues its reactive object-relationship state management approach with improved naming, documentation, and long-term support.
. What is LIVO?
Enter LIVO: a lightweight, reactive ORM-style state management library for Flutter.
It lets your UI react automatically when model properties change — no streams, ChangeNotifier, or boilerplate required.
. With LIVO, you get:
Object-wise and field-wise reactivity
Nested and shared models
Many → One and Many Many relationships
Minimal boilerplate, plain Dart models
. LIVO in action:
Object-wise: Update any field and rebuild the entire widget
Field-wise: Only rebuild widgets for selected fields
Many → One: Multiple models feeding a single observer
Many Many: Shared models reflected across multiple parents
. Getting Started:
livo: 1⃣ Creating a Reactive Model
_import 'package:livo/livo.dart';
class Task extends ReactiveModel {
Task({required String title}) : _title = title;
String get title =&gt; _title;
bool get completed =&gt; _completed;
String get status =&gt; status; This is just plain Dart. LIVO will handle notifying widgets when fields change.
2⃣ Object-wise Reactivity
_final objectWise = Task(title: "Object-wise Reactivity");
ReactiveBuilder(
Here, checking the checkbox triggers a rebuild of the entire widget.
3⃣ Field-wise Reactivity (Optimized)
Sometimes, you only want specific fields to trigger a rebuild:
_final fieldWise = Task(title: "Field-wise Reactivity");
ReactiveBuilder(
4⃣ Many → One (Aggregation)
Combine multiple models into a single reactive observer:
_class Dashboard extends ReactiveModel {
Dashboard(this.sources) {
final manyA = Task(title: "Task A");
ReactiveBuilder(
) =&gt; Column( Updating manyA or manyB automatically rebuilds the dashboard widget.
5⃣ Many Many (Shared Models)
Models can be shared across multiple parents, keeping the UI in sync everywhere:
_class Group extends ReactiveModel {
Group({required this.name, required this.tasks}) {
final group1 = Group(name: "Group 1", tasks: [objectWise, fieldWise]);
ReactiveBuilder(
Models extend ReactiveModel
Field setters call notifyListeners(#field) whenever a value changes
ReactiveBuilder widgets listen to either the whole object or specific fields
Nested models propagate changes upward automatically
No streams. No manual wiring. Everything updates safely and efficiently. Why LIVO
Clean Dart models with minimal boilerplate
Fine-grained reactivity for optimized performance
ORM-style mental model for easier app design
Works seamlessly for single fields, nested models, or shared models Links
Pub Package: LIVO
GitHub Repository Tip for Readers
LIVO makes Flutter state management intuitive and fun — without sacrificing performance.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:47:51 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/pravin_kunnure_f4663a859a/livo-next-level-flutter-state-management-2mk2</guid>
    </item>
    <item>
      <title><![CDATA[Bmalph: BMAD planning + Ralph autonomous loop, glued together in one command]]></title>
      <link>https://dev.to/lacow/bmalph-bmad-planning-ralph-autonomous-loop-glued-together-in-one-command-14ka</link>
      <description><![CDATA[I built Bmalph because I kept running into the same problem: AI coding assistants are great at writing code, but terrible at remembering why you made a decision three features ago. You re-explain context, the architecture drifts, things contradict each other. For small projects that's fine. For anything real, it's a slow disaster.
bmalph is a CLI that combines two great existing open source projects: BMAD-METHOD for structured AI planning, and Ralph for autonomous implementation. One command installs and wires them together in your project.
Until last week it only worked with Claude Code. That's changed.
The core idea is simple: separate thinking from doing. Phases 1-3 (BMAD — Planning)
Before any code gets written, you work interactively with specialized AI agents: an Analyst, a Product Manager, and an Architect. They guide you through producing a product brief, a PRD, UX specs, architecture decisions, and user stories. The AI asks questions. You make decisions. Nothing moves forward without your input.
The output is a set of real planning documents that live in your repo under _bmad-output/. Not chat history. Not a summary in your clipboard. Actual files that persist.
Phase 4 (Ralph — Implementation)
Once planning is done, Ralph takes those docs and implements them autonomously. It picks the next story from @fix_plan.md, writes tests first, implements the code, commits, and moves on to the next story. You don't babysit it. The loop runs until the board is empty or the circuit breaker trips.
The key thing Ralph has that vanilla Claude Code doesn't: a persistent spec. When it implements story 8, it still knows what was decided in story 1 because the planning docs are right there.
The initial release assumed Claude Code. That was limiting. This release adds support for six platforms across two tiers. Platform
Tier
What you get Claude Code
Full
Phases 1-4, BMAD + Ralph loop OpenAI Codex
Full
Phases 1-4, BMAD + Ralph loop Cursor
Instructions-only
Phases 1-3, BMAD planning only Windsurf
Instructions-only
Phases 1-3, BMAD planning only GitHub Copilot
Instructions-only
Phases 1-3, BMAD planning only Aider
Instructions-only
Phases 1-3, BMAD planning only The tier split comes down to one thing: whether the platform exposes a scriptable CLI. Ralph is a bash loop that spawns fresh AI sessions, so it needs claude or codex in your PATH. Cursor, Windsurf, Copilot, and Aider don't work that way, so they get the planning workflow only.
That said, Phases 1-3 alone are where most of the thinking happens. If you're on an instructions-only platform you still get the full BMAD workflow: analysts, PRDs, architecture, epics, stories.
Platform detection is automatic. Run bmalph init and it figures out which platform you're using from project markers. Or pass --platform explicitly.
npm install -g bmalph # auto-detect
bmalph init # or be explicit
bmalph init --platform codex
bmalph init --platform cursor This comes up a lot so let me be direct about it.
Just prompting Claude Code / Cursor / Copilot directly
Fine for small tasks. A bug fix, a component, a quick script. bmalph is overkill for that. If you're reaching for bmalph on a weekend project you're over-engineering your workflow.
Where it breaks down is anything with multiple moving parts spanning more than a few sessions. The AI has no memory of earlier decisions, you end up re-explaining context constantly, and the architecture slowly drifts into something nobody planned.
BMAD-METHOD standalone
BMAD is great but setting it up manually in each project is friction. bmalph bundles and installs it with one command, keeps it versioned, and wires it to Ralph. If you're already using BMAD standalone, bmalph init is essentially a migration path — it preserves your _bmad-output/ artifacts.
Devin, SWE-agent, and other fully autonomous systems
Different category. Those are trying to do everything autonomously from a single prompt. bmalph keeps you in the loop during planning — you're making decisions with the AI agents, not handing off a spec and waiting. Ralph only takes over once you've explicitly approved the plan and triggered the implementation.
Earlier versions had version drift issues where the bundled BMAD could get out of sync. That's fixed. The bundled version is now pinned, tested, and the upgrade path is clean.
bmalph upgrade This updates the bundled BMAD and Ralph assets without touching _bmad-output/ — your planning artifacts are never overwritten.
A few things on the roadmap:
Story-level diff summary. After each story, Ralph automatically generates a readable summary of what changed, which files were touched, and whether tests passed. Right now that's buried in raw logs.
Automatic context regeneration. When Ralph gets stuck, the circuit breaker detects it and stops. Instead: automatically run GPC (Generate Project Context) and refresh the context before the next iteration.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:06:10 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/lacow/bmalph-bmad-planning-ralph-autonomous-loop-glued-together-in-one-command-14ka</guid>
    </item>
    <item>
      <title><![CDATA[Des projets open source se révoltent contre les codes "non humains" et veulent fixer des règles et des limites]]></title>
      <link>https://www.programmez.com/actualites/des-projets-open-source-se-revoltent-contre-les-codes-non-humains-et-veulent-fixer-des-regles-et-des-39045</link>
      <description><![CDATA[Plusieurs projets open source ne veulent plus voir de code généré par des agents et autres pratiques de « vibe coding ». Par exemple, Widelands, un jeu open source de stratégie en temps réel, a annoncé le 24 février le blocage des contenus générés par une IA, un agent ou tout autre outil d’IA. "L'équipe de développement de Widelands rédige une résolution visant à refuser les contributions de contenu généré par l'IA (code, graphismes, musique, etc.) au code source et aux dépôts du jeu. Nous estimons que le contenu généré par l'IA est généralement problématique sur les plans éthique et juridique, car il viole les droits d'auteur des créateurs dont les œuvres ont été utilisées pour l'entraînement de l'IA sans leur autorisation et sans mention de leur nom. De plus, nous constatons qu'il est souvent de faible qualité et/ou trop générique, et qu'il ne répond pas aux exigences spécifiques de Widelands. Les demandes de fusion générées par l'IA pourront être fermées sans examen. Widelands est créé par des humains, pour des humains."
Le projet fixe donc des limites sur ce qui est autorisé ou non. Ainsi, les extensions et la documentation sont exclues de cette résolution anti-IA.
L’Electronic Frontier Foundation n'est pas opposée au code généré ni à l'IA en général, mais la fondation veut des règles claires et fixer des limites. Ainsi, dans ses nouvelles politiques, les contributions générées par une IA (au sens global du terme) doivent respecter plusieurs principes : Veuillez vous abstenir de soumettre des contributions que vous n’avez pas parfaitement comprises, examinées et testées. Veuillez indiquer si votre contribution a été générée par une IA. Les descriptions et doivent être rédigés par vos soins. Les responsables du projet peuvent déterminer si les contributions ne sont pas raisonnablement examinables. Il est intéressant de noter qu'ici, l'EFF veut imposer de bonnes pratiques. Le développeur doit comprendre le code généré et ne pas lui faire une confiance absolue, ni le soumettre sans aucune revue. Pour obliger le développeur à lire le code généré, l'EFF veut que les soient rédigés par le développeur lui-même.
Le moteur Godot est dans la même situation et avertit que les PR déposées sont de plus en plus générées par une IA et, à force, cela épuise les mainteneurs. L'alerte est venue de Rémi Verschelde. Un autre développeur, Adriaan, va plus loin et critique même le sens de nombreuses requêtes : "Le dépôt GitHub de Godot est saturé de demandes de fusion générées par des développeurs débutants, ce qui représente une perte de temps considérable pour les relecteurs – surtout si cela n'est pas signalé. Les modifications sont souvent incompréhensibles, les descriptions sont excessivement verbeuses et les utilisateurs eux-mêmes ne comprennent pas leurs propres modifications… C'est un véritable désastre."
Dans la communauté Blender, le problème est identique et une réglementation sur les contributions IA est discutée :
"On observe une augmentation des demandes de fusion générées par l'IA, sans que cela soit clairement indiqué, et dont l'auteur ne comprend pas toujours pleinement la contribution. La revue de code et la maintenance ont toujours constitué un goulot d'étranglement majeur. Lorsque les contributions de l'IA ne sont pas entièrement comprises par le contributeur lui-même, la charge de travail des relecteurs s'en trouve encore alourdie. Dans le pire des cas, les relecteurs communiquent avec l'IA par un jeu du téléphone arabe.
Il est important de former les nouveaux développeurs pour faire évoluer le projet, même lorsqu'il aurait été plus rapide pour le relecteur d'effectuer lui-même les modifications (avec ou sans IA). C'est pourquoi les relecteurs devraient savoir s'ils collaborent avec un humain, afin de pouvoir décider du moment opportun pour fournir cet effort.
Par ailleurs, la revue de code assistée par l'IA et les vérifications basées sur nos directives peuvent potentiellement alléger la charge de travail des relecteurs, contribuer à la correction des bogues et à l'amélioration de la stabilité."
Le projet open source Coolify dit exactement la même chose : trop de PR de mauvaise qualité ou incompréhensibles. Le mainteneur a mis en place une GitHub Action pour détecter une PR trop mauvaise ou générée par une IA. Résultat : 98 % des PR pourraient être fermées automatiquement...
Notre : trop d'IA tue l'IA ? Pas exactement, mais on constate depuis quelques semaines une volonté croissante de projets open source d'agir pour préserver la qualité du code et surtout éviter les requêtes inutiles générées par une IA, sans que la personne comprenne réellement le sens de la demande ni le code soumis. Et c'est là le véritable problème. L'objectif n'est pas de bannir l'IA, mais d'encadrer son usage et de responsabiliser les développeurs. Catégorie actualité: IA Godot, IA Image actualité AMP:]]></description>
      <pubDate>Mon, 23 Feb 2026 09:06:09 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/des-projets-open-source-se-revoltent-contre-les-codes-non-humains-et-veulent-fixer-des-regles-et-des-39045</guid>
    </item>
    <item>
      <title><![CDATA[Revue de presse de l’April pour la semaine 7 de l’année 2026]]></title>
      <link>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Cette revue de presse sur Internet fait partie du travail de veille mené par l’April dans le cadre de son action de défense et de promotion du logiciel libre. Les positions exposées dans les articles sont celles de leurs auteurs et ne rejoignent pas forcément celles de l’April.
[Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€)
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre?
[ZDNET] LibreOffice dénonce le format OOXML
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte lien nᵒ 1 : April
lien nᵒ 2 : Revue de presse de l'April
lien nᵒ 3 : Revue de presse de la semaine précédente
lien nᵒ 4 : Fils du Net [Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux Tiago Gil, le jeudi 12 février 2026.
La centrale d’achat informatique hospitalière (CAIH) engage une nouvelle feuille de route sur cinq ans et initie le programme Alternative, destiné à bâtir un socle numérique souverain pour les systèmes d’information de santé.
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€) Valéry Rieß-Marchive, le mercredi 11 février 2026.
L’Agence nationale de la sécurité des systèmes d’information vient de réitérer son engagement en faveur du logiciel libre. Dans la continuité d’une politique établie et confortée de longue date.
Et aussi: [Le Monde Informatique] L'Anssi formalise sa doctrine open source
[Silicon] L’ANSSI affirme l’open source comme levier de sa politique industrielle
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre? Bertrand Lemaire, le mercredi 11 février 2026.
L’APRIL relance son initiative «Pacte du Logiciel Libre» à l’occasion du prochain scrutin municipal.
Et aussi: [Goodtech] Municipales 2026 en France: l'April lance son pacte du logiciel libre
Voir aussi: L’April propose le pacte du logiciel libre à l’occasion des élections municipales et communautaires de 2026
[ZDNET] LibreOffice dénonce le format OOXML
Le mercredi 11 février 2026.
The Document Foundation (TDF) intensifie sa critique contre Microsoft, accusant le géant américain de privilégier ses intérêts commerciaux au détriment de l’interopérabilité.
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte Aymeric Geoffre-Rouland, le lundi 9 février 2026.
Quand un développeur demande à Claude ou ChatGPT d’écrire du code, l’IA pioche dans des milliers de bibliothèques libres sans que l’humain ne lise jamais leur documentation. Résultat: les mainteneurs de ces projets open-source, qui vivent de la visibilité générée par les visites et les interactions, voient leur audience s’effondrer. Une étude économique chiffre ce paradoxe: l’IA qui accélère le développement logiciel asphyxie l’écosystème qui le rend possible.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:40 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[Nouveautés de février 2026 de la communauté Scenari]]></title>
      <link>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</link>
      <description><![CDATA[Scenari est un ensemble de logiciels open source dédiés à la production collaborative, publication et diffusion de documents multi-support. Vous rédigez une seule fois votre contenu et vous pouvez les générer sous plusieurs formes : site web, PDF, OpenDocument, diaporama, paquet SCORM (Sharable Content Object Reference Model)… Vous ne vous concentrez que sur le contenu et l’outil se charge de créer un rendu professionnel accessible et responsive (qui s’adapte à la taille de l’écran).
À chaque métier/contexte son modèle Scenari :
Opale pour la formation Dokiel pour la documentation Optim pour les présentations génériques Topaze pour les études de cas Parcours pour créer des scénarios de formation et bien d’autres… lien nᵒ 1 : Explication de Scenari
lien nᵒ 2 : Pour démarrer
lien nᵒ 3 : Téléchargements
lien nᵒ 4 : Communauté Scenari
lien nᵒ 5 : Mastodon
lien nᵒ 6 : Bluesky
lien nᵒ 7 : Telegram
lien nᵒ 8 : LinkedIn
lien nᵒ 9 : Canal Peertube Sommaire Visio de découverte de Scenari Parole de Scenariste Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Tu peux parler de Scenari aux conférences éclair de l’April ? Nouvel habillage web pour Optim 24 Mise-à-jour de Myscenari Nouvelles versions d’outils Scenari Le savais-tu ? Le chiffre du mois Nouvelles adhésions d’organisations Visio de découverte de Scenari Tu as des questions sur Scenari avant de tester ?
Cette visio est faite pour toi : jeudi 26 février à 16h sur https://scenari.org/visio/miniwebinaire
Lien Agenda du Libre
Lien Mobilizon Parole de Scenariste
Utilisateur de Canoprof depuis 2019, cet outil est devenu un des piliers de ma pratique d’enseignement en Physique-Chimie (4ᵉ, 5ᵉ, 3ᵉ) et en Sciences (6ᵉ). Je l’utilise pour concevoir l’ensemble de mes supports aussi bien papier que numériques, ce qui me permet de maintenir une cohérence didactique forte sur l’ensemble du cursus collège.
La force de Canoprof réside dans la séparation claire entre le contenu et la forme. En tant qu’enseignant, cela me permet de me concentrer sur le fond pédagogique et la structuration de mes séquences, sans perdre de temps dans les contraintes techniques de mise en page. La richesse de mon fond documentaire, construit depuis plus de six ans, évolue ainsi sereinement au fil des réformes et de mes retours d’expérience.
Canoprof m’aide à formaliser une progression spiralaire efficace tout en générant des supports propres, structurés et accessibles. C’est un gain de productivité précieux qui me permet de consacrer plus d’énergie à l’accompagnement de mes élèves en classe. Guillaume Marmin, enseignant de physique-chimie au Collège Isabelle Autissier. Modèle utilisé : Canoprof Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Les Rencontres Scenari 2026 auront lieu du lundi 22 juin (midi) au vendredi 26 juin (midi) sous le soleil provençal à l'ENSAM Aix-en-Provence.
Bloque ces dates dès maintenant, les détails seront précisés bientôt. Tu peux parler de Scenari aux conférences éclair de l’April ? Lors de la prochaine assemblée générale de l’April (samedi 28 mars 2026 à Paris) il y aura un temps de conférences éclairs (6 minutes) de 10h à 12h qui s’enchaîneront sur des sujets variés, en lien avec le Libre, entendu au sens large.
Si tu utilises Scenari, c’est une bonne opportunité pour parler de tes usages auprès des adhérent⋅e⋅s de l’April. Date limite pour proposer : 15 mars. Envoyer un courriel à confseclairs@april.org.
Il n’est pas nécessaire d’être adhérent⋅e à l’April pour pouvoir proposer une conférence éclair.
Plus de détails sur l’annonce de l’April. Nouvel habillage web pour Optim 24 Un nouvel habillage graphique pour Optim 24 fait son apparition sur la plateforme de téléchargement.
Il existe pour tous les supports web des 3 modalités d’Optim : site normal, site web simple, site web en tuiles. Mise-à-jour de Myscenari MyScenari vient de passer en version 6.4.5 (corrections de bugs dans le cœur et dans les modèles en version 25). Attention : cette version est la dernière à contenir Dokiel 5 et 6, Opale 5 et 24, Optim 3 À partir de la prochaine mise à jour de MyScenari, nous n’aurons plus que Dokiel 25, Opale 25, Optim 24. Pense à migrer tes modèles (et skins) pour ne pas être pris⋅e au dépourvu au dernier moment. Nouvelles versions d’outils Scenari Opale, le modèle phare pour créer vos contenus pédagogiques, passe en version 25.1.1. Au menu, entre autres : corrections dans les outils d’accessibilité, et amélioration de l’intégration de MindMap dans la publication Diapo. Et Opale est maintenant disponible en allemand ! Parcours, pour concevoir des conducteurs pédagogiques, passe en version 25.0.2 (corrections mineures sur le skin, l’éditeur et les vidéos HLS) et est disponible maintenant en français et Anglais. Dokiel, le modèle pour la documentation technique et logicielle, passe en version 25.0.6. Cette version apporte entre autres des corrections dans la publication de relecture et l’écran de contrôle, et l’amélioration des écrans décrits dans les publications Web (maintenant responsive). Optim monte en version dans ses deux saveurs Optim 24.0.7 et OptimPlus 24.0.3 avec des corrections mineures sur les publications Web et Diaporama, et dans le styage. LTI-suite, le serveur pour exploiter des ressources SCORM dans des LMS via LTI, passe en version 2.0.3. Lexico, votre modèle pour créer des lexiques, glossaires, thesaurus, vocabulaires, monte en version 25.0.1 pour apporter des corrections mineures dans la publication Web. SCENARIchain-desktop est à présent disponible en français, en anglais et en espagnol. Le savais-tu ?
En contexte d’ateliers complexes (plusieurs calques de dérivation et/ou de travail), les détails dans le bandeau de l’item listent les variantes de cet item dans les autres ateliers calques ou de travail, s’il en existe.
Dans l’exemple ci-dessous, l’item _Module-LeThe.xml dans l’atelier maître (icone d’atelier bleu) est modifié dans un atelier de travail (icone d’atelier vert) et modifié aussi dans un atelier dérivé (icone d’atelier marron). On peut passer facilement d’une version à l’autre en un seul clic. La popup est détachable pour plus d’aisance si besoin.
Exemple Le chiffre du mois 20, c’est le nombre d’années qui se sont écoulées depuis la première sortie d’Opale le 18/09/2006 (les développements avaient commencé en novembre 2005). Nouvelles adhésions d’organisations
Souhaitons la bienvenue à :
Institution Azahrae qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
L’Université Bourgogne Europe qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
URBILOG qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 15:59:55 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</guid>
    </item>
    <item>
      <title><![CDATA[The world of open source metadata]]></title>
      <link>https://changelog.com/podcast/665</link>
      <description><![CDATA[Andrew Nesbitt builds tools and open datasets to support, sustain, and secure critical digital infrastructure. He's been exploring the world of open source metadata for over a decade. First with libraries.io and now with ecosyste.ms, which tracks over 12 million packages, 287 million repos, 24.5 billion dependencies, and 1.9 million maintainers. What has Andrew learned from all this, who is using this open dataset, and how does he hope others can build on top of it all? Tune in to find out.]]></description>
      <pubDate>Wed, 05 Nov 2025 20:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/665</guid>
    </item>
    <item>
      <title><![CDATA[GitHub热门项目: visual-explainer]]></title>
      <link>https://dev.to/liu5580a/githubre-men-xiang-mu-visual-explainer-2i1c</link>
      <description><![CDATA[visual-explainer Agent skill + prompt templates that generate rich HTML pages for visual diff reviews, architecture overviews, plan audits, data tables, and project recaps
仓库: nicobailon/visual-explainer
stars: 2.4K
forks: 153
语言: HTML]]></description>
      <pubDate>Mon, 23 Feb 2026 10:41:18 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/liu5580a/githubre-men-xiang-mu-visual-explainer-2i1c</guid>
    </item>
    <item>
      <title><![CDATA[I Built a Free Statistics Calculator with Next.js — Here's What I Learned]]></title>
      <link>https://dev.to/ahnhyeongkyu/i-built-a-free-statistics-calculator-with-nextjs-heres-what-i-learned-4ic6</link>
      <description><![CDATA[As a developer who also does research, I got tired of switching between SPSS, R, and Excel just to run basic statistical tests. So I built StatMate — a free, browser-based statistics calculator that handles 20 different tests and outputs APA-formatted results.
Here's the story of how I built it and what I learned along the way.
Every semester, thousands of students and researchers face the same struggle:
SPSS costs $100+/month
R has a steep learning curve
Excel can't format APA tables automatically
Online calculators only handle one test at a time
I wanted to build something that just works — paste your data, pick a test, get properly formatted results.
Next.js 16 with Turbopack for fast builds
TypeScript for type-safe statistics calculations
Tailwind CSS v4 for styling
next-intl for trilingual support (English, Korean, Japanese)
All statistics computed client-side in the browser — no data leaves your machine
StatMate currently supports 20 statistical tests:
Parametric: t-tests (independent, paired, one-sample), ANOVA (one-way, two-way, repeated measures), simple &amp; multiple regression, logistic regression
Non-parametric: Mann-Whitney U, Wilcoxon, Kruskal-Wallis, Friedman
Other: Chi-square, Fisher's exact, McNemar, correlation, descriptive stats, sample size/power, Cronbach's alpha, factor analysis (EFA)
Every test includes:
APA 7th edition formatted results — copy-paste ready
Assumption checks (normality, homogeneity of variance)
Interactive charts (box plots, scatter plots, residual plots)
PDF export (free) and Word/DOCX export (Pro)
Example data so users can try before entering their own
No external stats library — I wrote all 20 statistical modules in TypeScript. The trickiest parts:
Matrix operations for factor analysis (eigenvalue decomposition, varimax rotation)
Probability distributions (t, F, chi-square) using series approximations
Post-hoc tests (Bonferroni, Tukey HSD, Dunn's test)
I validated every calculator against R 4.3 to ensure accuracy.
APA 7th edition has very specific formatting rules. Each test has its own reporting format:
t(58) = 2.45, p = .017, d = 0.63
F(2, 87) = 4.12, p = .019, partial eta-squared = .086 Building a system that automatically generates these strings with correct rounding, italics handling, and effect size interpretation was more work than expected.
Translating a statistics app into 3 languages isn't just about UI text. Statistical terminology, APA conventions, and number formatting all vary by locale. next-intl handled most of it, but edge cases required custom logic.
Client-side computation is underrated. Users trust a tool more when their data never leaves the browser.
APA formatting is a product feature, not a nice-to-have. It's the #1 thing users mention.
Trilingual from day one opened up markets I wouldn't have reached otherwise.
Free tools need a Pro tier to be sustainable. Ours includes AI-powered result interpretation and DOCX export.
StatMate is live at statmate.org. Everything is free — 20 calculators, PDF export, assumption checks, and charts.
If you're building tools for researchers or students, I'd love to hear about your experience. Drop a or check out the project!
Have questions about implementing statistics in JavaScript, APA formatting, or building multilingual Next.js apps? Happy to discuss in the .]]></description>
      <pubDate>Mon, 23 Feb 2026 10:24:56 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/ahnhyeongkyu/i-built-a-free-statistics-calculator-with-nextjs-heres-what-i-learned-4ic6</guid>
    </item>
    <item>
      <title><![CDATA[Electrobun : une nouvelle solution pour développer des apps desktop en TypeScript]]></title>
      <link>https://www.programmez.com/actualites/electrobun-une-nouvelle-solution-pour-developper-des-apps-desktop-en-typescript-39042</link>
      <description><![CDATA[Electrobun est une nouvelle solution pour développer des apps desktops en TypeScript. Il se veut léger, rapide et basé sur Zig et Bun. Il permet un binding écrit en C++, Zig, le backend est assuré par Bun, et il est multiplateforme (macOS, Windows et Linux). Il a l'ambition de créer des binaires les plus petits possibles, un temps de démarrage très bas et 100 % natif pour l'interface. Electrobun se veut une meilleure réponse qu'Electron. On écrit le coeur de l'application en TypeScript avec des WebView, les deux ensembles sont isolés et communiquent en RPC. Attention, il faut un environnement de développement desktop complet :
- Xcode + cmake sur macOS
- Visual Studio Build Tools our Visual Studio avec les extensions C++ et cmake sur Windows
- webkit2gtk et les paquets GTC, cmake et paquet build-essantial sur Linux
Pour en savoir : https://blackboard.sh/electrobun/docs/ Catégorie actualité: Langages Electrobun Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 14:34:34 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/electrobun-une-nouvelle-solution-pour-developper-des-apps-desktop-en-typescript-39042</guid>
    </item>
    <item>
      <title><![CDATA[AsteroidOS 2.0 : nouvelles montres, optimisations]]></title>
      <link>https://www.programmez.com/actualites/asteroidos-20-nouvelles-montres-optimisations-39041</link>
      <description><![CDATA[AsteroidOS revient en version 2.0. Il s'agit d'une version majeure. Cette distribution Linux est dédiée aux montres connectés pour les rendre totalement indépendantes et redonner vie à des montres qui ne sont plus supportées par les constructeurs. AsteroidOS a été créé en 2015. Pour développer les apps, l'OS utilise Qt et QML. La v2 permet d'assurer l'affichage constant sur plus de montres, proposer un nouveau launcher, des paramètres personnalisables, de meilleures performances, une synchronisation améliorée. La v2 supporte 49 langages, soit 20 de plus par rapport à la dernière version. De nouvelles montres sont supportées : Fossil Gen 4 Watches (firefish/ray)
Fossil Gen 5 Watches (triggerfish)
Fossil Gen 6 Watches (hoki)
Huawei Watch (sturgeon)
Huawei Watch 2 (sawfish/sawshark)
LG Watch W7 (narwhal)
Moto 360 2015 (smelt)
MTK6580 (harmony/inharmony)
OPPO Watch (beluga)
Polar M600 (pike)
Ticwatch C2+ &amp; C2 (skipjack)
Ticwatch E &amp; S (mooneye)
Ticwatch E2 &amp; S2 (tunny)
Ticwatch Pro, Pro 2020 and LTE (catfish/catfish-ext/catshark)
Ticwatch Pro 3 (rover/rubyfish) Et d'autres le sont partiellement. Pour la synchronisation, on dispose de différents apps : AsteroidOS Sync, Telescope, Amazfish, Gadgetbridge. Au-delà, l'équipe a de grandes ambitions : application fitness, configuration WiFi, AppStore, nouvelles apps.
Annonce : https://asteroidos.org/news/2-0-release/ Catégorie actualité: Open Source AsteroidOS Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 09:19:04 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/asteroidos-20-nouvelles-montres-optimisations-39041</guid>
    </item>
    <item>
      <title><![CDATA[Rasbperry Pi vs ESP32 : vraies questions, mauvaises comparaisons]]></title>
      <link>https://www.programmez.com/actualites/rasbperry-pi-vs-esp32-vraies-questions-mauvaises-comparaisons-39038</link>
      <description><![CDATA[Raspberry Pi vs ESP32 vs Arduino, cette question revient régulièrement quand on choisit la bonne plateforme pour son projet IoT. Comme nous le disons souvent quand nous comparons les platesformes, il faut comparer ce qui est comporable. Il ne faut pas opposer une Raspberry Pi 5 avec une ESP32. La Pi est une SBC, une Single Board Computer. Il s'agit donc d'un véritable micro-ordinateur sur une unique carte. Elle contient toute l'électronique (SoC, mémoire, vidéo, audio, stockage, réseau). Et une Pi 5 a besoin d'une alimentation puissante et d'un OS. L'ESP32 repose sur un firmware, qu'il est possible de changer.
L'autre différence est le form factor. La Pi 5 exige de la place et une dissipation thermique active pour les fortes charges.
La Pi 5 fait 8,5 cm sur 4,9 cm contre 5,5 cm sur 2,6 cm pour une ESP32 WROOM-32 (qui n'est pas le modèle le plus petit). L'ESP32 pourrait se comparer à la Pi Pico 2 avec 5,1 cm sur 2,1 cm. Nous ne tenons pas compte de la hauteur des headers.
L'équivalent d'une ESP32 côté Pi est donc la Pi Pico 2, aussi bien par le positionnement, le hardware et le form factor.
Petite comparaison : les specs Pi Pico 2 : un SoC RP235x + cœurs ARM, 520 Ko de SRAM, 4 Mo de stockage, 26 GPIO, UART / SPI / I2C, USB, réseau sans fil selon le modèle. De 6 à 9 € selon le modèle. - ESP32 Wroom32 : SoC ESP32, 512 Ko de RAM, 4 Mo de stockage, 34 GPIO, SPI / I2C / CAN / UART, WiFi + Bluetooth, env. 7-9 € Si vous êtes habitué(e) à coder avec Arduino, vous pouvez sans problème coder depuis l’Arduino IDE, les ESP sont parfaitement supportés. Vous pourrez utiliser peu ou prou les mêmes capteurs. Si vous cherchez une carte réactive avec des interruptions plus rapides, le Pi Pico 2 est souvent considéré comme meilleur. L’ESP32 propose plus de protocoleset de GPIO. Sur la partie connectivité sans fil, les deux cartes supportent le Wi-Fi et le Bluetooth, mais petit avantage à l’ESP32, car le réseau sans fil est une des fonctionnalités intégrées dès la conception. Et le support OTA (mise à jour over the air) peut être un avantage certain dans un contexte contraint ou industriel.
L’ESP32 est plus consommatrice, notamment en charge maximale. La Pi Pico 2 est plus économique. Si vous cherchez avant tout la basse consommation, la Pi sera sans doute la meilleure option.
Sur le modèle de développement, nous avons toujours apprécié la diversité de l’ESP32. Si vous voulez faire du MicroPython, vous devrez flasher le bon firmware. Catégorie actualité: Hardware Raspberry pi, ESP32 Image actualité AMP:]]></description>
      <pubDate>Thu, 19 Feb 2026 16:27:48 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/rasbperry-pi-vs-esp32-vraies-questions-mauvaises-comparaisons-39038</guid>
    </item>
    <item>
      <title><![CDATA[Physiocab : un logiciel libre de gestion pour kinésithérapeutes]]></title>
      <link>https://linuxfr.org/news/physiocab-un-logiciel-libre-de-gestion-pour-kinesitherapeutes</link>
      <description><![CDATA[Physiocab est un logiciel libre de gestion de cabinet de kinésithérapie, développé sous licence Affero GPL 3.0 et hébergé sur Codeberg. Le projet est porté par la société Allium SAS, dans le cadre de la plateforme communautaire Kalinka, dédiée aux kinésithérapeutes francophones.
Le projet vient de passer en beta publique (v0.9) et cherche des testeurs et contributeurs.
Pourquoi un logiciel libre pour les kinés ? Le secteur de la santé libérale souffre d'une offre logicielle dominée par des solutions propriétaires onéreuses, souvent opaques sur le traitement des données de santé. Physiocab propose une alternative : un code auditable, des données stockées localement sous la responsabilité du praticien. lien nᵒ 1 : La page de présentation du projet
lien nᵒ 2 : Le dépôt codeberg
lien nᵒ 3 : PeerJs (MIT) Fonctionnalités
La beta couvre déjà un large périmètre fonctionnel :
Planning hebdomadaire en drag &amp; drop, avec export PDF et gestion des semaines exceptionnelles, particulièrement orienté vers les kinés intervenant en multi-établissements.
Bilans Diagnostiques Kinésithérapiques (BDK) avec tests standardisés (TUG, Tinetti, Handgrip, EVA, évaluation du risque de chute…), export de PDF et historique comparatif.
Suivi des séances avec de multiples exercices structurés (équilibre, force, endurance, mobilisation), chronométrage automatique et calcul de progression.
Application tablette en PWA : fonctionne hors connexion grâce à un Service Worker, s'installe sans passer par un store, interface optimisée tactile.
Stack technique
Backend : Python 3.10+
L'application est multi-plateforme côté client (Windows, macOS, Linux, iOS, Android). La communication entre l'appli de bureau et l'appli PWA se fait de manière directe via PeerJs. Cette méthode ne nécessite pas de préparation contraignante comme l'ouverture de ports.
Les données sont stockées localement, ce qui implique que le praticien reste maître de ses sauvegardes et de sa conformité RGPD.
Le logiciel a été testé par un kinésithérapeute en situation réelle plusieurs jours d'affilée.
Modèle économique
L'utilisation est gratuite, sans limite dans le temps et sans frais cachés, la licence Affero GPL 3.0 en étant la garantie. Un support payant sur devis est proposé pour les praticiens souhaitant une installation assistée, une formation à distance, des développements sur mesure ou un audit de sécurité.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Thu, 19 Feb 2026 13:42:53 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/physiocab-un-logiciel-libre-de-gestion-pour-kinesitherapeutes</guid>
    </item>
    <item>
      <title><![CDATA[What to expect for open source in 2026]]></title>
      <link>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</link>
      <description><![CDATA[Let’s dig into the 2025’s open source data on GitHub to see what we can learn about the future.]]></description>
      <pubDate>Wed, 18 Feb 2026 18:41:42 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</guid>
    </item>
    <item>
      <title><![CDATA[Securing the AI software supply chain: Security results across 67 open source projects]]></title>
      <link>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</link>
      <description><![CDATA[Learn how The GitHub Secure Open Source Fund helped 67 critical AI‑stack projects accelerate fixes, strengthen ecosystems, and advance open source resilience.]]></description>
      <pubDate>Tue, 17 Feb 2026 19:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</guid>
    </item>
    <item>
      <title><![CDATA[Concours - Gagnez une Raspberry Pi 5 avec Macé Robotics]]></title>
      <link>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</link>
      <description><![CDATA[À l’occasion de ses 10 ans de Macé Robotics, l’entreprise organise un concours qui se déroulera jusqu'au 26 février 2026.
Macé Robotics est une entreprise individuelle fondée et gérée par moi-même (Nicolas), basée en Bretagne, spécialisée dans la conception et la réparation électronique, aussi bien pour les entreprises que pour les particuliers. Depuis 2016, je fabrique aussi du matériel Open Source également des robots mobiles Open Source destinés à l’enseignement supérieur et à la recherche. Ces robots sont basés sur un système Linux (Raspberry Pi OS), intégrant une carte Raspberry Pi ainsi qu’un microcontrôleur (Pico) dédié à la gestion des moteurs et des capteurs. J’utilise la suite logicielle KiCad sous licence GNU GPL (https://www.kicad.org/) pour la conception des circuits imprimés de ces robots. Attribution des lots par tirage au sort :
→ 1er lot : une carte Raspberry Pi 5 (2 Go) → 2e lot : une carte Raspberry Pi Pico 2W
La livraison est offerte en France. lien nᵒ 1 : Le concours pour participer Retour sur la course de robots – Saint-Brock Robot Race d'une dépêche précédente
Suite à la dépêche de décembre 2024 concernant l’organisation de la course de robots mobiles, voici quelques retours sur cet événement : malgré plusieurs annulations d’écoles survenues quelques semaines avant la compétition, la course a tout de même pu avoir lieu.
Environ quinze participants ont pris part à la compétition. Parmi les robots engagés, on comptait un robot DIY piloté par un microcontrôleur ESP32, aux côtés de plusieurs robots basé sur Raspberry Pi, offrant ainsi une belle diversité technologique.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Sat, 14 Feb 2026 08:47:09 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</guid>
    </item>
    <item>
      <title><![CDATA[L’ANSSI révise sa doctrine vis-à-vis du logiciel libre]]></title>
      <link>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</link>
      <description><![CDATA[L’ANSSI (Agence nationale de la sécurité des systèmes d’information) vient de publier une mise à jour substantielle de sa doctrine vis-à-vis du logiciel libre. L’agence confirme que le logiciel libre et la transparence sont essentiels à la sécurité des systèmes d’information. Elle assume sa contribution au libre et la publication de logiciels sous licence libre.
Cette posture très favorable au logiciel libre et open source est une belle avancée et un signal fort. Jusque-là, la posture de l’ANSSI était beaucoup plus floue et sa contribution à des projets libres et open source pouvait même apparaitre en contradiction avec sa doctrine. J’avais l’impression que les collaborateurs de l’ANSSI qui le faisaient reprenaient à leur compte le dicton « Pour vivre heureux, vivons cachés ».
La politique de l’agence est désormais claire : l’ANSSI contribue, l’ANSSI publie, l’ANSSI a une stratégie pragmatique qui peut l’amener à s’engager ou non sur le long terme en fonction de la finalité de l’outil et des motivations de l’ANSSI.
Détail qui a son importance, l’ANSSI indique privilégier, sauf exception justifiée, la licence Apache v2.0 pour les projets qu’elle publie. Je suis ravi de voir ce service privilégier une licence mondialement connue à une licence franco-française ou européenne (elles ont le don de doucher nombre de velléités d’utilisation et de contribution). lien nᵒ 1 : L’ANSSI met à jour sa politique open source (9 février 2026)
lien nᵒ 2 : Posture générale et actions de l'ANSSI sur l'open-source Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 18:55:42 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</guid>
    </item>
    <item>
      <title><![CDATA[Le prochain Drupalcamp se déroulera à Grenoble les 9, 10 et 11 avril 2026 prochain]]></title>
      <link>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</link>
      <description><![CDATA[L’association Drupal France &amp; Francophonie organise la 13ème édition du Drupalcamp les 9, 10 et 11 avril 2026 au campus Universitaire Grenoble Alpes de Grenoble (France, Isère 38). Drupal est « un système de gestion de contenu (CMS) libre et open-source publié sous la licence publique générale GNU et écrit en PHP ».
Après Rennes en 2024, puis un Barcamp à Perpignan en 2025, cette année 2026 nous emmène au pied des montagnes à Grenoble pour un format de 3 jours de rencontres, soit deux journées de conférences les jeudi et vendredi. La journée du samedi est réservée à la contribution.
Des moments d’ateliers et micro-formation sont également au programme, pour faire de cet évènement une réussite d’un point de vue communauté autour du projet Open Source Drupal.
Le Drupalcamp Grenoble c’est la rencontre de la communauté francophone autour du logiciel libre Drupal. Ouvert à toutes et tous, les rencontres, conférences et ateliers permettent d’adresser à un public toujours plus large des sujets et thématiques diversifiées.
Notre objectif principal est de rendre la création de sites plus simple et la gestion des contenus plus intuitive pour tous. Comme de fédérer les utilisateurs et professionnels qui utilisent Drupal au quotidien.
Du simple curieux au développeur expert, tous ceux qui s’intéressent à Drupal et aux logiciels libres pourront participer à cette manifestation rythmée par :
des conférences (jeudi 9 et vendredi 10 avril), données par des professionnels reconnus et des membres de la communauté Drupal au cours desquels des thématiques nouvelles seront explorées,
des sessions de découverte étayées par des démonstrations à l’intention d’un public plus néophyte,
une journée de formation gratuite (Drupal in a Day) dédiée à l’initiation pour que les curieux puissent se lancer dans la création de leur premier site (sur inscription)
des moments de réseautage et de convivialité avec, notamment, la très attendue soirée communautaire !
Informations pratiques : Campus Universitaire Grenoble Alpes qui se situe à Saint-Martin d'Hères
https://grenoble2026.drupalcamp.fr/
Contact : drupalcamp@drupal.fr lien nᵒ 1 : https://grenoble2026.drupalcamp.fr Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 10 Feb 2026 09:16:59 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</guid>
    </item>
    <item>
      <title><![CDATA[The GitHub problem (and other predictions)]]></title>
      <link>https://changelog.com/friends/123</link>
      <description><![CDATA[Mat Ryer is back and he brought his impromptu musical abilities with him! We discuss Rob Pike vs thankful AI, Microsoft's GitHub monopoly (and what it means for open source), and Tom Tunguz' 12 predictions for 2026: agent-first design, the rise of vector databases, and are we about to pay more for AI than people?!]]></description>
      <pubDate>Wed, 14 Jan 2026 21:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/123</guid>
    </item>
    <item>
      <title><![CDATA[There will be bleeps]]></title>
      <link>https://changelog.com/friends/113</link>
      <description><![CDATA[Mike McQuaid and Justin Searls join Jerod in the wake of the RubyGems debacle to discuss what happened, what it says about money in open source, what sustainability really means for our community, making a career out of open source (or not), and more. Bleep!]]></description>
      <pubDate>Fri, 17 Oct 2025 18:15:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/113</guid>
    </item>
    <item>
      <title><![CDATA[anthropics/claude-code]]></title>
      <link>https://github.com/anthropics/claude-code</link>
      <description><![CDATA[anthropics/claude-code]]></description>
      <pubDate>Mon, 23 Feb 2026 10:48:19 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/anthropics/claude-code</guid>
    </item>
    <item>
      <title><![CDATA[Stremio/stremio-web]]></title>
      <link>https://github.com/Stremio/stremio-web</link>
      <description><![CDATA[Stremio - Freedom to Stream Stremio - Freedom to Stream Stremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons. Build Prerequisites Node.js 12 or higher pnpm 10 or higher Install dependencies pnpm install Start development server pnpm start Production build pnpm run build Run with Docker docker build -t stremio-web .
docker run -p 8080:8080 stremio-web Screenshots Board Discover Meta Details License Stremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the LICENSE file in the project for more information.]]></description>
      <pubDate>Mon, 23 Feb 2026 10:47:59 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/Stremio/stremio-web</guid>
    </item>
    <item>
      <title><![CDATA[La plateforme collaborative open source Nextcloud Hub 26 Winter est disponible, avec des outils de migration, le chiffrement du navigateur, des mises à jour Office, et offrant des performances amélior]]></title>
      <link>https://cloud-computing.developpez.com/actu/380459/La-plateforme-collaborative-open-source-Nextcloud-Hub-26-Winter-est-disponible-avec-des-outils-de-migration-le-chiffrement-du-navigateur-des-mises-a-jour-Office-et-offrant-des-performances-ameliorees/</link>
      <description><![CDATA[La plateforme collaborative open source Nextcloud Hub 26 Winter est disponible, avec des outils de migration, le chiffrement du navigateur, des mises à jour Office, et offrant des performances améliorées
Depuis 2016, Nextcloud est passé d'un simple outil de partage de fichiers à une plateforme de collaboration numérique souveraine complète avec des clients français comme le ministère de l'Éducation, le ministère de la Transition énergétique, la Ville de Lyon, la region Île-de-France et la Sorbonne....]]></description>
      <pubDate>Mon, 23 Feb 2026 03:38:00 GMT</pubDate>
      <source>Developpez.com</source>
      <category>opensource</category>
      <guid>https://cloud-computing.developpez.com/actu/380459/La-plateforme-collaborative-open-source-Nextcloud-Hub-26-Winter-est-disponible-avec-des-outils-de-migration-le-chiffrement-du-navigateur-des-mises-a-jour-Office-et-offrant-des-performances-ameliorees/</guid>
    </item>
    <item>
      <title><![CDATA[Les craintes liées à la sécurité d'OpenClaw poussent Meta et d'autres entreprises d'IA à en restreindre l'utilisation, l'outil est réputé pour ses capacités exceptionnelles et son extrême imprévisibil]]></title>
      <link>https://intelligence-artificielle.developpez.com/actu/380460/Les-craintes-liees-a-la-securite-d-OpenClaw-poussent-Meta-et-d-autres-entreprises-d-IA-a-en-restreindre-l-utilisation-l-outil-est-repute-pour-ses-capacites-exceptionnelles-et-son-extreme-imprevisibilite/</link>
      <description><![CDATA[Les craintes liées à la sécurité d'OpenClaw poussent Meta et d'autres entreprises d'IA à en restreindre l'utilisation, l'outil est réputé pour ses capacités exceptionnelles et son extrême imprévisibilité En l'espace de quelques mois, OpenClaw est passé du statut de projet GitHub confidentiel à celui d'épouvantail sécuritaire numéro un de l'industrie tech. Lancé en novembre 2025 sous le nom Clawdbot par un développeur autrichien travaillant seul, l'agent IA autonome a accumulé 145 000 étoiles GitHub...]]></description>
      <pubDate>Fri, 20 Feb 2026 22:58:00 GMT</pubDate>
      <source>Developpez.com</source>
      <category>opensource</category>
      <guid>https://intelligence-artificielle.developpez.com/actu/380460/Les-craintes-liees-a-la-securite-d-OpenClaw-poussent-Meta-et-d-autres-entreprises-d-IA-a-en-restreindre-l-utilisation-l-outil-est-repute-pour-ses-capacites-exceptionnelles-et-son-extreme-imprevisibilite/</guid>
    </item>
    <item>
      <title><![CDATA[Qui paie réellement pour votre utilisation quotidienne gratuite de ChatGPT ? OpenAI brûle plusieurs millions de dollars par jour, une facture astronomique que les offres payantes ne suffisent pas à co]]></title>
      <link>https://intelligence-artificielle.developpez.com/actu/380444/Qui-paie-reellement-pour-votre-utilisation-quotidienne-gratuite-de-ChatGPT-OpenAI-brule-plusieurs-millions-de-dollars-par-jour-une-facture-astronomique-que-les-offres-payantes-ne-suffisent-pas-a-couvrir/</link>
      <description><![CDATA[Qui paie réellement pour votre utilisation quotidienne gratuite de ChatGPT ? OpenAI brûle plusieurs millions de dollars par jour une facture astronomique que les offres payantes ne suffisent pas à couvrir
L'usage gratuit quotidien de ChatGPT repose sur une infrastructure extrêmement coûteuse. L'exécution des modèles, l'électricité et les serveurs représentent des dépenses de plusieurs dizaines de millions de dollars. Même des interactions anodines contribuent à cette facture colossale. OpenAI supporte...]]></description>
      <pubDate>Fri, 20 Feb 2026 10:05:00 GMT</pubDate>
      <source>Developpez.com</source>
      <category>opensource</category>
      <guid>https://intelligence-artificielle.developpez.com/actu/380444/Qui-paie-reellement-pour-votre-utilisation-quotidienne-gratuite-de-ChatGPT-OpenAI-brule-plusieurs-millions-de-dollars-par-jour-une-facture-astronomique-que-les-offres-payantes-ne-suffisent-pas-a-couvrir/</guid>
    </item>
    <item>
      <title><![CDATA[MySQL : tentative de relance à la FOSDEM, MariaDB peu convaincu, une lettre ouverte pour créer une fondation indépendante...]]></title>
      <link>https://www.programmez.com/actualites/mysql-tentative-de-relance-la-fosdem-mariadb-peu-convaincu-une-lettre-ouverte-pour-creer-une-39040</link>
      <description><![CDATA[Oracle avait profité de la FOSDEM 2026 pour mettre en avant MySQL avec un événement dédié "MySQL and friends". L'éditeur en profitait pour affimer que des fonctionnalités réservées aux versions payantes allaient bientôt rejoindre la version communautaire, notamment, les fonctions autour des vecteurs. Oracle parlait d'une nouvelle ère. Oracle cherchait à relancer les relations avec la communauté open source et rassurer sur l'avenir de mySQL. MariaDB a rapidement réagi : ""MariaDB a passé des années à livrer des innovations qui ont forcé Oracle à mettre à jour MySQL – des analyses en colonnes à la réplication avancée parallèle, en passant par le lancement de la recherche vectorielle native l'an dernier. Nous n'avons pas attendu le moment opportun pour ouvrir la porte à ces avancées ; nous les avons intégrées au cœur de notre serveur, car c'est ce que requiert une base de données open source moderne. Les utilisateurs MySQL ont désormais un choix clair : rester avec un éditeur qui n'innove que sous la contrainte, ou rejoindre MariaDB, qui se consacre à 100% à l'avenir. Puisque MariaDB devient l'option simple pour migrer depuis MySQL, sécuriser l'avenir de votre stack n'est plus qu'à un clic."
Il faut dire que la MySQL n'avait pas évolué depuis l'automne 2025 et qu'aucune communication claire n'avait été faite par Oracle sur l'avenir de la base de données. Il y a quelques jours, une lettre ouverte a été publiée pour demander à Oracle un changement de gouvernance : créer une gouvernance indépendante sous la forme d'une fondation pour reprendre en main MySQL et retrouver une stratégie claire. Les défis sont nombreux :
- une popularité en baisse constante
- manque de transparence sur le projet
- une version communautaire incomplète
- une partie des équipes transférées au cloud d'Oracle
La nouvelle gouvernance pourrait aider à relancer la confiance, définir une roadmap claire et transparence, unifier et fédérer l'écosystème.
Peu de chances que cette initiative puisse réellement influencer Oracle. Est-ce que l'éditeur veut réellement relancer MySQL et redonner une véritable dimension open source à la base de données ? Les annonces à la FOSDEM vont dans le bon sens mais le plus difficile reste à faire : concrétiser réellement ces annonces. La lettre ouverte : https://letter.3306-db.org/ Catégorie actualité: Outils MySQL Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 08:12:19 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/mysql-tentative-de-relance-la-fosdem-mariadb-peu-convaincu-une-lettre-ouverte-pour-creer-une-39040</guid>
    </item>
    <item>
      <title><![CDATA[10 pratiques de codes et humaines à intégrer]]></title>
      <link>https://www.programmez.com/actualites/10-pratiques-de-codes-et-humaines-integrer-39039</link>
      <description><![CDATA[Parfois, il est bon de revenir aux fondamentaux et de se rappeler quelques concepts qui peuvent sembler simplistes, mais qui restent toujours utiles. 1 / On lit plus de code qu’on en écrit
Même si on essaie de faire simple et clair, quand on rouvre un code six mois plus tard, on se demande souvent : qu’est-ce que j’ai voulu faire ? Bref : * des noms de variables simples, mais répondant à une logique claire
* la lisibilité du code doit primer sur les performances pures Et gardez toujours en tête : serai-je capable de relire et comprendre ce code dans six mois ? 2 / Faire simple, c’est difficile Il est plus facile de faire compliqué que de faire simple. On ajoute des couches, des dépendances, des patterns dans tous les sens. Faire simple permet : * moins de risques d’erreurs
* un code plus facile à tester
* une meilleure maintenance sur le long terme 3 / Vous n’avez pas besoin de tout savoir, mais vous devez apprendre Un développeur ne sait pas tout. Mais il apprend. Il faut lire la documentation, décomposer les problèmes, expérimenter et apprendre en continu pour progresser. 4 / Le debug est une compétence Certains développeurs savent mieux debugger que d’autres. C’est un fait. Ils trouvent plus facilement les bugs, restent calmes et savent observer et comprendre le problème. Pour corriger un bug, il faut : * savoir le reproduire clairement
* modifier un élément à la fois
* lire et comprendre les logs, warnings et messages d’erreur 5 / Les frameworks ne changent pas les fondamentaux Maîtriser les fondamentaux est toujours un avantage. Cette maîtrise vous aidera à migrer plus sereinement d’une version à une autre, ou même à changer de technologie. Les frameworks évoluent. Les fondamentaux restent. 6 / Les problèmes de performances sont souvent des problèmes de conception Avant d’optimiser avec du caching, du tuning ou des micro-optimisations, regardez d’abord l’architecture et la conception du projet : * vos requêtes fonctionnent-elles correctement et sont-elles bien écrites ?
* le modèle de données est-il adapté ?
* pouvez-vous réduire les appels réseau inutiles ?
* certaines boucles peuvent-elles être optimisées ? Les gains les plus importants viennent souvent de la conception, pas des optimisations mineures. 7 / Écrire des tests Les tests permettent : * de refactoriser en toute sécurité
* de détecter plus rapidement les problèmes
* d’améliorer la qualité globale du code Les tests ne ralentissent pas le développement. Ils le sécurisent. 8 / La communication fait partie de notre métier Cela inclut notamment les et la documentation du code. Un code documenté est plus facile à comprendre, maintenir et faire évoluer dans le temps. Le code explique le « ». Les expliquent le « pourquoi ». 9 / Le burn-out est aussi un problème technique La pression des délais, les longues heures de développement ou le manque de vision peuvent conduire à un code de mauvaise qualité, difficile à maintenir. Un code de qualité nécessite : * du temps
* de la réflexion
* et des conditions de travail saines La qualité technique est aussi une question d’organisation. 10 / La progression n’est pas linéaire Développer, c’est traverser des périodes très productives, où tout fonctionne rapidement, et d’autres beaucoup plus difficiles : bugs incompréhensibles, code instable, spécifications floues. C’est normal. Dans ces moments-là, il faut revenir aux fondamentaux, reprendre le problème étape par étape et garder son calme. La progression se fait sur le long terme. Source : https://medium.com/@gopi_ck/10-basic-concepts-every-developer-should-know-even-seniors-too-93e1b69a83fd Catégorie actualité: Langages tips Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 07:21:53 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/10-pratiques-de-codes-et-humaines-integrer-39039</guid>
    </item>
    <item>
      <title><![CDATA[Changes to test merge commit generation for pull requests]]></title>
      <link>https://github.blog/changelog/2026-02-19-changes-to-test-merge-commit-generation-for-pull-requests</link>
      <description><![CDATA[To reduce delays when determining the mergeability for a pull request and improve system reliability, we’ve changed the frequency at which we generate test merge commits for open pull requests.…]]></description>
      <pubDate>Thu, 19 Feb 2026 22:01:57 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-changes-to-test-merge-commit-generation-for-pull-requests</guid>
    </item>
    <item>
      <title><![CDATA[Selected Anthropic and OpenAI models are now deprecated]]></title>
      <link>https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated</link>
      <description><![CDATA[We have deprecated the following models across all GitHub Copilot experiences (including Copilot Chat, inline edits, ask and agent modes, and code completions) on February 17, 2026: Model Deprecation Date…]]></description>
      <pubDate>Thu, 19 Feb 2026 19:47:37 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated</guid>
    </item>
    <item>
      <title><![CDATA[GitHub Projects: Import items based on a query and hierarchy view improvements]]></title>
      <link>https://github.blog/changelog/2026-02-19-github-projects-import-items-based-on-a-query-and-hierarchy-view-improvements</link>
      <description><![CDATA[Import project items with a search query When creating a new project, you can now add items using a search query, in addition to importing directly from a repository. This…]]></description>
      <pubDate>Thu, 19 Feb 2026 19:33:33 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-github-projects-import-items-based-on-a-query-and-hierarchy-view-improvements</guid>
    </item>
    <item>
      <title><![CDATA[Python Environnements Extension : pour unifier les environnements Python sur Visual Studio Code]]></title>
      <link>https://www.programmez.com/actualites/python-environnements-extension-pour-unifier-les-environnements-python-sur-visual-studio-code-39036</link>
      <description><![CDATA[Pour simplifier et unifier l'environnement de développement Python sur Visual Studio Code, on dispose de la nouvelle extension Python Environnements. Il doit unifier le modèle de développement, gérer les environnements et les workflows, gérer les interpréteurs et les packages. Jusqu'é présent, l'expérience Python était fragmenté à travers les différents outils (venv, conda, pyenv, etc.). Après plus d'un an d'ajustements et de développement, l'extension est disponible. A terme, tous les flux Python migreront vers l'extension Environnements. Il est possible d'activer dès maintenant : python.useEnvsExtension. L'extension fonctionne en parallèle de l'extension Python et aucune configuration particulière n'est requise : vous ouvrez un fichier Python et l'environnement utilisé est automatiquement détecté. Les environnements supportés sont : venv
conda
pyenv
poetry
pipenv
System Python installs La découverte est assurée par PET (Python Environment Tool), un outil de scan codé en Rust. Si vous utilisez uv, l'extension va automatiquement créer un environnement venv et installer les paquets nécessaires. Pour le moment, il n'est pas possible de créer rapidement des projets sur tous les environnements, seuls venv et conda sont supportés. Sans doute que les autres le seront dans les prochaines versions. Mauvaise nouvelle : l'extension fonctionne UNIQUEMENT sur Windows x64 et Windows ARM et l'édition Web ! Il faut Python soit installé.
Pour le moment, les retours sont plutôt mauvais : extension difficile à utiliser, perte de temps pour créer les environnements depuis Pylance, etc. Et les mises à jour se succèdent. Heureusement que l'extension est officiellement en preview. Page de l'extension : https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-python-envs Catégorie actualité: Outils Visual Studio Code, Python Image actualité AMP:]]></description>
      <pubDate>Thu, 19 Feb 2026 07:33:25 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/python-environnements-extension-pour-unifier-les-environnements-python-sur-visual-studio-code-39036</guid>
    </item>
    <item>
      <title><![CDATA[Quantique : Comcast, Classiq et AMD testent un algorithme quantique pour les réseaux]]></title>
      <link>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</link>
      <description><![CDATA[Comcast, Classiq et AMD mènent des tests pour améliorer le trafic Internet en utilisant des algorithmes quantiques pour renforcer la résistance du routage réseau. "L’essai conjoint s’est concentré sur un défi clé de la conception des réseaux : identifier des chemins de secours indépendants pour les nœuds du réseau lors des opérations de maintenance ou de modifications. L’objectif était de garantir que, si un site est mis hors ligne et que soudainement, un deuxième tombe en panne, le trafic puisse être redirigé sans interruption ni dégradation du service pour les clients. Pour y parvenir, les opérateurs doivent identifier des chemins de secours distincts, rapides et capables de résister à des pannes simultanées, tout en minimisant la latence. Cette tâche devient de plus en plus complexe à mesure que le réseau s’étend." explique l'annonce. Le schéma présente le design et l'implémentation du flux et de l'algo quantique sur la plateforme Classiq. L’expérimentation a combiné des techniques de calcul quantique et des méthodes classiques haute performance afin d’évaluer la capacité des algorithmes quantiques à identifier, en temps réel, des chemins de secours dans des scénarios de gestion des changements. Elle a été menée à la fois sur du matériel quantique et dans des environnements de simulation accélérés utilisant des GPU AMD Instinct, afin d’atteindre une capacité de calcul (à l’échelle des qubits) encore hors de portée du matériel quantique seul.
« L’avenir du calcul repose sur la convergence entre le classique et le quantique », explique Madhu Rangarajan, vice-président corporate en charge des produits Compute et Enterprise AI chez AMD. « En tant qu’acteur du calcul haute performance, nous cherchons à comprendre nos technologies peuvent accompagner l’émergence du quantique. Cette collaboration montre un cas concret où la simulation accélérée et l’exécution quantique sont combinées pour répondre à un enjeu opérationnel réel dans les réseaux. »
Détail sur l'algo quantique utilisé : https://www.amd.com/en/developer/resources/technical-articles/2026/designing-resilient-routing-using-quantum-algorithms.html Catégorie actualité: Technologies quantique Image actualité AMP:]]></description>
      <pubDate>Wed, 18 Feb 2026 08:34:25 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</guid>
    </item>
    <item>
      <title><![CDATA[IDE Kiro : Checkmarx apporte plus de sécurité applicative]]></title>
      <link>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</link>
      <description><![CDATA[Checkmarx annonce que son Developer Assist supporte l'IDE Kiro, pour l'étendre la sécurité applicative directement dans l'enviornnement. Cette intégration permet à ces derniers d'identifier et de résoudre les problèmes de sécurité au fil de l'écriture du code, sans quitter leur IDE ni dépendre de scans en aval dans la chaîne CI/CD.
En utilisant l’extension IDE officielle de Checkmarx, les développeurs peuvent activer Developer Assist dans Kiro en quelques étapes seulement, sans configuration lourde. La prise en charge d’autres flux de développement, y compris via la ligne de commande, sera bientôt disponible. Une fois authentifié, Developer Assist analyse automatiquement le code source et les dépendances de l’espace de travail actif, appliquant les politiques existantes de Checkmarx One. Aucune configuration spécifique à Kiro, API propriétaire ou intégration expérimentale n’est nécessaire. Developer Assist est disponible sur Cursor, Visual Studio Code et Windsurf.
Pour en savoir plus : https://dev.checkmarx.com/ Catégorie actualité: Outils Checkmarx Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:25:38 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</guid>
    </item>
    <item>
      <title><![CDATA[WebMCP : un standard pour rendre un site web "agent ready" ?]]></title>
      <link>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</link>
      <description><![CDATA[concilier agents IA et sites web et la manière dont les pages web pourraient interagir, travailler avec les agents ? WebMCP veut fournir une méthode standard pour définir les actions des agents sur un site web, sur une page web sans pénaliser au bon fonctionnement du site web. "Vous indiquez aux agents et où interagir avec votre site, qu'il s'agisse de réserver un vol, de soumettre une demande d'assistance ou de naviguer dans des données complexes. Ce canal de communication direct élimine toute ambiguïté et permet des flux de travail plus rapides et plus efficaces pour les agents." expliquer Google. WebMCP preview repose sur 2 API :
- API déclarative : Permet d’effectuer des actions standard définies directement dans les formulaires HTML. - API impérative : Permet d’effectuer des interactions plus complexes et dynamiques nécessitant l’exécution de JavaScript. C'est une interface proposé en preview par Google et accessible dans Chrome. Ces API forment un "pont" rendant votre site web "agent ready" et permet de créer des flux agentiques que se veulent plus fiables qu'en passant par du DOM. Ces API sont JavaScript. Pour le moment, la spécification est en cours de rédaction. Elle ne dépend pas de W3C et n'est pas un standard du consortium. Site : https://webmachinelearning.github.io/webmcp/ Catégorie actualité: IA MCP Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:18:02 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</guid>
    </item>
    <item>
      <title><![CDATA[Parcours libriste d’Isabella Vanni — « Libre à vous ! » du 10 février 2026 — Podcasts et références]]></title>
      <link>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</link>
      <description><![CDATA[268ème émission « Libre à vous ! » de l’April. Podcast et programme :
sujet principal : parcours libriste d’Isabella Vanni, coordinatrice vie associative et responsable projets à l’April. Un parcours libriste est l’interview d’une seule personne pour parler de son parcours personnel et professionnel
chronique « Que libérer d’autre que du logiciel avec Antanak » sur « Les assises de l’attention »
chronique de Benjamin Bellamy sur « L’antéchrist et les petits hommes verts »
Quoi de Libre ? Actualités et annonces concernant l’April et le monde du Libre lien nᵒ 1 : Podcast de la 268ᵉ émission
lien nᵒ 2 : Les références pour la 268ᵉ émission et les podcasts par sujets
lien nᵒ 3 : S'abonner au podcast
lien nᵒ 4 : S'abonner à la lettre d'actus
lien nᵒ 5 : Libre à vous !
lien nᵒ 6 : Radio Cause Commune Rendez‐vous en direct chaque mardi de 15 h 30 à 17 h sur 93,1 MHz en Île‐de‐France. L’émission est diffusée simultanément sur le site Web de la radio Cause Commune. Vous pouvez nous laisser un message sur le répondeur de la radio : pour réagir à l’un des sujets de l’émission, pour partager un témoignage, vos idées, vos suggestions, vos encouragements ou pour nous poser une question. Le numéro du répondeur : +33 9 72 51 55 46. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:24 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</guid>
    </item>
    <item>
      <title><![CDATA[.Net 11 Preview 1 : nouvelles librairies, peu de changements dans C#]]></title>
      <link>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</link>
      <description><![CDATA[.Net 10 a été distribuée en novembre 2025. La version 11 est désormais disponible en preview 1. Comme à chaque fois, de nombreuses évolutions sont attendues. L'ensemble des frameworks et des langages sont concernées : C#, F#, ASP.Net Core, Blazor, MAUI, le compilateur Jit, le support de CoreCLR dans WebAssembly, meilleure compression / décompression avec Zstandard. Sur la partie librairie, retenons déjà les évolutions suivantes :
- Zstandard est natif à .Net pour la compression. La librairie promet une nette amélioration des performances :
// Compress data using ZstandardStream
using var compressStream = new ZstandardStream(outputStream, CompressionMode.Compress);
await inputStream.CopyToAsync(compressStream); // Decompress data
using var decompressStream = new ZstandardStream(inputStream, CompressionMode.Decompress);
await decompressStream.CopyToAsync(outputStream);
- BFloat16 intègre par défaut toutes les interfaces standards pour le numérique
- amélioration de TimeZone
Note de version sur les librairies : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/libraries.md
Sur la partie runtime, il faut s'attendre à de bonnes nouvelles :
- Runtime async : une nouvelle fonction majeure du runtime et méthodes asynchrones pour améliorer les performances. CoreCLR supporte RuntimeAsync par défaut, idem pour Native AOT
- CoreCLR est supporté dans WebAssembly. Il n'est pas encore disponible en preview 1.
- diverses améliorations de performances sur le JIT - meilleur support de RISC-V
Sur C#, pour le moment, peu de nouveautés annoncées. Deux nouvelles fonctions sont attendues : arguments pour les expresssions Collection et support Extended layout. .Net 11 n'introduira aucune nouvelle fonctionnalité pour Visual Basic. Sur ASP.Net Core et Blazor, les développeurs vont avoir beaucoup de nouveautés : EnvironmentBoundary, nouveau composant Label dans les formulaires Blazor, nouveau composant DisplayName, navigation relative Uri, support "propre" des éléments MathML dans un rendu interactif. Tous les détails dans la note de version : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/aspnetcore.md
La génération de source XAML est par défaut pour les applications .Net MAUI, cela doit permettre un build plus rapide et un debug plus performant. Sur Android, CoreCLR devient le runtime par défaut. Sur Container Images et Winfows Forms, pas de nouveautés annoncées. Annonce de .Net 11 : https://devblogs.microsoft.com/dotnet/dotnet-11-preview-1/ Catégorie actualité: Frameworks .Net 11 Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 09:52:19 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</guid>
    </item>
    <item>
      <title><![CDATA[Automate repository tasks with GitHub Agentic Workflows]]></title>
      <link>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</link>
      <description><![CDATA[Discover GitHub Agentic Workflows, now in technical preview. Build automations using coding agents in GitHub Actions to handle triage, documentation, code quality, and more.]]></description>
      <pubDate>Fri, 13 Feb 2026 14:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</guid>
    </item>
    <item>
      <title><![CDATA[LibreOffice 26.2 : Markdown, accessibilité et plein d’autres nouveautés et améliorations]]></title>
      <link>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</link>
      <description><![CDATA[En février, il y a la corvée commerciale de la Saint-Valentin et les réjouissances intellectuelles consécutives à la sortie d’une nouvelle version de la suite bureautique LibreOffice. C’est, bien évidemment, sur LibreOffice 26.2 que l’on va se pencher. Au menu, du très visible, comme les boites de dialogues, du très attendu comme la prise en compte du Markdown ou du moins visible comme le travail sur l’accessibilité.
Il va de soi que les notes de version sont plus exhaustives et qu’il ne s’agit ici que d’une sélection. lien nᵒ 1 : Notes de version Sommaire
L’accessibilité
Support du Markdown
L’interface et les boites de dialogue
Writer
Calc
En vrac
Pour finir
Avant de commencer : toutes les captures d’écran ont été faites, volontairement, sur une interface très personnalisée.
L’accessibilité
L’accessibilité de la suite bureautique est un important chantier pour lequel une personne a été recrutée en 2023 (en). Cette version-ci a fait l’objet d’améliorations sensibles. Parallèlement, Sophie Gautier, coordinatrice de The Document Foundation1 (Foundation coordinator) est en train de monter un groupe de travail qui a pour objectif la publication d’un rapport de conformité en matière d’accessibilité pour répondre à la norme européenne EN 301 549 (en) d’accessiblité numérique. La langue de travail de ce groupe est l’anglais.
Concernant les améliorations de cette version :
la boite de dialogue « Vérifier les mises à jour », Aide &gt; Vérifier les mises à jour… est devenue accessible aux lecteurs d’écran ;
les fonctions d’accessibilité des aperçus des bordures, onglet « Bordures » des boites de dialogue, ont été revues afin qu’elles ne perturbent plus les dispositifs d’assistance ;
sur Linux : la boite de dialogue Outils&gt; Orthographe est annoncée correctement par le lecteur d’écran ;
quand on supprimait la sélection accessible, le curseur se déplaçait automatiquement au début du texte, ce comportement perturbant est supprimé ;
dans Writer, les fautes d’orthographe ne sont plus signalées par les dispositifs d’assistance si la vérification orthographique n’est pas activée ;
l’accessibilité au clavier de la boite de dialogue des extensions : Outils &gt; Extensions est accessible aux lecteurs d’écran ;
et enfin, il est possible de naviguer entre les onglets verticaux avec des raccourcis clavier.
Support du Markdown
Le Markdown est devenu le format de balisage léger standard « de fait ». Et c’est celui supporté par LinuxFR. Son support a été introduit dans cette version, c’est un des formats d’enregistrement qui s’est ajouté à la série des autres formats de la suite, pas un format d’export. Pour l’utiliser pour vos sites, passant pour LinuxFR, vous devrez :
soit ouvrir le fichier .md dans un éditeur de texte, n’importe lequel, même Mousepad fait l’affaire par exemple, et copier-coller ensuite le tout à partir de l’éditeur de texte là où vous le voulez ;
soit, si cela est possible, importer le fichier .md dans ce qui vous sert pour gérer le site comme le fait par exemple l’extension ODT2SPIP pour le système de gestion de contenu SPIP qui permet de créer une nouvelle page dans SPIP avec un fichier.ODT. ça marche avec LinuxFR ? Plutôt bien. Les styles de caractère Accentuation (ici en italiques) et Accentuation forte (ici gras) sont bien reconnu ainsi que Texte source pour « télétype », les indications in-texte encadrées de l’accent grave U+0060. Les styles de paragraphes :
Bloc de citation (paragraphes de citation précédés d’une ligne blanche et du signe « &gt; » dans la saisie de contenu sur LinuxFR) ;
Contenu de tableau ;
Corps de texte ;
Liste, par contre la numérotation des listes ordonnée ne semble pas bien fonctionner, il faut saisir les numéros à la main ;
Texte préformaté pour écrire des blocs de code ;
Titre 1, Titre 2, Titre 3 et Titre de tableau.
Les tableaux sont bien repris ainsi que les liens insérés via l’insertion d’hyperliens.
Ce qui ne semble pas fonctionner du tout : ce sont les notes, elles disparaissent corps et biens. C’est peut-être dû au passage dans l’éditeur de texte qui transforme un peu le document. Et, évidemment, il faut rajouter les images avec la syntaxe LinuxFR.
La version de Mardown de LibreOffice est CommonMark (en) et la bibliothèque utilisée est MD4C avec quelques extensions prises en charge par cette bibliothèque (cf ce rapport de bug (en) et ses réponses), pour en savoir plus, voir cette note (en) du blog de The Document Foundation.
Petite remarque, si vous utilisez un LibreOffice 25.8, vous avez peut-être pu constater qu’il était question d’enregistrement au format .md, cette information a été ajoutée trop précocement car la version 25.8 ne gère pas le Markdown.
L’interface et les boites de dialogue
Les boites de dialogue, notamment de styles et de formats, ont beaucoup changé. Longtemps elles se sont affichées avec une présentation par onglets en haut et le contenu dessous.
Puis il y a une période de transition en 2025 qui a fait grincer une collection complète de dents où on avait, selon l’endroit où on était, soit des onglets soit une navigation par menu latéral. Cette dernière avait un gros défaut : par exemple pour la configuration des styles dans Writer il fallait descendre tout en bas pour accéder aux options qui étaient cachées. Et il n’y avait pas de barre de défilement pour aller plus vite.
LibreOffice 26.2 voit ces défauts corrigés : les boites de dialogue sont harmonisées dans toute la suite et leur menu latéral, toujours sans barre de défilement qui s’avère finalement inutile, montre clairement tous les types de paramètres auxquels on peut accéder. Et, comme on peut le voir, LibreOffice a intégré une meilleure prise en charge des systèmes d’écritures asiatiques et complexes en affichant deux colonnes, une pour les polices occidentales, ou pour les polices asiatiques ou complexes. Une personne a également été recrutée en 2023 (en) pour travailler sur le support des systèmes d’écriture de droite à gauche (RTL) et complexes (CTL). Si toutefois, vous préférez revenir à l’affichage avec les onglets, il suffit d’aller dans le menu Outils &gt; Options &gt; Apparenceau niveau de « Boites de dialogue » et cocher l’option Horizontal en haut. Il faut savoir que les onglets en haut ne s’affichent que sur une seule ligne et qu’il faudra donc naviguer avec les flèches quand il y a de nombreuses options. Writer
Il y a un certain nombre d’amélioration autour de la compatibilité avec le format DOCX : séparation de tableaux flottants en plusieurs tableaux, suppression de la numérotation des notes de bas de page à l’ouverture d’un fichier DOCX, etc.
On relèvera deux nouvelles options d’alignement des paragraphes : « Début » et « Fin ». Si vous utilisez l’alphabet latin, vous ne verrez aucune différence avec les deux options « Forcer à gauche/en haut » et « Forcer à droite/en bas ». Elles ont été développées pour réutiliser plus facilement les styles entre les divers systèmes d’écriture. Pour continuer sur la lancée du travail pour la prise en compte des systèmes d’écriture dont le fonctionnement est différent de celui de l’alphabet latin, il est possible de changer la direction du texte : de gauche à droite ou de droite à gauche en cours de travail. Cela peut se paramétrer dans les styles. Calc
Un gros travail sur les performances a été fait : vitesse de défilement, rapidité des classeurs avec de nombreuses formes et du rejet des modifications. On voit apparaître de nouvelles options de tri (Données &gt;Trier) qui dépendent de la « locale » (langue définie dans les Options de LibreOffice). On peut ainsi déterminer quel caractère est utilisé comme séparateur de décimal pour le tri naturel. On peut relever aussi une avancée ergonomique qui va plaire à toutes celles et ceux qui utilisent les matrices, on peut maintenant modifier les formules matricielles avec la combinaison de touches : F2 + ↑ Maj + Ctrl + Entrée, il n’est plus nécessaire de modifier la formule elle-même.
Et aussi : si vous utilisez (pourquoi diable ?) le format d’enregistrement XLSX, c’est le format EXCEL2010+ qui est le format par défaut, il change de nom pour devenir « Classeur Excel 2010-365 ».2
En vrac
Base est devenu complètement multi-utilisateur, TDF a, d’ailleurs, recruté une personne pour travailler sur l’application.
Concernant les diagrammes (ou chart) : dans le Volet latéral, quand le graphique est en mode modification et que l’on va, au niveau de « Couleurs », sur la palette, on a une prévisualisation en direct dans le diagramme ce qui permet de tester le choix de couleurs plus facilement.
Les polices embarquées dont la licence ne permettait pas l’édition étaient jusqu’à présent ignorées et remplacées à l’affichage, ni vu, ni connu par une fonte de substitution. Ce défaut a été corrigé.
L’export PDF gère les liens avec les documents externes : Fichier &gt; Exporter au format PDF &gt; Liens. Les dictionnaires hongrois, mongol et portugais du Portugal ont été mis à jour ainsi que les règles de césure de la langue hongroise.
JSON, pour JavaScript Object Notation, est un format standard utilisé pour représenter des données structurées. Il est utilisé notamment pour échanger les informations entre un navigateur et un serveur. C’est, par exemple, le format de sauvegarde des marques-pages de Firefox ou de certains fichiers d’archives de Mastodon. Les documents XML et JSON génériques avec des plages pouvant être liées sont maintenant automatiquement mappés à des feuilles dans Calc. Une plage pouvant être liée est une section d’un document contenant des enregistrements tabulaires. Lorsqu’un document contient plusieurs plages pouvant être liées, chaque plage est mappée à une seule feuille3.
Et si vous avez envie de vous amuser avec les fonctions expérimentales (à activer dansOutils &gt; Options &gt; LibreOffice &gt; Avancé), vous pouvez jouer avec la nouvelle de boite de dialogue « Gestion des macros ».
Pour finir
Cette dépêche a, bien, évidemment, été rédigée avec LibreOffice et, cette fois-ci dans un fichier enregistré en Markdown. Les seules balises que j’ai dû entrer à la main sont celles des images. Kate a l’air de modifier le fichier et, quand je réouvre le .md dans LibreOffice, il y a des styles qui ont sauté mais la mise en forme reste visuellement la même. Kate rajoute aussi des barres obliques devant les « &gt; », aux crochets [ ] et même à certains hyperliens (images). Il y a peut-être des éditeurs de texte plus adaptés ou des réglages à faire.
J’ai rédigé cette dépêche en même temps qu’un article sur LibreOffice 26.2 pour mon site. Si l’article n’est pas vraiment dupliqué, il n’est pas étonnant d’y trouver des morceaux ici. Que tout cela ne nous empêche d’adresser tous nos remerciements à celles et ceux qui font de LibreOffice une suite bureautique si agréable à utiliser et si performante.
Post-scriptum : si vous voulez savoir modifier les couleurs de l’interface comme sur les captures d’écran, ça peut s’envisager, demandez gentiment, avec un peu de chance.
The Document Foundation ou TDF est la fondation de droit allemand qui pilote le projet LibreOffice. Il y a deux formats OOXML différents et donc deux formats XLSX différents, la version 2007 et la version actuelle depuis 2010. S’il vous est vraiment nécessaire d’enregistrer au format XLSX, il faut utiliser la version de 2010. Notes de version. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Fri, 13 Feb 2026 09:09:23 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</guid>
    </item>
    <item>
      <title><![CDATA[Projets Libres saison 4 épisode 11 : PVH éditions, une maison d'édition libérée et dans le Fediverse]]></title>
      <link>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</link>
      <description><![CDATA[Nous avons eu le plaisir de rencontrer Lionel Jeannerat durant les Rencontres Hivernales du libre à Saint-Cergue (VD) en janvier 2026. son parcours
la maison d'édition et ses œuvres
le passage au libre que ce soit pour les licences mais aussi pour leurs outils métiers
Bonne écoute ou lecture lien nᵒ 1 : Lien vers l'épisode
lien nᵒ 2 : S'abonner au podcast
lien nᵒ 3 : Le site de PVH éditions
lien nᵒ 4 : Soutenir le podcast
lien nᵒ 5 : L'épisode traduit en anglais
lien nᵒ 6 : Le site des Rencontres Hivernales du libre Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 07:40:57 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</guid>
    </item>
    <item>
      <title><![CDATA[Les journaux LinuxFr.org les mieux notés de janvier 2026]]></title>
      <link>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</link>
      <description><![CDATA[LinuxFr.org propose des dépêches et articles, soumis par tout un chacun, puis revus et corrigés par l’équipe de modération avant publication. C’est la partie la plus visible de LinuxFr.org, ce sont les dépêches qui sont le plus lues et suivies, sur le site, via Atom/RSS, ou bien via partage par messagerie instantanée, par courriel, ou encore via médias sociaux. Ce que l’on sait moins, c’est que LinuxFr.org vous propose également de publier directement vos propres articles, sans validation a priori de lʼéquipe de modération. Ceux-ci s’appellent des journaux. Voici un florilège d’une dizaine de ces journaux parmi les mieux notés par les utilisateurs et les utilisatrices… qui notent. Lumière sur ceux du mois de janvier passé.
« lecteur mp3 pour personne handicapée mentale » par ChocolatineFlying ;
« À la recherche du Linuxfrien type » par Ysabeau ;
« hacker sa pompe de relevage 3 et fin ! » par ChocolatineFlying ;
« [Hors sujet] Des tablettes lave-vaisselle tout-en-un » par Tanguy Ortolo ;
« Francis Hallé Bronsonisé » par Joris Dedieu ;
« 10 ans après, Modoboa est toujours là pour prendre soin de votre serveur de messagerie » par mirtouf ;
« À table ! » par JaguarWan ;
« Retour d'expérience sur le développement d'une application par l'utilisation d'IA » par phoenix ;
« Algoo lance un bulletin d'information mensuel « veille techno et logiciels libres » » par LeBouquetin ;
« Linux : les planètes s'alignent en 2026 » par vmagnin. lien nᵒ 1 : Participez à l’écriture d’un article
lien nᵒ 2 : Publiez votre journal
lien nᵒ 3 : Proposez une dépêche Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 09:23:50 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[Meilleures contributions LinuxFr.org : les primées de janvier 2026]]></title>
      <link>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</link>
      <description><![CDATA[Nous continuons sur notre lancée de récompenser celles et ceux qui chaque mois contribuent au site LinuxFr.org (dépêches, , logo, journaux, correctifs, etc.). Vous n’êtes pas sans risquer de gagner un livre des éditions Eyrolles, ENI et D-Booker. Voici les gagnants du mois de janvier 2026 :
Stefane Fermigier, pour sa dépêche « Appel à de la Commission "Vers des écosystèmes numériques ouverts européens" » ;
ChocolatineFlying, pour son journal « lecteur mp3 pour personne handicapé mental » ;
YvanM, pour sa dépêche « MeshCentral, alternative à TeamViewer et RustDesk » ;
Christophe Bliard, pour sa dépêche « Sortie de OpenProject 17.0 ».
Les livres gagnés sont détaillés en seconde partie de la dépêche. N’oubliez pas de contribuer, LinuxFr.org vit pour vous et par vous ! lien nᵒ 1 : Contribuez à LinuxFr.org !
lien nᵒ 2 : Tous les moyens (ou presque) de participer
lien nᵒ 3 : Récompenses précédentes (décembre 2025) Les livres sélectionnés
Linux — Maîtrisez l'administration du système — 7e édition. Certaines personnes n’ont pas pu être jointes ou n’ont pas répondu. Les lots ont été réattribués automatiquement. N’oubliez pas de mettre une adresse de courriel valable dans votre compte ou lors de la proposition d’une dépêche. En effet, c’est notre seul moyen de vous contacter, que ce soit pour les lots ou des questions sur votre dépêche lors de sa modération. Tous nos remerciements aux contributeurs du site ainsi qu’aux éditions Eyrolles, ENI et D-Booker. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 07:09:14 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[Continuous AI in practice: What developers can automate today with agentic CI]]></title>
      <link>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</link>
      <description><![CDATA[Think of Continuous AI as background agents that operate in your repository for tasks that require reasoning.]]></description>
      <pubDate>Thu, 05 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</guid>
    </item>
    <item>
      <title><![CDATA[Setting Docker Hardened Images free]]></title>
      <link>https://changelog.com/podcast/675</link>
      <description><![CDATA[In May of 2025, Docker launched Hardened Images, a secure, minimal, production-ready set of images. In December, they made DHI freely available and open source to everyone who builds software. On this episode, we're joined by Tushar Jain, EVP of Engineering at Docker to learn all about it.]]></description>
      <pubDate>Wed, 04 Feb 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/675</guid>
    </item>
    <item>
      <title><![CDATA[Pick your agent: Use Claude and Codex on Agent HQ]]></title>
      <link>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</link>
      <description><![CDATA[Claude by Anthropic and OpenAI Codex are now available in public preview on GitHub and VS Code with a Copilot Pro+ or Copilot Enterprise subscription. Here's what you need to know and how to get started today.]]></description>
      <pubDate>Wed, 04 Feb 2026 17:00:19 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</guid>
    </item>
    <item>
      <title><![CDATA[What the fastest-growing tools reveal about how software is being built]]></title>
      <link>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</link>
      <description><![CDATA[What languages are growing fastest, and why? What about the projects that people are interested in the most? Where are new developers cutting their teeth? Let’s take a look at Octoverse data to find out.]]></description>
      <pubDate>Tue, 03 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</guid>
    </item>
    <item>
      <title><![CDATA[The state of homelab tech (2026)]]></title>
      <link>https://changelog.com/friends/125</link>
      <description><![CDATA[Techno Tim joins Adam to dive deep into the state of homelab'ing in 2026. Hardware is scarce and expensive due to the AI gold rush, but software has never been better. From unleashing Claude on your UDM Pro to building custom Proxmox CLIs, they explores how AI is transforming what's possible in the homelab. Tim declares 2026 the "Year of Self-Hosted Software" while Adam reveals his homelab's secret weapons: DNSHole (a Pi-hole replacement written in Rust) and PXM (a Proxmox automation CLI).]]></description>
      <pubDate>Sat, 24 Jan 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/125</guid>
    </item>
  </channel>
</rss>