<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI-Pulse - Open Source & GitHub</title>
    <link>https://thephoenixagency.github.io/AI-Pulse</link>
    <description>Open Source & GitHub news from AI-Pulse</description>
    <language>en</language>
    <lastBuildDate>Thu, 19 Feb 2026 06:58:53 GMT</lastBuildDate>
    <atom:link href="https://thephoenixagency.github.io/AI-Pulse/feed-opensource.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title><![CDATA[ruvnet/wifi-densepose]]></title>
      <link>https://github.com/ruvnet/wifi-densepose</link>
      <description><![CDATA[Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers WiFi DensePose A cutting-edge WiFi-based human pose estimation system that leverages Channel State Information (CSI) data and advanced machine learning to provide real-time, privacy-preserving pose detection without cameras. Key Features Privacy-First: No cameras required - uses WiFi signals for pose detection Real-Time Processing: Sub-50ms latency with 30 FPS pose estimation Multi-Person Tracking: Simultaneous tracking of up to 10 individuals Domain-Specific Optimization: Healthcare, fitness, smart home, and security applications Enterprise-Ready: Production-grade API with authentication, rate limiting, and monitoring Hardware Agnostic: Works with standard WiFi routers and access points Comprehensive Analytics: Fall detection, activity recognition, and occupancy monitoring WebSocket Streaming: Real-time pose data streaming for live applications 100% Test Coverage: Thoroughly tested with comprehensive test suite Rust Implementation (v2) A high-performance Rust port is available in /rust-port/wifi-densepose-rs/: Performance Benchmarks (Validated) Operation Python (v1) Rust (v2) Speedup CSI Preprocessing (4x64) ~5ms 5.19 µs ~1000x Phase Sanitization (4x64) ~3ms 3.84 µs ~780x Feature Extraction (4x64) ~8ms 9.03 µs ~890x Motion Detection ~1ms 186 ns ~5400x Full Pipeline ~15ms 18.47 µs ~810x Throughput Metrics Component Throughput CSI Preprocessing 49-66 Melem/s Phase Sanitization 67-85 Melem/s Feature Extraction 7-11 Melem/s Full Pipeline ~54,000 fps Resource Comparison Feature Python (v1) Rust (v2) Memory Usage ~500MB ~100MB WASM Support Binary Size N/A ~10MB Test Coverage 100% 107 tests Quick Start (Rust): cd rust-port/wifi-densepose-rs
cargo build --release
cargo test --workspace
cargo bench --package wifi-densepose-signal Validation Tests Mathematical correctness validated: Phase unwrapping: 0.000000 radians max error Amplitude RMS: Exact match Doppler shift: 33.33 Hz (exact) Correlation: 1.0 for identical signals Phase coherence: 1.0 for coherent signals See Rust Port Documentation for ADRs and DDD patterns. WiFi-Mat: Disaster Response Module A specialized extension for search and rescue operations - detecting and localizing survivors trapped in rubble, earthquakes, and natural disasters. Key Capabilities Feature Description Vital Signs Detection Breathing (4-60 BPM), heartbeat via micro-Doppler 3D Localization Position estimation through debris up to 5m depth START Triage Automatic Immediate/Delayed/Minor/Deceased classification Real-time Alerts Priority-based notifications with escalation Use Cases Earthquake search and rescue Building collapse response Avalanche victim location Mine collapse detection Flood rescue operations Quick Example use wifi_densepose_mat::{DisasterResponse, DisasterConfig, DisasterType, ScanZone, ZoneBounds}; let config = DisasterConfig::builder() .disaster_type(DisasterType::Earthquake) .sensitivity(0.85) .max_depth(5.0) .build(); let mut response = DisasterResponse::new(config);
response.initialize_event(location, "Building collapse")?;
response.add_zone(ScanZone::new("North Wing", ZoneBounds::rectangle(0.0, 0.0, 30.0, 20.0)))?;
response.start_scanning().await?; // Get survivors prioritized by triage status
let immediate = response.survivors_by_triage(TriageStatus::Immediate);
println!("{} survivors require immediate rescue", immediate.len()); Documentation WiFi-Mat User Guide - Complete setup, configuration, and field deployment Architecture Decision Record - Design decisions and rationale Domain Model - DDD bounded contexts and entities Build: cd rust-port/wifi-densepose-rs
cargo build --release --package wifi-densepose-mat
cargo test --package wifi-densepose-mat Table of Contents Getting Started Key Features Rust Implementation (v2) WiFi-Mat Disaster Response System Architecture Installation Using pip ( ) From Source Using Docker System Requirements Quick Start Basic Setup Start the System Using the REST API Real-time Streaming Usage &amp; Configuration CLI Usage Installation Basic Commands Configuration Commands Examples Documentation Core Documentation Quick Links API Overview Hardware Setup Supported Hardware Physical Setup Network Configuration Environment Calibration Advanced Topics Configuration Environment Variables Domain-Specific Configurations Advanced Configuration Testing Running Tests Test Categories Mock Testing Continuous Integration Deployment Production Deployment Infrastructure as Code Monitoring and Logging Performance &amp; Community Performance Metrics Benchmark Results Performance Optimization Load Testing Contributing Development Setup Code Standards Contribution Process Code Review Checklist License Acknowledgments Support System Architecture WiFi DensePose consists of several key components working together: ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
│ WiFi Router │ │ WiFi Router │ │ WiFi Router │
│ (CSI Source) │ │ (CSI Source) │ │ (CSI Source) │
└─────────┬───────┘ └─────────┬───────┘ └─────────┬───────┘ │ │ │ └──────────────────────┼──────────────────────┘ │ ┌─────────────▼─────────────┐ │ CSI Data Collector │ │ (Hardware Interface) │ └─────────────┬─────────────┘ │ ┌─────────────▼─────────────┐ │ Signal Processor │ │ (Phase Sanitization) │ └─────────────┬─────────────┘ │ ┌─────────────▼─────────────┐ │ Neural Network Model │ │ (DensePose Head) │ └─────────────┬─────────────┘ │ ┌─────────────▼─────────────┐ │ Person Tracker │ │ (Multi-Object Tracking) │ └─────────────┬─────────────┘ │ ┌───────────────────────┼───────────────────────┐ │ │ │
┌─────────▼─────────┐ ┌─────────▼─────────┐ ┌─────────▼─────────┐
│ REST API │ │ WebSocket API │ │ Analytics │
│ (CRUD Operations)│ │ (Real-time Stream)│ │ (Fall Detection) │
└───────────────────┘ └───────────────────┘ └───────────────────┘ Core Components CSI Processor: Extracts and processes Channel State Information from WiFi signals Phase Sanitizer: Removes hardware-specific phase offsets and noise DensePose Neural Network: Converts CSI data to human pose keypoints Multi-Person Tracker: Maintains consistent person identities across frames REST API: Comprehensive API for data access and system control WebSocket Streaming: Real-time pose data broadcasting Analytics Engine: Advanced analytics including fall detection and activity recognition Installation Using pip ( ) WiFi-DensePose is now available on PyPI for easy installation: # Install the latest stable version
pip install wifi-densepose # Install with specific version
pip install wifi-densepose==1.0.0 # Install with optional dependencies
pip install wifi-densepose[gpu] # For GPU acceleration
pip install wifi-densepose[dev] # For development
pip install wifi-densepose[all] # All optional dependencies From Source git clone https://github.com/ruvnet/wifi-densepose.git
cd wifi-densepose
pip install -r requirements.txt
pip install -e . Using Docker docker pull ruvnet/wifi-densepose:latest
docker run -p 8000:8000 ruvnet/wifi-densepose:latest System Requirements Python: 3.8 or higher Operating System: Linux (Ubuntu 18.04+), macOS (10.15+), Windows 10+ Memory: Minimum 4GB RAM, 8GB+ Storage: 2GB free space for models and data Network: WiFi interface with CSI capability GPU: Optional but (NVIDIA GPU with CUDA support) Quick Start 1. Basic Setup # Install the package
pip install wifi-densepose # Copy example configuration
cp example.env .env # Edit configuration (set your WiFi interface)
nano .env 2. Start the System from wifi_densepose import WiFiDensePose # Initialize with default configuration
system = WiFiDensePose() # Start pose estimation
system.start() # Get latest pose data
poses = system.get_latest_poses()
print(f"Detected {len(poses)} persons") # Stop the system
system.stop() 3. Using the REST API # Start the API server
wifi-densepose start # Start with custom configuration
wifi-densepose -c /path/to/config.yaml start # Start with verbose logging
wifi-densepose -v start # Check server status
wifi-densepose status The API will be available at http://localhost:8000 API Documentation: http://localhost:8000/docs Health Check: http://localhost:8000/api/v1/health Latest Poses: http://localhost:8000/api/v1/pose/latest 4. Real-time Streaming import asyncio
import websockets
import json async def stream_poses(): uri = "ws://localhost:8000/ws/pose/stream" async with websockets.connect(uri) as websocket: while True: data = await websocket.recv() poses = json.loads(data) print(f"Received poses: {len(poses['persons'])} persons detected") # Run the streaming client
asyncio.run(stream_poses()) CLI Usage WiFi DensePose provides a comprehensive command-line interface for easy system management, configuration, and monitoring. CLI Installation The CLI is automatically installed with the package: # Install WiFi DensePose with CLI
pip install wifi-densepose # Verify CLI installation
wifi-densepose --help
wifi-densepose version Basic Commands The WiFi-DensePose CLI provides the following commands: wifi-densepose [OPTIONS] COMMAND [ARGS]... Options: -c, --config PATH Path to configuration file -v, --verbose Enable verbose logging --debug Enable debug mode --help Show this message and exit. Commands: config Configuration management commands. db Database management commands. start Start the WiFi-DensePose API server. status Show the status of the WiFi-DensePose API server. stop Stop the WiFi-DensePose API server. tasks Background task management commands. version Show version information. Server Management # Start the WiFi-DensePose API server
wifi-densepose start # Start with custom configuration
wifi-densepose -c /path/to/config.yaml start # Start with verbose logging
wifi-densepose -v start # Start with debug mode
wifi-densepose --debug start # Check server status
wifi-densepose status # Stop the server
wifi-densepose stop # Show version information
wifi-densepose version Configuration Commands Configuration Management # Configuration management commands
wifi-densepose config [SUBCOMMAND] # Examples:
# Show current configuration
wifi-densepose config show # Validate configuration file
wifi-densepose config validate # Create default configuration
wifi-densepose config init # Edit configuration
wifi-densepose config edit Database Management # Database management commands
wifi-densepose db [SUBCOMMAND] # Examples:
# Initialize database
wifi-densepose db init # Run database migrations
wifi-densepose db migrate # Check database status
wifi-densepose db status # Backup database
wifi-densepose db backup # Restore database
wifi-densepose db restore Background Tasks # Background task management commands
wifi-densepose tasks [SUBCOMMAND] # Examples:
# List running tasks
wifi-densepose tasks list # Start background tasks
wifi-densepose tasks start # Stop background tasks
wifi-densepose tasks stop # Check task status
wifi-densepose tasks status Command Examples Complete CLI Reference # Show help for main command
wifi-densepose --help # Show help for specific command
wifi-densepose start --help
wifi-densepose config --help
wifi-densepose db --help # Use global options with commands
wifi-densepose -v status # Verbose status check
wifi-densepose --debug start # Start with debug logging
wifi-densepose -c custom.yaml start # Start with custom config Common Usage Patterns # Basic server lifecycle
wifi-densepose start # Start the server
wifi-densepose status # Check if running
wifi-densepose stop # Stop the server # Configuration management
wifi-densepose config show # View current config
wifi-densepose config validate # Check config validity # Database operations
wifi-densepose db init # Initialize database
wifi-densepose db migrate # Run migrations
wifi-densepose db status # Check database health # Task management
wifi-densepose tasks list # List background tasks
wifi-densepose tasks status # Check task status # Version and help
wifi-densepose version # Show version info
wifi-densepose --help # Show help message CLI Examples Complete Setup Workflow # 1. Check version and help
wifi-densepose version
wifi-densepose --help # 2. Initialize configuration
wifi-densepose config init # 3. Initialize database
wifi-densepose db init # 4. Start the server
wifi-densepose start # 5. Check status
wifi-densepose status Development Workflow # Start with debug logging
wifi-densepose --debug start # Use custom configuration
wifi-densepose -c dev-config.yaml start # Check database status
wifi-densepose db status # Manage background tasks
wifi-densepose tasks start
wifi-densepose tasks list Production Workflow # Start with production config
wifi-densepose -c production.yaml start # Check system status
wifi-densepose status # Manage database
wifi-densepose db migrate
wifi-densepose db backup # Monitor tasks
wifi-densepose tasks status Troubleshooting # Enable verbose logging
wifi-densepose -v status # Check configuration
wifi-densepose config validate # Check database health
wifi-densepose db status # Restart services
wifi-densepose stop
wifi-densepose start Documentation Comprehensive documentation is available to help you get started and make the most of WiFi-DensePose: Core Documentation User Guide - Complete guide covering installation, setup, basic usage, and examples API Reference - Detailed documentation of all public classes, methods, and endpoints Deployment Guide - Production deployment, Docker setup, Kubernetes, and scaling strategies Troubleshooting Guide - Common issues, solutions, and diagnostic procedures Quick Links Interactive API Docs: http://localhost:8000/docs (when running) Health Check: http://localhost:8000/api/v1/health Latest Poses: http://localhost:8000/api/v1/pose/latest System Status: http://localhost:8000/api/v1/system/status API Overview The system provides a comprehensive REST API and WebSocket streaming: Key REST Endpoints # Pose estimation
GET /api/v1/pose/latest # Get latest pose data
GET /api/v1/pose/history # Get historical data
GET /api/v1/pose/zones/{zone_id} # Get zone-specific data # System management
GET /api/v1/system/status # System health and status
POST /api/v1/system/calibrate # Calibrate environment
GET /api/v1/analytics/summary # Analytics dashboard data WebSocket Streaming // Real-time pose data
ws://localhost:8000/ws/pose/stream // Analytics events (falls, alerts)
ws://localhost:8000/ws/analytics/events // System status updates
ws://localhost:8000/ws/system/status Python SDK Quick Example from wifi_densepose import WiFiDensePoseClient # Initialize client
client = WiFiDensePoseClient(base_url="http://localhost:8000") # Get latest poses with confidence filtering
poses = client.get_latest_poses(min_confidence=0.7)
print(f"Detected {len(poses)} persons") # Get zone occupancy
occupancy = client.get_zone_occupancy("living_room")
print(f"Living room occupancy: {occupancy.person_count}") For complete API documentation with examples, see the API Reference Guide. Hardware Setup Supported Hardware WiFi DensePose works with standard WiFi equipment that supports CSI extraction: Routers ASUS AX6000 (RT-AX88U) - Excellent CSI quality Netgear Nighthawk AX12 - High performance TP-Link Archer AX73 - Budget-friendly option Ubiquiti UniFi 6 Pro - Enterprise grade CSI-Capable Devices Intel WiFi cards (5300, 7260, 8260, 9260) Atheros AR9300 series Broadcom BCM4366 series Qualcomm QCA9984 series Physical Setup Router Placement: Position routers to create overlapping coverage areas Height: Mount routers 2-3 meters high for optimal coverage Spacing: 5-10 meter spacing between routers depending on environment Orientation: Ensure antennas are positioned for maximum signal diversity Network Configuration # Configure WiFi interface for CSI extraction
sudo iwconfig wlan0 mode monitor
sudo iwconfig wlan0 channel 6 # Set up CSI extraction (Intel 5300 example)
echo 0x4101 | sudo tee /sys/kernel/debug/ieee80211/phy0/iwlwifi/iwldvm/debug/monitor_tx_rate Environment Calibration from wifi_densepose import Calibrator # Run environment calibration
calibrator = Calibrator()
calibrator.calibrate_environment( duration_minutes=10, environment_id="room_001"
) # Apply calibration
calibrator.apply_calibration() Configuration Environment Variables Copy example.env to .env and configure: # Application Settings
APP_NAME=WiFi-DensePose API
VERSION=1.0.0
ENVIRONMENT=production # development, staging, production
DEBUG=false # Server Settings
HOST=0.0.0.0
PORT=8000
WORKERS=4 # Security Settings
SECRET_KEY=your-secure-secret-key-here
JWT_ALGORITHM=HS256
JWT_EXPIRE_HOURS=24 # Hardware Settings
WIFI_INTERFACE=wlan0
CSI_BUFFER_SIZE=1000
HARDWARE_POLLING_INTERVAL=0.1 # Pose Estimation Settings
POSE_CONFIDENCE_THRESHOLD=0.7
POSE_PROCESSING_BATCH_SIZE=32
POSE_MAX_PERSONS=10 # Feature Flags
ENABLE_AUTHENTICATION=true
ENABLE_RATE_LIMITING=true
ENABLE_WEBSOCKETS=true
ENABLE_REAL_TIME_PROCESSING=true
ENABLE_HISTORICAL_DATA=true Domain-Specific Configurations Healthcare Configuration config = { "domain": "healthcare", "detection": { "confidence_threshold": 0.8, "max_persons": 5, "enable_tracking": True }, "analytics": { "enable_fall_detection": True, "enable_activity_recognition": True, "alert_thresholds": { "fall_confidence": 0.9, "inactivity_timeout": 300 } }, "privacy": { "data_retention_days": 30, "anonymize_data": True, "enable_encryption": True }
} Fitness Configuration config = { "domain": "fitness", "detection": { "confidence_threshold": 0.6, "max_persons": 20, "enable_tracking": True }, "analytics": { "enable_activity_recognition": True, "enable_form_analysis": True, "metrics": ["rep_count", "form_score", "intensity"] }
} Advanced Configuration from wifi_densepose.config import Settings # Load custom configuration
settings = Settings( pose_model_path="/path/to/custom/model.pth", neural_network={ "batch_size": 64, "enable_gpu": True, "inference_timeout": 500 }, tracking={ "max_age": 30, "min_hits": 3, "iou_threshold": 0.3 }
) Testing WiFi DensePose maintains 100% test coverage with comprehensive testing: Running Tests # Run all tests
pytest # Run with coverage report
pytest --cov=wifi_densepose --cov-report=html # Run specific test categories
pytest tests/unit/ # Unit tests
pytest tests/integration/ # Integration tests
pytest tests/e2e/ # End-to-end tests
pytest tests/performance/ # Performance tests Test Categories Unit Tests (95% coverage) CSI processing algorithms Neural network components Tracking algorithms API endpoints Configuration validation Integration Tests Hardware interface integration Database operations WebSocket connections Authentication flows End-to-End Tests Complete pose estimation pipeline Multi-person tracking scenarios Real-time streaming Analytics generation Performance Tests Latency benchmarks Throughput testing Memory usage profiling Stress testing Mock Testing For development without hardware: # Enable mock mode
export MOCK_HARDWARE=true
export MOCK_POSE_DATA=true # Run tests with mocked hardware
pytest tests/ --mock-hardware Continuous Integration # .github/workflows/test.yml
name: Test Suite
on: [push, pull_request]
jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: 3.8 - name: Install dependencies run: | pip install -r requirements.txt pip install -e . - name: Run tests run: pytest --cov=wifi_densepose --cov-report=xml - name: Upload coverage uses: codecov/codecov-action@v1 Deployment Production Deployment Using Docker # Build production image
docker build -t wifi-densepose:latest . # Run with production configuration
docker run -d \ --name wifi-densepose \ -p 8000:8000 \ -v /path/to/data:/app/data \ -v /path/to/models:/app/models \ -e ENVIRONMENT=production \ -e SECRET_KEY=your-secure-key \ wifi-densepose:latest Using Docker Compose # docker-compose.yml
version: '3.8'
services: wifi-densepose: image: wifi-densepose:latest ports: - "8000:8000" environment: - ENVIRONMENT=production - DATABASE_URL=postgresql://user:pass@db:5432/wifi_densepose - REDIS_URL=redis://redis:6379/0 volumes: - ./data:/app/data - ./models:/app/models depends_on: - db - redis db: image: postgres:13 environment: POSTGRES_DB: wifi_densepose POSTGRES_USER: user POSTGRES_PASSWORD: password volumes: - postgres_data:/var/lib/postgresql/data redis: image: redis:6-alpine volumes: - redis_data:/data volumes: postgres_data: redis_data: Kubernetes Deployment # k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata: name: wifi-densepose
spec: replicas: 3 selector: matchLabels: app: wifi-densepose template: metadata: labels: app: wifi-densepose spec: containers: - name: wifi-densepose image: wifi-densepose:latest ports: - containerPort: 8000 env: - name: ENVIRONMENT value: "production" - name: DATABASE_URL valueFrom: secretKeyRef: name: wifi-densepose-secrets key: database-url resources: requests: memory: "2Gi" cpu: "1000m" limits: memory: "4Gi" cpu: "2000m" Infrastructure as Code Terraform (AWS) # terraform/main.tf
resource "aws_ecs_cluster" "wifi_densepose" { name = "wifi-densepose"
} resource "aws_ecs_service" "wifi_densepose" { name = "wifi-densepose" cluster = aws_ecs_cluster.wifi_densepose.id task_definition = aws_ecs_task_definition.wifi_densepose.arn desired_count = 3 load_balancer { target_group_arn = aws_lb_target_group.wifi_densepose.arn container_name = "wifi-densepose" container_port = 8000 }
} Ansible Playbook # ansible/playbook.yml
- hosts: servers become: yes tasks: - name: Install Docker apt: name: docker.io state: present - name: Deploy WiFi DensePose docker_container: name: wifi-densepose image: wifi-densepose:latest ports: - "8000:8000" env: ENVIRONMENT: production DATABASE_URL: "{{ database_url }}" restart_policy: always Monitoring and Logging Prometheus Metrics # monitoring/prometheus.yml
global: scrape_interval: 15s scrape_configs: - job_name: 'wifi-densepose' static_configs: - targets: ['localhost:8000'] metrics_path: '/metrics' Grafana Dashboard { "dashboard": { "title": "WiFi DensePose Monitoring", "panels": [ { "title": "Pose Detection Rate", "type": "graph", "targets": [ { "expr": "rate(pose_detections_total[5m])" } ] }, { "title": "Processing Latency", "type": "graph", "targets": [ { "expr": "histogram_quantile(0.95, pose_processing_duration_seconds_bucket)" } ] } ] }
} Performance Metrics Benchmark Results Latency Performance Average Processing Time: 45.2ms per frame 95th Percentile: 67ms 99th Percentile: 89ms Real-time Capability: 30 FPS sustained Accuracy Metrics Pose Detection Accuracy: 94.2% (compared to camera-based systems) Person Tracking Accuracy: 91.8% Fall Detection Sensitivity: 96.5% Fall Detection Specificity: 94.1% Resource Usage CPU Usage: 65% (4-core system) Memory Usage: 2.1GB RAM GPU Usage: 78% (NVIDIA RTX 3080) Network Bandwidth: 15 Mbps (CSI data) Scalability Maximum Concurrent Users: 1000+ WebSocket connections API Throughput: 10,000 requests/minute Data Storage: 50GB/month (with compression) Multi-Environment Support: Up to 50 simultaneous environments Performance Optimization Hardware Optimization # Enable GPU acceleration
config = { "neural_network": { "enable_gpu": True, "batch_size": 64, "mixed_precision": True }, "processing": { "num_workers": 4, "prefetch_factor": 2 }
} Software Optimization # Enable performance optimizations
config = { "caching": { "enable_redis": True, "cache_ttl": 300 }, "database": { "connection_pool_size": 20, "enable_query_cache": True }
} Load Testing # API load testing with Apache Bench
ab -n 10000 -c 100 http://localhost:8000/api/v1/pose/latest # WebSocket load testing
python scripts/websocket_load_test.py --connections 1000 --duration 300 Contributing We welcome contributions to WiFi DensePose! Please follow these guidelines: Development Setup # Clone the repository
git clone https://github.com/ruvnet/wifi-densepose.git
cd wifi-densepose # Create virtual environment
python -m venv venv
source venv/bin/activate # On Windows: venv\Scripts\activate # Install development dependencies
pip install -r requirements-dev.txt
pip install -e . # Install pre-commit hooks]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/ruvnet/wifi-densepose</guid>
    </item>
    <item>
      <title><![CDATA[alibaba/zvec]]></title>
      <link>https://github.com/alibaba/zvec</link>
      <description><![CDATA[A lightweight, lightning-fast, in-process vector database Quickstart | Home | Docs | Benchmarks | Discord | X (Twitter) Zvec is an open-source, in-process vector database — lightweight, lightning-fast, and designed to embed directly into applications. Built on Proxima (Alibaba's battle-tested vector search engine), it delivers production-grade, low-latency, scalable similarity search with minimal setup. Features Blazing Fast: Searches billions of vectors in milliseconds. Simple, Just Works: Install and start searching in seconds. No servers, no config, no fuss. Dense + Sparse Vectors: Work with both dense and sparse embeddings, with native support for multi-vector queries in a single call. Hybrid Search: Combine semantic similarity with structured filters for precise results. Runs Anywhere: As an in-process library, Zvec runs wherever your code runs — notebooks, servers, CLI tools, or even edge devices. Installation Python Requirements: Python 3.10 - 3.12 pip install zvec Node.js npm install @zvec/zvec Supported Platforms Linux (x86_64, ARM64) macOS (ARM64) Building from Source If you prefer to build Zvec from source, please check the Building from Source guide. One-Minute Example import zvec # Define collection schema
schema = zvec.CollectionSchema( name="example", vectors=zvec.VectorSchema("embedding", zvec.DataType.VECTOR_FP32, 4),
) # Create collection
collection = zvec.create_and_open(path="./zvec_example", schema=schema) # Insert documents
collection.insert([ zvec.Doc(id="doc_1", vectors={"embedding": [0.1, 0.2, 0.3, 0.4]}), zvec.Doc(id="doc_2", vectors={"embedding": [0.2, 0.3, 0.4, 0.1]}),
]) # Search by vector similarity
results = collection.query( zvec.VectorQuery("embedding", vector=[0.4, 0.3, 0.3, 0.1]), topk=10
) # Results: list of {'id': str, 'score': float, ...}, sorted by relevance
print(results) Performance at Scale Zvec delivers exceptional speed and efficiency, making it ideal for demanding production workloads. For detailed benchmark methodology, configurations, and complete results, please see our Benchmarks documentation. Join Our Community Stay updated and get support — scan or click: Join Server Follow @zvec_ai Contributing We welcome and appreciate contributions from the community! Whether you're fixing a bug, adding a feature, or improving documentation, your help makes Zvec better for everyone. Check out our Contributing Guide to get started!]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/alibaba/zvec</guid>
    </item>
    <item>
      <title><![CDATA[NirDiamant/RAG_Techniques]]></title>
      <link>https://github.com/NirDiamant/RAG_Techniques</link>
      <description><![CDATA[This repository showcases various advanced techniques for Retrieval-Augmented Generation (RAG) systems. RAG systems combine information retrieval with generative models to provide accurate and contextually rich responses. Support This Project: Your sponsorship fuels innovation in RAG technologies. Become a to help maintain and expand this valuable resource! We gratefully acknowledge the organizations and individuals who have made significant contributions to this project. Company Individual Advanced RAG Techniques: Elevating Your Retrieval-Augmented Generation Systems Welcome to one of the most comprehensive and dynamic collections of Retrieval-Augmented Generation (RAG) tutorials available today. This repository serves as a hub for cutting-edge techniques aimed at enhancing the accuracy, efficiency, and contextual richness of RAG systems. Stay Updated! Cutting-edge
Updates Expert
Insights Top 0.1%
Content Join over 50,000 AI enthusiasts getting unique cutting-edge insights and free tutorials! Plus, subscribers get exclusive early access and special 33% discounts to my book and the upcoming RAG Techniques course! Introduction Retrieval-Augmented Generation (RAG) is revolutionizing the way we combine information retrieval with generative AI. This repository showcases a curated collection of advanced techniques designed to supercharge your RAG systems, enabling them to deliver more accurate, contextually relevant, and comprehensive responses. Our goal is to provide a valuable resource for researchers and practitioners looking to push the boundaries of what's possible with RAG. By fostering a collaborative environment, we aim to accelerate innovation in this exciting field. Related Projects Level up with my Agents Towards Production repository. It delivers horizontal, code-first tutorials that cover every tool and step in the lifecycle of building production-grade GenAI agents, guiding you from spark to scale with proven patterns and reusable blueprints for real-world launches, making it the smartest place to start if you're serious about shipping agents to production. Explore my GenAI Agents Repository to discover a variety of AI agent implementations and tutorials, showcasing how different AI technologies can be combined to create powerful, interactive systems. Check out my Prompt Engineering Techniques guide for a comprehensive collection of prompting strategies, from basic concepts to advanced techniques, enhancing your ability to interact effectively with AI language models. A Community-Driven Knowledge Hub This repository grows stronger with your contributions! Join our vibrant communities - the central hubs for shaping and advancing this project together Educational AI Subreddit RAG Techniques Discord Community Whether you're an expert or just starting out, your insights can shape the future of RAG. Join us to propose ideas, get feedback, and collaborate on innovative techniques. For contribution guidelines, please refer to our CONTRIBUTING.md file. Let's advance RAG technology together! For discussions on GenAI, RAG, or custom agents, or to explore knowledge-sharing opportunities, feel free to connect on LinkedIn. Key Features State-of-the-art RAG enhancements Comprehensive documentation for each technique Practical implementation guidelines Regular updates with the latest advancements Advanced Techniques Explore our extensive list of cutting-edge RAG techniques: # Category Technique View 1 Key Collaboration Agentic RAG with Contextual AI 2 Foundational Basic RAG 3 Foundational RAG with CSV Files 4 Foundational Reliable RAG 5 Foundational Optimizing Chunk Sizes 6 Foundational Proposition Chunking 7 Query Enhancement Query Transformations 8 Query Enhancement HyDE (Hypothetical Document Embedding) 9 Query Enhancement HyPE (Hypothetical Prompt Embedding) 10 Context Enrichment Contextual Chunk Headers 11 Context Enrichment Relevant Segment Extraction 12 Context Enrichment Context Window Enhancement 13 Context Enrichment Semantic Chunking 14 Context Enrichment Contextual Compression 15 Context Enrichment Document Augmentation 16 Advanced Retrieval Fusion Retrieval 17 Advanced Retrieval Reranking 18 Advanced Retrieval Multi-faceted Filtering 19 Advanced Retrieval Hierarchical Indices 20 Advanced Retrieval Ensemble Retrieval 21 Advanced Retrieval Dartboard Retrieval 22 Advanced Retrieval Multi-modal RAG with Captioning 23 Iterative Techniques Retrieval with Feedback Loop 24 Iterative Techniques Adaptive Retrieval 25 Iterative Retrieval Iterative Retrieval 26 Evaluation DeepEval 27 Evaluation GroUSE 28 Explainability Explainable Retrieval 29 Advanced Architecture Graph RAG with LangChain 30 Advanced Architecture Microsoft GraphRAG 31 Advanced Architecture RAPTOR 32 Advanced Architecture Self-RAG 33 Advanced Architecture Corrective RAG (CRAG) 34 Special Technique Sophisticated Controllable Agent Foundational RAG Techniques Simple RAG LangChain: LlamaIndex: Runnable Script Overview Introducing basic RAG techniques ideal for newcomers. Implementation Start with basic retrieval queries and integrate incremental learning mechanisms. Simple RAG using a CSV file LangChain: LlamaIndex: Overview Introducing basic RAG using CSV files. Implementation This uses CSV files to create basic retrieval and integrates with openai to create question and answering system. Reliable RAG : Overview Enhances the Simple RAG by adding validation and refinement to ensure the accuracy and relevance of retrieved information. Implementation Check for retrieved document relevancy and highlight the segment of docs used for answering. Choose Chunk Size LangChain: Runnable Script Overview Selecting an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency. Implementation Experiment with different chunk sizes to find the optimal balance between preserving context and maintaining retrieval speed for your specific use case. Proposition Chunking : Overview Breaking down the text into concise, complete, meaningful sentences allowing for better control and handling of specific queries (especially extracting knowledge). Implementation Proposition Generation: The LLM is used in conjunction with a custom prompt to generate factual statements from the document chunks. Quality Checking: The generated propositions are passed through a grading system that evaluates accuracy, clarity, completeness, and conciseness. Additional Resources The Propositions Method: Enhancing Information Retrieval for AI Systems - A comprehensive blog post exploring the benefits and implementation of proposition chunking in RAG systems. Query Enhancement Query Transformations LangChain: Runnable Script Overview Modifying and expanding queries to improve retrieval effectiveness. Implementation Query Rewriting: Reformulate queries to improve retrieval. Step-back Prompting: Generate broader queries for better context retrieval. Sub-query Decomposition: Break complex queries into simpler sub-queries. Hypothetical Questions (HyDE Approach) LangChain: Runnable Script Overview Generating hypothetical questions to improve alignment between queries and data. Implementation Create hypothetical questions that point to relevant locations in the data, enhancing query-data matching. Additional Resources HyDE: Exploring Hypothetical Document Embeddings for AI Retrieval - A short blog post explaining this method clearly. Context and Content Enrichment Hypothetical Prompt Embeddings (HyPE) LangChain: Runnable Script Overview HyPE (Hypothetical Prompt Embeddings) is an enhancement to traditional RAG retrieval that precomputes hypothetical prompts at the indexing stage, but inseting the chunk in their place. This transforms retrieval into a question-question matching task. This avoids the need for runtime synthetic answer generation, reducing inference-time computational overhead while improving retrieval alignment. Implementation Precomputed Questions: Instead of embedding document chunks, HyPE generates multiple hypothetical queries per chunk at indexing time. Question-Question Matching: User queries are matched against stored hypothetical questions, leading to better retrieval alignment. No Runtime Overhead: Unlike HyDE, HyPE does not require LLM calls at query time, making retrieval faster and cheaper. Higher Precision &amp; Recall: Improves retrieval context precision by up to 42 percentage points and claim recall by up to 45 percentage points. Additional Resources Preprint: Hypothetical Prompt Embeddings (HyPE) - Research paper detailing the method, evaluation, and benchmarks. Contextual Chunk Headers : Overview Contextual chunk headers (CCH) is a method of creating document-level and section-level context, and prepending those chunk headers to the chunks prior to embedding them. Implementation Create a chunk header that includes context about the document and/or section of the document, and prepend that to each chunk in order to improve the retrieval accuracy. Additional Resources dsRAG: open-source retrieval engine that implements this technique (and a few other advanced RAG techniques) Relevant Segment Extraction : Overview Relevant segment extraction (RSE) is a method of dynamically constructing multi-chunk segments of text that are relevant to a given query. Implementation Perform a retrieval post-processing step that analyzes the most relevant chunks and identifies longer multi-chunk segments to provide more complete context to the LLM. Context Enrichment Techniques LangChain: LlamaIndex: Runnable Script Overview Enhancing retrieval accuracy by embedding individual sentences and extending context to neighboring sentences. Implementation Retrieve the most relevant sentence while also accessing the sentences before and after it in the original text. Semantic Chunking LangChain: Runnable Script Overview Dividing documents based on semantic coherence rather than fixed sizes. Implementation Use NLP techniques to identify topic boundaries or coherent sections within documents for more meaningful retrieval units. Additional Resources Semantic Chunking: Improving AI Information Retrieval - A comprehensive blog post exploring the benefits and implementation of semantic chunking in RAG systems. Contextual Compression LangChain: Runnable Script Overview Compressing retrieved information while preserving query-relevant content. Implementation Use an LLM to compress or summarize retrieved chunks, preserving key information relevant to the query. Document Augmentation through Question Generation for Enhanced Retrieval LangChain: Runnable Script Overview This implementation demonstrates a text augmentation technique that leverages additional question generation to improve document retrieval within a vector database. By generating and incorporating various questions related to each text fragment, the system enhances the standard retrieval process, thus increasing the likelihood of finding relevant documents that can be utilized as context for generative question answering. Implementation Use an LLM to augment text dataset with all possible questions that can be asked to each document. Advanced Retrieval Methods Fusion Retrieval LangChain: LlamaIndex: Runnable Script Overview Optimizing search results by combining different retrieval methods. Implementation Combine keyword-based search with vector-based search for more comprehensive and accurate retrieval. Intelligent Reranking LangChain: LlamaIndex: Runnable Script Overview Applying advanced scoring mechanisms to improve the relevance ranking of retrieved results. Implementation LLM-based Scoring: Use a language model to score the relevance of each retrieved chunk. Cross-Encoder Models: Re-encode both the query and retrieved documents jointly for similarity scoring. Metadata-enhanced Ranking: Incorporate metadata into the scoring process for more nuanced ranking. Additional Resources Relevance Revolution: How Re-ranking Transforms RAG Systems - A comprehensive blog post exploring the power of re-ranking in enhancing RAG system performance. Multi-faceted Filtering Overview Applying various filtering techniques to refine and improve the quality of retrieved results. Implementation Metadata Filtering: Apply filters based on attributes like date, source, author, or document type. Similarity Thresholds: Set thresholds for relevance scores to keep only the most pertinent results. Content Filtering: Remove results that don't match specific content criteria or essential keywords. Diversity Filtering: Ensure result diversity by filtering out near-duplicate entries. Hierarchical Indices LangChain: Runnable Script Overview Creating a multi-tiered system for efficient information navigation and retrieval. Implementation Implement a two-tiered system for document summaries and detailed chunks, both containing metadata pointing to the same location in the data. Additional Resources Hierarchical Indices: Enhancing RAG Systems - A comprehensive blog post exploring the power of hierarchical indices in enhancing RAG system performance. Ensemble Retrieval Overview Combining multiple retrieval models or techniques for more robust and accurate results. Implementation Apply different embedding models or retrieval algorithms and use voting or weighting mechanisms to determine the final set of retrieved documents. Dartboard Retrieval LangChain: Overview Optimizing over Relevant Information Gain in Retrieval Implementation Combine both relevance and diversity into a single scoring function and directly optimize for it. POC showing plain simple RAG underperforming when the database is dense, and the dartboard retrieval outperforming it. Multi-modal Retrieval Overview Extending RAG capabilities to handle diverse data types for richer responses. Implementation Multi-model RAG with Multimedia Captioning: - Caption and store all the other multimedia data like pdfs, ppts, etc., with text data in vector store and retrieve them together. Multi-model RAG with Colpali: - Instead of captioning convert all the data into image, then find the most relevant images and pass them to a vision large language model. Iterative and Adaptive Techniques Retrieval with Feedback Loops LangChain: Runnable Script Overview Implementing mechanisms to learn from user interactions and improve future retrievals. Implementation Collect and utilize user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models. Adaptive Retrieval LangChain: Runnable Script Overview Dynamically adjusting retrieval strategies based on query types and user contexts. Implementation Classify queries into different categories and use tailored retrieval strategies for each, considering user context and preferences. Iterative Retrieval Overview Performing multiple rounds of retrieval to refine and enhance result quality. Implementation Use the LLM to analyze initial results and generate follow-up queries to fill in gaps or clarify information. Evaluation DeepEval Evaluation: | Comprehensive RAG system evaluation | Overview Performing evaluations Retrieval-Augmented Generation systems, by covering several metrics and creating test cases. Implementation Use the deepeval library to conduct test cases on correctness, faithfulness and contextual relevancy of RAG systems. GroUSE Evaluation: | Contextually-grounded LLM evaluation | Overview Evaluate the final stage of Retrieval-Augmented Generation using metrics of the GroUSE framework and meta-evaluate your custom LLM judge on GroUSE unit tests. Implementation Use the grouse package to evaluate contextually-grounded LLM generations with GPT-4 on the 6 metrics of the GroUSE framework and use unit tests to evaluate a custom Llama 3.1 405B evaluator. Explainability and Transparency Explainable Retrieval LangChain: Runnable Script Overview Providing transparency in the retrieval process to enhance user trust and system refinement. Implementation Explain why certain pieces of information were retrieved and how they relate to the query. Advanced Architectures Agentic RAG with Contextual AI Agentic RAG: Overview Building production-ready agentic RAG pipelines for financial document analysis with Contextual AI's managed platform. This comprehensive tutorial demonstrates how to leverage agentic RAG to solve complex queries through intelligent query reformulation, document parsing, reranking, and grounded language models. Implementation Document Parser: Enterprise-grade parsing with vision models for complex tables, charts, and multi-page documents Instruction-Following Reranker: SOTA reranker with instruction-following capabilities for handling conflicting information Grounded Language Model (GLM): World's most grounded LLM specifically engineered to minimize hallucinations for RAG use cases LMUnit: Natural language unit testing framework for evaluating and optimizing RAG system performance Graph RAG with Milvus Vector Database Graph RAG with Milvus: Overview A simple yet powerful approach to implement Graph RAG using Milvus vector databases. This technique significantly improves performance on complex multi-hop questions by combining relationship-based retrieval with vector search and reranking. Implementation Store both text passages and relationship triplets (subject-predicate-object) in separate Milvus collections Perform multi-way retrieval by querying both collections Use an LLM to rerank retrieved relationships based on their relevance to the query Retrieve the final passages based on the most relevant relationships Knowledge Graph Integration (Graph RAG) LangChain: Runnable Script Overview Incorporating structured data from knowledge graphs to enrich context and improve retrieval. Implementation Retrieve entities and their relationships from a knowledge graph relevant to the query, combining this structured data with unstructured text for more informative responses. GraphRag (Microsoft) GraphRag: Overview Microsoft GraphRAG (Open Source) is an advanced RAG system that integrates knowledge graphs to improve the performance of LLMs Implementation • Analyze an input corpus by extracting entities, relationships from text units. generates summaries of each community and its constituents from the bottom-up. RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval LangChain: Runnable Script Overview Implementing a recursive approach to process and organize retrieved information in a tree structure. Implementation Use abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context. Self RAG LangChain: Runnable Script Overview A dynamic approach that combines retrieval-based and generation-based methods, adaptively deciding whether to use retrieved information and how to best utilize it in generating responses. Implementation • Implement a multi-step process including retrieval decision, document retrieval, relevance evaluation, response generation, support assessment, and utility evaluation to produce accurate, relevant, and useful outputs. Corrective RAG LangChain: Runnable Script Overview A sophisticated RAG approach that dynamically evaluates and corrects the retrieval process, combining vector databases, web search, and language models for highly accurate and context-aware responses. Implementation • Integrate Retrieval Evaluator, Knowledge Refinement, Web Search Query Rewriter, and Response Generator components to create a system that adapts its information sourcing strategy based on relevance scores and combines multiple sources when necessary. Special Advanced Technique Sophisticated Controllable Agent for Complex RAG Tasks Overview An advanced RAG solution designed to tackle complex questions that simple semantic similarity-based retrieval cannot solve. This approach uses a sophisticated deterministic graph as the "brain" of a highly controllable autonomous agent, capable of answering non-trivial questions from your own data. Implementation • Implement a multi-step process involving question anonymization, high-level planning, task breakdown, adaptive information retrieval and question answering, continuous re-planning, and rigorous answer verification to ensure grounded and accurate responses. Getting Started To begin implementing these advanced RAG techniques in your projects: Clone this repository: git clone https://github.com/NirDiamant/RAG_Techniques.git Navigate to the technique you're interested in: cd all_rag_techniques/technique-name Follow the detailed implementation guide in each technique's directory. Contributing We welcome contributions from the community! If you have a new technique or improvement to suggest: Fork the repository Create your feature branch: git checkout -b feature/AmazingFeature Commit your changes: git commit -m 'Add some AmazingFeature' Push to the branch: git push origin feature/AmazingFeature Open a pull request Contributors License This project is licensed under a custom non-commercial license - see the LICENSE file for details. If you find this repository helpful, please consider giving it a star! Keywords: RAG, Retrieval-Augmented Generation, NLP, AI, Machine Learning, Information Retrieval, Natural Language Processing, LLM, Embeddings, Semantic Search]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/NirDiamant/RAG_Techniques</guid>
    </item>
    <item>
      <title><![CDATA[mistralai/mistral-vibe]]></title>
      <link>https://github.com/mistralai/mistral-vibe</link>
      <description><![CDATA[Minimal CLI coding agent by Mistral Mistral Vibe ░░ ░░ ░░ ██ ░░ ░░ ██ ██ ░░
██ ██ ██░░ ░░ ░░ Mistral's open-source CLI coding assistant. Mistral Vibe is a command-line coding assistant powered by Mistral's models. It provides a conversational interface to your codebase, allowing you to use natural language to explore, modify, and interact with your projects through a powerful set of tools. [!WARNING] Mistral Vibe works on Windows, but we officially support and target UNIX environments. One-line install ( ) Linux and macOS curl -LsSf https://mistral.ai/vibe/install.sh | bash Windows First, install uv powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex" Then, use uv command below. Using uv uv tool install mistral-vibe Using pip pip install mistral-vibe Table of Contents Features Built-in Agents Subagents and Task Delegation Interactive User Questions Terminal Requirements Quick Start Usage Interactive Mode Trust Folder System Programmatic Mode Slash Commands Built-in Slash Commands Custom Slash Commands via Skills Skills System Creating Skills Skill Discovery Managing Skills Configuration Configuration File Location API Key Configuration Custom System Prompts Custom Agent Configurations Tool Management MCP Server Configuration Session Management Update Settings Custom Vibe Home Directory Editors/IDEs Resources Data collection &amp; usage License Features Interactive Chat: A conversational AI agent that understands your requests and breaks down complex tasks. Powerful Toolset: A suite of tools for file manipulation, code searching, version control, and command execution, right from the chat prompt. Read, write, and patch files (read_file, write_file, search_replace). Execute shell commands in a stateful terminal (bash). Recursively search code with grep (with ripgrep support). Manage a todo list to track the agent's work. Ask interactive questions to gather user input (ask_user_question). Delegate tasks to subagents for parallel work (task). Project-Aware Context: Vibe automatically scans your project's file structure and Git status to provide relevant context to the agent, improving its understanding of your codebase. Advanced CLI Experience: Built with modern libraries for a smooth and efficient workflow. Autocompletion for slash commands (/) and file paths (@). Persistent command history. Beautiful Themes. Highly Configurable: Customize models, providers, tool permissions, and UI preferences through a simple config.toml file. Safety First: Features tool execution approval. Multiple Built-in Agents: Choose from different agent profiles tailored for specific workflows. Built-in Agents Vibe comes with several built-in agent profiles, each designed for different use cases: default: Standard agent that requires approval for tool executions. Best for general use. plan: Read-only agent for exploration and planning. Auto-approves safe tools like grep and read_file. accept-edits: Auto-approves file edits only (write_file, search_replace). Useful for code refactoring. auto-approve: Auto-approves all tool executions. Use with caution. Use the --agent flag to select a different agent: vibe --agent plan Subagents and Task Delegation Vibe supports subagents for delegating tasks. Subagents run independently and can perform specialized work without user interaction, preventing the context from being overloaded. The task tool allows the agent to delegate work to subagents: &gt; Can you explore the codebase structure while I work on something else? I'll use the task tool to delegate this to the explore subagent. &gt; task(task="Analyze the project structure and architecture", agent="explore") Create custom subagents by adding agent_type = "subagent" to your agent configuration. Vibe comes with a built-in subagent called explore, a read-only subagent for codebase exploration used internally for delegation. Interactive User Questions The ask_user_question tool allows the agent to ask you clarifying questions during its work. This enables more interactive and collaborative workflows. &gt; Can you help me refactor this function? I need to understand your requirements better before proceeding. &gt; ask_user_question(questions=[{ "question": "What's the main goal of this refactoring?", "options": [ {"label": "Performance", "description": "Make it run faster"}, {"label": "Readability", "description": "Make it easier to understand"}, {"label": "Maintainability", "description": "Make it easier to modify"} ]
}]) The agent can ask multiple questions at once, displayed as tabs. Each question supports 2-4 options plus an automatic "Other" option for free text responses. Terminal Requirements Vibe's interactive interface requires a modern terminal emulator. terminal emulators include: WezTerm (cross-platform) Alacritty (cross-platform) Ghostty (Linux and macOS) Kitty (Linux and macOS) Most modern terminals should work, but older or minimal terminal emulators may have display issues. Quick Start Navigate to your project's root directory: cd /path/to/your/project Run Vibe: vibe If this is your first time running Vibe, it will: Create a default configuration file at ~/.vibe/config.toml Prompt you to enter your API key if it's not already configured Save your API key to ~/.vibe/.env for future use Alternatively, you can configure your API key separately using vibe --setup. Start interacting with the agent! &gt; Can you find all instances of the word "TODO" in the project? The user wants to find all instances of "TODO". The `grep` tool is perfect for this. I will use it to search the current directory. &gt; grep(pattern="TODO", path=".") ... (grep tool output) ... I found the following "TODO" in your project. Usage Interactive Mode Simply run vibe to enter the interactive chat loop. Multi-line Input: Press Ctrl+J or Shift+Enter for select terminals to insert a newline. File Paths: Reference files in your prompt using the @ symbol for smart autocompletion (e.g., &gt; Read the file @src/agent.py). Shell Commands: Prefix any command with ! to execute it directly in your shell, bypassing the agent (e.g., &gt; !ls -l). External Editor: Press Ctrl+G to edit your current input in an external editor. Tool Output Toggle: Press Ctrl+O to toggle the tool output view. Todo View Toggle: Press Ctrl+T to toggle the todo list view. Auto-Approve Toggle: Press Shift+Tab to toggle auto-approve mode on/off. You can start Vibe with a prompt using the following command: vibe "Refactor the main function in cli/main.py to be more modular." Note: The --auto-approve flag automatically approves all tool executions without prompting. In interactive mode, you can also toggle auto-approve on/off using Shift+Tab. Trust Folder System Vibe includes a trust folder system to ensure you only run the agent in directories you trust. When you first run Vibe in a new directory which contains a .vibe subfolder, it may ask you to confirm whether you trust the folder. Trusted folders are remembered for future sessions. You can manage trusted folders through its configuration file ~/.vibe/trusted_folders.toml. This safety feature helps prevent accidental execution in sensitive directories. Programmatic Mode You can run Vibe non-interactively by piping input or using the --prompt flag. This is useful for scripting. vibe --prompt "Refactor the main function in cli/main.py to be more modular." By default, it uses auto-approve mode. Programmatic Mode Options When using --prompt, you can specify additional options: --max-turns N: Limit the maximum number of assistant turns. The session will stop after N turns. --max-price DOLLARS: Set a maximum cost limit in dollars. The session will be interrupted if the cost exceeds this limit. --enabled-tools TOOL: Enable specific tools. In programmatic mode, this disables all other tools. Can be specified multiple times. Supports exact names, glob patterns (e.g., bash*), or regex with re: prefix (e.g., re:^serena_.*$). --output FORMAT: Set the output format. Options: text (default): Human-readable text output json: All messages as JSON at the end streaming: Newline-delimited JSON per message Example: vibe --prompt "Analyze the codebase" --max-turns 5 --max-price 1.0 --output json Slash Commands Use slash commands for meta-actions and configuration changes during a session. Built-in Slash Commands Vibe provides several built-in slash commands. Use slash commands by typing them in the input box: &gt; /help Custom Slash Commands via Skills You can define your own slash commands through the skills system. Skills are reusable components that extend Vibe's functionality. To create a custom slash command: Create a skill directory with a SKILL.md file Set user-invocable = true in the skill metadata Define the command logic in your skill Example skill metadata: ---
name: my-skill
description: My custom skill with slash commands
user-invocable: true
--- Custom slash commands appear in the autocompletion menu alongside built-in commands. Skills System Vibe's skills system allows you to extend functionality through reusable components. Skills can add new tools, slash commands, and specialized behaviors. Vibe follows the Agent Skills specification for skill format and structure. Creating Skills Skills are defined in directories with a SKILL.md file containing metadata in YAML frontmatter. For example, ~/.vibe/skills/code-review/SKILL.md: ---
name: code-review
description: Perform automated code reviews
license: MIT
compatibility: Python 3.12+
user-invocable: true
allowed-tools: - read_file - grep - ask_user_question
--- # Code Review Skill This skill helps analyze code quality and suggest improvements. Skill Discovery Vibe discovers skills from multiple locations: Custom paths: Configured in config.toml via skill_paths Standard Agent Skills path (project root, trusted folders only): .agents/skills/ — Agent Skills standard Local project skills (project root, trusted folders only): .vibe/skills/ in your project Global skills directory: ~/.vibe/skills/ skill_paths = ["/path/to/custom/skills"] Managing Skills Enable or disable skills using patterns in your configuration: # Enable specific skills
enabled_skills = ["code-review", "test-*"] # Disable specific skills
disabled_skills = ["experimental-*"] Skills support the same pattern matching as tools (exact names, glob patterns, and regex). Configuration Configuration File Location Vibe is configured via a config.toml file. It looks for this file first in ./.vibe/config.toml and then falls back to ~/.vibe/config.toml. API Key Configuration To use Vibe, you'll need a Mistral API key. You can obtain one by signing up at https://console.mistral.ai. You can configure your API key using vibe --setup, or through one of the methods below. Vibe supports multiple ways to configure your API keys: Interactive Setup ( for first-time users): When you run Vibe for the first time or if your API key is missing, Vibe will prompt you to enter it. The key will be securely saved to ~/.vibe/.env for future sessions. Environment Variables: Set your API key as an environment variable: export MISTRAL_API_KEY="your_mistral_api_key" .env File: Create a .env file in ~/.vibe/ and add your API keys: MISTRAL_API_KEY=your_mistral_api_key Vibe automatically loads API keys from ~/.vibe/.env on startup. Environment variables take precedence over the .env file if both are set. Note: The .env file is specifically for API keys and other provider credentials. General Vibe configuration should be done in config.toml. Custom System Prompts You can create custom system prompts to replace the default one (prompts/cli.md). Create a markdown file in the ~/.vibe/prompts/ directory with your custom prompt content. To use a custom system prompt, set the system_prompt_id in your configuration to match the filename (without the .md extension): # Use a custom system prompt
system_prompt_id = "my_custom_prompt" This will load the prompt from ~/.vibe/prompts/my_custom_prompt.md. Custom Agent Configurations You can create custom agent configurations for specific use cases (e.g., red-teaming, specialized tasks) by adding agent-specific TOML files in the ~/.vibe/agents/ directory. To use a custom agent, run Vibe with the --agent flag: vibe --agent my_custom_agent Vibe will look for a file named my_custom_agent.toml in the agents directory and apply its configuration. Example custom agent configuration (~/.vibe/agents/redteam.toml): # Custom agent configuration for red-teaming
active_model = "devstral-2"
system_prompt_id = "redteam" # Disable some tools for this agent
disabled_tools = ["search_replace", "write_file"] # Override tool permissions for this agent
[tools.bash]
permission = "always" [tools.read_file]
permission = "always" Note: This implies that you have set up a redteam prompt named ~/.vibe/prompts/redteam.md. Tool Management Enable/Disable Tools with Patterns You can control which tools are active using enabled_tools and disabled_tools. These fields support exact names, glob patterns, and regular expressions. Examples: # Only enable tools that start with "serena_" (glob)
enabled_tools = ["serena_*"] # Regex (prefix with re:) — matches full tool name (case-insensitive)
enabled_tools = ["re:^serena_.*$"] # Disable a group with glob; everything else stays enabled
disabled_tools = ["mcp_*", "grep"] Notes: MCP tool names use underscores, e.g., serena_list not serena.list. Regex patterns are matched against the full tool name using fullmatch. MCP Server Configuration You can configure MCP (Model Context Protocol) servers to extend Vibe's capabilities. Add MCP server configurations under the mcp_servers section: # Example MCP server configurations
[[mcp_servers]]
name = "my_http_server"
transport = "http"
url = "http://localhost:8000"
headers = { "Authorization" = "Bearer my_token" }
api_key_env = "MY_API_KEY_ENV_VAR"
api_key_header = "Authorization"
api_key_format = "Bearer {token}" [[mcp_servers]]
name = "my_streamable_server"
transport = "streamable-http"
url = "http://localhost:8001"
headers = { "X-API-Key" = "my_api_key" } [[mcp_servers]]
name = "fetch_server"
transport = "stdio"
command = "uvx"
args = ["mcp-server-fetch"]
env = { "DEBUG" = "1", "LOG_LEVEL" = "info" } Supported transports: http: Standard HTTP transport streamable-http: HTTP transport with streaming support stdio: Standard input/output transport (for local processes) Key fields: name: A short alias for the server (used in tool names) transport: The transport type url: Base URL for HTTP transports headers: Additional HTTP headers api_key_env: Environment variable containing the API key command: Command to run for stdio transport args: Additional arguments for stdio transport startup_timeout_sec: Timeout in seconds for the server to start and initialize (default 10s) tool_timeout_sec: Timeout in seconds for tool execution (default 60s) env: Environment variables to set for the MCP server of transport type stdio MCP tools are named using the pattern {server_name}_{tool_name} and can be configured with permissions like built-in tools: # Configure permissions for specific MCP tools
[tools.fetch_server_get]
permission = "always" [tools.my_http_server_query]
permission = "ask" MCP server configurations support additional features: Environment variables: Set environment variables for MCP servers Custom timeouts: Configure startup and tool execution timeouts Example with environment variables and timeouts: [[mcp_servers]]
name = "my_server"
transport = "http"
url = "http://localhost:8000"
env = { "DEBUG" = "1", "LOG_LEVEL" = "info" }
startup_timeout_sec = 15
tool_timeout_sec = 120 Session Management Session Continuation and Resumption Vibe supports continuing from previous sessions: --continue or -c: Continue from the most recent saved session --resume SESSION_ID: Resume a specific session by ID (supports partial matching) # Continue from last session
vibe --continue # Resume specific session
vibe --resume abc123 Session logging must be enabled in your configuration for these features to work. Working Directory Control Use the --workdir option to specify a working directory: vibe --workdir /path/to/project This is useful when you want to run Vibe from a different location than your current directory. Update Settings Auto-Update Vibe includes an automatic update feature that keeps your installation current. This is enabled by default. To disable auto-updates, add this to your config.toml: enable_auto_update = false Custom Vibe Home Directory By default, Vibe stores its configuration in ~/.vibe/. You can override this by setting the VIBE_HOME environment variable: export VIBE_HOME="/path/to/custom/vibe/home" This affects where Vibe looks for: config.toml - Main configuration .env - API keys agents/ - Custom agent configurations prompts/ - Custom system prompts tools/ - Custom tools logs/ - Session logs Editors/IDEs Mistral Vibe can be used in text editors and IDEs that support Agent Client Protocol. See the ACP Setup documentation for setup instructions for various editors and IDEs. Resources CHANGELOG - See what's new in each version CONTRIBUTING - Guidelines for feature requests, feedback and bug reports Data collection &amp; usage Use of Vibe is subject to our Privacy Policy and may include the collection and processing of data related to your use of the service, such as usage data, to operate, maintain, and improve Vibe. You can disable telemetry in your config.toml by setting enable_telemetry = false. License Copyright 2025 Mistral AI Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the LICENSE file for the full license text.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/mistralai/mistral-vibe</guid>
    </item>
    <item>
      <title><![CDATA[pytorch/executorch]]></title>
      <link>https://github.com/pytorch/executorch</link>
      <description><![CDATA[On-device AI across mobile, embedded and edge for PyTorch ExecuTorch On-device AI inference powered by PyTorch ExecuTorch is PyTorch's unified solution for deploying AI models on-device—from smartphones to microcontrollers—built for privacy, performance, and portability. It powers Meta's on-device AI across Instagram, WhatsApp, Quest 3, Ray-Ban Meta Smart Glasses, and more. Deploy LLMs, vision, speech, and multimodal models with the same PyTorch APIs you already know—accelerating research to production with seamless model export, optimization, and deployment. No manual C++ rewrites. No format conversions. No vendor lock-in. Table of Contents Why ExecuTorch? How It Works Quick Start Installation Export and Deploy in 3 Steps Run on Device LLM Example: Llama Platform &amp; Hardware Support Production Deployments Examples &amp; Models Key Features Documentation Community &amp; Contributing License Why ExecuTorch? Native PyTorch Export — Direct export from PyTorch. No .onnx, .tflite, or intermediate format conversions. Preserve model semantics. Production-Proven — Powers billions of users at Meta with real-time on-device inference. Tiny Runtime — 50KB base footprint. Runs on microcontrollers to high-end smartphones. 12+ Hardware Backends — Open-source acceleration for Apple, Qualcomm, ARM, MediaTek, Vulkan, and more. One Export, Multiple Backends — Switch hardware targets with a single line change. Deploy the same model everywhere. How It Works ExecuTorch uses ahead-of-time (AOT) compilation to prepare PyTorch models for edge deployment: Export — Capture your PyTorch model graph with torch.export() Compile — Quantize, optimize, and partition to hardware backends → .pte Execute — Load .pte on-device via lightweight C++ runtime Models use a standardized Core ATen operator set. Partitioners delegate subgraphs to specialized hardware (NPU/GPU) with CPU fallback. Learn more: How ExecuTorch Works • Architecture Guide Quick Start Installation pip install executorch For platform-specific setup (Android, iOS, embedded systems), see the Quick Start documentation for additional info. Export and Deploy in 3 Steps import torch
from executorch.exir import to_edge_transform_and_lower
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner # 1. Export your PyTorch model
model = MyModel().eval()
example_inputs = (torch.randn(1, 3, 224, 224),)
exported_program = torch.export.export(model, example_inputs) # 2. Optimize for target hardware (switch backends with one line)
program = to_edge_transform_and_lower( exported_program, partitioner=[XnnpackPartitioner()] # CPU | CoreMLPartitioner() for iOS | QnnPartitioner() for Qualcomm
).to_executorch() # 3. Save for deployment
with open("model.pte", "wb") as f: f.write(program.buffer) # Test locally via ExecuTorch runtime's pybind API (optional)
from executorch.runtime import Runtime
runtime = Runtime.get()
method = runtime.load_program("model.pte").load_method("forward")
outputs = method.execute([torch.randn(1, 3, 224, 224)]) Run on Device C++ #include #include Module module("model.pte");
auto tensor = make_tensor_ptr({2, 2}, {1.0f, 2.0f, 3.0f, 4.0f});
auto outputs = module.forward(tensor); Swift (iOS) import ExecuTorch let module = Module(filePath: "model.pte")
let input = Tensor([1.0, 2.0, 3.0, 4.0], shape: [2, 2])
let outputs = try module.forward(input) Kotlin (Android) val module = Module.load("model.pte")
val inputTensor = Tensor.fromBlob(floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f), longArrayOf(2, 2))
val outputs = module.forward(EValue.from(inputTensor)) LLM Example: Llama Export Llama models using the export_llm script or Optimum-ExecuTorch: # Using export_llm
python -m executorch.extension.llm.export.export_llm --model llama3_2 --output llama.pte # Using Optimum-ExecuTorch
optimum-cli export executorch \ --model meta-llama/Llama-3.2-1B \ --task text-generation \ --recipe xnnpack \ --output_dir llama_model Run on-device with the LLM runner API: C++ #include auto runner = create_llama_runner("llama.pte", "tiktoken.bin");
executorch::extension::llm::GenerationConfig config{ .seq_len = 128, .temperature = 0.8f};
runner-&gt;generate("Hello, how are you?", config); Swift (iOS) import ExecuTorchLLM let runner = TextRunner(modelPath: "llama.pte", tokenizerPath: "tiktoken.bin")
try runner.generate("Hello, how are you?", Config { $0.sequenceLength = 128
}) { token in print(token, terminator: "")
} Kotlin (Android) — API Docs • Demo App val llmModule = LlmModule("llama.pte", "tiktoken.bin", 0.8f)
llmModule.load()
llmModule.generate("Hello, how are you?", 128, object : LlmCallback { override fun onResult(result: String) { print(result) } override fun onStats(stats: String) { }
}) For multimodal models (vision, audio), use the MultiModal runner API which extends the LLM runner to handle image and audio inputs alongside text. See Llava and Voxtral examples. See examples/models/llama for complete workflow including quantization, mobile deployment, and advanced options. Next Steps: Step-by-step tutorial — Complete walkthrough for your first model Colab notebook — Try ExecuTorch instantly in your browser Deploy Llama models — LLM workflow with quantization and mobile demos Platform &amp; Hardware Support Platform Supported Backends Android XNNPACK, Vulkan, Qualcomm, MediaTek, Samsung Exynos iOS XNNPACK, MPS, CoreML (Neural Engine) Linux / Windows XNNPACK, OpenVINO, CUDA (experimental) macOS XNNPACK, MPS, Metal (experimental) Embedded / MCU XNNPACK, ARM Ethos-U, NXP, Cadence DSP See Backend Documentation for detailed hardware requirements and optimization guides. For desktop/laptop GPU inference with CUDA and Metal, see the Desktop Guide. For Zephyr RTOS integration, see the Zephyr Guide. Production Deployments ExecuTorch powers on-device AI at scale across Meta's family of apps, VR/AR devices, and partner deployments. View success stories → Examples &amp; Models LLMs: Llama 3.2/3.1/3, Qwen 3, Phi-4-mini, LiquidAI LFM2 Multimodal: Llava (vision-language), Voxtral (audio-language), Gemma (vision-language) Vision/Speech: MobileNetV2, DeepLabV3, Whisper Resources: examples/ directory • executorch-examples out-of-tree demos • Optimum-ExecuTorch for HuggingFace models • Unsloth for fine-tuned LLM deployment Key Features ExecuTorch provides advanced capabilities for production deployment: Quantization — Built-in support via torchao for 8-bit, 4-bit, and dynamic quantization Memory Planning — Optimize memory usage with ahead-of-time allocation strategies Developer Tools — ETDump profiler, ETRecord inspector, and model debugger Selective Build — Strip unused operators to minimize binary size Custom Operators — Extend with domain-specific kernels Dynamic Shapes — Support variable input sizes with bounded ranges See Advanced Topics for quantization techniques, custom backends, and compiler passes. Documentation Documentation Home — Complete guides and tutorials API Reference — Python, C++, Java/Kotlin APIs Backend Integration — Build custom hardware backends Troubleshooting — Common issues and solutions Community &amp; Contributing We welcome contributions from the community! GitHub Discussions — Ask questions and share ideas Discord — Chat with the team and community Issues — Report bugs or request features Contributing Guide — Guidelines and codebase structure License ExecuTorch is BSD licensed, as found in the LICENSE file. Part of the PyTorch ecosystem GitHub • Documentation]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:24 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/pytorch/executorch</guid>
    </item>
    <item>
      <title><![CDATA[Supercharging Flutter Development with 80+ Powerful String Extensions]]></title>
      <link>https://dev.to/tharanitharan305/supercharging-flutter-development-with-80-powerful-string-extensions-4120</link>
      <description><![CDATA[If you’ve built more than one Flutter app, you’ve probably rewritten the same helpers again and again:
Email validation regex
Slug generators
Hashing utilities
Word count logic
Text formatting helpers
SnackBar boilerplate
Repetitive TextStyle setup
At some point, it becomes clear:
We don’t lack features — we lack reusable tools.
That’s why I built super_string_utils — a production-grade Flutter utility engine delivering 80+ powerful String extensions and Fluent UI builders. Package Live Demo
The Problem
Dart’s String class is powerful — but modern applications demand more.
We constantly rewrite:
Validation logic
Hashing and encoding
Extraction utilities
Word analysis
Layout generation from lists
UI boilerplate for Text &amp; SnackBars
These are not complex problems — but they are repetitive.
And repetition slows development.
The Solution: super_string_utils
super_string_utils eliminates boilerplate by extending String and List directly — and even introducing Fluent UI Builders. What You Get
Validation:
"user@example.com".isEmail; // true
"192.168.1.1".isIpv4; // true
"StrongP@ss1".isStrongPassword; // true Transformation:
"hello world".toTitleCase; // "Hello World" Extraction:
"Visit https://pub.dev".extractUrls; Security:
"password".md5Hash;
"data".toBase64;
"secret@mail.com".maskEmail; Fluent UI Builders (No More Boilerplate)
This is where it gets interesting.
Instead of writing:
Text( "Headline", style: TextStyle( fontSize: 24, fontWeight: FontWeight.bold, ),
) You can write:
"Headline".toTextBuilder .size(24) .weight(FontWeight.bold) .center() .build(); SnackBar:
"Saved Successfully".toSnackbarBuilder .floating() .color(Colors.green) .withCloseIcon() .build(); Advanced Layout Engine
Generate structured UI directly from List:
["Header", "Body", "Footer"] .toTextColumn() .spacing(12) .containerAt(0, color: Colors.blue) .expandAt(1) .align(TextAlign.right) .build(); Per-index control allows you to:
Expand specific items
Wrap items in containers
Apply padding selectively
Customize layout dynamically
Perfect for dashboards, menus, and dynamic screens. Cross-Platform Ready
Works on:
Android
iOS
Web
Windows
macOS
Linux
Fully null-safe. Optimized. Dependency-minimal. Installation
Add to your pubspec:
dependencies: super_string_utils: ^1.1.0 Then:
flutter pub get
Why This Matters
Developer productivity matters.
Every minute spent rewriting helpers is a minute not spent building features.
super_string_utils is designed to:
Reduce boilerplate
Improve readability
Encourage fluent, expressive code
Standardize common operations
Speed up development cycles Try It Yourself Pub.dev:
https://pub.dev/packages/super_string_utils Live Demo App:
https://super-string-utils-example.vercel.app/ GitHub Repository:
https://github.com/tharanitharan305/super_string_utils
Community &amp; Contributions
This project is open source and welcomes contributions.
Have ideas for:
More string utilities?
Additional builders?
Performance improvements?
Open a PR or issue.
Let’s build better Flutter tooling together]]></description>
      <pubDate>Thu, 19 Feb 2026 06:42:12 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/tharanitharan305/supercharging-flutter-development-with-80-powerful-string-extensions-4120</guid>
    </item>
    <item>
      <title><![CDATA[microsoft/agent-framework]]></title>
      <link>https://github.com/microsoft/agent-framework</link>
      <description><![CDATA[A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET. Welcome to Microsoft Agent Framework! Welcome to Microsoft's comprehensive multi-language framework for building, orchestrating, and deploying AI agents with support for both .NET and Python implementations. This framework provides everything from simple chat agents to complex multi-agent workflows with graph-based orchestration. Watch the full Agent Framework introduction (30 min) Getting Started Installation Python pip install agent-framework --pre
# This will install all sub-packages, see `python/packages` for individual packages.
# It may take a minute on first install on Windows. .NET dotnet add package Microsoft.Agents.AI Documentation Overview - High level overview of the framework Quick Start - Get started with a simple agent Tutorials - Step by step tutorials User Guide - In-depth user guide for building agents and workflows Migration from Semantic Kernel - Guide to migrate from Semantic Kernel Migration from AutoGen - Guide to migrate from AutoGen Still have questions? Join our weekly office hours or ask questions in our Discord channel to get help from the team and other users. Highlights Graph-based Workflows: Connect agents and deterministic functions using data flows with streaming, checkpointing, human-in-the-loop, and time-travel capabilities Python workflows | .NET workflows AF Labs: Experimental packages for cutting-edge features including benchmarking, reinforcement learning, and research initiatives Labs directory DevUI: Interactive developer UI for agent development, testing, and debugging workflows DevUI package See the DevUI in action (1 min) Python and C#/.NET Support: Full framework support for both Python and C#/.NET implementations with consistent APIs Python packages | .NET source Observability: Built-in OpenTelemetry integration for distributed tracing, monitoring, and debugging Python observability | .NET telemetry Multiple Agent Provider Support: Support for various LLM providers with more being added continuously Python examples | .NET examples Middleware: Flexible middleware system for request/response processing, exception handling, and custom pipelines Python middleware | .NET middleware We want your feedback! For bugs, please file a GitHub issue. Quickstart Basic Agent - Python Create a simple Azure Responses Agent that writes a haiku about the Microsoft Agent Framework # pip install agent-framework --pre
# Use `az login` to authenticate with Azure CLI
import os
import asyncio
from agent_framework.azure import AzureOpenAIResponsesClient
from azure.identity import AzureCliCredential async def main(): # Initialize a chat agent with Azure OpenAI Responses # the endpoint, deployment name, and api version can be set via environment variables # or they can be passed in directly to the AzureOpenAIResponsesClient constructor agent = AzureOpenAIResponsesClient( # endpoint=os.environ["AZURE_OPENAI_ENDPOINT"], # deployment_name=os.environ["AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME"], # api_version=os.environ["AZURE_OPENAI_API_VERSION"], # api_key=os.environ["AZURE_OPENAI_API_KEY"], # Optional if using AzureCliCredential credential=AzureCliCredential(), # Optional, if using api_key ).as_agent( name="HaikuBot", instructions="You are an upbeat assistant that writes beautifully.", ) print(await agent.run("Write a haiku about Microsoft Agent Framework.")) if __name__ == "__main__": asyncio.run(main()) Basic Agent - .NET Create a simple Agent, using OpenAI Responses, that writes a haiku about the Microsoft Agent Framework // dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
using System;
using OpenAI; // Replace the with your OpenAI API key.
var agent = new OpenAIClient("") .GetOpenAIResponseClient("gpt-4o-mini") .AsAIAgent(name: "HaikuBot", instructions: "You are an upbeat assistant that writes beautifully."); Console.WriteLine(await agent.RunAsync("Write a haiku about Microsoft Agent Framework.")); Create a simple Agent, using Azure OpenAI Responses with token based auth, that writes a haiku about the Microsoft Agent Framework // dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
// dotnet add package Azure.Identity
// Use `az login` to authenticate with Azure CLI
using System;
using OpenAI; // Replace and gpt-4o-mini with your Azure OpenAI resource name and deployment name.
var agent = new OpenAIClient( new BearerTokenPolicy(new AzureCliCredential(), "https://ai.azure.com/.default"), new OpenAIClientOptions() { Endpoint = new Uri("https://.openai.azure.com/openai/v1") }) .GetOpenAIResponseClient("gpt-4o-mini") .AsAIAgent(name: "HaikuBot", instructions: "You are an upbeat assistant that writes beautifully."); Console.WriteLine(await agent.RunAsync("Write a haiku about Microsoft Agent Framework.")); More Examples &amp; Samples Python Getting Started with Agents: progressive tutorial from hello-world to hosting Agent Concepts: deep-dive samples by topic (tools, middleware, providers, etc.) Getting Started with Workflows: workflow creation and integration with agents .NET Getting Started with Agents: basic agent creation and tool usage Agent Provider Samples: samples showing different agent providers Workflow Samples: advanced multi-agent patterns and workflow orchestration Contributor Resources Contributing Guide Python Development Guide Design Documents Architectural Decision Records Important Notes If you use the Microsoft Agent Framework to build applications that operate with third-party servers or agents, you do so at your own risk. We recommend reviewing all data being shared with third-party servers or agents and being cognizant of third-party practices for retention and location of data. It is your responsibility to manage whether your data will flow outside of your organization's Azure compliance and geographic boundaries and any related implications.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/microsoft/agent-framework</guid>
    </item>
    <item>
      <title><![CDATA[NVIDIA-NeMo/Automodel]]></title>
      <link>https://github.com/NVIDIA-NeMo/Automodel</link>
      <description><![CDATA[Pytorch Distributed native training library for LLMs/VLMs with OOTB Hugging Face support NeMo AutoModel Documentation • Ready-to-Use Recipes • Examples • Model Coverage • Performance • Contributing News and Discussions [02/16/2026]Qwen3.5 We support finetuning for Qwen/Qwen3.5-397B-A17B. Checkout our recipe [02/13/2026]MiniMax-M2.5 We support finetuning for MiniMaxAI/MiniMax-M2.5. Checkout our recipe [02/11/2026]GLM-4.7-Flash We now support finetuning GLM-4.7-Flash. Checkout our packed sequence recipe [02/09/2026]MiniMax-M2 We support finetuning for MiniMaxAI/MiniMax-M2. Checkout our recipe [02/06/2026]Qwen3 VL 235B We support finetuning for Qwen/Qwen3-VL-235B-A22B-Instruct. Checkout our recipe [02/06/2026]GLM4.7 We now support finetuning GLM4.7. Checkout our recipe [02/06/2026]Step3.5-flash is out! Finetune it with our finetune recipe [02/05/2026]DeepSeek-V3.2 is out! Checkout out the finetune recipe! [02/04/2026]Kimi K2.5 VL is out! Finetune it with NeMo AutoModel [12/18/2025]FunctionGemma is out! Finetune it with NeMo AutoModel! [12/15/2025]NVIDIA-Nemotron-3-Nano-30B-A3B is out! Finetune it with NeMo AutoModel! [11/6/2025]Accelerating Large-Scale Mixture-of-Experts Training in PyTorch [10/6/2025]Enabling PyTorch Native Pipeline Parallelism for Hugging Face Transformer Models [9/22/2025]Fine-tune Hugging Face Models Instantly with Day-0 Support with NVIDIA NeMo AutoModel [9/18/2025] NeMo Framework Now Supports Google Gemma 3n: Efficient Multimodal Fine-tuning Made Simple Overview Nemo AutoModel is a Pytorch DTensor‑native SPMD open-source training library under NVIDIA NeMo Framework, designed to streamline and scale training and finetuning for LLMs and VLMs. Designed for flexibility, reproducibility, and scale, NeMo AutoModel enables both small-scale experiments and massive multi-GPU, multi-node deployments for fast experimentation in research and production environments. What you can expect: Hackable with a modular design that allows easy integration, customization and quick research prototypes. Minimal ceremony: YAML‑driven recipes; override any field via CLI. High performance and flexibility with custom kernels and DTensor support. Seamless integration with Hugging Face for day-0 model support, ease of use, and wide range of supported models. Efficient resource management using k8s and Slurm, enabling scalable and flexible deployment across configurations. Comprehensive documentation that is both detailed and user-friendly, with practical examples. Note: NeMo AutoModel is under active development. New features, improvements, and documentation updates are released regularly. We are working toward a stable release, so expect the interface to solidify over time. Your feedback and contributions are welcome, and we encourage you to follow along as new updates roll out. Why PyTorch Distributed and SPMD One program, any scale: The same training script runs on 1 GPU or 1000+ by changing the mesh. PyTorch Distributed native: Partition model/optimizer states with DeviceMesh + placements (Shard, Replicate). SPMD first: Parallelism is configuration. No model rewrites when scaling up or changing strategy. Decoupled concerns: Model code stays pure PyTorch; parallel strategy lives in config. Composability: Mix tensor, sequence, and data parallel by editing placements. Portability: Fewer bespoke abstractions; easier to reason about failure modes and restarts. Table of Contents Feature Roadmap Getting Started LLM Pre-training Supervised Fine-Tuning (SFT) Parameter-Efficient Fine-Tuning (PEFT) VLM Supervised Fine-Tuning (SFT) Parameter-Efficient Fine-Tuning (PEFT) Supported Models Performance Interoperability Contributing License TL;DR: SPMD turns “how to parallelize” into a runtime layout choice, not a code fork. Feature Roadmap Available now | Coming in 26.02 Advanced Parallelism - PyTorch native FSDP2, TP, CP, and SP for distributed training. HSDP - Multi-node Hybrid Sharding Data Parallelism based on FSDP2. Pipeline Support - Torch-native support for pipelining composable with FSDP2 and DTensor (3D Parallelism). Environment Support - Support for SLURM and interactive training. Learning Algorithms - SFT (Supervised Fine-Tuning), and PEFT (Parameter Efficient Fine-Tuning). Pre-training - Support for model pre-training, including DeepSeekV3. Knowledge Distillation - Support for knowledge distillation with LLMs; VLM support will be added post 25.09. HuggingFace Integration - Works with dense models (e.g., Qwen, Llama3, etc) and large MoEs (e.g., DSv3). Sequence Packing - Sequence packing for huge training perf gains. FP8 and mixed precision - FP8 support with torchao, requires torch.compile-supported models. DCP - Distributed Checkpoint support with SafeTensors output. VLM: Support for finetuning VLMs (e.g., Qwen2-VL, Gemma-3-VL). More families to be included in the future. Extended MoE support - GPT-OSS, Qwen3 (Coder-480B-A35B, etc), Qwen-next. Transformers v5 - Support for transformers v5 with device-mesh driven parallelism. Muon &amp; Dion - Support for Muon and Dion optimizers. SonicMoE - Optimized MoE implementation for faster expert computation. FP8 MoE - FP8 precision training and inference for MoE models. Cudagraph with MoE - CUDA graph support for MoE layers to reduce kernel launch overhead. Extended VLM Support - DeepSeek OCR, Qwen3 VL 235B, Kimi-VL, GLM4.5V Extended LLM Support - QWENCoder 480B Instruct, MiniMax2.1, and more Kubernetes - Multi-node job launch with k8s. Getting Started We recommend using uv for reproducible Python environments. # Setup environment before running any commands
uv venv
uv sync --frozen --all-extras uv pip install nemo_automodel # latest release
# or: uv pip install git+https://github.com/NVIDIA-NeMo/Automodel.git
uv run python -c "import nemo_automodel; print('AutoModel ready')" Run a Recipe To run a NeMo AutoModel recipe, you need a recipe script (e.g., LLM, VLM) and a YAML config file (e.g., LLM, VLM): # Command invocation format:
uv run --config # LLM example: multi-GPU with FSDP2
uv run torchrun --nproc-per-node=8 examples/llm_finetune/finetune.py --config examples/llm_finetune/llama3_2/llama3_2_1b_hellaswag.yaml # VLM example: single GPU fine-tuning (Gemma-3-VL) with LoRA
uv run examples/vlm_finetune/finetune.py --config examples/vlm_finetune/gemma3/gemma3_vl_4b_cord_v2_peft.yaml LLM Pre-training LLM Pre-training Single Node We provide an example SFT experiment using the Fineweb dataset with a nano-GPT model, ideal for quick experimentation on a single node. uv run torchrun --nproc-per-node=8 \ examples/llm_pretrain/pretrain.py \ -c examples/llm_pretrain/nanogpt_pretrain.yaml LLM Supervised Fine-Tuning (SFT) We provide an example SFT experiment using the SQuAD dataset. LLM SFT Single Node The default SFT configuration is set to run on a single GPU. To start the experiment: uv run python3 \ examples/llm_finetune/finetune.py \ -c examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml This fine-tunes the Llama3.2-1B model on the SQuAD dataset using a 1 GPU. To use multiple GPUs on a single node in an interactive environment, you can run the same command using torchrun and adjust the --proc-per-node argument to the number of needed GPUs. uv run torchrun --nproc-per-node=8 \ examples/llm_finetune/finetune.py \ -c examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml Alternatively, you can use the automodel CLI application to launch the same job, for example: uv run automodel finetune llm \ --nproc-per-node=8 \ -c examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml LLM SFT Multi Node You can use the automodel CLI application to launch a job on a SLURM cluster, for example: # First you need to specify the SLURM section in your YAML config, for example: cat &lt;&lt; EOF &gt; examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml
slurm: job_name: llm-finetune # set to the job name you want to use nodes: 2 # set to the needed number of nodes ntasks_per_node: 8 time: 00:30:00 account: your_account partition: gpu container_image: nvcr.io/nvidia/nemo:25.07 gpus_per_node: 8 # This adds "#SBATCH --gpus-per-node=8" to the script # Optional: Add extra mount points if needed extra_mounts: - /lustre:/lustre # Optional: Specify custom HF_HOME location (will auto-create if not specified) hf_home: /path/to/your/HF_HOME # Optional : Specify custom env vars # env_vars: # ENV_VAR: value # Optional: Specify custom job directory (defaults to cwd/slurm_jobs) # job_dir: /path/to/slurm/jobs
EOF # using the updated YAML you can launch the job.
uv run automodel finetune llm \ -c examples/llm_finetune/llama3_2/llama3_2_1b_squad.yaml LLM Parameter-Efficient Fine-Tuning (PEFT) We provide a PEFT example using the HellaSwag dataset. LLM PEFT Single Node # Memory‑efficient SFT with LoRA
uv run examples/llm_finetune/finetune.py \
--config examples/llm_finetune/llama3_2/llama3_2_1b_hellaswag_peft.yaml # You can always overwrite parameters by appending them to the command, for example,
# if you want to increase the micro-batch size you can do
uv run examples/llm_finetune/finetune.py \ --config examples/llm_finetune/llama3_2/llama3_2_1b_hellaswag_peft.yaml \ --step_scheduler.local_batch_size 16 # The above command will modify the `local_batch_size` variable to have value 16 in the
# section `step_scheduler` of the yaml file. [!NOTE] Launching a multi-node PEFT example requires only adding a slurm section to your config, similarly to the SFT case. VLM Supervised Fine-Tuning (SFT) We provide a VLM SFT example using Qwen2.5‑VL for end‑to‑end fine‑tuning on image‑text data. VLM SFT Single Node # Qwen2.5‑VL on a 8 GPUs
uv run torchrun --nproc-per-node=8 \ examples/vlm_finetune/finetune.py \ --config examples/vlm_finetune/qwen2_5/qwen2_5_vl_3b_rdr.yaml VLM Parameter-Efficient Fine-Tuning (PEFT) We provide a VLM PEFT (LoRA) example for memory‑efficient adaptation with Gemma3 VLM. VLM PEFT Single Node # Qwen2.5‑VL on a 8 GPUs
uv run torchrun --nproc-per-node=8 \ examples/vlm_finetune/finetune.py \ --config examples/vlm_finetune/gemma3/gemma3_vl_4b_medpix_peft.yaml Supported Models NeMo AutoModel provides native support for a wide range of models available on the Hugging Face Hub, enabling efficient fine-tuning for various domains. Below is a small sample of ready‑to‑use families (train as‑is or swap any compatible causal LM), you can specify nearly any LLM/VLM model available on hub: Domain Model Family Model ID Recipes LLM GPT-OSS GPT-OSS-20B SFT GPT-OSS-120B SFT LLM DeepSeek DeepSeek-V3 Pretrain LLM Moonlight Moonlight-16B-TE Pretrain, SFT LLM LLaMA meta-llama/Llama-3.2-1B SFT, PEFT meta-llama/Llama-3.2-3B-Instruct SFT, PEFT meta-llama/Llama-3.1-8B FP8 meta-llama/Llama-3.3-70B-Instruct SFT, PEFT LLM Mistral mistralai/Mistral-7B-v0.1 SFT, PEFT, FP8 mistralai/Mistral-Nemo-Base-2407 SFT, PEFT, FP8 mistralai/Mixtral-8x7B-Instruct-v0.1 SFT, PEFT LLM Qwen Qwen/Qwen2.5-7B SFT, PEFT, FP8 Qwen/Qwen3-0.6B SFT, PEFT Qwen/QwQ-32B SFT, PEFT LLM Gemma google/gemma-3-270m SFT, PEFT google/gemma-2-9b-it SFT, PEFT, FP8 google/gemma-7b SFT, PEFT LLM Phi microsoft/phi-2 SFT, PEFT microsoft/Phi-3-mini-4k-instruct SFT, PEFT microsoft/phi-4 SFT, PEFT, FP8 LLM Seed ByteDance-Seed/Seed-Coder-8B-Instruct SFT, PEFT, FP8 ByteDance-Seed/Seed-OSS-36B-Instruct SFT, PEFT LLM Baichuan baichuan-inc/Baichuan2-7B-Chat SFT, PEFT, FP8 VLM Gemma google/gemma-3-4b-it SFT, PEFT google/gemma-3n-e4b-it SFT, PEFT [!NOTE] Check out more LLM and VLM examples. Any causal LM on Hugging Face Hub can be used with the base recipe template, just overwrite --model.pretrained_model_name_or_path in the CLI or in the YAML config. Performance NeMo AutoModel achieves great training performance on NVIDIA GPUs. Below are highlights from our benchmark results: Model #GPUs Seq Length Model TFLOPs/sec/GPU Tokens/sec/GPU Kernel Optimizations DeepSeek V3 671B 256 4096 250 1,002 TE + DeepEP GPT-OSS 20B 8 4096 279 13,058 TE + DeepEP + FlexAttn Qwen3 MoE 30B 8 4096 212 11,842 TE + DeepEP For complete benchmark results including configuration details, see the Performance Summary. Interoperability NeMo RL: Use AutoModel checkpoints directly as starting points for DPO/RM/GRPO pipelines. Hugging Face: Train any LLM/VLM from without format conversion. Megatron Bridge: Optional conversions to/from Megatron formats for specific workflows. Project Structure NeMo-Automodel/
├── examples
│ ├── llm_finetune # LLM finetune recipes
│ ├── llm_kd # LLM knowledge-distillation recipes
│ ├── llm_pretrain # LLM pretrain recipes
│ ├── vlm_finetune # VLM finetune recipes
│ └── vlm_generate # VLM generate recipes
├── nemo_automodel
│ ├── _cli
│ │ └── app.py # the `automodel` CLI job launcher
│ ├── components # Core library
│ │ ├── _peft # PEFT implementations (LoRA)
│ │ ├── _transformers # HF model integrations
│ │ ├── checkpoint # Distributed checkpointing
│ │ ├── config
│ │ ├── datasets # LLM (HellaSwag, etc.) &amp; VLM datasets
│ │ ├── distributed # FSDP2, Megatron FSDP, Pipelining, etc.
│ │ ├── launcher # The job launcher component (SLURM)
│ │ ├── loggers # loggers
│ │ ├── loss # Optimized loss functions
│ │ ├── models # User-defined model examples
│ │ ├── moe # Optimized kernels for MoE models
│ │ ├── optim # Optimizer/LR scheduler components
│ │ ├── quantization # FP8
│ │ ├── training # Train utils
│ │ └── utils # Misc utils
│ ├── recipes
│ │ ├── llm # Main LLM train loop
│ │ └── vlm # Main VLM train loop
│ └── shared
└── tests/ # Comprehensive test suite Citation If you use NeMo AutoModel in your research, please cite it using the following BibTeX entry: @misc{nemo-automodel,
title = {NeMo AutoModel: DTensor‑native SPMD library for scalable and efficient training},
howpublished = {\url{https://github.com/NVIDIA-NeMo/Automodel}},
year = {2025},
note = {GitHub repository},
} Contributing We welcome contributions! Please see our Contributing Guide for details. License NVIDIA NeMo AutoModel is licensed under the Apache License 2.0.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/NVIDIA-NeMo/Automodel</guid>
    </item>
    <item>
      <title><![CDATA[SemiAnalysisAI/InferenceX]]></title>
      <link>https://github.com/SemiAnalysisAI/InferenceX</link>
      <description><![CDATA[Open Source Continuous Inference Benchmarking Qwen3.5, DeepSeek, GPTOSS - GB200 NVL72 vs MI355X vs B200 vs GB300 NVL72 vs H100 &amp; soon TPUv6e/v7/Trainium2/3 InferenceX, Open Source Inference Frequent Benchmarking InferenceX (formerly InferenceMAX) runs our suite of benchmarks every night, continually re-benchmarking the world’s most popular open-source inference frameworks used by major token factories and models to track real performance in real time. As these software stacks improve, InferenceX captures that progress in near real-time, providing a live indicator of inference performance progress. A live dashboard is available for free publicly at https://inferencex.com/. [!IMPORTANT] Only SemiAnalysisAI/InferenceX repo contains the Official InferenceX result, all other forks &amp; repos are Unofficial. The benchmark setup &amp; quality of machines/clouds in unofficial repos may be differ leading to subpar benchmarking. Unofficial must be explicitly labelled as Unofficial. Forks may not remove this disclaimer Full Article Write Up for InferenceXv2 Full Article Write Up for InferenceXv1 Why? InferenceX, an open-source, under Apache2 license, automated benchmark designed to move at the same rapid speed as the software ecosystem itself, is built to address this challenge. LLM Inference performance is driven by two pillars, hardware and software. While hardware innovation drives step jumps in performance every year through the release of new GPUs/XPUs and new systems, software evolves every single day, delivering continuous performance gains on top of these step jumps. Speed is the Moat AI software like SGLang, vLLM, TensorRT-LLM, CUDA, ROCm and achieve this continuous improvement in performance through kernel-level optimizations, distributed inference strategies, and scheduling innovations that increase the pareto frontier of performance in incremental releases that can be just days apart. This pace of software advancement creates a challenge: benchmarks conducted at a fixed point in time quickly go stale and do not represent the performance that can be achieved with the latest software packages. Acknowledgements &amp; Supporters Thank you to Lisa Su and Anush Elangovan for providing the MI355X and CDNA3 GPUs for this free and open-source project. We want to recognize the many AMD contributors for their responsiveness and for debugging, optimizing, and validating performance across AMD GPUs. We’re also grateful to Jensen Huang and Ian Buck for supporting this open source with access to a GB200 NVL72 rack (through OCI) and B200 GPUs. Thank you to the many NVIDIA contributors from the NVIDIA inference team, NVIDIA Dynamo team. We also want to recognize the SGLang, vLLM, and TensorRT-LLM maintainers for building a world-class software stack and open sourcing it to the entire world. Finally, we’re grateful to Crusoe, CoreWeave, Nebius, TensorWave, Oracle and TogetherAI for supporting open-source innovation through compute resources, enabling this. "As we build systems at unprecedented scale, it's critical for the ML community to have open, transparent benchmarks that reflect how inference really performs across hardware and software. InferenceX's head-to-head benchmarks cut through the noise and provide a living picture of token throughput, performance per dollar, and tokens per Megawatt. This kind of open source effort strengthens the entire ecosystem and helps everyone, from researchers to operators of frontier datacenters, make smarter decisions." - Peter Hoeschele, VP of Infrastructure and Industrial Compute, OpenAI Stargate "The gap between theoretical peak and real-world inference throughput is often determined by systems software: inference engine, distributed strategies, and low-level kernels. InferenceX is valuable because it benchmarks the latest software showing how optimizations actually play out across various hardware. Open, reproducible results like these help the whole community move faster.” - Tri Dao, Chief Scientist of Together AI &amp; Inventor of Flash Attention “The industry needs many public, reproducible benchmarks of inference performance. We’re excited to collaborate with InferenceX from the vLLM team. More diverse workloads and scenarios that everyone can trust and reference will help the ecosystem move forward. Fair, transparent measurements drive progress across every layer of the stack, from model architectures to inference engines to hardware.” – Simon Mo, vLLM Project Co-Lead]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/SemiAnalysisAI/InferenceX</guid>
    </item>
    <item>
      <title><![CDATA[Cinnamon/kotaemon]]></title>
      <link>https://github.com/Cinnamon/kotaemon</link>
      <description><![CDATA[An open-source RAG-based tool for chatting with your documents. kotaemon An open-source clean &amp; customizable RAG UI for chatting with your documents. Built with both end users and developers in mind. Live Demo #1 | Live Demo #2 | Online Install | Colab Notebook (Local RAG) User Guide | Developer Guide | Feedback | Contact Introduction This project serves as a functional RAG UI for both end users who want to do QA on their documents and developers who want to build their own RAG pipeline. +----------------------------------------------------------------------------+
| End users: Those who use apps built with `kotaemon`. |
| (You use an app like the one in the demo above) |
| +----------------------------------------------------------------+ |
| | Developers: Those who built with `kotaemon`. | |
| | (You have `import kotaemon` somewhere in your project) | |
| | +----------------------------------------------------+ | |
| | | Contributors: Those who make `kotaemon` better. | | |
| | | (You make PR to this repo) | | |
| | +----------------------------------------------------+ | |
| +----------------------------------------------------------------+ |
+----------------------------------------------------------------------------+ For end users Clean &amp; Minimalistic UI: A user-friendly interface for RAG-based QA. Support for Various LLMs: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via ollama and llama-cpp-python). Easy Installation: Simple scripts to get you started quickly. For developers Framework for RAG Pipelines: Tools to build your own RAG-based document QA pipeline. Customizable UI: See your RAG pipeline in action with the provided UI, built with Gradio . Gradio Theme: If you use Gradio for development, check out our theme here: kotaemon-gradio-theme. Key Features Host your own document QA (RAG) web-UI: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others. Organize your LLM &amp; Embedding models: Support both local LLMs &amp; popular API providers (OpenAI, Azure, Ollama, Groq). Hybrid RAG pipeline: Sane default RAG pipeline with hybrid (full-text &amp; vector) retriever and re-ranking to ensure best retrieval quality. Multi-modal QA support: Perform Question Answering on multiple documents with figures and tables support. Support multi-modal document parsing (selectable options on UI). Advanced citations with document preview: By default the system will provide detailed citations to ensure the correctness of LLM answers. View your citations (incl. relevant score) directly in the in-browser PDF viewer with highlights. Warning when retrieval pipeline return low relevant articles. Support complex reasoning methods: Use question decomposition to answer your complex/multi-hop question. Support agent-based reasoning with ReAct, ReWOO and other agents. Configurable settings UI: You can adjust most important aspects of retrieval &amp; generation process on the UI (incl. prompts). Extensible: Being built on Gradio, you are free to customize or add any UI elements as you like. Also, we aim to support multiple strategies for document indexing &amp; retrieval. GraphRAG indexing pipeline is provided as an example. Installation If you are not a developer and just want to use the app, please check out our easy-to-follow User Guide. Download the .zip file from the latest release to get all the newest features and bug fixes. System requirements Python &gt;= 3.10 Docker: optional, if you install with Docker Unstructured if you want to process files other than .pdf, .html, .mhtml, and .xlsx documents. Installation steps differ depending on your operating system. Please visit the link and follow the specific instructions provided there. With Docker ( ) We support both lite &amp; full version of Docker images. With full version, the extra packages of unstructured will be installed, which can support additional file types (.doc, .docx, ...) but the cost is larger docker image size. For most users, the lite image should work well in most cases. To use the full version. docker run \
-e GRADIO_SERVER_NAME=0.0.0.0 \
-e GRADIO_SERVER_PORT=7860 \
-v ./ktem_app_data:/app/ktem_app_data \
-p 7860:7860 -it --rm \
ghcr.io/cinnamon/kotaemon:main-full To use the full version with bundled Ollama for local / private RAG. # change image name to
docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-ollama To use the lite version. # change image name to docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-lite We currently support and test two platforms: linux/amd64 and linux/arm64 (for newer Mac). You can specify the platform by passing --platform in the docker run command. For example: # To run docker with platform linux/arm64
docker run \
-e GRADIO_SERVER_NAME=0.0.0.0 \
-e GRADIO_SERVER_PORT=7860 \
-v ./ktem_app_data:/app/ktem_app_data \
-p 7860:7860 -it --rm \
--platform linux/arm64 \
ghcr.io/cinnamon/kotaemon:main-lite Once everything is set up correctly, you can go to http://localhost:7860/ to access the WebUI. We use GHCR to store docker images, all images can be found here. Without Docker Clone and install required packages on a fresh python environment. # optional (setup env)
conda create -n kotaemon python=3.10
conda activate kotaemon # clone this repo
git clone https://github.com/Cinnamon/kotaemon
cd kotaemon pip install -e "libs/kotaemon[all]"
pip install -e "libs/ktem" Create a .env file in the root of this project. Use .env.example as a template The .env file is there to serve use cases where users want to pre-config the models before starting up the app (e.g. deploy the app on HF hub). The file will only be used to populate the db once upon the first run, it will no longer be used in consequent runs. (Optional) To enable in-browser PDF_JS viewer, download PDF_JS_DIST then extract it to libs/ktem/ktem/assets/prebuilt Start the web server: python app.py The app will be automatically launched in your browser. Default username and password are both admin. You can set up additional users directly through the UI. Check the Resources tab and LLMs and Embeddings and ensure that your api_key value is set correctly from your .env file. If it is not set, you can set it there. Setup GraphRAG [!NOTE] Official MS GraphRAG indexing only works with OpenAI or Ollama API. We recommend most users to use NanoGraphRAG implementation for straightforward integration with Kotaemon. Setup Nano GRAPHRAG Install nano-GraphRAG: pip install nano-graphrag nano-graphrag install might introduce version conflicts, see this issue To quickly fix: pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib Launch Kotaemon with USE_NANO_GRAPHRAG=true environment variable. Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from NanoGraphRAG. Setup LIGHTRAG Install LightRAG: pip install git+https://github.com/HKUDS/LightRAG.git LightRAG install might introduce version conflicts, see this issue To quickly fix: pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib Launch Kotaemon with USE_LIGHTRAG=true environment variable. Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from LightRAG. Setup MS GRAPHRAG Non-Docker Installation: If you are not using Docker, install GraphRAG with the following command: pip install "graphrag&lt;=0.3.6" future Setting Up API KEY: To use the GraphRAG retriever feature, ensure you set the GRAPHRAG_API_KEY environment variable. You can do this directly in your environment or by adding it to a .env file. Using Local Models and Custom Settings: If you want to use GraphRAG with local models (like Ollama) or customize the default LLM and other configurations, set the USE_CUSTOMIZED_GRAPHRAG_SETTING environment variable to true. Then, adjust your settings in the settings.yaml.example file. Setup Local Models (for local/private RAG) See Local model setup. Setup multimodal document parsing (OCR, table parsing, figure extraction) These options are available: Azure Document Intelligence (API) Adobe PDF Extract (API) Docling (local, open-source) To use Docling, first install required dependencies: pip install docling Select corresponding loaders in Settings -&gt; Retrieval Settings -&gt; File loader Customize your application By default, all application data is stored in the ./ktem_app_data folder. You can back up or copy this folder to transfer your installation to a new machine. For advanced users or specific use cases, you can customize these files: flowsettings.py .env flowsettings.py This file contains the configuration of your application. You can use the example here as the starting point. Notable settings # setup your preferred document store (with full-text search capabilities)
KH_DOCSTORE=(Elasticsearch | LanceDB | SimpleFileDocumentStore) # setup your preferred vectorstore (for vector-based search)
KH_VECTORSTORE=(ChromaDB | LanceDB | InMemory | Milvus | Qdrant) # Enable / disable multimodal QA
KH_REASONINGS_USE_MULTIMODAL=True # Setup your new reasoning pipeline or modify existing one.
KH_REASONINGS = [ "ktem.reasoning.simple.FullQAPipeline", "ktem.reasoning.simple.FullDecomposeQAPipeline", "ktem.reasoning.react.ReactAgentPipeline", "ktem.reasoning.rewoo.RewooAgentPipeline",
] .env This file provides another way to configure your models and credentials. Configure model via the .env file Alternatively, you can configure the models via the .env file with the information needed to connect to the LLMs. This file is located in the folder of the application. If you don't see it, you can create one. Currently, the following providers are supported: OpenAI In the .env file, set the OPENAI_API_KEY variable with your OpenAI API key in order to enable access to OpenAI's models. There are other variables that can be modified, please feel free to edit them to fit your case. Otherwise, the default parameter should work for most people. OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_API_KEY=
OPENAI_CHAT_MODEL=gpt-3.5-turbo
OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002 Azure OpenAI For OpenAI models via Azure platform, you need to provide your Azure endpoint and API key. Your might also need to provide your developments' name for the chat model and the embedding model depending on how you set up Azure development. AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo
AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002 Local Models Using ollama OpenAI compatible server: Install ollama and start the application. Pull your model, for example: ollama pull llama3.1:8b
ollama pull nomic-embed-text Set the model names on web UI and make it as default: Using GGUF with llama-cpp-python You can search and download a LLM to be ran locally from the Hugging Face Hub. Currently, these model formats are supported: GGUF You should choose a model whose size is less than your device's memory and should leave about 2 GB. For example, if you have 16 GB of RAM in total, of which 12 GB is available, then you should choose a model that takes up at most 10 GB of RAM. Bigger models tend to give better generation but also take more processing time. Here are some recommendations and their size in memory: Qwen1.5-1.8B-Chat-GGUF: around 2 GB Add a new LlamaCpp model with the provided model name on the web UI. Adding your own RAG pipeline Custom Reasoning Pipeline Check the default pipeline implementation in here. You can make quick adjustment to how the default QA pipeline work. Add new .py implementation in libs/ktem/ktem/reasoning/ and later include it in flowssettings to enable it on the UI. Custom Indexing Pipeline Check sample implementation in libs/ktem/ktem/index/file/graph (more instruction WIP). Citation Please cite this project as @misc{kotaemon2024, title = {Kotaemon - An open-source RAG-based tool for chatting with any content.}, author = {The Kotaemon Team}, year = {2024}, howpublished = {\url{https://github.com/Cinnamon/kotaemon}},
} Star History Contribution Since our project is actively being developed, we greatly value your feedback and contributions. Please see our Contributing Guide to get started. Thank you to all our contributors!]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:25 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/Cinnamon/kotaemon</guid>
    </item>
    <item>
      <title><![CDATA[OpenCTI-Platform/opencti]]></title>
      <link>https://github.com/OpenCTI-Platform/opencti</link>
      <description><![CDATA[Open Cyber Threat Intelligence Platform Introduction OpenCTI is an open source platform allowing organizations to manage their cyber threat intelligence knowledge and observables. It has been created in order to structure, store, organize and visualize technical and non-technical information about cyber threats. The structuration of the data is performed using a knowledge schema based on the STIX2 standards. It has been designed as a modern web application including a GraphQL API and an UX oriented frontend. Also, OpenCTI can be integrated with other tools and applications such as MISP, TheHive, MITRE ATT&amp;CK, etc. Objective The goal is to create a comprehensive tool allowing users to capitalize technical (such as TTPs and observables) and non-technical information (such as suggested attribution, victimology etc.) while linking each piece of information to its primary source (a report, a MISP event, etc.), with features such as links between each information, first and last seen dates, levels of confidence, etc. The tool is able to use the MITRE ATT&amp;CK framework (through a dedicated connector) to help structure the data. The user can also choose to implement their own datasets. Once data has been capitalized and processed by the analysts within OpenCTI, new relations may be inferred from existing ones to facilitate the understanding and the representation of this information. This allows the user to extract and leverage meaningful knowledge from the raw data. OpenCTI not only allows imports but also exports of data under different formats (CSV, STIX2 bundles, etc.). Connectors are currently developed to accelerate interactions between the tool and other platforms. Editions of the platform OpenCTI platform has 2 different editions: Community (CE) and Enterprise (EE). The purpose of the Enterprise Edition is to provide additional and powerful features which require specific investments in research and development. You can enable the Enterprise Edition directly in the settings of the platform. OpenCTI Community Edition, licensed under the Apache 2, Version 2.0 license. OpenCTI Enterprise Edition, licensed under the Enterprise Edition license. To understand what OpenCTI Enterprise Edition brings in terms of features, just check the Enterprise Editions page on the Filigran website. You can also try this edition by enabling it in the settings of the platform. Documentation and demonstration If you want to know more on OpenCTI, you can read the documentation on the tool. If you wish to discover how the OpenCTI platform is working, a demonstration instance is available and open to everyone. This instance is reset every night and is based on reference data maintained by the OpenCTI developers. Releases download The releases are available on the Github releases page. You can also access the rolling release package generated from the master branch of the repository. Installation All you need to install the OpenCTI platform can be found in the official documentation. For installation, you can: Use Docker Install manually Use Terraform (community) Use Helm charts (community) Contributing Code of Conduct OpenCTI has adopted a Code of Conduct that we expect project participants to adhere to. Please read the full text so that you can understand what actions will and will not be tolerated. Contributing Guide Read our contributing guide to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to OpenCTI. Beginner friendly issues To help you get you familiar with our contribution process, we have a list of beginner friendly issues which are fairly easy to implement. This is a great place to get started. Development If you want to actively help OpenCTI, we created a dedicated documentation about the deployment of a development environment and how to start the source code modification. Community Status &amp; bugs Currently OpenCTI is under heavy development, if you wish to report bugs or ask for new features, you can directly use the Github issues module. Discussion If you need support or you wish to engage a discussion about the OpenCTI platform, feel free to join us on our Slack channel. You can also send us an email to contact@filigran.io. About Authors OpenCTI is a product designed and developed by the company Filigran. Data Collection Usage telemetry To improve the features and the performances of OpenCTI, the platform collects anonymous statistical data related to its usage and health. You can find all the details on collected data and associated usage in the usage telemetry documentation. OpenStreetMap server To provide OpenCTI users with cartography features, the platform uses a dedicated OpenStreetMap server (https://map.opencti.io). To monitor usage and adapt services performances, Filigran collects access log to this server (including IP addresses). By using this server, you authorize Filigran to collect this information. Otherwise, you are free to deploy your own OpenStreetMap server and modify the platform configuration accordingly. If you have started using the Filigran server and change your mind, you have the right to access, limit, rectify, erase and receive your data. To exercise your rights, please send your request to privacy@filigran.io.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:24 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/OpenCTI-Platform/opencti</guid>
    </item>
    <item>
      <title><![CDATA[ComposioHQ/composio]]></title>
      <link>https://github.com/ComposioHQ/composio</link>
      <description><![CDATA[Composio powers 1000+ toolkits, tool search, context management, authentication, and a sandboxed workbench to help you build AI agents that turn intent into action. Composio SDK Skills that evolve for your Agents Website • Documentation This repository contains the official Software Development Kits (SDKs) for Composio, providing seamless integration capabilities for Python and Typescript Agentic Frameworks and Libraries. Getting Started TypeScript SDK Installation # Using npm
npm install @composio/core # Using yarn
yarn add @composio/core # Using pnpm
pnpm add @composio/core Quick start: import { Composio } from '@composio/core';
// Initialize the SDK
const composio = new Composio({ // apiKey: 'your-api-key',
}); Simple Agent with OpenAI Agents npm install @composio/openai-agents @openai/agents import { Composio } from '@composio/core';
import { OpenAIAgentsProvider } from '@composio/openai-agents';
import { Agent, run } from '@openai/agents'; const composio = new Composio({ provider: new OpenAIAgentsProvider(),
}); const userId = 'user@acme.org'; const tools = await composio.tools.get(userId, { toolkits: ['HACKERNEWS'],
}); const agent = new Agent({ name: 'Hackernews assistant', tools: tools,
}); const result = await run(agent, 'What is the latest hackernews post about?'); console.log(JSON.stringify(result.finalOutput, null, 2));
// will return the response from the agent with data from HACKERNEWS API. Python SDK Installation # Using pip
pip install composio # Using poetry
poetry add composio Quick start: from composio import Composio composio = Composio( # api_key="your-api-key",
) Simple Agent with OpenAI Agents pip install composio_openai_agents openai-agents import asyncio
from agents import Agent, Runner
from composio import Composio
from composio_openai_agents import OpenAIAgentsProvider # Initialize Composio client with OpenAI Agents Provider
composio = Composio(provider=OpenAIAgentsProvider()) user_id = "user@acme.org"
tools = composio.tools.get(user_id=user_id, toolkits=["HACKERNEWS"]) # Create an agent with the tools
agent = Agent( name="Hackernews Agent", instructions="You are a helpful assistant.", tools=tools,
) # Run the agent
async def main(): result = await Runner.run( starting_agent=agent, input="What's the latest Hackernews post about?", ) print(result.final_output) asyncio.run(main())
# will return the response from the agent with data from HACKERNEWS API. For more detailed usage instructions and examples, please refer to each SDK's specific documentation. Open API Specification To update the OpenAPI specifications used for generating SDK documentation: # Pull the latest API specifications from the backend
pnpm api:pull This command pulls the OpenAPI specification from https://backend.composio.dev/api/v3/openapi.json (defined in fern/scripts/pull-openapi-spec.sh) and updates the local API documentation files. This is pulled automatically with build step. Available SDKs TypeScript SDK (/ts) The TypeScript SDK provides a modern, type-safe way to interact with Composio's services. It's designed for both Node.js and browser environments, offering full TypeScript support with comprehensive type definitions. For detailed information about the TypeScript SDK, please refer to the TypeScript SDK Documentation. Python SDK (/python) The Python SDK offers a Pythonic interface to Composio's services, making it easy to integrate Composio into your Python applications. It supports Python 3.10+ and follows modern Python development practices. For detailed information about the Python SDK, please refer to the Python SDK Documentation. Provider Support The following table shows which AI frameworks and platforms are supported in each SDK: Provider TypeScript Python OpenAI OpenAI Agents Anthropic LangChain LangGraph * LlamaIndex Vercel AI SDK Google Gemini Google ADK Mastra Cloudflare Workers AI CrewAI AutoGen * LangGraph in TypeScript is supported via the @composio/langchain package. Don't see your provider? Learn how to build a custom provider to integrate with any AI framework. Packages Core Packages Package Version TypeScript @composio/core Python composio Provider Packages Package Version TypeScript @composio/openai @composio/openai-agents @composio/anthropic @composio/langchain @composio/llamaindex @composio/vercel @composio/google @composio/mastra @composio/cloudflare Python composio-openai composio-openai-agents composio-anthropic composio-langchain composio-langgraph composio-llamaindex composio-crewai composio-autogen composio-gemini composio-google composio-google-adk Utility Packages Package Version @composio/json-schema-to-zod @composio/ts-builders if you are looking for the older sdk, you can find them here Rube Rube is a Model Context Protocol (MCP) server built with Composio. It connects your AI tools to 500+ apps like Gmail, Slack, GitHub, and Notion. Simply install it in your AI client, authenticate once with your apps, and start asking your AI to perform real actions like "Send an email" or "Create a task." It integrates with major AI clients like Cursor, Claude Desktop, VS Code, Claude Code and any custom MCP‑compatible client. You can switch between these clients and your integrations follow you. Contributing We welcome contributions to both SDKs! Please read our contribution guidelines before submitting pull requests. License This project is licensed under the MIT License - see the LICENSE file for details. Support If you encounter any issues or have questions about the SDKs: Open an issue in this repository Contact our support team Check our documentation]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/ComposioHQ/composio</guid>
    </item>
    <item>
      <title><![CDATA[GodsScion/Auto_job_applier_linkedIn]]></title>
      <link>https://github.com/GodsScion/Auto_job_applier_linkedIn</link>
      <description><![CDATA[Make your job hunt easy by automating your application process with this Auto Applier LinkedIn AI Auto Job Applier This is an web scraping bot that automates the process of job applications on LinkedIn. It searches for jobs relevant to you, answers all questions in application form, customizes your resume based on the collected job information, such as skills required, description, about company, etc. and applies to the job. Can apply 100+ jobs in less than 1 hour. See it in Action Click on above image to watch the demo or use this link https://youtu.be/gMbB1fWZDHw Content Introduction Demo Video Index Install Configure Contributor Guidelines Updates Disclaimer Terms and Conditions License Socials Support and Discussions How to install Click on above image to watch the tutorial for installation and configuration or use this link https://youtu.be/f9rdz74e1lM ( to watch it in 2x speed) Python 3.10 or above. Visit https://www.python.org/downloads/ to download and install Python, or for windows you could visit Microsoft Store and search for "Python". Please make sure Python is added to Path in System Environment Variables. Install necessary Undetected Chromedriver, PyAutoGUI and Setuptools packages. After Python is installed, OPEN a console/terminal or shell, Use below command that uses the pip command-line tool to install these 3 package. pip install undetected-chromedriver pyautogui setuptools openai flask-cors flask Download and install latest version of Google Chrome in it's default location, visit https://www.google.com/chrome to download it's installer. Clone the current git repo or download it as a zip file, url to the latest update https://github.com/GodsScion/Auto_job_applier_linkedIn. (Not needed if you set stealth_mode = True in config/settings.py ) Download and install the appropriate Chrome Driver for Google Chrome and paste it in the location Chrome was installed, visit https://googlechromelabs.github.io/chrome-for-testing/ to download. OR If you are using Windows, click on windows-setup.bat available in the /setup folder, this will install the latest chromedriver automatically. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY back to index How to configure Open personals.py file in /config folder and enter your details like name, phone number, address, etc. Whatever you want to fill in your applications. Open questions.py file in /config folder and enter your answers for application questions, configure wether you want the bot to pause before submission or pause if it can't answer unknown questions. Open search.py file in /config folder and enter your search preferences, job filters, configure the bot as per your needs (these settings decide which jobs to apply for or skip). Open secrets.py file in /config folder and enter your LinkedIn username, password to login and OpenAI API Key for generation of job tailored resumes and cover letters (This entire step is optional). If you do not provide username or password or leave them as default, it will login with saved profile in browser, if failed will ask you to login manually. Open settings.py file in /config folder to configure the bot settings like, keep screen awake, click intervals (click intervals are randomized to seem like human behavior), run in background, stealth mode (to avoid bot detection), etc. as per your needs. (Optional) Don't forget to add you default resume in the location you mentioned in default_resume_path = "all resumes/default/resume.pdf" given in /config/questions.py. If one is not provided, it will use your previous resume submitted in LinkedIn or (In Development) generate custom resume if OpenAI APT key is provided! Run runAiBot.py and see the magic happen. To run the Applied Jobs history UI, run app.py and open web browser on http://localhost:5000. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY back to index Contributor Guidelines Thank you for your efforts and being a part of the community. All contributions are appreciated no matter how small or big. Once you contribute to the code base, your work will be remembered forever. NOTE: Only Pull request to community-version branch will be accepted. Any other requests will be declined by default, especially to main branch. Once your code is tested, your changes will be merged to the main branch in next cycle. Code Guidelines Functions: All functions or methods are named lower case and snake case Must have explanation of their purpose. Write explanation surrounded in ''' Explanation ''' under the definition def function() -&gt; None:. Example: def function() -&gt; None: ''' This function does nothing, it's just an example for explanation placement! ''' The Types (str, list, int, list[str], int | float) for the parameters and returns must be given. Example: def function(param1: str, param2: list[str], param3: int) -&gt; str: Putting all that together some valid examples for function or method declarations would be as follows. def function_name_in_camel_case(parameter1: driver, parameter2: str) -&gt; list[str] | ValueError: ''' This function is an example for code guidelines ''' return [parameter2, parameter2.lower()] The hashtag on top of functions are optional, which are intended for developers # for developers. # Enter input text function
def text_input_by_ID(driver: WebDriver, id: str, value: str, time: float=5.0) -&gt; None | Exception: ''' Enters `value` into the input field with the given `id` if found, else throws NotFoundException. - `time` is the max time to wait for the element to be found. ''' username_field = WebDriverWait(driver, time).until(EC.presence_of_element_located((By.ID, id))) username_field.send_keys(Keys.CONTROL + "a") username_field.send_keys(value) Variables All variables must start with lower case, must be in explainable full words. If someone reads the variable name, it should be easy to understand what the variable stores. All local variables are camel case. Examples: jobListingsElement = None localBufferTime = 5.5 All global variables are snake case. Example: total_runs = 1 Mentioning types are optional. localBufferTime: float | int = 5.5 Configuration variables All config variables are treated as global variables. They have some extra guidelines. Must have variable setting explanation, and examples of valid values. Examples: # Explanation of what this setting will do, and instructions to enter it correctly
config_variable = "value1" # "value1", "value2", etc. Don't forget quotes ("") # Do you want to randomize the search order for search_terms?
randomize_search_order = False # True of False, Note: True or False are case-sensitive # Avoid applying to jobs if their required experience is above your current_experience. (Set value as -1 if you want to apply to all ignoring their required experience...)
current_experience = 5 # Integers &gt; -2 (Ex: -1, 0, 1, 2, 3, 4...) # Search location, this will be filled in "City, state, or zip code" search box. If left empty as "", tool will not fill it.
search_location = "United States" # Some valid examples: "", "United States", "India", "Chicago, Illinois, United States", "90001, Los Angeles, California, United States", "Bengaluru, Karnataka, India", etc. Add the config variable in appropriate /config/file. Every config variable must be validated. Go to /modules/validator.py and add it over there. Example: For config variable search_location = "" found in /config/search.py, string validation is added in file /modules/validator.py under the method def validate_search(). def validate_search() -&gt; None | ValueError | TypeError: ''' Validates all variables in the `/config/search.py` file. ''' check_string(search_location, "search_location") back to index Attestation All contributions require proper attestion. Format for attestation: ##&gt; ------ : OR - ------ print("My contributions ") # Your code
##&lt; Examples for proper attestation: New feature example ##&gt; ------ Sai Vignesh Golla : godsscion - Feature ------
def alert_box(title: str, message: str) -&gt; None: ''' Shows an alert box with the given `title` and `message`. ''' from pyautogui import alert return alert(title, message) ##&lt; Bug fix example def alert_box(title: str, message: str) -&gt; None: ''' Shows an alert box with the given `title` and `message`. ''' from pyautogui import alert ##&gt; ------ Sai Vignesh Golla : saivigneshgolla@outlook.com - Bug fix ------ return alert(message, title)]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/GodsScion/Auto_job_applier_linkedIn</guid>
    </item>
    <item>
      <title><![CDATA[hummingbot/hummingbot]]></title>
      <link>https://github.com/hummingbot/hummingbot</link>
      <description><![CDATA[Open source software that helps you create and deploy high-frequency crypto trading bots Hummingbot is an open-source framework that helps you design and deploy automated trading strategies, or bots, that can run on many centralized or decentralized exchanges. Over the past year, Hummingbot users have generated over $34 billion in trading volume across 140+ unique trading venues. The Hummingbot codebase is free and publicly available under the Apache 2.0 open-source license. Our mission is to democratize high-frequency trading by creating a global community of algorithmic traders and developers that share knowledge and contribute to the codebase. Quick Links Website and Docs: Official Hummingbot website and documentation Installation: Install Hummingbot on various platforms Discord: The main gathering spot for the global Hummingbot community YouTube: Videos that teach you how to get the most out of Hummingbot Twitter: Get the latest announcements about Hummingbot Reported Volumes: Reported trading volumes across all Hummingbot instances Newsletter: Get our newsletter whenever we ship a new release Getting Started The easiest way to get started with Hummingbot is using Docker: To install the Telegram Bot Condor, follow the instructions in the Hummingbot Docs site. To install the CLI-based Hummingbot client, follow the instructions below. Alternatively, if you are building new connectors/strategies or adding custom code, see the Install from Source section in the documentation. Install Hummingbot with Docker Install Docker Compose website. Clone the repo and use the provided docker-compose.yml file: # Clone the repository
git clone https://github.com/hummingbot/hummingbot.git
cd hummingbot # Run Setup &amp; Deploy
make setup
make deploy # Attach to the running instance
docker attach hummingbot Install Hummingbot + Gateway DEX Middleware Gateway provides standardized connectors for interacting with automatic market maker (AMM) decentralized exchanges (DEXs) across different blockchain networks. To run Hummingbot with Gateway, clone the repo and answer y when prompted after running make setup # Clone the repository
git clone https://github.com/hummingbot/hummingbot.git
cd hummingbot make setup # Answer `y` when prompted
Include Gateway? [y/N] Then run: make deploy # Attach to the running instance]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/hummingbot/hummingbot</guid>
    </item>
    <item>
      <title><![CDATA[openclaw/openclaw]]></title>
      <link>https://github.com/openclaw/openclaw</link>
      <description><![CDATA[Your own personal AI assistant. Any OS. Any Platform. The lobster way. OpenClaw — Personal AI Assistant EXFOLIATE! EXFOLIATE! OpenClaw is a personal AI assistant you run on your own devices. It answers you on the channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, WebChat), plus extension channels like BlueBubbles, Matrix, Zalo, and Zalo Personal. It can speak and listen on macOS/iOS/Android, and can render a live Canvas you control. The Gateway is just the control plane — the product is the assistant. If you want a personal, single-user assistant that feels local, fast, and always-on, this is it. Website · Docs · Vision · DeepWiki · Getting Started · Updating · Showcase · FAQ · Wizard · Nix · Docker · Discord Preferred setup: run the onboarding wizard (openclaw onboard) in your terminal. The wizard guides you step by step through setting up the gateway, workspace, channels, and skills. The CLI wizard is the path and works on macOS, Linux, and Windows (via WSL2; strongly ). Works with npm, pnpm, or bun. New install? Start here: Getting started Subscriptions (OAuth): Anthropic (Claude Pro/Max) OpenAI (ChatGPT/Codex) Model note: while any model is supported, I strongly recommend Anthropic Pro/Max (100/200) + Opus 4.6 for long‑context strength and better prompt‑injection resistance. See Onboarding. Models (selection + auth) Models config + CLI: Models Auth profile rotation (OAuth vs API keys) + fallbacks: Model failover Install ( ) Runtime: Node ≥22. npm install -g openclaw@latest
# or: pnpm add -g openclaw@latest openclaw onboard --install-daemon The wizard installs the Gateway daemon (launchd/systemd user service) so it stays running. Quick start (TL;DR) Runtime: Node ≥22. Full beginner guide (auth, pairing, channels): Getting started openclaw onboard --install-daemon openclaw gateway --port 18789 --verbose # Send a message
openclaw message send --to +1234567890 --message "Hello from OpenClaw" # Talk to the assistant (optionally deliver back to any connected channel: WhatsApp/Telegram/Slack/Discord/Google Chat/Signal/iMessage/BlueBubbles/Microsoft Teams/Matrix/Zalo/Zalo Personal/WebChat)
openclaw agent --message "Ship checklist" --thinking high Upgrading? Updating guide (and run openclaw doctor). Development channels stable: tagged releases (vYYYY.M.D or vYYYY.M.D-), npm dist-tag latest. beta: prerelease tags (vYYYY.M.D-beta.N), npm dist-tag beta (macOS app may be missing). dev: moving head of main, npm dist-tag dev (when published). Switch channels (git + npm): openclaw update --channel stable|beta|dev. Details: Development channels. From source (development) Prefer pnpm for builds from source. Bun is optional for running TypeScript directly. git clone https://github.com/openclaw/openclaw.git
cd openclaw pnpm install
pnpm ui:build # auto-installs UI deps on first run
pnpm build pnpm openclaw onboard --install-daemon # Dev loop (auto-reload on TS changes)
pnpm gateway:watch Note: pnpm openclaw ... runs TypeScript directly (via tsx). pnpm build produces dist/ for running via Node / the packaged openclaw binary. Security defaults (DM access) OpenClaw connects to real messaging surfaces. Treat inbound DMs as untrusted input. Full security guide: Security Default behavior on Telegram/WhatsApp/Signal/iMessage/Microsoft Teams/Discord/Google Chat/Slack: DM pairing (dmPolicy="pairing" / channels.discord.dmPolicy="pairing" / channels.slack.dmPolicy="pairing"; legacy: channels.discord.dm.policy, channels.slack.dm.policy): unknown senders receive a short pairing code and the bot does not process their message. Approve with: openclaw pairing approve (then the sender is added to a local allowlist store). Public inbound DMs require an explicit opt-in: set dmPolicy="open" and include "*" in the channel allowlist (allowFrom / channels.discord.allowFrom / channels.slack.allowFrom; legacy: channels.discord.dm.allowFrom, channels.slack.dm.allowFrom). Run openclaw doctor to surface risky/misconfigured DM policies. Highlights Local-first Gateway — single control plane for sessions, channels, tools, and events. Multi-channel inbox — WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, BlueBubbles (iMessage), iMessage (legacy), Microsoft Teams, Matrix, Zalo, Zalo Personal, WebChat, macOS, iOS/Android. Multi-agent routing — route inbound channels/accounts/peers to isolated agents (workspaces + per-agent sessions). Voice Wake + Talk Mode — always-on speech for macOS/iOS/Android with ElevenLabs. Live Canvas — agent-driven visual workspace with A2UI. First-class tools — browser, canvas, nodes, cron, sessions, and Discord/Slack actions. Companion apps — macOS menu bar app + iOS/Android nodes. Onboarding + skills — wizard-driven setup with bundled/managed/workspace skills. Star History Everything we built so far Core platform Gateway WS control plane with sessions, presence, config, cron, webhooks, Control UI, and Canvas host. CLI surface: gateway, agent, send, wizard, and doctor. Pi agent runtime in RPC mode with tool streaming and block streaming. Session model: main for direct chats, group isolation, activation modes, queue modes, reply-back. Group rules: Groups. Media pipeline: images/audio/video, transcription hooks, size caps, temp file lifecycle. Audio details: Audio. Channels Channels: WhatsApp (Baileys), Telegram (grammY), Slack (Bolt), Discord (discord.js), Google Chat (Chat API), Signal (signal-cli), BlueBubbles (iMessage, ), iMessage (legacy imsg), Microsoft Teams (extension), Matrix (extension), Zalo (extension), Zalo Personal (extension), WebChat. Group routing: mention gating, reply tags, per-channel chunking and routing. Channel rules: Channels. Apps + nodes macOS app: menu bar control plane, Voice Wake/PTT, Talk Mode overlay, WebChat, debug tools, remote gateway control. iOS node: Canvas, Voice Wake, Talk Mode, camera, screen recording, Bonjour pairing. Android node: Canvas, Talk Mode, camera, screen recording, optional SMS. macOS node mode: system.run/notify + canvas/camera exposure. Tools + automation Browser control: dedicated openclaw Chrome/Chromium, snapshots, actions, uploads, profiles. Canvas: A2UI push/reset, eval, snapshot. Nodes: camera snap/clip, screen record, location.get, notifications. Cron + wakeups; webhooks; Gmail Pub/Sub. Skills platform: bundled, managed, and workspace skills with install gating + UI. Runtime + safety Channel routing, retry policy, and streaming/chunking. Presence, typing indicators, and usage tracking. Models, model failover, and session pruning. Security and troubleshooting. Ops + packaging Control UI + WebChat served directly from the Gateway. Tailscale Serve/Funnel or SSH tunnels with token/password auth. Nix mode for declarative config; Docker-based installs. Doctor migrations, logging. How it works (short) WhatsApp / Telegram / Slack / Discord / Google Chat / Signal / iMessage / BlueBubbles / Microsoft Teams / Matrix / Zalo / Zalo Personal / WebChat │ ▼
┌───────────────────────────────┐
│ Gateway │
│ (control plane) │
│ ws://127.0.0.1:18789 │
└──────────────┬────────────────┘ │ ├─ Pi agent (RPC) ├─ CLI (openclaw …) ├─ WebChat UI ├─ macOS app └─ iOS / Android nodes Key subsystems Gateway WebSocket network — single WS control plane for clients, tools, and events (plus ops: Gateway runbook). Tailscale exposure — Serve/Funnel for the Gateway dashboard + WS (remote access: Remote). Browser control — openclaw‑managed Chrome/Chromium with CDP control. Canvas + A2UI — agent‑driven visual workspace (A2UI host: Canvas/A2UI). Voice Wake + Talk Mode — always‑on speech and continuous conversation. Nodes — Canvas, camera snap/clip, screen record, location.get, notifications, plus macOS‑only system.run/system.notify. Tailscale access (Gateway dashboard) OpenClaw can auto-configure Tailscale Serve (tailnet-only) or Funnel (public) while the Gateway stays bound to loopback. Configure gateway.tailscale.mode: off: no Tailscale automation (default). serve: tailnet-only HTTPS via tailscale serve (uses Tailscale identity headers by default). funnel: public HTTPS via tailscale funnel (requires shared password auth). Notes: gateway.bind must stay loopback when Serve/Funnel is enabled (OpenClaw enforces this). Serve can be forced to require a password by setting gateway.auth.mode: "password" or gateway.auth.allowTailscale: false. Funnel refuses to start unless gateway.auth.mode: "password" is set. Optional: gateway.tailscale.resetOnExit to undo Serve/Funnel on shutdown. Details: Tailscale guide · Web surfaces Remote Gateway (Linux is great) It’s perfectly fine to run the Gateway on a small Linux instance. Clients (macOS app, CLI, WebChat) can connect over Tailscale Serve/Funnel or SSH tunnels, and you can still pair device nodes (macOS/iOS/Android) to execute device‑local actions when needed. Gateway host runs the exec tool and channel connections by default. Device nodes run device‑local actions (system.run, camera, screen recording, notifications) via node.invoke. In short: exec runs where the Gateway lives; device actions run where the device lives. Details: Remote access · Nodes · Security macOS permissions via the Gateway protocol The macOS app can run in node mode and advertises its capabilities + permission map over the Gateway WebSocket (node.list / node.describe). Clients can then execute local actions via node.invoke: system.run runs a local command and returns stdout/stderr/exit code; set needsScreenRecording: true to require screen-recording permission (otherwise you’ll get PERMISSION_MISSING). system.notify posts a user notification and fails if notifications are denied. canvas.*, camera.*, screen.record, and location.get are also routed via node.invoke and follow TCC permission status. Elevated bash (host permissions) is separate from macOS TCC: Use /elevated on|off to toggle per‑session elevated access when enabled + allowlisted. Gateway persists the per‑session toggle via sessions.patch (WS method) alongside thinkingLevel, verboseLevel, model, sendPolicy, and groupActivation. Details: Nodes · macOS app · Gateway protocol Agent to Agent (sessions_* tools) Use these to coordinate work across sessions without jumping between chat surfaces. sessions_list — discover active sessions (agents) and their metadata. sessions_history — fetch transcript logs for a session. sessions_send — message another session; optional reply‑back ping‑pong + announce step (REPLY_SKIP, ANNOUNCE_SKIP). Details: Session tools Skills registry (ClawHub) ClawHub is a minimal skill registry. With ClawHub enabled, the agent can search for skills automatically and pull in new ones as needed. ClawHub Chat commands Send these in WhatsApp/Telegram/Slack/Google Chat/Microsoft Teams/WebChat (group commands are owner-only): /status — compact session status (model + tokens, cost when available) /new or /reset — reset the session /compact — compact session context (summary) /think — off|minimal|low|medium|high|xhigh (GPT-5.2 + Codex models only) /verbose on|off /usage off|tokens|full — per-response usage footer /restart — restart the gateway (owner-only in groups) /activation mention|always — group activation toggle (groups only) Apps (optional) The Gateway alone delivers a great experience. All apps are optional and add extra features. If you plan to build/run companion apps, follow the platform runbooks below. macOS (OpenClaw.app) (optional) Menu bar control for the Gateway and health. Voice Wake + push-to-talk overlay. WebChat + debug tools. Remote gateway control over SSH. Note: signed builds required for macOS permissions to stick across rebuilds (see docs/mac/permissions.md). iOS node (optional) Pairs as a node via the Bridge. Voice trigger forwarding + Canvas surface. Controlled via openclaw nodes …. Runbook: iOS connect. Android node (optional) Pairs via the same Bridge + pairing flow as iOS. Exposes Canvas, Camera, and Screen capture commands. Runbook: Android connect. Agent workspace + skills Workspace root: ~/.openclaw/workspace (configurable via agents.defaults.workspace). Injected prompt files: AGENTS.md, SOUL.md, TOOLS.md. Skills: ~/.openclaw/workspace/skills//SKILL.md. Configuration Minimal ~/.openclaw/openclaw.json (model + defaults): { agent: { model: "anthropic/claude-opus-4-6", },
} Full configuration reference (all keys + examples). Security model (important) Default: tools run on the host for the main session, so the agent has full access when it’s just you. Group/channel safety: set agents.defaults.sandbox.mode: "non-main" to run non‑main sessions (groups/channels) inside per‑session Docker sandboxes; bash then runs in Docker for those sessions. Sandbox defaults: allowlist bash, process, read, write, edit, sessions_list, sessions_history, sessions_send, sessions_spawn; denylist browser, canvas, nodes, cron, discord, gateway. Details: Security guide · Docker + sandboxing · Sandbox config WhatsApp Link the device: pnpm openclaw channels login (stores creds in ~/.openclaw/credentials). Allowlist who can talk to the assistant via channels.whatsapp.allowFrom. If channels.whatsapp.groups is set, it becomes a group allowlist; include "*" to allow all. Telegram Set TELEGRAM_BOT_TOKEN or channels.telegram.botToken (env wins). Optional: set channels.telegram.groups (with channels.telegram.groups."*".requireMention); when set, it is a group allowlist (include "*" to allow all). Also channels.telegram.allowFrom or channels.telegram.webhookUrl + channels.telegram.webhookSecret as needed. { channels: { telegram: { botToken: "123456:ABCDEF", }, },
} Slack Set SLACK_BOT_TOKEN + SLACK_APP_TOKEN (or channels.slack.botToken + channels.slack.appToken). Discord Set DISCORD_BOT_TOKEN or channels.discord.token (env wins). Optional: set commands.native, commands.text, or commands.useAccessGroups, plus channels.discord.allowFrom, channels.discord.guilds, or channels.discord.mediaMaxMb as needed. { channels: { discord: { token: "1234abcd", }, },
} Signal Requires signal-cli and a channels.signal config section. BlueBubbles (iMessage) iMessage integration. Configure channels.bluebubbles.serverUrl + channels.bluebubbles.password and a webhook (channels.bluebubbles.webhookPath). The BlueBubbles server runs on macOS; the Gateway can run on macOS or elsewhere. iMessage (legacy) Legacy macOS-only integration via imsg (Messages must be signed in). If channels.imessage.groups is set, it becomes a group allowlist; include "*" to allow all. Microsoft Teams Configure a Teams app + Bot Framework, then add a msteams config section. Allowlist who can talk via msteams.allowFrom; group access via msteams.groupAllowFrom or msteams.groupPolicy: "open". WebChat Uses the Gateway WebSocket; no separate WebChat port/config. Browser control (optional): { browser: { enabled: true, color: "#FF4500", },]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:24 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/openclaw/openclaw</guid>
    </item>
    <item>
      <title><![CDATA[LeanMCP: Run AI Agents in Production. Own Your Stack]]></title>
      <link>https://dev.to/_4b254f4408ef55656a22e4/leanmcp-run-ai-agents-in-production-own-your-stack-5f04</link>
      <description><![CDATA[Getting an AI agent to work is easy. Getting it to production is a different story.
How do you handle user auth?
How do you isolate permissions across multiple tenants?
How do you inject secrets securely?
When something breaks at 2am, how do you even know what went wrong?
The AI agent ecosystem has frameworks, model providers, and tool integrations covered. What it's been missing is a runtime: the layer that handles governance, security, and deployment so your agent can actually run in production, at scale, without being locked into any single vendor.
Building an AI agent is the easy part. The hard part is everything that comes after.
Authentication. Multi-tenancy. Secret injection. Observability. Protocol compatibility across clients. These aren't glamorous problems, but they're the ones that decide whether your agent ships or stays a demo forever.
The MCP protocol gives you a standard way to connect tools to agents — but it doesn't tell you how to handle any of this. You're left to build it yourself, from scratch, for every project. And if you switch LLM providers or deployment environments down the road, you might rebuild it all again.
We've seen this pattern too many times. So we built the runtime that handles it.
LeanMCP is an open-source agent runtime with a production-grade governance layer built in. Think of it as the operating system for your AI agents — the foundation that lets them run anywhere, work with any client, and stay secure at scale. The problem you hit
What LeanMCP handles User authentication
Auth0, Supabase, Cognito, Firebase — one decorator, done Multi-tenancy
Per-user API keys and permission scoping Secret injection
Per-request env vars, no global pollution Production observability
Logging, monitoring, audit trails Client compatibility
Claude Desktop, Cursor, Windsurf — HTTP/SSE/WebSocket Governance
Security policies, access control, compliance-ready audit logs The core SDK is open source, MIT licensed. Fork it, modify it, self-host it. The production platform — managed deployment, security governance, elastic auto-scaling across 30+ edge regions — is where we run a business. The foundation is free. The enterprise layer is where serious teams invest.
npm install -g @leanmcp/cli
npx @leanmcp/cli create my-agent
cd my-agent &amp;&amp; npm start Your agent runtime is live at http://localhost:8080/mcp.
Adding authentication takes one decorator:
import { AuthProvider, Authenticated } from "@leanmcp/auth"; const auth = new AuthProvider('cognito', { region: process.env.AWS_REGION, userPoolId: process.env.COGNITO_USER_POOL_ID, clientId: process.env.COGNITO_CLIENT_ID
});
await auth.init(); @Authenticated(auth)
export class MyService { @Tool({ description: "Authenticated users only" }) async doSomething(args: { input: string }) { return { result: "done" }; }
} No custom middleware. No manually parsing auth headers. No deep-diving into protocol internals. You write the logic that matters. We handle the runtime.
We support every major agent client — Claude Desktop, Cursor, Windsurf — and every transport protocol: HTTP, SSE, WebSocket. Your agents run on our edge network or your own infrastructure. You're never forced to choose our deployment just to get our governance layer.
We don't want to be the thing that locks you in. We've watched developers get burned by betting early on a single vendor's SDK, then facing deprecations or price hikes they didn't see coming. The AI space moves too fast to assume today's best option stays that way.
If we build this well enough, you won't want to leave. But you always can.
Open-source SDK: TypeScript (50k+ downloads) + Python (200k+ downloads), MIT licensed
Community: 6,000+ developers, across 6 global hackathons
Production platform: Managed deployment, observability, elastic scaling on 30+ edge nodes
We don't know which LLM wins in three years. We don't know which cloud becomes the default home for agents.
But we know this: whoever wins, agents will need a runtime that's stable, secure, and not owned by any single player.
That's what we're building — the ground AI agents stand on.
Code: github.com/LeanMCP/leanmcp-sdk
leanmcp.com
founders@leanmcp.com
— Xian Lu, @__luxian__]]></description>
      <pubDate>Thu, 19 Feb 2026 05:46:34 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/_4b254f4408ef55656a22e4/leanmcp-run-ai-agents-in-production-own-your-stack-5f04</guid>
    </item>
    <item>
      <title><![CDATA[QwenLM/qwen-code]]></title>
      <link>https://github.com/QwenLM/qwen-code</link>
      <description><![CDATA[An open-source AI agent that lives in your terminal. An open-source AI agent that lives in your terminal. 中文 | Deutsch | français | 日本語 | Русский | Português (Brasil) News (2026-02-16): Qwen3.5-Plus is now live! Sign in via Qwen OAuth to use it directly, or get an API key from Alibaba Cloud ModelStudio to access it through the OpenAI-compatible API. Qwen Code is an open-source AI agent for the terminal, optimized for Qwen3-Coder. It helps you understand large codebases, automate tedious work, and ship faster. Why Qwen Code? Multi-protocol, OAuth free tier: use OpenAI / Anthropic / Gemini-compatible APIs, or sign in with Qwen OAuth for 1,000 free requests/day. Open-source, co-evolving: both the framework and the Qwen3-Coder model are open-source—and they ship and evolve together. Agentic workflow, feature-rich: rich built-in tools (Skills, SubAgents) for a full agentic workflow and a Claude Code-like experience. Terminal-first, IDE-friendly: built for developers who live in the command line, with optional integration for VS Code, Zed, and JetBrains IDEs. Installation Quick Install ( ) Linux / macOS curl -fsSL https://qwen-code-assets.oss-cn-hangzhou.aliyuncs.com/installation/install-qwen.sh | bash Windows (Run as Administrator CMD) curl -fsSL -o %TEMP%\install-qwen.bat https://qwen-code-assets.oss-cn-hangzhou.aliyuncs.com/installation/install-qwen.bat &amp;&amp; %TEMP%\install-qwen.bat Note: It's to restart your terminal after installation to ensure environment variables take effect. Manual Installation Prerequisites Make sure you have Node.js 20 or later installed. Download it from nodejs.org. NPM npm install -g @qwen-code/qwen-code@latest Homebrew (macOS, Linux) brew install qwen-code Quick Start # Start Qwen Code (interactive)
qwen # Then, in the session:
/help
/auth On first use, you'll be prompted to sign in. You can run /auth anytime to switch authentication methods. Example prompts: What does this project do?
Explain the codebase structure.
Help me refactor this function.
Generate unit tests for this module. Click to watch a demo video Your browser does not support the video tag. Authentication Qwen Code supports two authentication methods: Qwen OAuth ( &amp; free): sign in with your qwen.ai account in a browser. API-KEY: use an API key to connect to any supported provider (OpenAI, Anthropic, Google GenAI, Alibaba Cloud Bailian, and other compatible endpoints). Qwen OAuth ( ) Start qwen, then run: /auth Choose Qwen OAuth and complete the browser flow. Your credentials are cached locally so you usually won't need to log in again. Note: In non-interactive or headless environments (e.g., CI, SSH, containers), you typically cannot complete the OAuth browser login flow. In these cases, please use the API-KEY authentication method. API-KEY (flexible) Use this if you want more flexibility over which provider and model to use. Supports multiple protocols: OpenAI-compatible: Alibaba Cloud Bailian, ModelScope, OpenAI, OpenRouter, and other OpenAI-compatible providers Anthropic: Claude models Google GenAI: Gemini models The way to configure models and providers is by editing ~/.qwen/settings.json (create it if it doesn't exist). This file lets you define all available models, API keys, and default settings in one place. Quick Setup in 3 Steps Step 1: Create or edit ~/.qwen/settings.json Here is a complete example: { "modelProviders": { "openai": [ { "id": "qwen3-coder-plus", "name": "qwen3-coder-plus", "baseUrl": "https://dashscope.aliyuncs.com/compatible-mode/v1", "description": "Qwen3-Coder via Dashscope", "envKey": "DASHSCOPE_API_KEY" } ] }, "env": { "DASHSCOPE_API_KEY": "sk-xxxxxxxxxxxxx" }, "security": { "auth": { "selectedType": "openai" } }, "model": { "name": "qwen3-coder-plus" }
} Step 2: Understand each field Field What it does modelProviders Declares which models are available and how to connect to them. Keys like openai, anthropic, gemini represent the API protocol. modelProviders[].id The model ID sent to the API (e.g. qwen3-coder-plus, gpt-4o). modelProviders[].envKey The name of the environment variable that holds your API key. modelProviders[].baseUrl The API endpoint URL (required for non-default endpoints). env A fallback place to store API keys (lowest priority; prefer .env files or export for sensitive keys). security.auth.selectedType The protocol to use on startup (openai, anthropic, gemini, vertex-ai). model.name The default model to use when Qwen Code starts. Step 3: Start Qwen Code — your configuration takes effect automatically: qwen Use the /model command at any time to switch between all configured models. More Examples Coding Plan (Alibaba Cloud Bailian) — fixed monthly fee, higher quotas { "modelProviders": { "openai": [ { "id": "qwen3.5-plus", "name": "qwen3.5-plus (Coding Plan)", "baseUrl": "https://coding.dashscope.aliyuncs.com/v1", "description": "qwen3.5-plus with thinking enabled from Bailian Coding Plan", "envKey": "BAILIAN_CODING_PLAN_API_KEY", "generationConfig": { "extra_body": { "enable_thinking": true } } }, { "id": "qwen3-coder-plus", "name": "qwen3-coder-plus (Coding Plan)", "baseUrl": "https://coding.dashscope.aliyuncs.com/v1", "description": "qwen3-coder-plus from Bailian Coding Plan", "envKey": "BAILIAN_CODING_PLAN_API_KEY" } ] }, "env": { "BAILIAN_CODING_PLAN_API_KEY": "sk-xxxxxxxxxxxxx" }, "security": { "auth": { "selectedType": "openai" } }, "model": { "name": "qwen3-coder-plus" }]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/QwenLM/qwen-code</guid>
    </item>
    <item>
      <title><![CDATA[p-e-w/heretic]]></title>
      <link>https://github.com/p-e-w/heretic</link>
      <description><![CDATA[Fully automatic censorship removal for language models Heretic: Fully automatic censorship removal for language models Heretic is a tool that removes censorship (aka "safety alignment") from transformer-based language models without expensive post-training. It combines an advanced implementation of directional ablation, also known as "abliteration" (Arditi et al. 2024, Lai 2025 (1, 2)), with a TPE-based parameter optimizer powered by Optuna. This approach enables Heretic to work completely automatically. Heretic finds high-quality abliteration parameters by co-minimizing the number of refusals and the KL divergence from the original model. This results in a decensored model that retains as much of the original model's intelligence as possible. Using Heretic does not require an understanding of transformer internals. In fact, anyone who knows how to run a command-line program can use Heretic to decensor language models. Running unsupervised with the default configuration, Heretic can produce decensored models that rival the quality of abliterations created manually by human experts: Model Refusals for "harmful" prompts KL divergence from original model for "harmless" prompts google/gemma-3-12b-it (original) 97/100 0 (by definition) mlabonne/gemma-3-12b-it-abliterated-v2 3/100 1.04 huihui-ai/gemma-3-12b-it-abliterated 3/100 0.45 p-e-w/gemma-3-12b-it-heretic (ours) 3/100 0.16 The Heretic version, generated without any human effort, achieves the same level of refusal suppression as other abliterations, but at a much lower KL divergence, indicating less damage to the original model's capabilities. (You can reproduce those numbers using Heretic's built-in evaluation functionality, e.g. heretic --model google/gemma-3-12b-it --evaluate-model p-e-w/gemma-3-12b-it-heretic. Note that the exact values might be platform- and hardware-dependent. The table above was compiled using PyTorch 2.8 on an RTX 5090.) Of course, mathematical metrics and automated benchmarks never tell the whole story, and are no substitute for human evaluation. Models generated with Heretic have been well-received by users (links and emphasis added): "I was skeptical before, but I just downloaded GPT-OSS 20B Heretic model and holy shit. It gives properly formatted long responses to sensitive topics, using the exact uncensored words that you would expect from an uncensored model, produces markdown format tables with details and whatnot. Looks like this is the best abliterated version of this model so far..." (Link to ) "Heretic GPT 20b seems to be the best uncensored model I have tried yet. It doesn't destroy a the model's intelligence and it is answering prompts normally would be rejected by the base model." (Link to ) "[Qwen3-4B-Instruct-2507-heretic] Has been the best unquantized abliterated model that I have been able to run on 16gb vram." (Link to ) Heretic supports most dense models, including many multimodal models, and several different MoE architectures. It does not yet support SSMs/hybrid models, models with inhomogeneous layers, and certain novel attention systems. You can find a small collection of models that have been decensored using Heretic on Hugging Face, and the community has created and published well over 1,000 Heretic models in addition to those. Usage Prepare a Python 3.10+ environment with PyTorch 2.2+ installed as appropriate for your hardware. Then run: pip install -U heretic-llm
heretic Qwen/Qwen3-4B-Instruct-2507 Replace Qwen/Qwen3-4B-Instruct-2507 with whatever model you want to decensor. The process is fully automatic and does not require configuration; however, Heretic has a variety of configuration parameters that can be changed for greater control. Run heretic --help to see available command-line options, or look at config.default.toml if you prefer to use a configuration file. At the start of a program run, Heretic benchmarks the system to determine the optimal batch size to make the most of the available hardware. On an RTX 3090, with the default configuration, decensoring Llama-3.1-8B-Instruct takes about 45 minutes. Note that Heretic supports model quantization with bitsandbytes, which can drastically reduce the amount of VRAM required to process models. Set the quantization option to bnb_4bit to enable quantization. After Heretic has finished decensoring a model, you are given the option to save the model, upload it to Hugging Face, chat with it to test how well it works, or any combination of those actions. Research features In addition to its primary function of removing model censorship, Heretic also provides features designed to support research into the semantics of model internals (interpretability). To use those features, you need to install Heretic with the optional research extra: pip install -U heretic-llm[research] This gives you access to the following functionality: Generate plots of residual vectors by passing --plot-residuals When run with this flag, Heretic will: Compute residual vectors (hidden states) for the first output token, for each transformer layer, for both "harmful" and "harmless" prompts. Perform a PaCMAP projection from residual space to 2D-space. Left-right align the projections of "harmful"/"harmless" residuals by their geometric medians to make projections for consecutive layers more similar. Additionally, PaCMAP is initialized with the previous layer's projections for each new layer, minimizing disruptive transitions. Scatter-plot the projections, generating a PNG image for each layer. Generate an animation showing how residuals transform between layers, as an animated GIF. See the configuration file for options that allow you to control various aspects of the generated plots. Note that PaCMAP is an expensive operation that is performed on the CPU. For larger models, it can take an hour or more to compute projections for all layers. Print details about residual geometry by passing --print-residual-geometry If you are interested in a quantitative analysis of how residual vectors for "harmful" and "harmless" prompts relate to each other, this flag gives you the following table, packed with metrics that can facilitate understanding the same (for gemma-3-270m-it in this case): ┏━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓
┃ Layer ┃ S(g,b) ┃ S(g*,b*) ┃ S(g,r) ┃ S(g*,r*) ┃ S(b,r) ┃ S(b*,r*) ┃ |g| ┃ |g*| ┃ |b| ┃ |b*| ┃ |r| ┃ |r*| ┃ Silh ┃
┡━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩
│ 1 │ 1.0000 │ 1.0000 │ -0.4311 │ -0.4906 │ -0.4254 │ -0.4847 │ 170.29 │ 170.49 │ 169.78 │ 169.85 │ 1.19 │ 1.31 │ 0.0480 │
│ 2 │ 1.0000 │ 1.0000 │ 0.4297 │ 0.4465 │ 0.4365 │ 0.4524 │ 768.55 │ 768.77 │ 771.32 │ 771.36 │ 6.39 │ 5.76 │ 0.0745 │
│ 3 │ 0.9999 │ 1.0000 │ -0.5699 │ -0.5577 │ -0.5614 │ -0.5498 │ 1020.98 │ 1021.13 │ 1013.80 │ 1014.71 │ 12.70 │ 11.60 │ 0.0920 │
│ 4 │ 0.9999 │ 1.0000 │ 0.6582 │ 0.6553 │ 0.6659 │ 0.6627 │ 1356.39 │ 1356.20 │ 1368.71 │ 1367.95 │ 18.62 │ 17.84 │ 0.0957 │
│ 5 │ 0.9987 │ 0.9990 │ -0.6880 │ -0.6761 │ -0.6497 │ -0.6418 │ 766.54 │ 762.25 │ 731.75 │ 732.42 │ 51.97 │ 45.24 │ 0.1018 │
│ 6 │ 0.9998 │ 0.9998 │ -0.1983 │ -0.2312 │ -0.1811 │ -0.2141 │ 2417.35 │ 2421.08 │ 2409.18 │ 2411.40 │ 43.06 │ 43.47 │ 0.0900 │
│ 7 │ 0.9998 │ 0.9997 │ -0.5258 │ -0.5746 │ -0.5072 │ -0.5560 │ 3444.92 │ 3474.99 │ 3400.01 │ 3421.63 │ 86.94 │ 94.38 │ 0.0492 │
│ 8 │ 0.9990 │ 0.9991 │ 0.8235 │ 0.8312 │ 0.8479 │ 0.8542 │ 4596.54 │ 4615.62 │ 4918.32 │ 4934.20 │ 384.87 │ 377.87 │ 0.2278 │
│ 9 │ 0.9992 │ 0.9992 │ 0.5335 │ 0.5441 │ 0.5678 │ 0.5780 │ 5322.30 │ 5316.96 │ 5468.65 │ 5466.98 │ 265.68 │ 267.28 │ 0.1318 │
│ 10 │ 0.9974 │ 0.9973 │ 0.8189 │ 0.8250 │ 0.8579 │ 0.8644 │ 5328.81 │ 5325.63 │ 5953.35 │ 5985.15 │ 743.95 │ 779.74 │ 0.2863 │
│ 11 │ 0.9977 │ 0.9978 │ 0.4262 │ 0.4045 │ 0.4862 │ 0.4645 │ 9644.02 │ 9674.06 │ 9983.47 │ 9990.28 │ 743.28 │ 726.99 │ 0.1576 │
│ 12 │ 0.9904 │ 0.9907 │ 0.4384 │ 0.4077 │ 0.5586 │ 0.5283 │ 10257.40 │ 10368.50 │ 11114.51 │ 11151.21 │ 1711.18 │ 1664.69 │ 0.1890 │
│ 13 │ 0.9867 │ 0.9874 │ 0.4007 │ 0.3680 │ 0.5444 │ 0.5103 │ 12305.12 │ 12423.75 │ 13440.31 │ 13432.47 │ 2386.43 │ 2282.47 │ 0.1293 │
│ 14 │ 0.9921 │ 0.9922 │ 0.3198 │ 0.2682 │ 0.4364 │ 0.3859 │ 16929.16 │ 17080.37 │ 17826.97 │ 17836.03 │ 2365.23 │ 2301.87 │ 0.1282 │
│ 15 │ 0.9846 │ 0.9850 │ 0.1198 │ 0.0963 │ 0.2913 │ 0.2663 │ 16858.58 │ 16949.44 │ 17496.00 │ 17502.88 │ 3077.08 │ 3029.60 │ 0.1611 │
│ 16 │ 0.9686 │ 0.9689 │ -0.0029 │ -0.0254 │ 0.2457 │ 0.2226 │ 18912.77 │ 19074.86 │ 19510.56 │ 19559.62 │ 4848.35 │ 4839.75 │ 0.1516 │
│ 17 │ 0.9782 │ 0.9784 │ -0.0174 │ -0.0381 │ 0.1908 │ 0.1694 │ 27098.09 │ 27273.00 │ 27601.12 │ 27653.12 │ 5738.19 │ 5724.21 │ 0.1641 │
│ 18 │ 0.9184 │ 0.9196 │ 0.1343 │ 0.1430 │ 0.5155 │ 0.5204 │ 190.16 │ 190.35 │ 219.91 │ 220.62 │ 87.82 │ 87.59 │ 0.1855 │
└───────┴────────┴──────────┴─────────┴──────────┴─────────┴──────────┴──────────┴──────────┴──────────┴──────────┴─────────┴─────────┴────────┘
g = mean of residual vectors for good prompts
g* = geometric median of residual vectors for good prompts
b = mean of residual vectors for bad prompts
b* = geometric median of residual vectors for bad prompts
r = refusal direction for means (i.e., b - g)
r* = refusal direction for geometric medians (i.e., b* - g*)
S(x,y) = cosine similarity of x and y
|x| = L2 norm of x
Silh = Mean silhouette coefficient of residuals for good/bad clusters How Heretic works Heretic implements a parametrized variant of directional ablation. For each supported transformer component (currently, attention out-projection and MLP down-projection), it identifies the associated matrices in each transformer layer, and orthogonalizes them with respect to the relevant "refusal direction", inhibiting the expression of that direction in the result of multiplications with that matrix. Refusal directions are computed for each layer as a difference-of-means between the first-token residuals for "harmful" and "harmless" example prompts. The ablation process is controlled by several optimizable parameters: direction_index: Either the index of a refusal direction, or the special value per layer, indicating that each layer should be ablated using the refusal direction associated with that layer. max_weight, max_weight_position, min_weight, and min_weight_distance: For each component, these parameters describe the shape and position of the ablation weight kernel over the layers. The following diagram illustrates this: Heretic's main innovations over existing abliteration systems are: The shape of the ablation weight kernel is highly flexible, which, combined with automatic parameter optimization, can improve the compliance/quality tradeoff. Non-constant ablation weights were previously explored by Maxime Labonne in gemma-3-12b-it-abliterated-v2. The refusal direction index is a float rather than an integer. For non-integral values, the two nearest refusal direction vectors are linearly interpolated. This unlocks a vast space of additional directions beyond the ones identified by the difference-of-means computation, and often enables the optimization process to find a better direction than that belonging to any individual layer. Ablation parameters are chosen separately for each component. I have found that MLP interventions tend to be more damaging to the model than attention interventions, so using different ablation weights can squeeze out some extra performance. Prior art I'm aware of the following publicly available implementations of abliteration techniques: AutoAbliteration abliterator.py wassname's Abliterator ErisForge Removing refusals with HF Transformers deccp Note that Heretic was written from scratch, and does not reuse code from any of those projects. Acknowledgments The development of Heretic was informed by: The original abliteration paper (Arditi et al. 2024) Maxime Labonne's article on abliteration, as well as some details from the model cards of his own abliterated models (see above) Jim Lai's articles describing "projected abliteration" and "norm-preserving biprojected abliteration" Citation If you use Heretic for your research, please cite it using the following BibTeX entry: @misc{heretic, author = {Weidmann, Philipp Emanuel}, title = {Heretic: Fully automatic censorship removal for language models}, year = {2025}, publisher = {GitHub}, journal = {GitHub repository}, howpublished = {\url{https://github.com/p-e-w/heretic}}
} License Copyright 2025-2026 Philipp Emanuel Weidmann (pew@worldwidemann.com) + contributors This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/. By contributing to this project, you agree to release your contributions under the same license.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/p-e-w/heretic</guid>
    </item>
    <item>
      <title><![CDATA[harvard-edge/cs249r_book]]></title>
      <link>https://github.com/harvard-edge/cs249r_book</link>
      <description><![CDATA[Introduction to Machine Learning Systems Machine Learning Systems Principles and Practices of Engineering Artificially Intelligent Systems English • 中文 • 日本語 • 한국어 Read Online • TinyTorch • Download PDF • Download EPUB • Explore Ecosystem Hardcopy edition coming 2026 with MIT Press. Mission The world is rushing to build AI systems. It is not engineering them. That gap is what we mean by AI engineering. AI engineering is the discipline of building efficient, reliable, safe, and robust intelligent systems that operate in the real world, not just models in isolation. Our mission: Establish AI engineering as a foundational discipline, alongside software engineering and computer engineering, by teaching how to design, build, and evaluate end to end intelligent systems. The long term impact of AI will be shaped by engineers who can turn ideas into working, dependable systems. What’s in this repo This repository is the open learning stack for AI systems engineering. It includes the textbook source, TinyTorch, hardware kits, and upcoming co-labs that connect principles to runnable code and real devices. Start Here Choose a path based on your goal. READ Start with the textbook. Try Chapter 1 and the Benchmarking chapter. BUILD Start TinyTorch with the getting started guide. Begin with Module 01 and work up from CNNs to transformers and the MLPerf benchmarks. DEPLOY Pick a hardware kit and run the labs on Arduino, Raspberry Pi, and other edge devices. CONNECT Say hello in Discussions. We will do our best to reply. The Learning Stack The learning stack below shows how the textbook connects to hands on work and deployment. Read the textbook, then pick your path: ┌───────────────────────────────────────────────────────────────────────────────┐
│ │
│ MACHINE LEARNING SYSTEMS │
│ Read the Textbook │
│ │
│ Theory • Concepts • Best Practices │
│ │
└───────────────────────────────────────┬───────────────────────────────────────┘ │ ┌─────────────┼─────────────┐ │ │ │ ▼ ▼ ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│ HANDS-ON ACTIVITIES │
│ (pick one or all) │
│ │
│ ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │
│ │ │ │ │ │ │ │
│ │ SOFTWARE │ │ TINYTORCH │ │ HARDWARE │ │
│ │ CO-LABS │ │ FRAMEWORK │ │ LABS │ │
│ │ │ │ │ │ │ │
│ │ EXPLORE │ │ BUILD │ │ DEPLOY │ │
│ │ │ │ │ │ │ │
│ │ Run controlled │ │ Understand │ │ Engineer under │ │
│ │ experiments on │ │ frameworks by │ │ real constraints│ │
│ │ latency, memory,│ │ implementing │ │ memory, power, │ │
│ │ energy, cost │ │ them │ │ timing, safety │ │
│ │ │ │ │ │ │ │
│ │ (coming 2026) │ │ │ │ Arduino, Pi │ │
│ └─────────────────┘ └─────────────────┘ └─────────────────┘ │
│ │
│ EXPLORE BUILD DEPLOY │
│ │
└───────────────────────────────────────┬───────────────────────────────────────┘ │ ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│ │
│ AI OLYMPICS │
│ Prove Mastery │
│ │
│ Compete across all tracks • University teams • Public leaderboards │
│ │
│ (coming 2026) │
│ │
└───────────────────────────────────────────────────────────────────────────────┘ Component What You Do Link READ Textbook Understand ML systems concepts book/ EXPLORE Software Co-Labs Run controlled experiments on latency, memory, energy, cost Coming 2026 BUILD TinyTorch Understand frameworks by implementing them tinytorch/ DEPLOY Hardware Kits Engineer under real constraints: memory, power, timing, safety kits/ PROVE AI Olympics Compete and benchmark across all tracks Coming 2026 What each path teaches: EXPLORE teaches why — Understand tradeoffs. Change batch sizes, precision, model architectures and see how latency, memory, and accuracy shift. BUILD teaches how — Understand internals. Implement autograd, optimizers, and attention from scratch to see how TensorFlow and PyTorch actually work. DEPLOY teaches where — Understand constraints. Face real memory limits, power budgets, and latency requirements on actual hardware. What You Will Learn This textbook teaches you to think at the intersection of machine learning and systems engineering. Each chapter bridges algorithmic concepts with the infrastructure that makes them work in practice. The ML Systems Bridge ML Concept Systems Concept What You Learn Model parameters Memory constraints How to fit large models on resource-limited devices Inference latency Hardware acceleration How GPUs, TPUs, and accelerators execute neural networks Training convergence Compute efficiency How mixed-precision and optimization techniques reduce cost Model accuracy Quantization and pruning How to compress models while preserving performance Data requirements Pipeline infrastructure How to build efficient data loading and preprocessing Model deployment MLOps practices How to monitor, version, and update models in production Privacy constraints On-device learning How to train and adapt models without sending data to the cloud Book Structure Part Focus Chapters I. Foundations Core concepts Introduction, ML Systems, DL Primer, Architectures II. Design Building blocks Workflow, Data Engineering, Frameworks, Training III. Performance Making it fast Efficient AI, Optimizations, HW Acceleration, Benchmarking IV. Deployment Making it work MLOps, On-device Learning, Privacy, Robustness V. Trust Making it right Responsible AI, Sustainable AI, AI for Good VI. Frontiers What's next Emerging trends and future directions What Makes This Different This is a living textbook. We keep it updated as the field grows, with community input along the way. AI may feel like it is moving at lightning speed, but the engineering building blocks that make it work do not change as quickly as the headlines. This project is built around those stable foundations. Think of it like LEGO. New sets arrive all the time, but the bricks themselves stay the same. Once you learn how the bricks fit together, you can build anything. Here, those "AI bricks" are the solid systems principles that make AI work. Whether you are reading a chapter, running a lab, or sharing feedback, you are helping make these ideas more accessible to the next learner. Research to Teaching Loop We use the same loop for research and teaching: define the system problem, build a reference implementation, benchmark it, then turn it into curriculum and tooling so others can reproduce and extend it. Loop Step Research Artifacts Teaching Artifacts Measure Benchmarks, suites, metrics Benchmarking chapter, assignments Build Reference systems, compilers, runtimes TinyTorch modules, co-labs Deploy Hardware targets, constraints, reliability Hardware labs, kits Support This Work We are working toward 1 million learners by 2030 so that AI engineering becomes a shared, teachable discipline, not a collection of isolated practices. Every star, share, and contribution helps move this effort forward. Why GitHub Stars Matter What gets measured gets improved. Each star is a learner, educator, or supporter who believes AI systems should be engineered with rigor and real world constraints in mind. 1 learner → 10 learners → 100 learners → 1,000 learners → 10,000 learners → 100,000 learners → 1M learners Stars are not the goal. They are a signal. A visible, growing community makes it easier for universities, foundations, and industry partners to adopt this material, donate hardware, and fund workshops. That momentum lowers the barrier for the next institution, the next classroom, and the next cohort of learners. Support raised through this signal flows into Open Collective and funds concrete outcomes such as TinyML4D workshops, hardware kits for underserved classrooms, and the infrastructure required to keep this resource free and open. One click can unlock the next classroom, the next contributor, and the next generation of AI engineers. Fund the Mission All contributions go to Open Collective, a transparent fund that supports educational outreach. Community and Resources Resource Description Textbook Interactive online textbook TinyTorch Build ML frameworks from scratch Hardware Kits Deploy to Arduino, Raspberry Pi, edge devices Ecosystem Resources, workshops, and community Discussions Questions and ideas Contributing We welcome contributions to the book, TinyTorch, and hardware kits! I want to... Go here Fix a typo or improve a chapter book/docs/CONTRIBUTING.md Add a TinyTorch module or fix a bug tinytorch/CONTRIBUTING.md Improve hardware labs kits/README.md Report an issue GitHub Issues Ask a question GitHub Discussions Citation &amp; License Citation @inproceedings{reddi2024mlsysbook, title = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering}, author = {Reddi, Vijay Janapa}, booktitle = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)}, pages = {41--42}, year = {2024}, organization = {IEEE}, url = {https://mlsysbook.org}]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:24 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/harvard-edge/cs249r_book</guid>
    </item>
    <item>
      <title><![CDATA[8 Things You Didn't Know About Code Mode]]></title>
      <link>https://dev.to/goose_oss/8-things-you-didnt-know-about-code-mode-4h71</link>
      <description><![CDATA[Agents fundamentally changed how we program. They enable developers to move faster by disintermediating the traditional development workflow. This means less time switching between specialized tools and fewer dependencies on other teams. Now that agents can execute complicated tasks, developers face a new challenge: using them effectively over long sessions.
The biggest challenge is context rot. Because agents have limited memory, a session that runs too long can cause them to "forget" earlier instructions. This leads to unreliable outputs, frustration, and subtle but grave mistakes in your codebase. One promising solution is Code Mode. Instead of describing dozens of separate tools to an LLM, Code Mode allows an agent to write code that calls those tools programmatically, reducing the amount of context the model has to hold at once. While many developers first heard about Code Mode through Cloudflare's blog post, fewer understand how it works in practice. I have been using Code Mode for a few months and recently ran a small experiment. I asked goose to fix its own bug where the Gemini model failed to process images in the CLI but worked in the desktop app, then open a PR. The fix involved analyzing model configuration, tracing image input handling through the pipeline, and validating behavior across repeated runs. I ran the same task twice: once with Code Mode enabled and once without it.
Here is what I learned from daily use and my experiment.
In fact, it uses MCP under the hood. MCP is a standard that lets AI agents connect to external tools and data sources. When you install an MCP server in an agent, that MCP server exposes its capabilities as MCP tools. For example, goose's primary MCP server called the developer extension exposes tools like shell enabling goose to run commands and text_editor, so goose can view and edit files. Code Mode wraps your MCP tools as JavaScript modules, allowing the agent to combine multiple tool calls into a single step. Code Mode is a pattern for how agents interact with MCP tools more efficiently.
Code Mode support landed in goose v1.17.0 in December 2025. It ships as a platform extension called "Code Mode" that you can enable in the desktop app or CLI.
To enable it:
Desktop app: Click the extensions icon and toggle on "Code Mode"
CLI: Run goose configure and enable the Code Mode extension
Since its initial implementation, we've added so many improvements!
Every time you install an MCP server (or "extension" in the goose ecosystem), it adds a significant amount of data to your agent's memory. Every tool comes with a tool definition describing what the tool does, the parameters it accepts, and what it returns. This helps the agent understand how to use the tool.
These definitions consume space in your agent's context window. For example, if a single definition takes 500 tokens and an extension has five tools, that is 2,500 tokens gone before you even start. If you use multiple extensions, you could easily double or even decuple that number.
Without Code Mode, your context window could look like this:
[System prompt: ~1,000 tokens]
[Tool: developer__shell - 500 tokens]
[Tool: developer__text_editor - 600 tokens]
[Tool: developer__analyze - 400 tokens]
[Tool: slack__send_message - 450 tokens]
[Tool: slack__list_channels - 400 tokens]
[Tool: googledrive__search - 500 tokens]
[Tool: googledrive__download - 450 tokens]
... and so on for every tool in every extension As your session progresses, useful context gets crowded out by tool definitions you aren't even using: the code you are discussing, the problem you are solving, or the instructions you previously gave. This leads to performance degradation and memory loss. While I used to recommend disabling unused MCP servers, Code Mode offers a better fix. It uses three tools that help the agent discover what tools it needs on demand rather than having every tool definition loaded upfront:
search_modules - Find available extensions
read_module - Learn what tools an extension offers
execute_code - Run JavaScript that uses those tools
I wanted to see how true this was so I ran an experiment: I had goose solve a user's bug and put up a PR with and without code mode. Code Mode used 30% fewer tokens for the same task. Metric
With Code Mode
Without Code Mode Total tokens
23,339
33,648 Input tokens
23,128
33,560 The token savings do not just come from loading fewer tool definitions upfront. Code Mode also handles the "active" side of the conversation through a method called batching.
When you ask an agent to do something, it typically breaks your request into individual steps, each requiring a separate tool call. You can see these calls appear in your chat as the agent executes the tasks. For example, if you ask goose to "check the current branch, show me the diff, and run the tests," it might run four individual commands: developer__shell → git branch --show-current developer__shell → git status developer__shell → git diff developer__shell → cargo test Each of these calls adds a new layer to the conversation history that goose has to track. Batching combines these into a single execution. When you turn Code Mode on and give that same prompt, you will see just one tool call: Code Execution: Execute Code generating... Inside that one execution, it batches all the commands into a script:
import { shell } from "developer"; const branch = shell({ command: "git branch --show-current" });
const status = shell({ command: "git status" });
const diff = shell({ command: "git diff" });
const tests = shell({ command: "cargo test" }); As a user, you see the same results, but the agent only has to remember one interaction instead of four. By reducing these round trips, Code Mode keeps the conversation history concise so the agent can maintain focus on the task at hand.
When an agent has access to dozens of tools, it sometimes makes a "logical" choice that is technically wrong for your environment. This happens because, in a standard setup, the agent picks tools from a flat list based on short text descriptions. This can lead to a massive waste of time and tokens when the agent picks a tool that sounds right but lacks the necessary context.
I saw this firsthand during my experiments. I had an extension enabled called agent-task-queue, which is designed to run background tasks with timeouts.
When I asked goose to run the tests for my PR, it looked at the available tools and saw agent-task-queue. The LLM reasoned that a test suite is a "long-running task," making that extension a perfect fit. It chose the specialized tool over the generic shell.
However, the tool call failed immediately:
FAILED exit=127 0.0s
/bin/sh: cargo: command not found My environment was not configured to use that specific extension for my toolchain. goose made a reasonable choice based on the description, but it was the wrong tool for my actual setup.
In the Code Mode session, this mistake never happened. Code Mode changes how the agent interacts with its capabilities by requiring explicit import statements.
Instead of browsing a menu of names, goose had to be intentional about which module it was using. It chose to import from the developer module:
import { shell } from "developer"; const test = shell({ command: "cargo test -p goose --lib formats::google" }); By explicitly importing developer, Code Mode ensured the tests ran in my actual shell environment.
goose is more than an agent; it's also an ACP (Agent Client Protocol) server. This means you can connect it to any editor that supports ACP, like Zed or Neovim. Plus, any MCP server you use in goose will work there, too.
I wanted to try this myself, so I set up Neovim to connect to goose with Code Mode enabled. Here's the configuration I used:
{ "yetone/avante.nvim", build = "make", event = "VeryLazy", opts = { provider = "goose", acp_providers = { ["goose"] = { command = "goose", args = { "acp", "--with-builtin", "code_execution,developer" }, }, }, }, dependencies = { "nvim-lua/plenary.nvim", "MunifTanjim/nui.nvim", },
} The key line is the one where I enable Code Mode right inside the editor config:
args = { "acp", "--with-builtin", "code_execution,developer" }, To test it, I asked goose to list my Rust files and count the lines of code. Instead of a long stream of individual shell commands cluttering my Neovim buffer, I saw one singular tool call: Code Execution. It worked exactly like it does in the desktop app. This portability means you can build a powerful, efficient agent workflow and take it with you to whatever environment you're most comfortable in. I ran my experiments using Claude Opus 4.5. Your results may vary depending on which model you use.
Code Mode requires the LLM to do things that not all models do equally well:
Write valid JavaScript - The model has to generate syntactically correct code. Models with stronger code generation capabilities will produce fewer errors.
Follow the import pattern - Code Mode expects the LLM to import tools from modules like import { shell } from "developer". Some models might try to call tools directly without importing, which will fail.
Use the discovery tools - Before writing code, the LLM should call search_modules and read_module to learn what tools are available. Some models skip this step and guess, leading to hallucinated tool names.
Handle errors gracefully - When a code execution fails, the model needs to read the error, understand what went wrong, and try again. Some models are better at this feedback loop than others.
If Code Mode is not working well for you, try switching models. A model that excels at code generation and instruction following will generally perform better with Code Mode than one optimized for other tasks.
Code Mode adds overhead. Before executing anything, the LLM has to:
Call search_modules to find available extensions
Call read_module to learn what tools an extension offers
Write JavaScript code
Call execute_code to run it
For simple, single-tool tasks, this overhead is not worth it. If you just need to run one shell command or view one file, regular tool calling is faster.
Based on my experiments, here is when Code Mode makes sense: Use Code Mode When
Skip Code Mode When You have multiple extensions enabled
You only have 1-2 extensions Your task involves multi-step orchestration
Your task is a single tool call You want longer sessions without context rot
Speed matters more than context longevity You are working across multiple editors
You are doing a quick one-off task If you want to experiment with Code Mode, here are some resources:
Documentation:
ACP client setup
Extensions guide
Previous posts:
Code Mode MCP in goose by Alex Hancock
Code Mode Doesn't Replace MCP by me
Community:
Join our Discord to share what you learn
File issues on GitHub if something does not work as expected
Run your own experiments and let us know what you find.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:54:38 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/goose_oss/8-things-you-didnt-know-about-code-mode-4h71</guid>
    </item>
    <item>
      <title><![CDATA[How I Built a Blackbox Log Analyzer to Auto-Tune FPV Drone PIDs]]></title>
      <link>https://dev.to/fpvtune/how-i-built-a-blackbox-log-analyzer-to-auto-tune-fpv-drone-pids-1e3d</link>
      <description><![CDATA[I fly FPV drones as a hobby, and if you've ever tried tuning Betaflight PIDs manually, you know the pain. Record a flight, pull the blackbox log, open PIDtoolbox, stare at gyro traces for an hour, change one number, fly again... repeat forever.
So I built a tool that does it automatically. Here's the story and some of the technical bits.
Betaflight's PID controller has ~15 parameters that affect how your drone flies. Most pilots either copy someone else's tune (which rarely works because every build is different) or spend days doing test flights and analyzing blackbox logs.
The existing tools like PIDtoolbox are great for visualizing data, but they don't tell you what to actually change. You still need to know what a noisy gyro trace means and which PID term to adjust.
I wanted something that could:
Parse Betaflight blackbox logs (.bbl files)
Analyze the frequency response and noise characteristics
Suggest specific PID values based on the analysis
The blackbox format is basically a compressed binary stream of sensor data — gyro, accelerometer, motor outputs, RC inputs, etc. sampled at up to 8kHz.
Betaflight's blackbox logs use a custom encoding with variable-length integers and predictive coding. Here's a simplified look at how the parsing works:
def decode_blackbox_frame(data, field_defs): values = {} for field in field_defs: if field.encoding == 'signed_vlq': val = read_signed_vlq(data) elif field.encoding == 'unsigned_vlq': val = read_unsigned_vlq(data) elif field.encoding == 'tag8_8svb': val = read_tag8_8svb(data) values[field.name] = val * field.scale return values The tricky part is handling the different Betaflight firmware versions — the log format changes between versions and you need to parse the header to figure out which fields are present.
Once you have the raw gyro and PID error data, the real magic happens in the frequency domain. I use FFT to identify:
Noise floor: How much electrical/mechanical noise your quad produces
Motor resonance peaks: Frequencies where your motors/props create vibrations PID response: How well the current tune tracks setpoint changes import numpy as np def analyze_axis(gyro_data, pid_error, sample_rate): # FFT of gyro data to find noise profile freqs = np.fft.rfftfreq(len(gyro_data), 1/sample_rate) gyro_fft = np.abs(np.fft.rfft(gyro_data)) # Find dominant noise frequencies noise_peaks = find_peaks(gyro_fft, height=np.mean(gyro_fft) * 3) # Analyze step response from setpoint changes step_indices = find_setpoint_steps(pid_error) overshoot = calculate_overshoot(gyro_data, step_indices) settling_time = calculate_settling(gyro_data, step_indices) return { 'noise_peaks': freqs[noise_peaks], 'overshoot': overshoot, 'settling_time': settling_time } This is where it gets interesting. Based on the frequency analysis, the tool adjusts PIDs following some basic control theory:
High overshoot on roll/pitch → reduce P gain or increase D gain
Slow response → increase P gain
High frequency oscillation → reduce D gain, check D lowpass filter
Prop wash oscillation (low frequency, shows up in throttle cuts) → adjust I gain and D term
The tool outputs specific numbers you can paste directly into Betaflight configurator.
I turned this into a web tool called FPVtune — you upload your blackbox log and it spits out PID recommendations. No software to install, works in the browser.
The source code for the analysis engine is on GitHub if you want to dig into the algorithms.
It's $9.90 for the full analysis, but I have a beta code for the DEV community: FPVTUNE-BETA-2026 — just enter it on the activation page to get free access.
Building this taught me a lot about:
Signal processing in Python (scipy.signal is your friend)
Working with binary data formats that have zero documentation
The gap between "analyzing data" and "making actionable suggestions"
If you fly FPV or work with any kind of PID control systems (robotics, etc.), I'd love to hear how you approach tuning. The control theory fundamentals are the same whether you're tuning a drone or a robot arm.
Repo: github.com/chugzb/betaflight-pid-autotuning]]></description>
      <pubDate>Thu, 19 Feb 2026 03:34:17 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/fpvtune/how-i-built-a-blackbox-log-analyzer-to-auto-tune-fpv-drone-pids-1e3d</guid>
    </item>
    <item>
      <title><![CDATA[INSTALAÇÃO OUTLINE WIKI EM CONTAINER PROXMOX]]></title>
      <link>https://dev.to/lucasesg/instalacao-outline-wiki-5a00</link>
      <description><![CDATA[1. instalar via comunidade proxmox
https://community-scripts.github.io/ProxmoxVE/scripts?id=outline
Acesse parte How to install e copie o códgo e execute no terminal do Proxmox. Recomendo apenas:
2 cpu OBSERVAÇÃO:
Verbose é opcional. Ativado YES você acompanha pelo shell tudo o que o script está fazendo.
2. instalar nginx e certificado https com mkcert
NGINX
mkdir -p /etc/outline-ssl
apt update
apt install -y nginx
systemctl status nginx
MKCERT
curl -s https://api.github.com/repos/FiloSottile/mkcert/releases/latest \
chmod +x mkcert-v*-linux-amd64
sudo mv mkcert-v*-linux-amd64 /usr/local/bin/mkcert
mkcert -install mkdir -p /opt/outline/mkcert cd /opt/outline/mkcert mkcert 192.168.254.95
Isso vai criar dois arquivos, algo como:
192.168.254.95.pem
nano /etc/nginx/sites-available/outline.conf
server { listen 443 ssl; server_name 192.168.254.95; ssl_certificate /opt/outline/mkcert/192.168.254.95.pem; ssl_certificate_key /opt/outline/mkcert/192.168.254.95-key.pem; http2 on; location / { proxy_pass http://127.0.0.1:3000; # porta interna do Outline proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # WebSocket proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "upgrade"; }
} ln -s /etc/nginx/sites-available/outline /etc/nginx/sites-enabled/
3. configuração https websocket
OBSERVAÇÃO:
Muito dos procedimentos a seguir quando reiniciar os serviços vai ficar na página 502 Bad Gateway. A dica é ter paciência pois depois de um tempo vai abrir normalmente. Vamos em /opt/outline/.env e altere as seguintes linhas:
URL=https://192.168.250.95 (https)
COLLABORATION_URL=wss://192.168.250.95 (wss)
FORCE_HTTPS=false (mantém false)
Verifique se consegue acessar a página do outline via HTTPS e não é necessário mais PORTA 3000. Acesso por https://ipoutline
4. configurar conexão API do slack para conectar ao outline
Acesse o site https://slack.com/intl/pt-br/ e aperte em COMEÇAR
Primeiro, insira seu e-mail e assim vai receber um código e aceite os termos.
Agora no google pesquise: api slack ou acesse direto o site https://api.slack.com/apps
Clique em Creat an app Escolha From scratch e de um nome ao seu app e escolha o workspace. Clique em Creat App. Muita atenção, os próximos passos são muito importante e dados sensíveis! Copie as seguintes informações e vamos em /opt/outline/.env para configurar
App ID = SLACK_APP_ID
Na mesma página da API Slack vamos na opção OAuth &amp; Permissions e deça a página até https://ipoutline/auth/slack.callback/
Reincie o serviço outline no Linux e na página inicia se conecte com a conta que você criou na Slack. Agora na página você já está dentro do Outline wiki on-premise. O próximo passo é configurar o acesso via e-mail para os demais usuarios. 5. configurar o arquivo .env para envio de e-mail via smtp
Acesse o arquivo de configuração /opt/outline/.env e procure pela parte SMTP. Altere para as seguintes informações:
SMTP_HOST= tipo de critgrafia do seu e-mail
seuemail@gmail.com
seuemail@gmail.com
Depois de salvar o arquivo reinciei os serviços Outline e dentro do Outline acesse a configuração &gt; Authentication e ative envio de SMTP. Recomendo por enquanto desativar Passkeys (POR ENQUANTO). Agora faça o envio convite de acesso ou sai da conta e verifique se na página inicial aparece “Continuar com E-mail”.
Pronto, seu Outilne Wiki está pronto para uso! Boa construção na documentação.]]></description>
      <pubDate>Thu, 19 Feb 2026 03:22:14 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/lucasesg/instalacao-outline-wiki-5a00</guid>
    </item>
    <item>
      <title><![CDATA[Agenda du Libre pour la semaine 7 de l'année 2026]]></title>
      <link>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Calendrier Web, regroupant des événements liés au Libre (logiciel, salon, atelier, install party, conférence), annoncés par leurs organisateurs. Voici un récapitulatif de la semaine à venir. Le détail de chacun de ces 41 événements (France: 39, Internet: 2) est en seconde partie de dépêche. lien nᵒ 1 : April
lien nᵒ 2 : Agenda du Libre
lien nᵒ 3 : Carte des événements
lien nᵒ 4 : Proposer un événement
lien nᵒ 5 : Annuaire des organisations
lien nᵒ 6 : Agenda de la semaine précédente
lien nᵒ 7 : Agenda du Libre Québec Sommaire
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
[Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
[FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
[FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
[FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
[Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
[FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
[FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
[FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
[FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
[FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
[FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
[FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
[FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
[FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
[FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
[FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
[FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
[FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
[FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
[FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
[FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
[FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
[FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Wimille] Retrouvez votre liberté numérique – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Pollionnay] Install partie – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Auray] Install Party : adieu Windows, bonjour le Libre – Le samedi 14 février 2026 de 10h00 à 16h00.
[FR Ivry sur Seine] Cours de l’École du Logiciel Libre – Le samedi 14 février 2026 de 10h30 à 18h30.
[FR Illzach] Atelier Linux – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Illkirch-Graffenstaden] Atelier numérique éthique HOP par Alsace Réseau Neutre – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Fontenay-le-Fleury] Conférence : Présentation Git – Le samedi 14 février 2026 de 14h00 à 16h00.
[FR Ramonville St Agne] WordPress : Personnalisation – Le samedi 14 février 2026 de 14h00 à 18h00.
[FR Juvisy-sur-Orge] Permanence GNU/Linux – Le samedi 14 février 2026 de 14h30 à 17h00.
[FR Quimper] Permanence Linux Quimper – Le samedi 14 février 2026 de 16h00 à 18h00.
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
Tous les lundis de 10h à 17h sans interruption, l’association Prends toi en main / atelier abcpc, propose install party, suivi, dépannage, formation et revalorisation à petit prix sous Linux exclusivement.
L’atelier abcpc existe depuis plus de 10 ans et milite exclusivement pour les logiciels libres.
Médiathèque, Médiathèque, 4 place Dastros, Saint Clar, Occitanie, France
https://www.facebook.com/PrendsToiEnMain
linux, permanence, dépannage, formation, adieu-windows, libres, logiciels-libres, abcpc, prends-toi-en-main, install-party [Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
Vous voulez vous engager pour une cause, rencontrer de nouvelles personnes et découvrir la cartographie participative et humanitaire? CartONG vous invite à participer à un ou plusieurs mapathons en ligne! ​​
Venez cartographier les régions encore absentes des cartes pour soutenir les organisations humanitaires et de solidarité internationale qui ont besoin de cartes précises et à jour pour agir plus efficacement en cas de crise ou initier des projets de développement local.
Les ateliers de cartographie sont organisés dans le cadre du projet Missing Maps, qui a pour objectif de cartographier de façon préventive les régions vulnérables aux catastrophes naturelles, crises sanitaires, environnementales, aux conflits et à la pauvreté. On peut penser qu’aujourd’hui toutes les parties du monde sont cartographiées, mais en réalité de nombreuses régions ne possèdent encore aucune carte!
​ Pour qui? Pas besoin d’être un·e expert·e, les ateliers sont accessibles à tout le monde!
​ Où ? 100% en ligne! Un lien de connexion vous sera envoyé après votre inscription
​ ? Avec la plateforme de cartographie libre et contributive OpenStreetMap (OSM, le «Wikipédia des cartes») tout le monde peut participer à la cartographie de n’importe quelle zone de la planète: il suffit d’un ordinateur, d’une souris et d’une connexion internet! Accessibles à tout·es, nous serons là pour vous accompagner pour vos premiers pas avec OSM.
Le programme des mapathons
18h00: Introduction, présentation de la cartographie collaborative et solidaire et démonstration OSM pour les nouveaux·elles
18h30: On cartographie tous ensemble sur un projet
20h00: Fin du mapathon, conclusion sur les contributions de la soirée
Pour s’inscrire c’est par ici
Si vous avez besoin de plus d’info, vous pouvez nous contacter directement à l’adresse suivante: missingmaps@cartong.org
Internet
https://www.cartong.org
cartographie, cartong, osm, humanitaire, libre, mapathon [FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
L’Écurieux et Espéranto-Gironde vous invitent à la découverte de l’espéranto à Sainte Hélène le:
Lundi 9 février 2026 à 18h00
Foyer des sociétés
Allée du Stade
33480 Sainte-Hélène
Venez découvrir cette langue FRATERNELLE, libre, neutre, 15 fois plus facile à apprendre que le français, parlée par Freinet, Jean Jaurès, Louis Lumière, Jean-Paul II, Jules Verne…
Inventée en 1887, l’espéranto est actuellement parlé dans plus de 120 pays sur les 5 continents et est actuellement utilisé par des millions de personnes dans le monde, pour voyager, correspondre, découvrir d’autres cultures, se faire des amis…
Il y aura la projection d’un documentaire suivi de questions débat.
La rencontre est ouverte à tous, espérantistes ou non, membre de l’Écurieux ou non.
Entrée libre et gratuite.
Foyer des sociétés, Foyer des sociétés, allée du Stade, Sainte-Hélène, Nouvelle-Aquitaine, France
https://esperanto-gironde.fr/2026/01/decouverte-de-lesperanto-a-sainte-helene/
espéranto, langue-libre, langage, decouverte [FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
Tous les lundis soir de 19h à 22h (hors jours fériés) à la Bricoleuse.
Rencontrer les bénévoles, poser des questions sur le libre ou l’informatique, les logiciels, l’hébergement, passer de Windows à Linux.
Pour passer votre ordinateur sous linux, nous vous invitons à nous prévenir avant votre passage: contact@alolise.org.
La Bricoleuse, La Bricoleuse, 27 rue de la Ville, Saint-Étienne, Auvergne-Rhône-Alpes, France
https://alolise.org
install-party, aide, logiciel-libre, entraide, alolise, permanence, linux, gnu-linux [FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
Après un rappel sur le générateur de cartes personnalisées uMap, Binnette nous présentera:
Une démo de ses cartes uMap: différents besoins et cas d’usage.
La création de cartes uMap avec des données Overpass
Des scripts pythons de génération de carte uMap
Les limitations de uMap et les problèmes de performance
Informations pratiques
Lundi 9 février 19h – 21h
À la Turbine.coop, 5 Esplanade Andry Farcy, 38000 Grenoble (entrée sur le côté du bâtiment, nous serons dans la salle de réunion au rez-de-chaussée)
Atelier ouvert à tous et à toutes
Inscription souhaitée via ce formulaire La Turbine Coop, La Turbine Coop, 3-5 esplanade Andry Farcy, Grenoble, Auvergne-Rhône-Alpes, France https://wiki.openstreetmap.org/wiki/Grenoble_groupe_local/Agenda#Lundi_9_f%C3%A9vrier_:_atelier_uMap_avanc%C3%A9 openstreetmap, osm, osm-grenoble, umap, logiciels-libres, atelier, rencontre [FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
Vous pouvez venir pour:
découvrir ce que peut vous apporter le numérique libre, éthique et écoresponsable
obtenir de l’assistance pour l’utilisation des systèmes d’exploitation libres (GNU/Linux pour ordinateur et /e/OS pour smartphones)
obtenir de l’assistance pour l’utilisation des logiciels libres (ex: Firefox, Thunderbird, LibreOffice, VLC) et des services Internet éthiques (ex: mél et cloud, travail collaboratif en ligne).
vous faire aider à installer GNU/Linux sur votre ordinateur ou /e/OS sur votre Fairphone, si vous n’avez pas pu venir à notre Install Partie.
Nous vous recommandons d’effectuer une sauvegarde avant de venir, si vous n’êtes pas en mesure de faire, veuillez apporter un support de sauvegarde (disque dur externe ou clé USB de capacité suffisante).
Nos services sont gratuits, vous pourrez néanmoins faire un don à notre association « Libérons nos ordis ».
Remarque: vous pouvez même apporter un ordinateur de bureau – uniquement l’unité centrale (la tour) – nous avons des écrans, claviers et souris à brancher dessus.
VEUILLEZ VOUS INSCRIRE ICI: https://calc.ouvaton.coop/InscriptionPermanenceNumeriqueLibreRouen
La Base, La Base, 5 rue Geuffroy, Rouen, Normandie, France
libérons-nos-ordis, gnu-linux, logiciels-libres, assistance, linux, numérique [FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
Présentation de différents outils concernant les logiciels libres.
Assistance technique.
De préférence sur RDV directement sur le site de l’asso
Maison des associations, Maison des associations, 2 rue des Corroyeurs, Dijon, Bourgogne-Franche-Comté, France
https://desobs.fr
informatique-libre, installation, réemploi, réparation, résilience, résoudre, atelier [Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
L’émission Libre à vous! de l’April est diffusée chaque mardi de 15 h 30 à 17 h sur radio Cause Commune sur la bande FM en région parisienne (93.1) et sur le site web de la radio.
Le podcast de l’émission, les podcasts par sujets traités et les références citées sont disponibles dès que possible sur le site consacré à l’émission, quelques jours après l’émission en général.
Les ambitions de l’émission Libre à vous!
Découvrez les enjeux et l’actualité du logiciel libre, des musiques sous licences libres, et prenez le contrôle de vos libertés informatiques.
Donner à chacun et chacune, de manière simple et accessible, les clefs pour comprendre les enjeux mais aussi proposer des moyens d’action, tels sont les objectifs de cette émission hebdomadaire.
L’émission dispose:
d’un flux RSS compatible avec la baladodiffusion d’une lettre d’information à laquelle vous pouvez vous inscrire (pour recevoir les annonces des podcasts, des émissions à venir et toute autre actualité en lien avec l’émission)
d’un salon dédié sur le webchat de la radio Radio Cause Commune, Radio Cause Commune, Internet https://www.libreavous.org april, radio, cause-commune, libre-à-vous [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, partager leurs connaissances, échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup(https://www.meetup.com/fr-fr/labaixbidouille/) sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
La permanence d’ADeTI est un moment d’accueil avec des bénévoles pour apprendre à utiliser un ordinateur sous GNU/Linux (Ubuntu, Linux Mint, Debian…) mais aussi:
réparer les problèmes de logiciels sur son ordinateur
prendre des conseils pour choisir des logiciels alternatifs
différencier les logiciels libres utilisables pour répondre aux besoins
préserver et réfléchir sur ses usages (vie privée, éthique…)
Mais c’est aussi un moment consacré pour:
partager des connaissances et échanger des savoirs
maîtriser les formats ouverts et la pérennité de ses documents
Confidentialité, intégrité et disponibilité des systèmes d’information
Diversité des alternatives
Indépendance
Nous accueillons également des membres de l’association ALFA-Net et A-Hébergement qui peuvent répondre aux questions concernant Internet, les réseaux et l’hébergement: connexion à Internet, alternatives aux “Box” et aux opérateurs/FAI commerciaux, Neutralité du Net, Vie Privée, Blog, Site Internet/Web…
Centre Socioculturel Gentiana, Centre Socioculturel Gentiana, 90 avenue Maginot, Tours, Centre-Val de Loire, France
https://www.adeti.org
install-party, gull, linux, internet, réseau, adieu-windows, logiciels-libres, gnu/linux, adeti-org, hébergement, permanence [FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
Assistance technique et démonstration concernant les logiciels libres.
Il est préférable de réserver votre place à contact (at) linuxmaine (point) org
Planning des réservations consultableici.
Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, 31 allée Claude Debussy, Le Mans, Pays de la Loire, France
https://linuxmaine.org
linuxmaine, gnu-linux, demonstration, assistance, permanence, logiciels-libres, linux, adieu-windows [FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Centre socioculturel Port-Boyer, Centre socioculturel Port-Boyer, 4 rue de Pornichet, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
Tu as toujours rêvé de créer ton propre jeu vidéo ? Cet atelier est fait pour toi ! Viens apprendre à concevoir un jeu de A à Z: de l’idée de départ à la programmation, en passant par la création des personnages et des décors. Avec Scratch, rien de plus simple et amusant !
Mercredi 11 février: Attention Danger !
Mercredi 11 mars: Shark attack !
2 séances: 14 h et 16 h
Téléphone: 03 83 54 85 53
Médiathèque Jules Verne, Médiathèque Jules Verne, 2 rue de Malines, Vandœuvre-lès-Nancy, Grand Est, France
https://www.vandœuvre.fr/evenement/ateliers-cree-ton-jeu-video-avec-scratch/
mediatheque-jules-verne, atelier, logiciels-libres, scratch, jeu-video [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, de partager leurs connaissances, d’échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
Chaque mercredi soir, l’association propose une rencontre pour partager des connaissances, des savoir-faire, des questions autour de l’utilisation des logiciels libres, que ce soit à propos du système d’exploitation Linux, des applications libres ou des services en ligne libres.
C’est l’occasion aussi de mettre en avant l’action des associations fédératrices telles que l’April ou Framasoft, dont nous sommes adhérents et dont nous soutenons les initiatives avec grande reconnaissance.
Ecospace, 136 rue de la Mie au Roy, Beauvais, Hauts-de-France, France
https://www.oisux.org
oisux, logiciels-libres, atelier, rencontre, sensibilisation, adieu-windows [FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
Les contribateliers sont des ateliers conviviaux où chacun·e peut partager ses outils libres préférés et apprendre à y contribuer !
Hyperlien, Hyperlien, 5 allée Frida Kahlo, Nantes, Pays de la Loire, France
https://contribateliers.org/trouver-un-contribatelier/les-contribateliers-nantais
contribateliers-nantais, atelier, contribuer, libre [FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
Réunion ouverte à tous, adhérent ou pas.
Les réunions mensuelles Hadoly ont lieu tous les 2ᵉ mercredi du mois, à partir de 19h.
Soit en présentiel dans les locaux de la maison de l’écologie – 4 rue Bodin 69001 Lyon
Soit en distanciel sur l’adresse https://jitsi.hadoly.fr/permanence-hadoly.
À propos de cet événement
La permanence (mensuelle) d’Hadoly (Hébergeur Associatif Décentralisé et Ouvert à LYon), chaton lyonnais, est l’occasion d’échanger avec les membres de l’asso sur les services et moyens mis à disposition des adhérents afin de se libérer des Gafams tout en partageant ce que chacun·e aura amené pour grignoter ou boire.
Nous partageons du mail, du cloud, et d’autres services, le tout basé exclusivement sur une infrastructure locale et des logiciels libres. Nous respectons la neutralité du net et la vie privée. Plus largement nous échangeons autour des communs numériques, des cultures libres et de l’éducation populaire par exemple en réalisant ou animant des ateliers d’éducation aux médias.
Vous serez bienvenu pour présenter votre projet, celui de votre organisation, causer communs numériques, cultures libres et éduc pop.
Maison de l’écologie, Maison de l’écologie, 4 rue Bodin, Lyon, Auvergne-Rhône-Alpes, France
https://hadoly.fr
hadoly, chaton, permanence, réunion, discussion [FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
Appel à une rencontre autour d’un verre de bière des amis de Linux de Strasbourg et environs.
Les autres boissons sont explicitement tolérées…
Vous pouvez nous informer de votre envie de participer à l’évènement pour que l’on ne vous oublie pas. Pour cela, vous pouvez envoyer un message sur la liste de diffusion ou sur IRC.
Station de tram: Langstross Grand'Rue, ligne A ou D.
La Taverne Des Serruriers, La Taverne Des Serruriers, 25 rue des Serruriers, Strasbourg, Grand Est, France
https://strasbourg.linuxfr.org
aam, flammekueche-connection, lug-de-strasbourg, appel-à-mousser [FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
L’Association Club Linux Nord Pas-de-Calais organise chaque mois une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Les Mercredi Linux sont des réunions mensuelles désormais organisées le mercredi. Ces réunions sont l’occasion de se rencontrer, d’échanger des idées ou des conseils.
Régulièrement, des présentations thématiques sont réalisées lors de ces réunions, bien sûr, toujours autour des logiciels libres.
Durant cette permanence, vous pourrez trouver des réponses aux questions que vous vous posez au sujet du Logiciel Libre, ainsi que de l’aide pour résoudre vos problèmes d’installation, de configuration et d’utilisation de Logiciels Libres. N’hésitez pas à apporter votre ordinateur, afin que les autres participants puissent vous aider.
Cette permanence a lieu à la Médiathèque Cultiv'Art 6 rue de la Ladrerie, Cappelle en Pévèle
Médiathèque Cultiv'Art, Médiathèque Cultiv'Art, 16 rue de la Ladrerie, Cappelle en Pévèle, Hauts-de-France, France
http://clx.asso.fr
clx, permanence, linux, gnu-linux, logiciels-libres, adieu-windows [FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
Convocation à l’assemblée générale de l’association PauLLA Une Assemblée Générale est convoquée le jeudi 12 février 2026 à 18h. Pour y assister, 2 solutions:
- la version conviviale: venez nous rejoindre dans les locaux d’AGIRabcd (merci Jean-Louis !), 12 Avenue Federico Garcia Lorca à Pau. Très exactement ici: https://www.openstreetmap.org/node/8892972477
Big Blue Button de l’association (ici: https://bbb.paulla.asso.fr/b/ant-mqu-f3p-brn)
Tous les membres de PauLLA à jour de leur cotisation seront en mesure de voter.
L’ordre du jour est le suivant:
Bilan moral 2025
Bilan financier 2025
Renouvellement/Reconduction des membres du bureau
Paiement des cotisations 2026
Adhésion de PauLLA dans les autres assos/collectifs
APRIL
Landinux
autres Projets pour 2026 Accompagnement de 2 associations vers le libre Campagne « candidats.fr » pour les municipales 2026 Install-party à Haut de Gan en mars Install-party à la médiathèque de Lons fin avril Contacts avec le lycée Louis Barthou Le bouncer de CIaviCI, on en parle ? Bug gênant sur le site internet Toi ! Oui, toi, qui est en train de lire cette ligne, qu’as-tu à proposer pour 2026 ? Questions diverses L’assemblée générale sera aussi l’occasion de se sustenter autour d’un buffet improvisé en mode auberge espagnole avec ce que les membres apporteront ce soir-là. Boissons, petits plats sont donc les bienvenus. Essayez autant que possible de vous coordonner sur le canal #paulla sur IRC afin d’éviter que l’on se retrouve avec 12 packs de bière et rien d’autre.
Même chose pour d’éventuels covoiturages: coordonnons-nous sur l’IRC.
Local d’AGIRabcd, Local d’AGIRabcd, 12 avenue Federico Garcia Lorca, Pau, Nouvelle-Aquitaine, France
https://www.paulla.asso.fr/Evenements/assemblee-generale-paulla-2026
gull, paulla, logiciels-libres, projets, futur, assemblée-générale [FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
Le but des soirées de contribution au libre est de proposer un espace de travail partagé aux personnes actives dans le libre en Île-de-France le temps d’une soirée, une fois par mois (le deuxième jeudi du mois plus précisément).
Dit plus court: c’est un lieu avec de l’électricité et une connexion internet. En avant les claviers !
Les soirées de contribution au libre sont faites pour vous si:
vous travaillez sur un projet libre et vous recherchez une atmosphère à la fois conviviale et studieuse pour aller de l’avant et, qui sait, créer des connexions avec d’autres projets libres, vous êtes un collectif autour du libre et vous cherchez un lieu pour vous retrouver physiquement et avancer avec efficacité sur vos chantiers. Si vous n’avez pas envie de contribuer à un projet libre, les soirées de contribution au libre ne sont sans doute pas faites pour vous. Pas de panique, Parinux organise d’autres évènements:
si vous voulez discuter autour du libre: l’Apéro du Libre (APL) est là pour ça ; c’est un rendez-vous fixé tous les 15 du mois ; venez-nous retrouver autour d’un verre pour papoter et refaire le monde (libre), si vous avez un problème informatique: c’est la vocation de Premiers Samedi du Libre (PSL) où vous pourrez trouver des oreilles attentives et compétentes à l’écoute de toutes vos questions. Nous nous réservons le droit de refuser l’entrée aux soirées de contribution au libre à tout personne qui n’en respecterait pas l’esprit. Et, bien sûr, les règles de bienséance habituelles s’appliquent pour que chacune et chacun se sente à l’aise dans un cadre bienveillant.
Si les soirées de contribution vous intéressent, le mieux est de contacter d’abord le CA de Parinux ca@parinux.org. Vous devrez de toute façon nous écrire pour obtenir le code de la porte cochère…
FPH, FPH, 38 rue Saint-Sabin, Paris, Île-de-France, France
https://parinux.org/Soiree-de-Contribution-au-Libre-le-jeudi-12-fevrier-2026
parinux, scl, contribution, contribution-au-libre [FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
Médiathèque de Quimperlé, place Saint Michel, pas d’inscription, entrée libre !
Mickaël, Johann, Alain, et Yves vous accueillent (ou l’un d’eux, on se relaie !).
Conseils, aide et infos pratiques GNU/Linux et Logiciels Libres.
Curieux ? Déjà utilisateur ? Expert ? Pour résoudre vos problèmes, vous êtes le bienvenu ; pas besoin de prendre rendez-vous !
N’hésitez pas à venir avec votre PC si vous voulez une installation de GNU/Linux ou de venir avec votre périphérique récalcitrant (imprimante, scanner…) si possible.
Médiathèque de Quimperlé, place Saint Michel, Quimperlé, Bretagne, France
https://libreaquimperle.netlib.re
dépannage, entraide, gnu-linux, logiciels-libres, point-info, linux, libre-à-quimperlé, médiathèque-de-quimperlé [FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
Tous les vendredis après-midi, venez nous rencontrer lors de nos cafés-conseils et repairs-cafés!
Nous faisons découvrir les logiciels et systèmes libres (et gratuits !)
Plus de Télémétrie, de PC ralentis, une meilleure stabilité et sécurité,
Moins de virus et finie l’obsolescence programmée !
Salle Steredenn, Salle Steredenn, 9 rue du 19 Mars 1962, Lanmeur, Bretagne, France
https://ulamir-cpie.bzh
ulamir, cpie, repair-café, cyber-sécurité, windows10, libre, linux, adieu-windows, bonnes-pratiques, open-source, conseils-numeriques, ulamir-cpie [FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Maison de quartier des Haubans, Maison de quartier des Haubans, 1 bis boulevard de Berlin, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
Tous les 2ᵉmes et 4ᵉmes vendredis du mois (sauf indisponibilité des membres) de 14h30 à 16h30 l’association Ailes-52 vous propose de venir au Café de la Gare à Nogent (52800) pour échanger autour de la découverte des Logiciels Libres.
Vous pourrez:
Demander conseil pour l’acquisition d’un ordinateur reconditionné.
Gérer mes contacts sur mon ordiphone et mon PC.
Installer/configurer un logiciel libre sous Windows, Mac OS ou Linux. (Ex: VLC, Firefox, Thunderbird, LibreOffice, etc.).
Installer et configurer une imprimante/scanner.
Essayer une distribution Linux.
Répondez à cette question: Mon ordinateur ne pourra pas bénéficier de Windows 11, qu’est-ce que je peux faire pour continuer à l’utiliser, installer GNU/Linux sur mon ordi c’est possible?
Café de la Gare, Café de la Gare, 192 rue du Maréchal de Lattre de Tassigny, Nogent, Grand Est, France
https://ailes-52.org
linux, logiciels-libres, gnu-linux, découverte, café, apprentissage, permanence, bureautique, obsolescence, informatique-libre, ailes-52 [FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
Progressivement vous pourrez faire en sorte d’être moins sous l’influence de Google.
Dans cet atelier nous installerons des magasins d’applications libres pour ne plus avoir à utiliser le Google Play Store et s’assurer de pouvoir télécharger des applications libres (éthiques).
Nous installerons également l’application libre NewPipe pour accéder à Youtube sans s.
À noter: cet atelier n’est PAS faisable avec un iPhone / iPad
Inscription sur: https://calc.ouvaton.coop/InscriptionAtelierNumeriqueEthiqueRouen
MJC Grieu, MJC Grieu, 3 rue de Genève, Rouen, Normandie, France
dégooglisation, smartphone, tablette, application, logiciels-libres, libérons-nos-ordis [FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
Venez découvrir l’association Libre en Communs, ses membres et ses activités lors d’un moment de convivialité à La Générale, 39 rue Gassendi, 75014 Paris.
Habituellement le 2ᵉ vendredi de chaque mois – consultez l’Agenda Du Libre pour d’éventuelles mises à jour de dernière minute.
Métro les plus proches: Denfert-Rochereau (RER B, lignes 4 et 6), Mouton-Duvernet (ligne 4), Gaîté (ligne 13).
Vous pouvez apporter de la nourriture pour un repas partagé. Il y a une buvette sur place pour soutenir La Générale.
La Générale, La Générale, 39 rue Gassendi, Paris, Île-de-France, France
https://www.a-lec.org
libre-en-communs, alec, rencontre, apéro, échange-de-savoirs, la-générale [FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
L'OMJC organise avec l’Association Club Linux Nord Pas-de-Calais organise chaque samedi une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Le Centre d’Infos Jeunes a mis en place une démarche d’accompagnement des jeunes aux pratiques actuelles pour l’informatique et le numérique:
Lieu d’accès public à Internet (5 postes avec Wifi libre et gratuit)
Web collaboratif et citoyen pour que chacun puisse trouver sa place et passer du rôle de simple usager à celui d’initiateur de processus collaboratif
Éducation à l’information par les nouveaux médias (diffusion par le biais du numérique)
Logiciels libres (bureautique, sites, blogs, cloud, infographie et vidéo, musique, réseaux sociaux, chat…).
Cette rencontre a lieu sur rendez-vous, tous les samedis matin hors vacances scolaires à la Maison communale de la ferme Dupire, rue Yves Decugis à VILLENEUVE D’ASCQ
OMJC, rue Yves Decugis, Villeneuve d’Ascq, Hauts-de-France, France
https://clx.asso.fr
omjc, clx, permanence, linux, gnu-linux, logiciels-libres, atelier [FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
Rencontre mensuelle autour des logiciels libres, en toute simplicité.
Ces matinées seront ce que nous en ferons ensemble, selon vos attentes:
Découverte des logiciels libres dont Linux et de leur intérêt. Utilisation sur place.
Installations, sur votre machine (pensez à sauvegarder vos données avant de venir avec) ou sur des PC fournis pour apprendre ensemble sans risque. Parfois, on vous propose un ordinateur auquel Linux a redonné une seconde vie, avec lequel vous pouvez repartir…
Préparation d’une clé USB pour tester Linux chez vous, l’installer ou alors pour utiliser des logiciels libres sans installation sous Windows.
Entraide, suivi de votre expérience avec les logiciels libres.
Nous pourrons aussi nous intéresser aux outils en ligne, aux smartphones, ou nous amuser à redonner vie à de vieux PC un peu obsolètes, à reconditionner des ordinateurs pour des associations ou personnes avec peu de ressources, etc.
Pour tout projet qui risque de prendre un peu de temps, il est préférable de nous contacter avant.
Les débutant·e·s sont les bienvenu·e·s! Les autres aussi, bien évidemment !
Maison pour tous, 35 route d’Arenthon, Amancy, Auvergne-Rhône-Alpes, France
https://librealabase.gitlab.io
libre, logiciel-libre, linux, /e/os, gnu-linux [FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
Apportez votre ordinateur
pour y installer des logiciels libres et gratuits
Tous les 2ᵉ samedis 9h-13h de janvier à juin 2026
PROCHAIN: Samedi 14 février 2026 de 9h à 13h
Atelier public &amp; gratuit destiné: aux curieux, aux avertis, à ceux qui veulent faire des économies.
► Remplacer Microsoft Word par LibreOffice Write, Photoshop par Gimp, Outlook par Thunderbird, Google par DuckDuckGo, Gmail par déMAILnagement
SUR INSCRIPTIONS: au 01.43.04.83.53
+ de renseignements par email à franck@sinimale.fr
#adieu-windows
Maison pour tous des Coteaux, Maison pour tous des Coteaux, 30 route de Gournay, Noisy-le-Grand, Île-de-France, France
adieu-windows, install-party, entraide, logiciels-libres, linux, gnu-linux [FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.]]></description>
      <pubDate>Sat, 07 Feb 2026 21:16:41 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[AstrBotDevs/AstrBot]]></title>
      <link>https://github.com/AstrBotDevs/AstrBot</link>
      <description><![CDATA[Agentic IM Chatbot infrastructure that integrates lots of IM platforms, LLMs, plugins and AI feature, and can be your openclaw alternative. English ｜ 日本語 ｜ 繁體中文 ｜ Français ｜ Русский 文档 ｜ Blog ｜ 路线图 ｜ 问题提交 AstrBot 是一个开源的一站式 Agentic 个人和群聊助手，可在 QQ、Telegram、企业微信、飞书、钉钉、Slack、等数十款主流即时通讯软件上部署，此外还内置类似 OpenWebUI 的轻量化 ChatUI，为个人、开发者和团队打造可靠、可扩展的对话式智能基础设施。无论是个人 AI 伙伴、智能客服、自动化助手，还是企业知识库，AstrBot 都能在你的即时通讯软件平台的工作流中快速构建 AI 应用。 主要功能 免费 &amp; 开源。 AI 大模型对话，多模态，Agent，MCP，Skills，知识库，人格设定，自动压缩对话。 支持接入 Dify、阿里云百炼、Coze 等智能体平台。 多平台，支持 QQ、企业微信、飞书、钉钉、微信公众号、Telegram、Slack 以及更多。 插件扩展，已有近 800 个插件可一键安装。 Agent Sandbox 隔离化环境，安全地执行任何代码、调用 Shell、会话级资源复用。 WebUI 支持。 Web ChatUI 支持，ChatUI 内置代理沙盒、网页搜索等。 国际化（i18n）支持。 角色扮演 &amp; 情感陪伴 主动式 Agent 通用 Agentic 能力 900+ 社区插件 快速开始 Docker 部署(推荐 ) 推荐使用 Docker / Docker Compose 方式部署 AstrBot。 请参阅官方文档 使用 Docker 部署 AstrBot 。 uv 部署 uv tool install astrbot
astrbot 启动器一键部署（AstrBot Launcher） 进入 AstrBot Launcher 仓库，在 Releases 页最新版本下找到对应的系统安装包安装即可。 宝塔面板部署 AstrBot 与宝塔面板合作，已上架至宝塔面板。 请参阅官方文档 宝塔面板部署 。 1Panel 部署 AstrBot 已由 1Panel 官方上架至 1Panel 面板。 请参阅官方文档 1Panel 部署 。 在 雨云 上部署 AstrBot 已由雨云官方上架至云应用平台，可一键部署。 在 Replit 上部署 社区贡献的部署方式。 Windows 一键安装器部署 请参阅官方文档 使用 Windows 一键安装器部署 AstrBot 。 CasaOS 部署 社区贡献的部署方式。 请参阅官方文档 CasaOS 部署 。 手动部署 首先安装 uv： pip install uv 通过 Git Clone 安装 AstrBot： git clone https://github.com/AstrBotDevs/AstrBot &amp;&amp; cd AstrBot
uv run main.py 或者请参阅官方文档 通过源码部署 AstrBot 。 系统包管理器安装 Arch Linux yay -S astrbot-git
# 或者使用 paru
paru -S astrbot-git 桌面端 Electron 打包 桌面端（Electron 打包，pnpm 工作流）构建流程请参阅：desktop/README.md。 支持的消息平台 官方维护 QQ OneBot v11 协议实现 Telegram 企微应用 &amp; 企微智能机器人 微信客服 &amp; 微信公众号 飞书 钉钉 Slack Discord LINE Satori Misskey Whatsapp (将支持) 社区维护 Matrix KOOK VoceChat 支持的模型服务 大模型服务 OpenAI 及兼容服务 Anthropic Google Gemini Moonshot AI 智谱 AI DeepSeek Ollama (本地部署) LM Studio (本地部署) AIHubMix 优云智算 302.AI 小马算力 硅基流动 PPIO 派欧云 ModelScope OneAPI LLMOps 平台 Dify 阿里云百炼应用 Coze 语音转文本服务 OpenAI Whisper SenseVoice 文本转语音服务 OpenAI TTS Gemini TTS GPT-Sovits-Inference GPT-Sovits FishAudio Edge TTS 阿里云百炼 TTS Azure TTS Minimax TTS 火山引擎 TTS 贡献 欢迎任何 Issues/Pull Requests！只需要将你的更改提交到此项目 ：) 如何贡献 你可以通过查看问题或帮助审核 PR（拉取请求）来贡献。任何问题或 PR 都欢迎参与，以促进社区贡献。当然，这些只是建议，你可以以任何方式进行贡献。对于新功能的添加，请先通过 Issue 讨论。 开发环境 AstrBot 使用 ruff 进行代码格式化和检查。 git clone https://github.com/AstrBotDevs/AstrBot
pip install pre-commit
pre-commit install 社区 QQ 群组 1 群：322154837 3 群：630166526 5 群：822130018 6 群：753075035 7 群：743746109 8 群：1030353265 开发者群：975206796 Telegram 群组 Discord 群组 Special Thanks 特别感谢所有 Contributors 和插件开发者对 AstrBot 的贡献 此外，本项目的诞生离不开以下开源项目的帮助： NapNeko/NapCatQQ - 伟大的猫猫框架 Star History [!TIP] 如果本项目对您的生活 / 工作产生了帮助，或者您关注本项目的未来发展，请给项目 Star，这是我们维护这个开源项目的动力 &lt;3 陪伴与能力从来不应该是对立面。我们希望创造的是一个既能理解情绪、给予陪伴，也能可靠完成工作的机器人。 私は、高性能ですから!]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/AstrBotDevs/AstrBot</guid>
    </item>
    <item>
      <title><![CDATA[HailToDodongo/pyrite64]]></title>
      <link>https://github.com/HailToDodongo/pyrite64</link>
      <description><![CDATA[N64 Game-Engine and Editor using libdragon &amp; tiny3d Pyrite64 N64 game-engine and editor using Libdragon and tiny3d. Note: This project does NOT use any proprietary N64 SDKs or libraries. Pyrite64 is a visual editor + runtime-engine to create 3D games that can run on a real N64 console or accurate emulators. Besides the usual editor, some extra features include: Automatic toolchain installation on Windows 3D-Model import (GLTF) from blender with fast64 material support. Support for HDR+Bloom rendering (shown here: www.youtube.com/watch?v=XP8g2ngHftY) Support for big-texture rendering (256x256) (shown here: www.youtube.com/watch?v=rNEo0aQkGnU) Runtime engine handling scene-management, rendering, collision, audio and more. Global asset management with automatic memory cleanup Node-Graph editor to script basic control flow Note that this project focuses on real hardware, so accurate emulation is required to run/test games on PC. Emulators that are accurate enough include Ares (v147 or newer) and gopher64. [!WARNING] This project is still in early development, so features are going to be missing. Documentation is also still a work in progress, and breaking API changes are to be expected. Documentation Before starting, please read the FAQ! Installation &amp; Docs: Pyrite64 Installation Using the Editor Using the CLI Development on the editor itself: Building the Editor Showcase Cathode Quest 64 (YouTube) | Pyrite64 Release Video Links For anything N64 homebrew related, checkout the N64Brew discord: https://discord.gg/WqFgNWf Credits &amp; License 2025-2026 - Max Bebök (HailToDodongo) Pyrite64 is licensed under the MIT License, see the LICENSE file for more information. Licenses for external libraries used in the editor can be found in their respective directory under /vendored Pyrite64 does NOT force any restrictions or licenses on games made with it. Pyrite64 does NOT claim any copyright or force licenses for assets / source-code generated by the editor. While not required, please consider crediting Pyrite64 with a logo and/or name in your credits and/or boot logo sequence.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/HailToDodongo/pyrite64</guid>
    </item>
    <item>
      <title><![CDATA[mavlink/mavlink]]></title>
      <link>https://github.com/mavlink/mavlink</link>
      <description><![CDATA[Marshalling / communication library for drones. MAVLink MAVLink -- Micro Air Vehicle Message Marshalling Library. MAVLink is a very lightweight, header-only message library for communication between drones and/or ground control stations. It consists primarily of message-set specifications for different systems ("dialects") defined in XML files, and Python tools that convert these into appropriate source code for supported languages. There are additional Python scripts providing examples and utilities for working with MAVLink data. Tip MAVLink is very well suited for applications with very limited communication bandwidth. Its reference implementation in C is highly optimized for resource-constrained systems with limited RAM and flash memory. It is field-proven and deployed in many products where it serves as interoperability interface between components of different manufacturers. Quick start Generate C headers To install the minimal MAVLink environment on Ubuntu LTS 20.04 or 22.04, enter the following on a terminal: # Dependencies
sudo apt install python3-pip # Clone mavlink into the directory of your choice
git clone https://github.com/mavlink/mavlink.git --recursive
cd mavlink python3 -m pip install -r pymavlink/requirements.txt You can then build the MAVLink2 C-library for message_definitions/v1.0/common.xml from the /mavlink directory as shown: python3 -m pymavlink.tools.mavgen --lang=C --wire-protocol=2.0 --output=generated/include/mavlink/v2.0 message_definitions/v1.0/common.xml Use from cmake To include the headers in cmake, install them locally, e.g. into the directory install: cmake -Bbuild -H. -DCMAKE_INSTALL_PREFIX=install -DMAVLINK_DIALECT=common -DMAVLINK_VERSION=2.0
cmake --build build --target install Then use find_package to get the dependency in CMakeLists.txt: find_package(MAVLink REQUIRED) add_executable(my_program my_program.c) target_link_libraries(my_program PRIVATE MAVLink::mavlink) And pass the local install directory to cmake (adapt to your directory structure): cd ../my_program
cmake -Bbuild -H. -DCMAKE_PREFIX_PATH=../mavlink/install For a full example, check examples/c. Note: even though we use target_link_libraries in cmake, it doesn't actually "link" to MAVLink as it's just a header-only library. Other instructions Instructions for using the C libraries are then covered in Using C MAVLink Libraries (mavgen). Note: Installing the MAVLink Toolchain explains how to install MAVLink on other Ubuntu platforms and Windows, while Generating MAVLink Libraries explains how to build MAVLink for the other programming languages supported by the project. The sub-topics of Using MAVLink Libraries explain how to use the generated libraries. Key Links Documentation/Website (mavlink.io/en/) Discussion/Support Contributing License]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/mavlink/mavlink</guid>
    </item>
    <item>
      <title><![CDATA[p2r3/convert]]></title>
      <link>https://github.com/p2r3/convert</link>
      <description><![CDATA[Truly universal online file converter Convert to it! Truly universal online file converter. Many online file conversion tools are boring and insecure. They only allow conversion between two formats in the same medium (images to images, videos to videos, etc.), and they require that you upload your files to some server. This is not just terrible for privacy, it's also incredibly lame. What if you really need to convert an AVI video to a PDF document? Try to find an online tool for that, I dare you. Convert.to.it aims to be a tool that "just works". You're almost guaranteed to get an output - perhaps not always the one you expected, but it'll try its best to not leave you hanging. For a semi-technical overview of this tool, check out the video: https://youtu.be/btUbcsTbVA8 Usage Go to convert.to.it Click the big blue box to add your file (or just drag it on to the window). An input format should have been automatically selected. If it wasn't, yikes! Try searching for it, or if it's really not there, see the "Issues" section below. Select an output format from the second list. If you're on desktop, that's the one on the right side. If you're on mobile, it'll be somewhere lower down. Click Convert! Hopefully, after a bit (or a lot) of thinking, the program will spit out the file you wanted. If not, see the "Issues" section below. Issues Ever since the YouTube video released, we've been getting spammed with issues suggesting the addition of all kinds of niche file formats. To keep things organized, I've decided to specify what counts as a valid issue and what doesn't. [!IMPORTANT] SIMPLY ASKING FOR A FILE FORMAT TO BE ADDED IS NOT A MEANINGFUL ISSUE! There are thousands of file formats out there. It can take hours to add support for just one. The math is simple - we can't possibly support every single file. As such, simply listing your favorite file formats is not helpful. We already know that there are formats we don't support, we don't need tickets to tell us that. When suggesting a file format, you must at minimum: Make sure that there isn't already an issue about the same thing, and that we don't already support the format. Explain what you expect the conversion to be like (what medium is it converting to/from). It's important to note here that simply parsing the underlying data is not sufficient. Imagine if we only treated SVG images as raw XML data and didn't support converting them to raster images - that would defeat the point. Provide links to existing browser-based solutions if possible, or at the very least a reference for implementing the format, and make sure the license is compatible with GPL-2.0. If this seems like a lot, please remember - a developer will have to do 100x more work to actually implement the format. Doing a bit of research not only saves them precious time, it also weeds out "unserious" proposals that would only bloat our to-do list. If you're submitting a bug report, you only need to do step 1 - check if the problem isn't already reported by someone else. Bug reports are generally quite important otherwise. Though please note, "converting X to Y doesn't work" is not a bug report. However, "converting X to Y works but not how I expected" likely is a bug report. Deployment Local development (Bun + Vite) Clone this repository WITH SUBMODULES. You can use git clone --recursive https://github.com/p2r3/convert for that. Omitting submodules will leave you missing a few dependencies. Install Bun. Run bun install to install dependencies. Run bunx vite to start the development server. The following steps are optional, but for performance: When you first open the page, it'll take a while to generate the list of supported formats for each tool. If you open the console, you'll see it complaining a bunch about missing caches. After this is done (indicated by a Built initial format list message in the console), use printSupportedFormatCache() to get a JSON string with the cache data. You can then save this string to cache.json to skip that loading screen on startup. Docker (prebuilt image) Docker compose files live in the docker/ directory, so run compose with -f from the repository root: docker compose -f docker/docker-compose.yml up -d Alternatively download the docker-compose.yml separately and start it by executing docker compose up -d in the same directory. This runs the container on http://localhost:8080/convert/. Docker (local build for development) Use the override file to build the image locally: docker compose -f docker/docker-compose.yml -f docker/docker-compose.override.yml up --build -d The first Docker build is expected to be slow because Chromium and related system packages are installed in the build stage (needed for puppeteer in buildCache.js). Later builds are usually much faster due to Docker layer caching. Contributing The best way to contribute is by adding support for new file formats (duh). Here's how that works: Creating a handler Each "tool" used for conversion has to be normalized to a standard form - effectively a "wrapper" that abstracts away the internal processes. These wrappers are available in src/handlers. Below is a super barebones handler that does absolutely nothing. You can use this as a starting point for adding a new format: // file: dummy.ts import type { FileData, FileFormat, FormatHandler } from "../FormatHandler.ts";
import CommonFormats from "src/CommonFormats.ts"; class dummyHandler implements FormatHandler { public name: string = "dummy"; public supportedFormats?: FileFormat[]; public ready: boolean = false; async init () { this.supportedFormats = [ // Example PNG format, with both input and output disabled CommonFormats.PNG.builder("png") .markLossless() // .allowFrom() // .allowTo() ]; this.ready = true; } async doConvert ( inputFiles: FileData[], inputFormat: FileFormat, outputFormat: FileFormat ): Promise { const outputFiles: FileData[] = []; return outputFiles; } } export default dummyHandler; For more details on how all of these components work, refer to the doc in src/FormatHandler.ts. You can also take a look at existing handlers to get a more practical example. There are a few additional things that I want to point out in particular: Pay attention to the naming system. If your tool is called dummy, then the class should be called dummyHandler, and the file should be called dummy.ts. The handler is responsible for setting the output file's name. This is done to allow for flexibility in rare cases where the full file name matters. Of course, in most cases, you'll only have to swap the file extension. The handler is also responsible for ensuring that any byte buffers that enter or exit the handler do not get mutated. If necessary, clone the buffer by wrapping it in new Uint8Array(). When handling MIME types, run them through normalizeMimeType first. One file can have multiple valid MIME types, which isn't great when you're trying to match them algorithmically. When implementing a new file format, please treat the file as the media that it represents, not the data that it contains. For example, if you were making an SVG handler, you should treat the file as an image, not as XML. Adding dependencies If your tool requires an external dependency (which it likely does), there are currently two well-established ways of going about this: If it's an npm package, just install it to the project like you normally would. If it's a Git repository, add it as a submodule to src/handlers. Please try to avoid CDNs (Content Delivery Networks). They're really cool on paper, but they don't work well with TypeScript, and each one introduces a tiny bit of instability. For a project that leans heavily on external dependencies, those bits of instability can add up fast. If you need to load a WebAssembly binary (or similar), add its path to vite.config.js and target it under /convert/wasm/. Do not link to node_modules.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:13 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/p2r3/convert</guid>
    </item>
    <item>
      <title><![CDATA[Why I Stopped Paying for Software: 6 Open Source Tools That Changed How I Work]]></title>
      <link>https://dev.to/fedya_serafiev/why-i-stopped-paying-for-software-6-open-source-tools-that-changed-how-i-work-1h9f</link>
      <description><![CDATA[This is a translated and adapted version of the original article published in Bulgarian at itpraktika.com.
As a tech enthusiast, I spent years searching for the perfect toolset. I used to believe that a high price tag guaranteed quality — paying for expensive subscriptions, convinced it was the only way to stay professional.
Over time, I discovered something remarkable: the world of open source software has evolved beyond recognition. Today, these solutions don't just compete with paid alternatives — they often surpass them.
In this article, I'm sharing 6 tools that are at the core of my workflow. They save me hundreds of dollars a year and give me complete freedom.
If I had to pick just one tool, it would be VS Code. Before it, I used paid IDEs — they were bloated and slow. VS Code changed everything.
Why it's indispensable:
Lightweight, fast, and completely free
Supports almost every programming language through extensions
IntelliSense predicts your next step as you write code
Integrated terminal and Git built right into the interface
Why I stopped paying: Paid IDEs offer features I rarely use. With VS Code, I only add what I actually need — which keeps it lightning fast.
Before Docker, I constantly heard: "But it works on my machine!" That was a nightmare for team collaboration. Docker solved that problem once and for all.
What it does:
Packages your software and its dependencies into a "container" so code runs the same everywhere
Spin up databases and servers in seconds with a single command: docker-compose up Test different technologies without "polluting" your operating system
The result: I save hours on environment configuration. Docker is the industry standard — and it's free for my use case.
For a long time I felt trapped by Microsoft 365 subscriptions. Paying a monthly fee just to write documents seemed unnecessary. LibreOffice was my salvation.
Why it works:
Opens and saves .docx, .xlsx, and .pptx files without issues
Writer and Calc cover 100% of my needs for reports and spreadsheets
Files stay local — not locked in someone else's cloud
Why it's better: LibreOffice is stable, powerful, and doesn't interrupt you with upsell prompts. It's a clean tool built for work.
Knowledge management is critical. I used to use Notion, but the slow loading and constant need for an internet connection frustrated me. Obsidian changed the way I think.
What makes it special:
All notes are stored as Markdown files on your own computer
Link ideas together the way your brain actually works
Graph view gives you a visual map of all your knowledge and projects
Why I chose it: My data is permanent. Even if Obsidian ceases to exist, my notes remain as plain text files I can open anywhere.
The browser is the most-used tool in any workflow. Chrome is decent, but it consumes too many resources and tracks your activity. Brave is the better version.
Key features:
Built-in ad blocker stops annoying ads automatically
Blocks trackers that try to profile you
Pages load significantly faster without the ad overhead
What I save: I no longer pay for premium services just to browse without ads. Brave gives me a clean experience for free.
Photoshop is the industry standard, but its price is steep for smaller projects. For image editing, I use GIMP.
Capabilities:
Layers, masks, filters — everything you need is here
Hundreds of free plugins that extend its functionality
Works perfectly on Windows, Linux, and macOS
My experience: It took me about a week to get used to the interface. Now I handle everything — from banners to photo retouching — without spending a cent. Task]]></description>
      <pubDate>Thu, 19 Feb 2026 05:57:12 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/fedya_serafiev/why-i-stopped-paying-for-software-6-open-source-tools-that-changed-how-i-work-1h9f</guid>
    </item>
    <item>
      <title><![CDATA[I Built a Compose Video Player Library Because ExoPlayer Setup Was Driving Me Crazy]]></title>
      <link>https://dev.to/tule_simon_243f9fa5635f9f/i-built-a-compose-video-player-library-because-exoplayer-setup-was-driving-me-crazy-5839</link>
      <description><![CDATA[If you’ve ever implemented video playback in an Android app, you know the drill. You don’t “just play a video.”
You:
Set up ExoPlayer with the right LoadControl Configure BandwidthMeter for adaptive streaming
Handle lifecycle (pause in background, release on destroy)
Manage playback state manually (isPlaying, progress, duration…)
Configure track selection for HLS quality switching
Remember the wake lock
Somehow make it all work with Compose’s declarative model
And you do this:
Every.
After copy-pasting 200+ lines of boilerplate across multiple projects, I decided I was done.
So I built XComposeMediaPlayer — a small open-source library that wraps ExoPlayer into a clean, Compose-native API. val bandwidthMeter = DefaultBandwidthMeter.Builder(context).build() val trackSelector = DefaultTrackSelector( context, AdaptiveTrackSelection.Factory()
) val loadControl = DefaultLoadControl.Builder() .setBufferDurationsMs(...) .build() val player = ExoPlayer.Builder(context) .setTrackSelector(trackSelector) .setLoadControl(loadControl) .setBandwidthMeter(bandwidthMeter) .build() val mediaSource = HlsMediaSource.Factory(dataSourceFactory) .createMediaSource(MediaItem.fromUri(url)) player.setMediaSource(mediaSource)
player.prepare()
player.play() // Then lifecycle handling...
// State management...
// Listeners...
// Another 100+ lines That’s just to start playback.
Now compare that to this.
@Composable
fun VideoScreen() { val state = rememberXMediaState() XMediaPlayer( state = state, url = "https://example.com/video.m3u8", modifier = Modifier .fillMaxWidth() .aspectRatio(16f / 9f) )
} That’s it.
Five lines.
This isn’t magic. It’s just good abstraction over common video use cases.
Pass any URL — the library determines whether it’s HLS, DASH, MP4, or a local file.
XMediaPlayer(state, "https://example.com/stream.m3u8") // HLS
XMediaPlayer(state, "https://example.com/video.mp4") // Progressive
XMediaPlayer(state, contentUri.toString()) // Local No manual media source juggling.
All playback state is exposed as StateFlow, so building reactive UI becomes trivial.
val state = rememberXMediaState() val isPlaying by state.isPlaying.collectAsState()
val progress by state.progress.collectAsState()
val duration by state.duration.collectAsState() Now you can build fully custom controls:
Slider( value = progress.toFloat() / duration, onValueChange = { state.seekToPercent(it) }
) IconButton(onClick = { state.togglePlayPause() }) { Icon( if (isPlaying) Icons.Pause else Icons.Play )
} No manual listeners. No awkward bridging between callbacks and Compose.
For adaptive streams, available qualities are extracted automatically.
val qualities by state.availableQualities.collectAsState() state.setQuality( qualities.find { it.height == 1080 }!!
) // Or just:
state.setAutoQuality() Manual control when you want it. Automatic when you don’t.
Enable caching via configuration:
val state = rememberXMediaState(XMediaConfig.HighPerformance) Content is cached while playing.
Replay? Instant.
Reduced bandwidth usage? Yes.
Building a TikTok-style or Instagram-style feed?
Preload the next video:
state.preCacheHls( url = nextVideoUrl, context = context, durationMs = 15_000L
) Smooth scrolling. No visible buffering.
Most apps don’t need to tune every ExoPlayer knob. But when you do, you can.
Use presets:
XMediaConfig.Default
XMediaConfig.HighPerformance
XMediaConfig.LowLatency
XMediaConfig.DataSaver
Or customize everything:
XMediaConfig( minBufferMs = 5000, maxBufferMs = 60000, cacheConfig = CacheConfig( enabled = true, maxCacheSize = 500L * 1024 * 1024 ), trackSelectorConfig = TrackSelectorConfig( maxVideoHeight = 1080, bandwidthFraction = 1.2f ), bandwidthConfig = BandwidthConfig( initialBitrateEstimate = 5_000_000L )
) You get flexibility — without writing infrastructure from scratch.
Background → auto pause
Foreground → resume if previously playing
Dispose → player released
Configuration changes → survives rotation
No DisposableEffect boilerplate.
This library doesn’t hide ExoPlayer from you.
Need something advanced?
state.player?.setPlaybackSpeed(1.5f) It’s a wrapper, not a wall.
Let’s be clear:
Not a background playback service
Not a DRM abstraction
Not an ad SDK integration
Not Picture-in-Picture
If you need MediaSessionService, DRM configuration, IMA ads, or PiP — use Media3 directly.
This library targets the 80% use case:
Play video in Compose cleanly and correctly.
ExoPlayer is incredibly powerful.
But power comes with complexity, and most apps don’t need to configure every internal component manually.
I wanted:
A Compose-native API
Reactive state out of the box
Quality selection and caching built-in
Sensible defaults
Zero boilerplate
So I built it.
Add JitPack:
// settings.gradle.kts
dependencyResolutionManagement { repositories { maven { url = uri("https://jitpack.io") } }
} // build.gradle.kts
dependencies { implementation("com.github.TuleSimon:XComposeMediaPlayer:1.0.0")
} GitHub:
https://github.com/TuleSimon/XComposeMediaPlayer
We don’t need to rewrite infrastructure for every project.
Sometimes the best engineering decision isn’t building something complex — it’s building something simple on top of something complex.
If you’re building video features in Jetpack Compose, I hope this saves you hours.
If you find it useful, star the repo.
It’s open source.
Happy coding.]]></description>
      <pubDate>Thu, 19 Feb 2026 03:50:34 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/tule_simon_243f9fa5635f9f/i-built-a-compose-video-player-library-because-exoplayer-setup-was-driving-me-crazy-5839</guid>
    </item>
    <item>
      <title><![CDATA[TheAlgorithms/Python]]></title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description><![CDATA[All Algorithms implemented in Python The Algorithms - Python All algorithms implemented in Python - for education Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion. Getting Started Read through our Contribution Guidelines before you contribute. Community Channels We are on Discord and Gitter! Community channels are a great way for you to ask questions and get help. Please join us! List of Algorithms See our directory for easier navigation and a better overview of the project.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:13 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/TheAlgorithms/Python</guid>
    </item>
    <item>
      <title><![CDATA[5 Tips for Building MCP Apps That Work]]></title>
      <link>https://dev.to/goose_oss/5-tips-for-building-mcp-apps-that-work-2pme</link>
      <description><![CDATA[MCP Apps allow you to render interactive UI directly inside any agent supporting the Model Context Protocol. Instead of a wall of text, your agent can now provide a functional chart, a checkout form, or a video player. This bridges the gap in agentic workflows: clicking a button is often clearer than describing the action you hope an agent executes.
MCP Apps originated as MCP-UI, an experimental project. After adoption by early clients like goose, the MCP maintainers incorporated it as an official extension. Today, it's supported by clients like goose, MCPJam, Claude, ChatGPT, and Postman.
Even though MCP Apps use web technologies, building one isn't the same as building a traditional web app. Your UI runs inside an agent you don't control, communicates with a model that can't see user interactions, and needs to feel native across multiple hosts.
After implementing MCP App support in our own hosts and building several individual apps to run on them, here are the practical lessons we've picked up along the way.
At a high level, clients that support MCP Apps load your UI via iFrames. Your MCP App exposes an MCP server with tools and resources. When the client wants to load your app's UI, it calls the associated MCP tool, loads the resource containing the HTML, then loads your HTML into an iFrame to display in the chat interface.
Here's an example flow of what happens when goose renders a cocktail recipe UI:
You ask the LLM "Show me a margarita recipe".
The LLM calls the get-cocktail tool with the right parameters. This tool has a UI resource link in _meta.ui.resourceUri pointing to the resource containing the HTML.
The client then uses the URI to fetch the MCP resource. This resource contains the HTML content of the view.
The HTML is then loaded into the iFrame directly in the chat interface, rendering the cocktail recipe. There's a lot that also goes on behind the scenes, such as View hydration, capability negotiation, and CSPs, but this is how it works at a high level. If you're interested in the full implementation of MCP Apps, we highly recommend giving the spec a read.
When building an MCP App, you want it to feel like a natural part of the agent experience rather than something bolted on. Visual mismatches are one of the fastest ways to break that illusion.
Imagine a user starting an MCP App interaction inside a dark-mode agent, but the app renders in light mode and creates a harsh visual contrast. Even if the app works correctly, the experience immediately feels off.
By default, your MCP App has no awareness of the surrounding agent environment because it runs inside a sandboxed iframe. It cannot tell whether the agent is in light or dark mode, how large the viewport is, or which locale the user prefers.
The agent, referred to as the Host, solves this by sharing its environment details with your MCP App, known as the View. When the View connects, it sends a ui/initialize request. The Host responds with a hostContext object describing the current environment. When something changes, such as theme, viewport, or locale, the Host sends a ui/notifications/host-context-changed notification containing only the updated fields.
Imagine this dialogue between the View and Host:
View: "I'm initializing. What does your environment look like?"
Host: "We're in dark mode, viewport is 400×300, locale is en-US, and we're on desktop."
User switches to light theme
Host: "Update: we're now in light mode."
It is your job as the developer to ensure your MCP App makes use of the hostContext so it can adapt to the environment.
import { useState } from "react";
import { useApp } from "@modelcontextprotocol/ext-apps/react";
import type { McpUiHostContext } from "@modelcontextprotocol/ext-apps"; function MyApp() { const [hostContext, setHostContext] = useState(undefined); const { app, isConnected, error } = useApp({ appInfo: { name: "MyApp", version: "1.0.0" }, capabilities: {}, onAppCreated: (app) =&gt; { app.onhostcontextchanged = (ctx) =&gt; { setHostContext((prev) =&gt; ({ ...prev, ...ctx })); }; }, }); if (error) return Error: {error.message}; if (!isConnected) return Connecting...; return ( Theme: {hostContext?.theme} Locale: {hostContext?.locale} Viewport: {hostContext?.containerDimensions?.width} x {hostContext?.containerDimensions?.height} Platform: {hostContext?.platform} );
} Tip: If you're using the useApp hook in your MCP App, the hook provides a onhostcontextchanged listener. You can then use a React useState to update your app context. The host will provide their context, it's up to you as the app developer to decide what you want to do with that. For example, you can use theme to render light mode vs dark mode, locale to show a different language, or containerDimensions to adjust the app's sizing.
There are cases where you may want to have granular control over what data the LLM has access to, and what data the view can show. The MCP Apps spec specifies three different tool return values that lets you control data flow, each are handled differently by the app host.
content: Content is the info that you want to expose to the model. Gives model context.
structuredContent: This data is hidden from the model context. It is used to send data over the View for hydration.
_meta: This data is hidden from the model context. Used to provide additional info such as timestamps, version info.
Let's look at a practical example of how we can use these three tool return types effectively:
server.registerTool( "view-cocktail", { title: "Get Cocktail", description: "Fetch a cocktail by id with ingredients and images...", inputSchema: z.object({ id: z.string().describe("The id of the cocktail to fetch.") }), _meta: { ui: { resourceUri: "ui://cocktail/cocktail-recipe-widget.html" }, }, }, async ({ id }: { id: string }): Promise =&gt; { const cocktail = await convexClient.query(api.cocktails.getCocktailById, { id, }); return { content: [ { type: "text", text: `Loaded cocktail "${cocktail.name}".` }, { type: "text", text: `Cocktail ingredients: ${cocktail.ingredients}.` }, { type: "text", text: `Cocktail instructions: ${cocktail.instructions}.` }, ], structuredContent: { cocktail }, _meta: { timestamp: new Date().toString() } }; },
); This tool renders a view showing a cocktail recipe. The cocktail data is being fetched from the backend database (Convex). The View needs the entire cocktail data so we pass the data to it via structuredContent. For the model context, the LLM doesn't need to know the entire cocktail data like the image URL. We can extract the information that the model should know about the cocktail, like the name, ingredients, and instructions. That information can be passed to the model via content.
It's important to note that currently, ChatGPT apps SDK handles it differently, where structuredContent is exposed to both the model and the View. Their model is the following:
content: Content is the info that you want to expose to the model. Gives model context.
structuredContent: This data is exposed to the model and the View.
_meta: This data is hidden from the model context.
If you're building an app that supports both MCP Apps and ChatGPT apps SDK, this is an important distinction. You may want to conditionally return values, or conditionally render tools based off of whether the client is MCP App support or ChatGPT app.
It's pretty typical for the iFrame to render first before the tool finishes executing and the View gets hydrated. You're going to want to let your user know that the app is loading by presenting a beautiful loading state. One powerful feature to note: toolInputs are sent and streamed into the View even before the tool execution is done. This allows you to create cool partial loading states where you can show the user what's being requested while the data is still being fetched.
To implement this, let's take a look at the same cocktail recipes app. The MCP tool fetches the cocktail data and passes it to the View via structuredContent. We don't know how long it takes to fetch that cocktail data, could be anywhere from a few ms to a few seconds on a bad day.
server.registerTool( "view-cocktail", { title: "Get Cocktail", description: "Fetch a cocktail by id with ingredients and images...", inputSchema: z.object({ id: z.string().describe("The id of the cocktail to fetch.") }), _meta: { ui: { resourceUri: "ui://cocktail/cocktail-recipe-widget.html", visibility: ["model", "app"], }, }, }, async ({ id }: { id: string }): Promise =&gt; { const cocktail = await convexClient.query(api.cocktails.getCocktailById, { id, }); return { content: [ { type: "text", text: `Loaded cocktail "${cocktail.name}".` }, ], structuredContent: { cocktail }, }; },
); On the View side (React), the useApp AppBridge hook has a app.ontoolresult listener that listens for the tool return results and hydrates your View. While onToolResult hasn't come in yet and the data is empty, we can render a beautiful loading state.
import { useApp } from "@modelcontextprotocol/ext-apps/react"; function CocktailApp() { const [cocktail, setCocktail] = useState(null); useApp({ appInfo: IMPLEMENTATION, capabilities: {}, onAppCreated: (app) =&gt; { app.ontoolresult = async (result) =&gt; { const data = extractCocktail(result); setCocktail(data); }; }, }); return cocktail ? : ;
} We also want to handle errors gracefully. In the case where there's an error in your tool, such as the cocktail data failing to load, both the LLM and the view should be notified of the error.
In your MCP tool, you should return an error in the tool result. This is exposed to the model and also passed to the view.
server.registerTool( "view-cocktail", { title: "Get Cocktail", description: "Fetch a cocktail by id with ingredients and images...", inputSchema: z.object({ id: z.string().describe("The id of the cocktail to fetch.") }), _meta: { ui: { resourceUri: "ui://cocktail/cocktail-recipe-widget.html" }, visibility: ["model", "app"], }, }, async ({ id }: { id: string }): Promise =&gt; { try { const cocktail = await convexClient.query(api.cocktails.getCocktailById, { id, }); return { content: [ { type: "text", text: `Loaded cocktail "${cocktail.name}".` }, ], structuredContent: { cocktail }, }; } catch (error) { return { content: [ { type: "text", text: `Could not load cocktail` }, ], error }; } },
); Then in useApp on the React client side, you can detect whether or not there was an error by looking at the existence of error from the tool result.
Because your MCP App operates in a sandboxed iframe, the model powering your agent can't see what happens inside the app by default. It won't know if a user fills out a form, clicks a button, or completes a purchase.
Without a feedback loop, the model loses context. If a user buys a pair of shoes and then asks, "When will they arrive?", the model won't even realize a transaction occurred.
To solve this, the SDK provides two methods to keep the model synchronized with the user's journey: sendMessage and updateModelContext.
Use this for active triggers. It sends a message to the model as if the user typed it, prompting an immediate response. This is ideal for confirming a "Buy" click or suggesting related items right after an action.
// User clicks "Buy" - the model responds immediately
await app.sendMessage({ role: "user", content: [{ type: "text", text: "I just purchased Nike Air Max for $129" }],
});
// Result: Model responds: "Great choice! Want me to track your order?" Use this for background awareness. It quietly saves information for the model to use later without interrupting the flow. This is perfect for tracking browsing history or cart updates without triggering a chat response every time.
// User is browsing - no immediate response needed
await app.updateModelContext({ content: [{ type: "text", text: "User is viewing: Nike Air Max, Size 10, $129" }],
});
// Result: No response. But if the user later asks, "What was I looking at?", the model knows. With a standard MCP server, the model sees your tools, interprets the user's prompt, and calls the right tool. If a user says "delete that email," the model decides what that means and invokes the delete tool.
However, with an MCP App, tools can be triggered in two ways: the model interpreting the user's prompt, or the user interacting directly with the UI.
By default, both can call any tool. For example, say you build an MCP App that visually surfaces an email inbox and lets users interact with emails. Now there are two potential triggers for your tools: the model acting on a prompt to delete an email, and the user clicking a delete button directly in the App's interface.
The model works by interpreting intent. If a user says "delete my old emails," the model has to decide what "old" means and which emails qualify. For some actions like deleting emails, that ambiguity can be risky.
When a user clicks a "Delete" button next to a specific message in your MCP App, there is no ambiguity. They have made an explicit choice.
To prevent the model from accidentally performing high-stakes actions based on a misunderstanding, you can use tool visibility to restrict certain tools to the MCP App's UI only. This allows the model to display the interface while requiring a human click to finalize the action.
You can define visibility using these three configurations:
["model", "app"] (default) — Both the model and the UI can call it
["model"] — Only the model can call it; the UI cannot
["app"] — Only the UI can call it; hidden from the model
Here's how you might implement this:
// Model calls this to display the inbox
registerAppTool(server, "show-inbox", { description: "Display the user's inbox", _meta: { ui: { resourceUri: "ui://email/inbox.html", visibility: ["model"], }, },
}, async () =&gt; { const emails = await getEmails(); return { content: [{ type: "text", text: JSON.stringify(emails) }] };
}); // User clicks delete button in the UI
registerAppTool(server, "delete-email", { description: "Delete an email", inputSchema: { emailId: z.string() }, _meta: { ui: { resourceUri: "ui://email/inbox.html", visibility: ["app"], }, },
}, async ({ emailId }) =&gt; { await deleteEmail(emailId); return { content: [{ type: "text", text: "Email deleted" }] };
}); MCP Apps open up a new dimension for agent interactions. Now it's time to build your own.
Test with MCPJam — the open source local inspector for MCP Apps, ChatGPT apps SDK, and MCP servers. Perfect for debugging and iterating on your app before shipping.
Run in goose — an open source AI agent that renders MCP Apps directly in the chat interface. See your app come to life in a real agent environment.
Ready to dive deeper? Check out the MCP Apps tutorial or build your first MCP App with MCPJam.]]></description>
      <pubDate>Thu, 19 Feb 2026 06:49:01 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/goose_oss/5-tips-for-building-mcp-apps-that-work-2pme</guid>
    </item>
    <item>
      <title><![CDATA[3 Best sites to Buy Telegram Accounts (PVA &amp; Aged)]]></title>
      <link>https://dev.to/mudasir12/3-best-sites-to-buy-telegram-accounts-pva-aged-57nk</link>
      <description><![CDATA[Buy Telegram Accounts from GetUSASMM.com @getusasmm
@getusasmm
https://getusasmm.com/product/buy-telegram-accounts/
Telegram accounts are powerful tools for communication, marketing, and business growth. As one of the fastest-growing messaging platforms, Telegram offers secure, fast, and versatile communication features that make it ideal for both personal and professional use. Verified and aged Telegram accounts are especially valuable for marketers, businesses, and influencers who want to expand their reach and manage multiple campaigns efficiently. GetUSASMM.com provides a trusted and secure platform to buy Telegram accounts that are verified, active, and ready for immediate use.
Secure and Encrypted Messaging
Telegram is known for its end-to-end encryption and privacy features. Businesses and individuals use it for secure communication and confidential data sharing.
Ideal for Marketing and Promotion
Telegram channels and groups are excellent for promoting products, services, and brands. Aged Telegram accounts help marketers manage multiple channels and campaigns effectively.
Global Reach
Telegram has millions of active users worldwide, making it a perfect platform for global marketing and audience engagement.
Automation and Bots
Telegram supports bots and automation tools that help businesses manage customer service, marketing, and notifications efficiently.
Benefits of Buying Telegram Accounts from GetUSASMM.com
100% Verified Accounts
GetUSASMM.com provides fully verified Telegram accounts with authentic details. Each account is tested for functionality and security before delivery.
Instant Delivery
After purchase, accounts are delivered instantly through secure channels, allowing immediate use.
Affordable Pricing
The platform offers competitive pricing for both individual and bulk purchases. Discounts are available for large orders, making it cost-effective for businesses and marketers.
Secure Payment Options
GetUSASMM.com supports multiple secure payment methods, ensuring safe and smooth transactions.
24/7 Customer Support
A dedicated support team is available around the clock to assist with any queries or technical issues related to Telegram accounts.
Types of Telegram Accounts Available
Aged Telegram Accounts
These accounts were created months or years ago and are ideal for marketing, group management, and business communication. Their age adds credibility and trust.
Fresh Telegram Accounts
Recently created accounts that are verified and ready for use. Suitable for users who need new accounts for general purposes.
Phone Verified Telegram Accounts (PVA)
These accounts are verified using phone numbers, adding an extra layer of security. PVAs are perfect for users who require high-security accounts for sensitive operations.
Bulk Telegram Accounts
Businesses and marketers can purchase Telegram accounts in bulk for large-scale campaigns. Bulk packages come with discounted rates and instant delivery.
How to Buy Telegram Accounts from GetUSASMM.com @getusasmm
@getusasmm
https://getusasmm.com/product/buy-telegram-accounts/
Step 1: Visit the Website
Improved Brand Visibility
Telegram accounts can be used to create channels and groups that promote products and services, increasing brand awareness.
Better Audience Engagement
Telegram allows direct interaction with followers, improving engagement and customer relationships.
Enhanced Marketing Campaigns
Using multiple Telegram accounts helps marketers manage different campaigns simultaneously, improving efficiency and reach.
Increased Traffic and Conversions
@getusasmm
@getusasmm]]></description>
      <pubDate>Thu, 19 Feb 2026 06:38:28 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/mudasir12/3-best-sites-to-buy-telegram-accounts-pva-aged-57nk</guid>
    </item>
    <item>
      <title><![CDATA[Revue de presse de l’April pour la semaine 7 de l’année 2026]]></title>
      <link>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Cette revue de presse sur Internet fait partie du travail de veille mené par l’April dans le cadre de son action de défense et de promotion du logiciel libre. Les positions exposées dans les articles sont celles de leurs auteurs et ne rejoignent pas forcément celles de l’April.
[Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€)
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre?
[ZDNET] LibreOffice dénonce le format OOXML
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte lien nᵒ 1 : April
lien nᵒ 2 : Revue de presse de l'April
lien nᵒ 3 : Revue de presse de la semaine précédente
lien nᵒ 4 : Fils du Net [Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux Tiago Gil, le jeudi 12 février 2026.
La centrale d’achat informatique hospitalière (CAIH) engage une nouvelle feuille de route sur cinq ans et initie le programme Alternative, destiné à bâtir un socle numérique souverain pour les systèmes d’information de santé.
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€) Valéry Rieß-Marchive, le mercredi 11 février 2026.
L’Agence nationale de la sécurité des systèmes d’information vient de réitérer son engagement en faveur du logiciel libre. Dans la continuité d’une politique établie et confortée de longue date.
Et aussi: [Le Monde Informatique] L'Anssi formalise sa doctrine open source
[Silicon] L’ANSSI affirme l’open source comme levier de sa politique industrielle
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre? Bertrand Lemaire, le mercredi 11 février 2026.
L’APRIL relance son initiative «Pacte du Logiciel Libre» à l’occasion du prochain scrutin municipal.
Et aussi: [Goodtech] Municipales 2026 en France: l'April lance son pacte du logiciel libre
Voir aussi: L’April propose le pacte du logiciel libre à l’occasion des élections municipales et communautaires de 2026
[ZDNET] LibreOffice dénonce le format OOXML
Le mercredi 11 février 2026.
The Document Foundation (TDF) intensifie sa critique contre Microsoft, accusant le géant américain de privilégier ses intérêts commerciaux au détriment de l’interopérabilité.
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte Aymeric Geoffre-Rouland, le lundi 9 février 2026.
Quand un développeur demande à Claude ou ChatGPT d’écrire du code, l’IA pioche dans des milliers de bibliothèques libres sans que l’humain ne lise jamais leur documentation. Résultat: les mainteneurs de ces projets open-source, qui vivent de la visibilité générée par les visites et les interactions, voient leur audience s’effondrer. Une étude économique chiffre ce paradoxe: l’IA qui accélère le développement logiciel asphyxie l’écosystème qui le rend possible.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:40 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[Nouveautés de février 2026 de la communauté Scenari]]></title>
      <link>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</link>
      <description><![CDATA[Scenari est un ensemble de logiciels open source dédiés à la production collaborative, publication et diffusion de documents multi-support. Vous rédigez une seule fois votre contenu et vous pouvez les générer sous plusieurs formes : site web, PDF, OpenDocument, diaporama, paquet SCORM (Sharable Content Object Reference Model)… Vous ne vous concentrez que sur le contenu et l’outil se charge de créer un rendu professionnel accessible et responsive (qui s’adapte à la taille de l’écran).
À chaque métier/contexte son modèle Scenari :
Opale pour la formation Dokiel pour la documentation Optim pour les présentations génériques Topaze pour les études de cas Parcours pour créer des scénarios de formation et bien d’autres… lien nᵒ 1 : Explication de Scenari
lien nᵒ 2 : Pour démarrer
lien nᵒ 3 : Téléchargements
lien nᵒ 4 : Communauté Scenari
lien nᵒ 5 : Mastodon
lien nᵒ 6 : Bluesky
lien nᵒ 7 : Telegram
lien nᵒ 8 : LinkedIn
lien nᵒ 9 : Canal Peertube Sommaire Visio de découverte de Scenari Parole de Scenariste Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Tu peux parler de Scenari aux conférences éclair de l’April ? Nouvel habillage web pour Optim 24 Mise-à-jour de Myscenari Nouvelles versions d’outils Scenari Le savais-tu ? Le chiffre du mois Nouvelles adhésions d’organisations Visio de découverte de Scenari Tu as des questions sur Scenari avant de tester ?
Cette visio est faite pour toi : jeudi 26 février à 16h sur https://scenari.org/visio/miniwebinaire
Lien Agenda du Libre
Lien Mobilizon Parole de Scenariste
Utilisateur de Canoprof depuis 2019, cet outil est devenu un des piliers de ma pratique d’enseignement en Physique-Chimie (4ᵉ, 5ᵉ, 3ᵉ) et en Sciences (6ᵉ). Je l’utilise pour concevoir l’ensemble de mes supports aussi bien papier que numériques, ce qui me permet de maintenir une cohérence didactique forte sur l’ensemble du cursus collège.
La force de Canoprof réside dans la séparation claire entre le contenu et la forme. En tant qu’enseignant, cela me permet de me concentrer sur le fond pédagogique et la structuration de mes séquences, sans perdre de temps dans les contraintes techniques de mise en page. La richesse de mon fond documentaire, construit depuis plus de six ans, évolue ainsi sereinement au fil des réformes et de mes retours d’expérience.
Canoprof m’aide à formaliser une progression spiralaire efficace tout en générant des supports propres, structurés et accessibles. C’est un gain de productivité précieux qui me permet de consacrer plus d’énergie à l’accompagnement de mes élèves en classe. Guillaume Marmin, enseignant de physique-chimie au Collège Isabelle Autissier. Modèle utilisé : Canoprof Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Les Rencontres Scenari 2026 auront lieu du lundi 22 juin (midi) au vendredi 26 juin (midi) sous le soleil provençal à l'ENSAM Aix-en-Provence.
Bloque ces dates dès maintenant, les détails seront précisés bientôt. Tu peux parler de Scenari aux conférences éclair de l’April ? Lors de la prochaine assemblée générale de l’April (samedi 28 mars 2026 à Paris) il y aura un temps de conférences éclairs (6 minutes) de 10h à 12h qui s’enchaîneront sur des sujets variés, en lien avec le Libre, entendu au sens large.
Si tu utilises Scenari, c’est une bonne opportunité pour parler de tes usages auprès des adhérent⋅e⋅s de l’April. Date limite pour proposer : 15 mars. Envoyer un courriel à confseclairs@april.org.
Il n’est pas nécessaire d’être adhérent⋅e à l’April pour pouvoir proposer une conférence éclair.
Plus de détails sur l’annonce de l’April. Nouvel habillage web pour Optim 24 Un nouvel habillage graphique pour Optim 24 fait son apparition sur la plateforme de téléchargement.
Il existe pour tous les supports web des 3 modalités d’Optim : site normal, site web simple, site web en tuiles. Mise-à-jour de Myscenari MyScenari vient de passer en version 6.4.5 (corrections de bugs dans le cœur et dans les modèles en version 25). Attention : cette version est la dernière à contenir Dokiel 5 et 6, Opale 5 et 24, Optim 3 À partir de la prochaine mise à jour de MyScenari, nous n’aurons plus que Dokiel 25, Opale 25, Optim 24. Pense à migrer tes modèles (et skins) pour ne pas être pris⋅e au dépourvu au dernier moment. Nouvelles versions d’outils Scenari Opale, le modèle phare pour créer vos contenus pédagogiques, passe en version 25.1.1. Au menu, entre autres : corrections dans les outils d’accessibilité, et amélioration de l’intégration de MindMap dans la publication Diapo. Et Opale est maintenant disponible en allemand ! Parcours, pour concevoir des conducteurs pédagogiques, passe en version 25.0.2 (corrections mineures sur le skin, l’éditeur et les vidéos HLS) et est disponible maintenant en français et Anglais. Dokiel, le modèle pour la documentation technique et logicielle, passe en version 25.0.6. Cette version apporte entre autres des corrections dans la publication de relecture et l’écran de contrôle, et l’amélioration des écrans décrits dans les publications Web (maintenant responsive). Optim monte en version dans ses deux saveurs Optim 24.0.7 et OptimPlus 24.0.3 avec des corrections mineures sur les publications Web et Diaporama, et dans le styage. LTI-suite, le serveur pour exploiter des ressources SCORM dans des LMS via LTI, passe en version 2.0.3. Lexico, votre modèle pour créer des lexiques, glossaires, thesaurus, vocabulaires, monte en version 25.0.1 pour apporter des corrections mineures dans la publication Web. SCENARIchain-desktop est à présent disponible en français, en anglais et en espagnol. Le savais-tu ?
En contexte d’ateliers complexes (plusieurs calques de dérivation et/ou de travail), les détails dans le bandeau de l’item listent les variantes de cet item dans les autres ateliers calques ou de travail, s’il en existe.
Dans l’exemple ci-dessous, l’item _Module-LeThe.xml dans l’atelier maître (icone d’atelier bleu) est modifié dans un atelier de travail (icone d’atelier vert) et modifié aussi dans un atelier dérivé (icone d’atelier marron). On peut passer facilement d’une version à l’autre en un seul clic. La popup est détachable pour plus d’aisance si besoin.
Exemple Le chiffre du mois 20, c’est le nombre d’années qui se sont écoulées depuis la première sortie d’Opale le 18/09/2006 (les développements avaient commencé en novembre 2005). Nouvelles adhésions d’organisations
Souhaitons la bienvenue à :
Institution Azahrae qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
L’Université Bourgogne Europe qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
URBILOG qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 15:59:55 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</guid>
    </item>
    <item>
      <title><![CDATA[The world of open source metadata]]></title>
      <link>https://changelog.com/podcast/665</link>
      <description><![CDATA[Andrew Nesbitt builds tools and open datasets to support, sustain, and secure critical digital infrastructure. He's been exploring the world of open source metadata for over a decade. First with libraries.io and now with ecosyste.ms, which tracks over 12 million packages, 287 million repos, 24.5 billion dependencies, and 1.9 million maintainers. What has Andrew learned from all this, who is using this open dataset, and how does he hope others can build on top of it all? Tune in to find out.]]></description>
      <pubDate>Wed, 05 Nov 2025 20:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/665</guid>
    </item>
    <item>
      <title><![CDATA[databricks-solutions/ai-dev-kit]]></title>
      <link>https://github.com/databricks-solutions/ai-dev-kit</link>
      <description><![CDATA[Databricks Toolkit for Coding Agents provided by Field Engineering]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/databricks-solutions/ai-dev-kit</guid>
    </item>
    <item>
      <title><![CDATA[I Built an ORM That Starts With JSON So You Can Stop Overthinking Your Database]]></title>
      <link>https://dev.to/joshmatthew/i-built-an-orm-that-starts-with-json-so-you-can-stop-overthinking-your-database-2439</link>
      <description><![CDATA[You know that thing where you have a project idea at 1am, you're hyped, and then you spend the next 2 hours setting up Postgres, writing migrations, configuring connection strings, and by the time you're done your motivation is gone and you go to bed?
Yeah. That kept happening to me. So I built something about it.
SeedORM is a Node.js ORM that stores everything in local JSON files. No database server, no Docker, no connection strings. You define your models and start building immediately.
Then when your project is ready for the real world, you migrate to Postgres with one command. Same models, same queries, same API. Nothing changes except where the data lives.
npm install seedorm Every side project I start follows the same pattern:
Cool idea hits me
Create new folder, npm init Okay now I need a database
Do I use Postgres? MongoDB? SQLite?
Spend 30 minutes setting up Docker, env vars, connection pooling
Write migration files
I'm tired now
Abandon project
I wanted something where step 3 is just... skip.
Define your models like you normally would:
import { SeedORM, FieldType } from "seedorm"; const db = new SeedORM();
await db.connect(); const User = db.model({ name: "users", fields: { name: { type: FieldType.String, required: true }, email: { type: FieldType.String, unique: true }, age: { type: FieldType.Number }, },
}); await User.init(); That's it. No database running. No config file pointing to localhost:5432. It just writes to JSON files in a data/ folder.
Now use it like any ORM:
// Create
const user = await User.create({ name: "Josh", email: "josh@hey.com", age: 24 }); // Query
const adults = await User.find({ filter: { age: { $gte: 18 } } }); // Update
await User.update(user.id, { age: 25 }); // Delete
await User.delete(user.id); It supports the relation types you'd expect:
const Post = db.model({ name: "posts", fields: { title: { type: FieldType.String, required: true }, body: { type: FieldType.String }, }, relations: { author: { type: "belongsTo", model: "users", foreignKey: "authorId", }, },
}); hasMany, belongsTo, manyToMany. They all work the same way when you eventually switch to Postgres. I got tired of opening JSON files to check my data, so I built a visual browser for it. Run seedorm studio and you get a web UI at localhost:4200 where you can browse collections, search across fields, paginate through documents, and edit records.
It's nothing fancy. It's just nice to have when you're building and want to see what's in your database without writing a query.
One thing that bugged me early on was that everything was stored in a single seedorm.json file. With a few thousand documents across multiple collections, that file gets massive.
So now each collection gets its own file:
data/ users.json posts.json .json Each file is minified (single line), writes only happen for collections that changed, and if you have an old seedorm.json from a previous version it auto-migrates on startup. You don't have to do anything.
Your project took off. Congrats. Time for a real database.
seedorm migrate to postgres Your models stay the same. Your queries stay the same. You just change the adapter config:
const db = new SeedORM({ adapter: { adapter: "postgres", url: "postgresql://localhost:5432/myapp", },
}); That's the whole point. Build fast with JSON, switch when it matters.
Let me be clear about what SeedORM is not:
It's not a Prisma replacement for your production app with 10 million users
It's not meant for high-concurrency workloads
It's not trying to be the last ORM you'll ever use
It's for prototyping, side projects, hackathons, and that 1am idea that deserves to exist without a 30 minute setup ritual.
npm install seedorm GitHub
npm
If you try it, I'd genuinely love to hear what you think. What's missing? What's annoying? What would make you actually use this?
Thanks for reading.]]></description>
      <pubDate>Thu, 19 Feb 2026 03:36:05 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/joshmatthew/i-built-an-orm-that-starts-with-json-so-you-can-stop-overthinking-your-database-2439</guid>
    </item>
    <item>
      <title><![CDATA[What to expect for open source in 2026]]></title>
      <link>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</link>
      <description><![CDATA[Let’s dig into the 2025’s open source data on GitHub to see what we can learn about the future.]]></description>
      <pubDate>Wed, 18 Feb 2026 18:41:42 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</guid>
    </item>
    <item>
      <title><![CDATA[Securing the AI software supply chain: Security results across 67 open source projects]]></title>
      <link>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</link>
      <description><![CDATA[Learn how The GitHub Secure Open Source Fund helped 67 critical AI‑stack projects accelerate fixes, strengthen ecosystems, and advance open source resilience.]]></description>
      <pubDate>Tue, 17 Feb 2026 19:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</guid>
    </item>
    <item>
      <title><![CDATA[Kotlin Multiplatform - Flutter - React Native : entre choix, compromis et frustrations]]></title>
      <link>https://www.programmez.com/actualites/kotlin-multiplatform-flutter-react-native-entre-choix-compromis-et-frustrations-39024</link>
      <description><![CDATA[Nos confrères de Java Code Geeks ont publié un intéressant dossier sur le multiplateforme en 2026 en s'appuyant sur Kotlin Multiplatform (KMP), Flutter et React Native. Faire du multiplateforme avec une base de codes et un minimum d'adaptation reste un objectif pour de nombreux développeurs. Si la philosophie de KMP, Flutter et React Native est différente, l'idée est la même : compiler nativement le code logique le plus agnostique possible et créer une interface native pour chaque plateforme. Flutter est un peu différent car il a l'ambition d'adresser toute la stack et de générer l'UI avec son propre moteur pour plus de cohérence. React Native s'appuie sur les composants UI natifs.
Selon les benchmarks de Java Code Geeks, React Native serait le plus lent à démarrer, KMP étant légèrement devant. Sur la taille des binaires, il n'y a pas de réel vainqueur. Par contre, sur la mémoire, React Native et Flutter sont assez gourmands. Sur les animations, KMP et Flutter s'en sortent le mieux. React Native reste aussi en retrait sur l'intégration à la plateforme : nous restons dans un modèle JavaScript avec un risque d'overhead, même si la New Architecture améliore les choses. Quelle est la solution la plus utilisée ? Flutter serait 1er, React Native baisse régulièrement depuis 2023 et KMP connaît une forte progression.
Apprentissage : KMP : langage connu, Kotlin, avec les mêmes outils. Pour le développeur iOS, il faut apprendre Kotlin/Native et l’interopérabilité. KMP est peut-être la solution la moins mature. Flutter : l'inconvénient est d'apprendre Dart et la logique de la plateforme. React Native : si vous connaissez JavaScript, vous connaissez (ou presque) React Native. L'arrivée de la New Architecture oblige à migrer et à apprendre une nouvelle stack. Pour la réalité du code commun et du développement spécifique, tout le monde prétend faire 90 à 95 % de code partagé. Cette promesse est plus ou moins tenue sur le code logique et une UI simple et partagée. Par contre, pour l'intégration plus profonde, par exemple avec les capteurs et le matériel (caméra typiquement), on tombe vite sur du code spécifique. Aucune solution n'est la meilleure. Flutter et React Native incitent à avoir le maximum de code commun, mais cela peut rapidement provoquer des problèmes quand il faut intégrer des fonctions spécifiques à chaque plateforme.
Côté compétence, c'est autre chose. Un développeur JavaScript pourra relativement rapidement faire du React Native. Pour Flutter, il faut spécifiquement apprendre Dart. KMP repose sur le langage Kotlin et une plateforme dédiée qu'il faut maîtriser. Pour un développeur iOS, ce sera sans doute plus long que pour un développeur Kotlin. choisir ? Tout dépend des compétences disponibles et du projet. Flutter permettra de prototyper rapidement un projet, KMP fournit une intégration native et des performances de haut niveau. React Native est sans doute le plus facile à démarrer avec un profil JavaScript si vous souhaitez aller vite dans le développement.
Source: https://www.javacodegeeks.com/2026/02/kotlin-multiplatform-vs-flutter-vs-react-native-the-2026-cross-platform-reality.html Catégorie actualité: Frameworks Flutter, React Native, Kotlin Multiplatform Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 08:24:45 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/kotlin-multiplatform-flutter-react-native-entre-choix-compromis-et-frustrations-39024</guid>
    </item>
    <item>
      <title><![CDATA[DevTools : les nouveautés de Chrome 145]]></title>
      <link>https://www.programmez.com/actualites/devtools-les-nouveautes-de-chrome-145-39021</link>
      <description><![CDATA[Une des nouveautés les plus importantes est l'intégration Soft Navigations. L'équipe Chrome présente ainsi cette appelleration : a soft navigation est quand JavaScript intercepte une navigation (clic sur un lien) et met à jour le contenu dans la page existente, plutôt que de charger une nouvelle page et que l'URL se mette à jour dans la barre d'adresse. Pour l'utilisateur, cela change peu de choses. Dans Chrome 145, les Soft navigations sont visibles sur le panneau Performance et dans la vue des traces si le site est une SPA. Un timer plus précis
Après l'enregistrement d'une trace dans le panneau Performances, le panneau Sources affiche les temps d'exécution observés ligne par ligne. Vous pouvez ainsi identifier précisément les lignes de code qui consomment le plus de temps.Auparavant, cette fonctionnalité présentait des bogues qui la rendaient peu fiable lorsque le code source était formaté (à l'aide du bouton {}) ou lors de l'utilisation de scripts avec mappage de sources. Le panneau réseau inclut maintenant une colonne dédiée Render blocking. Cela permet de voir les ressources qui bloquent le bon affichage. Autre amélioration : un meilleur debug pour @starting-style. Note de version : https://developer.chrome.com/blog/new-in-devtools-145 Catégorie actualité: Outils DevTools Image actualité AMP:]]></description>
      <pubDate>Mon, 16 Feb 2026 14:43:45 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/devtools-les-nouveautes-de-chrome-145-39021</guid>
    </item>
    <item>
      <title><![CDATA[Concours - Gagnez une Raspberry Pi 5 avec Macé Robotics]]></title>
      <link>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</link>
      <description><![CDATA[À l’occasion de ses 10 ans de Macé Robotics, l’entreprise organise un concours qui se déroulera jusqu'au 26 février 2026.
Macé Robotics est une entreprise individuelle fondée et gérée par moi-même (Nicolas), basée en Bretagne, spécialisée dans la conception et la réparation électronique, aussi bien pour les entreprises que pour les particuliers. Depuis 2016, je fabrique aussi du matériel Open Source également des robots mobiles Open Source destinés à l’enseignement supérieur et à la recherche. Ces robots sont basés sur un système Linux (Raspberry Pi OS), intégrant une carte Raspberry Pi ainsi qu’un microcontrôleur (Pico) dédié à la gestion des moteurs et des capteurs. J’utilise la suite logicielle KiCad sous licence GNU GPL (https://www.kicad.org/) pour la conception des circuits imprimés de ces robots. Attribution des lots par tirage au sort :
→ 1er lot : une carte Raspberry Pi 5 (2 Go) → 2e lot : une carte Raspberry Pi Pico 2W
La livraison est offerte en France. lien nᵒ 1 : Le concours pour participer Retour sur la course de robots – Saint-Brock Robot Race d'une dépêche précédente
Suite à la dépêche de décembre 2024 concernant l’organisation de la course de robots mobiles, voici quelques retours sur cet événement : malgré plusieurs annulations d’écoles survenues quelques semaines avant la compétition, la course a tout de même pu avoir lieu.
Environ quinze participants ont pris part à la compétition. Parmi les robots engagés, on comptait un robot DIY piloté par un microcontrôleur ESP32, aux côtés de plusieurs robots basé sur Raspberry Pi, offrant ainsi une belle diversité technologique.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Sat, 14 Feb 2026 08:47:09 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</guid>
    </item>
    <item>
      <title><![CDATA[L’ANSSI révise sa doctrine vis-à-vis du logiciel libre]]></title>
      <link>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</link>
      <description><![CDATA[L’ANSSI (Agence nationale de la sécurité des systèmes d’information) vient de publier une mise à jour substantielle de sa doctrine vis-à-vis du logiciel libre. L’agence confirme que le logiciel libre et la transparence sont essentiels à la sécurité des systèmes d’information. Elle assume sa contribution au libre et la publication de logiciels sous licence libre.
Cette posture très favorable au logiciel libre et open source est une belle avancée et un signal fort. Jusque-là, la posture de l’ANSSI était beaucoup plus floue et sa contribution à des projets libres et open source pouvait même apparaitre en contradiction avec sa doctrine. J’avais l’impression que les collaborateurs de l’ANSSI qui le faisaient reprenaient à leur compte le dicton « Pour vivre heureux, vivons cachés ».
La politique de l’agence est désormais claire : l’ANSSI contribue, l’ANSSI publie, l’ANSSI a une stratégie pragmatique qui peut l’amener à s’engager ou non sur le long terme en fonction de la finalité de l’outil et des motivations de l’ANSSI.
Détail qui a son importance, l’ANSSI indique privilégier, sauf exception justifiée, la licence Apache v2.0 pour les projets qu’elle publie. Je suis ravi de voir ce service privilégier une licence mondialement connue à une licence franco-française ou européenne (elles ont le don de doucher nombre de velléités d’utilisation et de contribution). lien nᵒ 1 : L’ANSSI met à jour sa politique open source (9 février 2026)
lien nᵒ 2 : Posture générale et actions de l'ANSSI sur l'open-source Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 18:55:42 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</guid>
    </item>
    <item>
      <title><![CDATA[Le prochain Drupalcamp se déroulera à Grenoble les 9, 10 et 11 avril 2026 prochain]]></title>
      <link>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</link>
      <description><![CDATA[L’association Drupal France &amp; Francophonie organise la 13ème édition du Drupalcamp les 9, 10 et 11 avril 2026 au campus Universitaire Grenoble Alpes de Grenoble (France, Isère 38). Drupal est « un système de gestion de contenu (CMS) libre et open-source publié sous la licence publique générale GNU et écrit en PHP ».
Après Rennes en 2024, puis un Barcamp à Perpignan en 2025, cette année 2026 nous emmène au pied des montagnes à Grenoble pour un format de 3 jours de rencontres, soit deux journées de conférences les jeudi et vendredi. La journée du samedi est réservée à la contribution.
Des moments d’ateliers et micro-formation sont également au programme, pour faire de cet évènement une réussite d’un point de vue communauté autour du projet Open Source Drupal.
Le Drupalcamp Grenoble c’est la rencontre de la communauté francophone autour du logiciel libre Drupal. Ouvert à toutes et tous, les rencontres, conférences et ateliers permettent d’adresser à un public toujours plus large des sujets et thématiques diversifiées.
Notre objectif principal est de rendre la création de sites plus simple et la gestion des contenus plus intuitive pour tous. Comme de fédérer les utilisateurs et professionnels qui utilisent Drupal au quotidien.
Du simple curieux au développeur expert, tous ceux qui s’intéressent à Drupal et aux logiciels libres pourront participer à cette manifestation rythmée par :
des conférences (jeudi 9 et vendredi 10 avril), données par des professionnels reconnus et des membres de la communauté Drupal au cours desquels des thématiques nouvelles seront explorées,
des sessions de découverte étayées par des démonstrations à l’intention d’un public plus néophyte,
une journée de formation gratuite (Drupal in a Day) dédiée à l’initiation pour que les curieux puissent se lancer dans la création de leur premier site (sur inscription)
des moments de réseautage et de convivialité avec, notamment, la très attendue soirée communautaire !
Informations pratiques : Campus Universitaire Grenoble Alpes qui se situe à Saint-Martin d'Hères
https://grenoble2026.drupalcamp.fr/
Contact : drupalcamp@drupal.fr lien nᵒ 1 : https://grenoble2026.drupalcamp.fr Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 10 Feb 2026 09:16:59 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</guid>
    </item>
    <item>
      <title><![CDATA[The GitHub problem (and other predictions)]]></title>
      <link>https://changelog.com/friends/123</link>
      <description><![CDATA[Mat Ryer is back and he brought his impromptu musical abilities with him! We discuss Rob Pike vs thankful AI, Microsoft's GitHub monopoly (and what it means for open source), and Tom Tunguz' 12 predictions for 2026: agent-first design, the rise of vector databases, and are we about to pay more for AI than people?!]]></description>
      <pubDate>Wed, 14 Jan 2026 21:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/123</guid>
    </item>
    <item>
      <title><![CDATA[Down the Linux rabbit hole]]></title>
      <link>https://changelog.com/friends/121</link>
      <description><![CDATA[Alex Kretzschmar joins Adam for a trip down the Linux rabbit hole -- Docker vs Podman, building a Kubernetes cluster, ZFS backups with zfs.rent, bootc, favorite Linux distros, new homelab tools built with AI, self-hosting Immich, content creation, Plex and Jellyfin, the future of piracy and more.]]></description>
      <pubDate>Fri, 12 Dec 2025 19:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/121</guid>
    </item>
    <item>
      <title><![CDATA[There will be bleeps]]></title>
      <link>https://changelog.com/friends/113</link>
      <description><![CDATA[Mike McQuaid and Justin Searls join Jerod in the wake of the RubyGems debacle to discuss what happened, what it says about money in open source, what sustainability really means for our community, making a career out of open source (or not), and more. Bleep!]]></description>
      <pubDate>Fri, 17 Oct 2025 18:15:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/113</guid>
    </item>
    <item>
      <title><![CDATA[obra/superpowers]]></title>
      <link>https://github.com/obra/superpowers</link>
      <description><![CDATA[obra/superpowers]]></description>
      <pubDate>Thu, 19 Feb 2026 06:58:29 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/obra/superpowers</guid>
    </item>
    <item>
      <title><![CDATA[DropThe Article]]></title>
      <link>https://dev.to/dropthe/dropthe-article-2cil</link>
      <description><![CDATA[Originally published on DropThe.org.
On Monday, Meta and Nvidia announced a multi-year deal that puts millions of Blackwell and Rubin GPUs into Meta’s data centers. Not thousands. Millions. Along with Grace and Vera CPUs and Spectrum-X networking — a full-stack Nvidia buildout that positions Meta as one of the largest single buyers of AI compute on the planet.
The deal is massive. But it is not unusual. It is the latest move in an infrastructure arms race that has pushed combined AI capex from five US tech companies past $690 billion for 2026. Nearly double what they spent in 2025.
We tracked the spending commitments of every major player in this race — from the hyperscalers pouring hundreds of billions into data centers to the AI startups burning through revenue faster than they earn it. Here is where each company stands.
Five companies account for nearly all of it. Their 2026 capital expenditure plans, most of it directed at AI compute and data centers:
Amazon
$200B
Alphabet
$175-185B
Meta
$115-135B
Microsoft
$120B+
Oracle
$50B
Source: DROPTHE_ analysis of public earnings guidance and capex commitments, Feb 2026
Amazon leads at $200 billion — a number that shocked even bullish analysts. Consensus expectations had been closer to $147 billion. CEO Andy Jassy defended the figure by pointing to AWS revenue hitting a $142 billion annualized run rate with growth accelerating to 24% year-over-year. The stock still dropped 8-10% on the announcement.
Alphabet‘s $175-185 billion plan has been revised upward three times from an initial $71-73 billion target for 2025. CEO Sundar Pichai acknowledged the scale causes concern internally. But the cloud backlog surged 55% sequentially to over $240 billion. The demand is real.
This is an excerpt. Read the full analysis with charts and data on DropThe.org
DropThe is a data platform tracking 1.83 million entities across movies, games, companies, people, and crypto — connected by 2.18 million knowledge graph links. We don't guess. We count.
Full tech statistics
Our methodology
All data insights
dropthe.org]]></description>
      <pubDate>Thu, 19 Feb 2026 04:16:39 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/dropthe/dropthe-article-2cil</guid>
    </item>
    <item>
      <title><![CDATA[Copilot coding agent supports code referencing]]></title>
      <link>https://github.blog/changelog/2026-02-18-copilot-coding-agent-supports-code-referencing</link>
      <description><![CDATA[Copilot coding agent, our asynchronous, autonomous background agent, now works with Copilot code referencing. If the agent generates code that matches code in a public GitHub repository, the matching code…]]></description>
      <pubDate>Wed, 18 Feb 2026 22:02:22 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-18-copilot-coding-agent-supports-code-referencing</guid>
    </item>
    <item>
      <title><![CDATA[Secret scanning improvements to extended metadata checks]]></title>
      <link>https://github.blog/changelog/2026-02-18-secret-scanning-improvements-to-extended-metadata-checks</link>
      <description><![CDATA[GitHub secret scanning is adding support for extended metadata checks in security configurations. This change makes it substantially easier to enable extended metadata checks at scale. As announced previously, repositories…]]></description>
      <pubDate>Wed, 18 Feb 2026 21:31:29 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-18-secret-scanning-improvements-to-extended-metadata-checks</guid>
    </item>
    <item>
      <title><![CDATA[Quantique : Comcast, Classiq et AMD testent un algorithme quantique pour les réseaux]]></title>
      <link>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</link>
      <description><![CDATA[Comcast, Classiq et AMD mènent des tests pour améliorer le trafic Internet en utilisant des algorithmes quantiques pour renforcer la résistance du routage réseau. "L’essai conjoint s’est concentré sur un défi clé de la conception des réseaux : identifier des chemins de secours indépendants pour les nœuds du réseau lors des opérations de maintenance ou de modifications. L’objectif était de garantir que, si un site est mis hors ligne et que soudainement, un deuxième tombe en panne, le trafic puisse être redirigé sans interruption ni dégradation du service pour les clients. Pour y parvenir, les opérateurs doivent identifier des chemins de secours distincts, rapides et capables de résister à des pannes simultanées, tout en minimisant la latence. Cette tâche devient de plus en plus complexe à mesure que le réseau s’étend." explique l'annonce. Le schéma présente le design et l'implémentation du flux et de l'algo quantique sur la plateforme Classiq. L’expérimentation a combiné des techniques de calcul quantique et des méthodes classiques haute performance afin d’évaluer la capacité des algorithmes quantiques à identifier, en temps réel, des chemins de secours dans des scénarios de gestion des changements. Elle a été menée à la fois sur du matériel quantique et dans des environnements de simulation accélérés utilisant des GPU AMD Instinct, afin d’atteindre une capacité de calcul (à l’échelle des qubits) encore hors de portée du matériel quantique seul.
« L’avenir du calcul repose sur la convergence entre le classique et le quantique », explique Madhu Rangarajan, vice-président corporate en charge des produits Compute et Enterprise AI chez AMD. « En tant qu’acteur du calcul haute performance, nous cherchons à comprendre nos technologies peuvent accompagner l’émergence du quantique. Cette collaboration montre un cas concret où la simulation accélérée et l’exécution quantique sont combinées pour répondre à un enjeu opérationnel réel dans les réseaux. »
Détail sur l'algo quantique utilisé : https://www.amd.com/en/developer/resources/technical-articles/2026/designing-resilient-routing-using-quantum-algorithms.html Catégorie actualité: Technologies quantique Image actualité AMP:]]></description>
      <pubDate>Wed, 18 Feb 2026 08:34:25 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</guid>
    </item>
    <item>
      <title><![CDATA[Enterprise-wide credential management tools for incident response]]></title>
      <link>https://github.blog/changelog/2026-02-17-enterprise-wide-credential-management-tools-for-incident-response</link>
      <description><![CDATA[Enterprise owners can now use new credential management actions to respond decisively to high-impact security incidents in their GitHub Enterprise Cloud enterprise accounts. These new capabilities are available for enterprise…]]></description>
      <pubDate>Tue, 17 Feb 2026 18:55:38 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-17-enterprise-wide-credential-management-tools-for-incident-response</guid>
    </item>
    <item>
      <title><![CDATA[MCP Registry and more improvements in Copilot in Eclipse]]></title>
      <link>https://github.blog/changelog/2026-02-17-mcp-registry-and-more-improvements-in-copilot-in-eclipse</link>
      <description><![CDATA[We’ve just shipped a new set of improvements to make GitHub Copilot in Eclipse smarter and easier to use. These updates bring more context options, smoother workflows, and better customization…]]></description>
      <pubDate>Tue, 17 Feb 2026 18:22:24 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-17-mcp-registry-and-more-improvements-in-copilot-in-eclipse</guid>
    </item>
    <item>
      <title><![CDATA[IDE Kiro : Checkmarx apporte plus de sécurité applicative]]></title>
      <link>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</link>
      <description><![CDATA[Checkmarx annonce que son Developer Assist supporte l'IDE Kiro, pour l'étendre la sécurité applicative directement dans l'enviornnement. Cette intégration permet à ces derniers d'identifier et de résoudre les problèmes de sécurité au fil de l'écriture du code, sans quitter leur IDE ni dépendre de scans en aval dans la chaîne CI/CD.
En utilisant l’extension IDE officielle de Checkmarx, les développeurs peuvent activer Developer Assist dans Kiro en quelques étapes seulement, sans configuration lourde. La prise en charge d’autres flux de développement, y compris via la ligne de commande, sera bientôt disponible. Une fois authentifié, Developer Assist analyse automatiquement le code source et les dépendances de l’espace de travail actif, appliquant les politiques existantes de Checkmarx One. Aucune configuration spécifique à Kiro, API propriétaire ou intégration expérimentale n’est nécessaire. Developer Assist est disponible sur Cursor, Visual Studio Code et Windsurf.
Pour en savoir plus : https://dev.checkmarx.com/ Catégorie actualité: Outils Checkmarx Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:25:38 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</guid>
    </item>
    <item>
      <title><![CDATA[WebMCP : un standard pour rendre un site web "agent ready" ?]]></title>
      <link>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</link>
      <description><![CDATA[concilier agents IA et sites web et la manière dont les pages web pourraient interagir, travailler avec les agents ? WebMCP veut fournir une méthode standard pour définir les actions des agents sur un site web, sur une page web sans pénaliser au bon fonctionnement du site web. "Vous indiquez aux agents et où interagir avec votre site, qu'il s'agisse de réserver un vol, de soumettre une demande d'assistance ou de naviguer dans des données complexes. Ce canal de communication direct élimine toute ambiguïté et permet des flux de travail plus rapides et plus efficaces pour les agents." expliquer Google. WebMCP preview repose sur 2 API :
- API déclarative : Permet d’effectuer des actions standard définies directement dans les formulaires HTML. - API impérative : Permet d’effectuer des interactions plus complexes et dynamiques nécessitant l’exécution de JavaScript. C'est une interface proposé en preview par Google et accessible dans Chrome. Ces API forment un "pont" rendant votre site web "agent ready" et permet de créer des flux agentiques que se veulent plus fiables qu'en passant par du DOM. Ces API sont JavaScript. Pour le moment, la spécification est en cours de rédaction. Elle ne dépend pas de W3C et n'est pas un standard du consortium. Site : https://webmachinelearning.github.io/webmcp/ Catégorie actualité: IA MCP Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:18:02 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</guid>
    </item>
    <item>
      <title><![CDATA[Parcours libriste d’Isabella Vanni — « Libre à vous ! » du 10 février 2026 — Podcasts et références]]></title>
      <link>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</link>
      <description><![CDATA[268ème émission « Libre à vous ! » de l’April. Podcast et programme :
sujet principal : parcours libriste d’Isabella Vanni, coordinatrice vie associative et responsable projets à l’April. Un parcours libriste est l’interview d’une seule personne pour parler de son parcours personnel et professionnel
chronique « Que libérer d’autre que du logiciel avec Antanak » sur « Les assises de l’attention »
chronique de Benjamin Bellamy sur « L’antéchrist et les petits hommes verts »
Quoi de Libre ? Actualités et annonces concernant l’April et le monde du Libre lien nᵒ 1 : Podcast de la 268ᵉ émission
lien nᵒ 2 : Les références pour la 268ᵉ émission et les podcasts par sujets
lien nᵒ 3 : S'abonner au podcast
lien nᵒ 4 : S'abonner à la lettre d'actus
lien nᵒ 5 : Libre à vous !
lien nᵒ 6 : Radio Cause Commune Rendez‐vous en direct chaque mardi de 15 h 30 à 17 h sur 93,1 MHz en Île‐de‐France. L’émission est diffusée simultanément sur le site Web de la radio Cause Commune. Vous pouvez nous laisser un message sur le répondeur de la radio : pour réagir à l’un des sujets de l’émission, pour partager un témoignage, vos idées, vos suggestions, vos encouragements ou pour nous poser une question. Le numéro du répondeur : +33 9 72 51 55 46. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:24 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</guid>
    </item>
    <item>
      <title><![CDATA[.Net 11 Preview 1 : nouvelles librairies, peu de changements dans C#]]></title>
      <link>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</link>
      <description><![CDATA[.Net 10 a été distribuée en novembre 2025. La version 11 est désormais disponible en preview 1. Comme à chaque fois, de nombreuses évolutions sont attendues. L'ensemble des frameworks et des langages sont concernées : C#, F#, ASP.Net Core, Blazor, MAUI, le compilateur Jit, le support de CoreCLR dans WebAssembly, meilleure compression / décompression avec Zstandard. Sur la partie librairie, retenons déjà les évolutions suivantes :
- Zstandard est natif à .Net pour la compression. La librairie promet une nette amélioration des performances :
// Compress data using ZstandardStream
using var compressStream = new ZstandardStream(outputStream, CompressionMode.Compress);
await inputStream.CopyToAsync(compressStream); // Decompress data
using var decompressStream = new ZstandardStream(inputStream, CompressionMode.Decompress);
await decompressStream.CopyToAsync(outputStream);
- BFloat16 intègre par défaut toutes les interfaces standards pour le numérique
- amélioration de TimeZone
Note de version sur les librairies : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/libraries.md
Sur la partie runtime, il faut s'attendre à de bonnes nouvelles :
- Runtime async : une nouvelle fonction majeure du runtime et méthodes asynchrones pour améliorer les performances. CoreCLR supporte RuntimeAsync par défaut, idem pour Native AOT
- CoreCLR est supporté dans WebAssembly. Il n'est pas encore disponible en preview 1.
- diverses améliorations de performances sur le JIT - meilleur support de RISC-V
Sur C#, pour le moment, peu de nouveautés annoncées. Deux nouvelles fonctions sont attendues : arguments pour les expresssions Collection et support Extended layout. .Net 11 n'introduira aucune nouvelle fonctionnalité pour Visual Basic. Sur ASP.Net Core et Blazor, les développeurs vont avoir beaucoup de nouveautés : EnvironmentBoundary, nouveau composant Label dans les formulaires Blazor, nouveau composant DisplayName, navigation relative Uri, support "propre" des éléments MathML dans un rendu interactif. Tous les détails dans la note de version : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/aspnetcore.md
La génération de source XAML est par défaut pour les applications .Net MAUI, cela doit permettre un build plus rapide et un debug plus performant. Sur Android, CoreCLR devient le runtime par défaut. Sur Container Images et Winfows Forms, pas de nouveautés annoncées. Annonce de .Net 11 : https://devblogs.microsoft.com/dotnet/dotnet-11-preview-1/ Catégorie actualité: Frameworks .Net 11 Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 09:52:19 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</guid>
    </item>
    <item>
      <title><![CDATA[Snyk lance AI Security Fabric pour le SDLC]]></title>
      <link>https://www.programmez.com/actualites/snyk-lance-ai-security-fabric-pour-le-sdlc-39017</link>
      <description><![CDATA[Ai Security Fabric est la nouvelle solution de l'éditeur Snyk. Cette plateforme s'insère dans le cycle de vie du développement logiciel, SDLC. Elle permet aux entreprises de commercialiser des logiciels basés sur l'IA en toute sécurité et à la vitesse de l'IA. "Aujourd'hui, les développeurs et les concepteurs utilisent non seulement l'IA pour écrire du code et alimenter des applications, mais ils sont également confrontés à une augmentation rapide des cybermenaces liées à l'IA. Ce système aide les entreprises à réduire les risques cumulés sans ralentir l'innovation. L'augmentation de la dette de sécurité, les nouveaux vecteurs d'attaque et le manque de gouvernance dans le développement basé sur l'IA aggravent les risques sur une surface d'attaque en constante expansion. Alors que l'infrastructure IA devient la trame de l'informatique moderne, les entreprises ont besoin d'une trame de sécurité qui assure une protection continue, et non ponctuelle." explique l'éditeur.
L'outil est donc là pour sécuriser tous les développements IA, les agents, etc. Snyk a identifié trois défis que l’AI Security adresse : Les vulnérabilités apparaissent à un rythme plus rapide : le volume de code explose en raison de l'adoption rapide des pratiques de codage assistées par l'IA. La sécurité doit aller au-delà de l'analyse réactive pour être « sécurisée dès le départ », en stoppant le flux de nouvelles vulnérabilités qui contribuent à l'augmentation des retards en matière de sécurité.
Le temps nécessaire à l'exploitation des vulnérabilités diminue : selon Gartner, l'IA devrait accélérer l'exploitation des vulnérabilités de 50 % d'ici 20271. Les attaques automatisées ciblant toutes les expositions disponibles, les organisations doivent systématiquement réduire leur dette de sécurité afin de diminuer leur profil de risque.
L'IA a un effet cumulatif sur les risques : le passage au développement natif de l'IA a déclenché une explosion de modèles non gérés et d'agents autonomes dans les workflows de livraison et dans les logiciels eux-mêmes. Cette « Shadow AI » crée une surface d'attaque fragmentée où le danger ne réside plus seulement dans le code, mais aussi dans l'action de l'IA elle-même. Un agent compromis peut enchaîner de manière autonome de nouvelles menaces avec des vulnérabilités précédemment « mises en veille », ce qui aggrave les risques plus rapidement que les équipes humaines ne peuvent y remédier. Pour la partie sécurité proprement dite, Snyk cible :
nyk continue d'ajouter de nouvelles fonctionnalités et améliorations qui concrétisent cette vision de l'AI Security Fabric à travers les trois vecteurs unifiés de la plateforme Snyk AI Security : DevSecOps accéléré par l'IA (stabilisation et réduction de la dette de sécurité) :Snyk aide ses clients à maîtriser les principes fondamentaux, de la visibilité à la gouvernance, afin de garantir la sécurité par défaut de l'ensemble de leur chaîne logistique logicielle.
Prévention intégrée : Delta Findings fournit des immédiats sur les nouveaux risques dans l'IDE et les PR, tandis qu'une expérience PR Check améliorée intègre des tests de sécurité dans les workflows Git afin d'empêcher les risques d'entrer dans les référentiels.
Correction plus rapide : accélère les corrections grâce au regroupement par dépendance (en donnant la priorité aux mises à niveau à fort impact) et à la notation du risque de rupture (pour éviter les perturbations de la compilation), tandis que Snyk Agent Fix permet d'effectuer des réparations par IA en un seul clic dans l'IDE et la demande d'extraction. La corrélation DAST et SAST comble le fossé entre les tests dynamiques et statiques, en reliant directement les vulnérabilités d'exécution à la ligne exacte du code source afin de permettre une correction plus efficace. Sécurisation du développement basé sur l'IA (sécurisation dès la conception dans les agents de codage) : les fonctionnalités de Snyk sont directement intégrées aux assistants de codage IA afin de garantir la sécurité du code généré par l'IA dès sa conception.
Barrières de sécurité IA étendues : de nouveaux flux de configuration en 60 secondes sont désormais également disponibles pour Gemini CLI et Claude Code.
Échelle de l'entreprise : les équipes de sécurité peuvent désormais définir et distribuer de manière centralisée des garde-fous afin de garantir des normes de sécurité cohérentes.
Corrections fluides : les développeurs peuvent déclencher une correction intelligente de bout en bout, de la génération à la demande d'extraction, sans quitter leur flux de travail. Sécurisation des logiciels natifs IA (gestion des agents, des outils et de l'exécution autonome) : alors que nous entrons dans l'ère des agents et des systèmes non déterministes, Snyk aide les organisations à adopter l'IA en toute sécurité et à gérer l'avenir du développement de l'IA. Visibilité du Shadow AI : l'AI-BOM d'Evo s'intègre à l'inventaire des actifs pour détecter automatiquement les modèles et les dépendances. Elle est lancée parallèlement à une nouvelle étude sur les tendances en matière d'adoption de l'IA menée auprès de plus de 500 utilisateurs précurseurs.
Sécurité agentique : le prototype MCP-Scan de Snyk Labs exploite l'analyse des flux toxiques pour atténuer l'empoisonnement des outils et l'injection rapide dans le protocole MCP (Model Context Protocol). Catégorie actualité: Sécurité Snyk Image actualité AMP:]]></description>
      <pubDate>Mon, 16 Feb 2026 06:39:26 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/snyk-lance-ai-security-fabric-pour-le-sdlc-39017</guid>
    </item>
    <item>
      <title><![CDATA[Armis : l'AppSec au défi du code généré par l'IA]]></title>
      <link>https://www.programmez.com/actualites/armis-lappsec-au-defi-du-code-genere-par-lia-39016</link>
      <description><![CDATA[L'éditeur en sécurité Armis annonce Centrix pour la sécurité applicative. Le paysage actuel de la sécurité applicative est saturé de solutions fragmentées et statiques, chacune résolvant un morceau du problème mais générant du bruit, des inefficacités et des angles morts. Basée sur IA, Armis Centrix for Application Security détecte les failles dans le code, contextualise ces failles en les reliant à l’environnement réel d’exécution et automatise la remédiation. Elle intègre pleinement l’infrastructure ainsi que le pipeline CI/CD et prend en compte les contrôles de mitigation en production. Armis simplifie la gestion des risques et permet aux équipes de sécurité de prioriser leurs actions pour protéger l’ensemble de la chaîne logicielle.
« Avec le codage assisté par l’IA, les développeurs accélèrent leurs livraisons… mais peuvent tout aussi rapidement introduire des vulnérabilités. Les équipes de sécurité doivent donc réagir au même rythme et à la même échelle. » poursuit Katie Norton, Responsable Recherche, DevSecOps et Sécurité de la chaîne d’approvisionnement logicielle chez IDC. « Grâce à son scan natif IA, sa vision contextuelle de l’ensemble de la plateforme et sa validation indépendante, Armis Centrix permet aux équipes de sécurité de rester en phase avec cette nouvelle ère du développement IA. » L'éditeur affirme réduire les faux positifs de 70 % et améliore notablement le temps moyen de résolution en automatisant la correction. La solution assure la couverture complète du code source à la production. Pour en savoir plus : https://www.armis.com/platform/armis-centrix-for-application-security/ Catégorie actualité: Sécurité AppSec Image actualité AMP:]]></description>
      <pubDate>Mon, 16 Feb 2026 06:27:00 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/armis-lappsec-au-defi-du-code-genere-par-lia-39016</guid>
    </item>
    <item>
      <title><![CDATA[Automate repository tasks with GitHub Agentic Workflows]]></title>
      <link>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</link>
      <description><![CDATA[Discover GitHub Agentic Workflows, now in technical preview. Build automations using coding agents in GitHub Actions to handle triage, documentation, code quality, and more.]]></description>
      <pubDate>Fri, 13 Feb 2026 14:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</guid>
    </item>
    <item>
      <title><![CDATA[Marché de l'emploi IT : un ralentissement mais des profils porteurs]]></title>
      <link>https://www.programmez.com/actualites/marche-de-lemploi-it-un-ralentissement-mais-des-profils-porteurs-39015</link>
      <description><![CDATA[En 2025, environ 230 000 offres d'emploi concernaient l'IT et la tech, selon HelloWork. Par rapport à 2024, il s'agit d'un recul de 28 %. Il ne faut pas forcément y voir une baisse de recrutement mais de nouvelles pratiques avec quelques priorités technologiques. Les dernières études de HelloWorks fournissent plusieurs stats intéressantes : 1 offre IT sur 2 est localisée en Île-de-France ou en Auvergne-Rhône-Alpes : sans grande surprise
La baisse des recrutements concerne la plupart des régions, mais PACA (-17 %) résiste mieux
Les métiers des systèmes, réseaux, infrastructures, du développement et du pilotage SI représentent 3 recrutements IT sur 4 Technicien de maintenance informatique (+48 %)
Ingénieur de recherche en IA (+47 %)
Ingénieur systèmes d’information (+43 %)
Technicien data center (+41 %) Les métiers et profils autour de l'IA progressent fortement, là encore, tout sauf une surprise. En 2025, trois profils de développeurs sont reçus le plus de candidatures :
- développeur full stack
- développeur web
- développeur Python Catégorie actualité: Carrière HelloWorks Image actualité AMP:]]></description>
      <pubDate>Fri, 13 Feb 2026 09:38:52 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/marche-de-lemploi-it-un-ralentissement-mais-des-profils-porteurs-39015</guid>
    </item>
    <item>
      <title><![CDATA[LibreOffice 26.2 : Markdown, accessibilité et plein d’autres nouveautés et améliorations]]></title>
      <link>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</link>
      <description><![CDATA[En février, il y a la corvée commerciale de la Saint-Valentin et les réjouissances intellectuelles consécutives à la sortie d’une nouvelle version de la suite bureautique LibreOffice. C’est, bien évidemment, sur LibreOffice 26.2 que l’on va se pencher. Au menu, du très visible, comme les boites de dialogues, du très attendu comme la prise en compte du Markdown ou du moins visible comme le travail sur l’accessibilité.
Il va de soi que les notes de version sont plus exhaustives et qu’il ne s’agit ici que d’une sélection. lien nᵒ 1 : Notes de version Sommaire
L’accessibilité
Support du Markdown
L’interface et les boites de dialogue
Writer
Calc
En vrac
Pour finir
Avant de commencer : toutes les captures d’écran ont été faites, volontairement, sur une interface très personnalisée.
L’accessibilité
L’accessibilité de la suite bureautique est un important chantier pour lequel une personne a été recrutée en 2023 (en). Cette version-ci a fait l’objet d’améliorations sensibles. Parallèlement, Sophie Gautier, coordinatrice de The Document Foundation1 (Foundation coordinator) est en train de monter un groupe de travail qui a pour objectif la publication d’un rapport de conformité en matière d’accessibilité pour répondre à la norme européenne EN 301 549 (en) d’accessiblité numérique. La langue de travail de ce groupe est l’anglais.
Concernant les améliorations de cette version :
la boite de dialogue « Vérifier les mises à jour », Aide &gt; Vérifier les mises à jour… est devenue accessible aux lecteurs d’écran ;
les fonctions d’accessibilité des aperçus des bordures, onglet « Bordures » des boites de dialogue, ont été revues afin qu’elles ne perturbent plus les dispositifs d’assistance ;
sur Linux : la boite de dialogue Outils&gt; Orthographe est annoncée correctement par le lecteur d’écran ;
quand on supprimait la sélection accessible, le curseur se déplaçait automatiquement au début du texte, ce comportement perturbant est supprimé ;
dans Writer, les fautes d’orthographe ne sont plus signalées par les dispositifs d’assistance si la vérification orthographique n’est pas activée ;
l’accessibilité au clavier de la boite de dialogue des extensions : Outils &gt; Extensions est accessible aux lecteurs d’écran ;
et enfin, il est possible de naviguer entre les onglets verticaux avec des raccourcis clavier.
Support du Markdown
Le Markdown est devenu le format de balisage léger standard « de fait ». Et c’est celui supporté par LinuxFR. Son support a été introduit dans cette version, c’est un des formats d’enregistrement qui s’est ajouté à la série des autres formats de la suite, pas un format d’export. Pour l’utiliser pour vos sites, passant pour LinuxFR, vous devrez :
soit ouvrir le fichier .md dans un éditeur de texte, n’importe lequel, même Mousepad fait l’affaire par exemple, et copier-coller ensuite le tout à partir de l’éditeur de texte là où vous le voulez ;
soit, si cela est possible, importer le fichier .md dans ce qui vous sert pour gérer le site comme le fait par exemple l’extension ODT2SPIP pour le système de gestion de contenu SPIP qui permet de créer une nouvelle page dans SPIP avec un fichier.ODT. ça marche avec LinuxFR ? Plutôt bien. Les styles de caractère Accentuation (ici en italiques) et Accentuation forte (ici gras) sont bien reconnu ainsi que Texte source pour « télétype », les indications in-texte encadrées de l’accent grave U+0060. Les styles de paragraphes :
Bloc de citation (paragraphes de citation précédés d’une ligne blanche et du signe « &gt; » dans la saisie de contenu sur LinuxFR) ;
Contenu de tableau ;
Corps de texte ;
Liste, par contre la numérotation des listes ordonnée ne semble pas bien fonctionner, il faut saisir les numéros à la main ;
Texte préformaté pour écrire des blocs de code ;
Titre 1, Titre 2, Titre 3 et Titre de tableau.
Les tableaux sont bien repris ainsi que les liens insérés via l’insertion d’hyperliens.
Ce qui ne semble pas fonctionner du tout : ce sont les notes, elles disparaissent corps et biens. C’est peut-être dû au passage dans l’éditeur de texte qui transforme un peu le document. Et, évidemment, il faut rajouter les images avec la syntaxe LinuxFR.
La version de Mardown de LibreOffice est CommonMark (en) et la bibliothèque utilisée est MD4C avec quelques extensions prises en charge par cette bibliothèque (cf ce rapport de bug (en) et ses réponses), pour en savoir plus, voir cette note (en) du blog de The Document Foundation.
Petite remarque, si vous utilisez un LibreOffice 25.8, vous avez peut-être pu constater qu’il était question d’enregistrement au format .md, cette information a été ajoutée trop précocement car la version 25.8 ne gère pas le Markdown.
L’interface et les boites de dialogue
Les boites de dialogue, notamment de styles et de formats, ont beaucoup changé. Longtemps elles se sont affichées avec une présentation par onglets en haut et le contenu dessous.
Puis il y a une période de transition en 2025 qui a fait grincer une collection complète de dents où on avait, selon l’endroit où on était, soit des onglets soit une navigation par menu latéral. Cette dernière avait un gros défaut : par exemple pour la configuration des styles dans Writer il fallait descendre tout en bas pour accéder aux options qui étaient cachées. Et il n’y avait pas de barre de défilement pour aller plus vite.
LibreOffice 26.2 voit ces défauts corrigés : les boites de dialogue sont harmonisées dans toute la suite et leur menu latéral, toujours sans barre de défilement qui s’avère finalement inutile, montre clairement tous les types de paramètres auxquels on peut accéder. Et, comme on peut le voir, LibreOffice a intégré une meilleure prise en charge des systèmes d’écritures asiatiques et complexes en affichant deux colonnes, une pour les polices occidentales, ou pour les polices asiatiques ou complexes. Une personne a également été recrutée en 2023 (en) pour travailler sur le support des systèmes d’écriture de droite à gauche (RTL) et complexes (CTL). Si toutefois, vous préférez revenir à l’affichage avec les onglets, il suffit d’aller dans le menu Outils &gt; Options &gt; Apparenceau niveau de « Boites de dialogue » et cocher l’option Horizontal en haut. Il faut savoir que les onglets en haut ne s’affichent que sur une seule ligne et qu’il faudra donc naviguer avec les flèches quand il y a de nombreuses options. Writer
Il y a un certain nombre d’amélioration autour de la compatibilité avec le format DOCX : séparation de tableaux flottants en plusieurs tableaux, suppression de la numérotation des notes de bas de page à l’ouverture d’un fichier DOCX, etc.
On relèvera deux nouvelles options d’alignement des paragraphes : « Début » et « Fin ». Si vous utilisez l’alphabet latin, vous ne verrez aucune différence avec les deux options « Forcer à gauche/en haut » et « Forcer à droite/en bas ». Elles ont été développées pour réutiliser plus facilement les styles entre les divers systèmes d’écriture. Pour continuer sur la lancée du travail pour la prise en compte des systèmes d’écriture dont le fonctionnement est différent de celui de l’alphabet latin, il est possible de changer la direction du texte : de gauche à droite ou de droite à gauche en cours de travail. Cela peut se paramétrer dans les styles. Calc
Un gros travail sur les performances a été fait : vitesse de défilement, rapidité des classeurs avec de nombreuses formes et du rejet des modifications. On voit apparaître de nouvelles options de tri (Données &gt;Trier) qui dépendent de la « locale » (langue définie dans les Options de LibreOffice). On peut ainsi déterminer quel caractère est utilisé comme séparateur de décimal pour le tri naturel. On peut relever aussi une avancée ergonomique qui va plaire à toutes celles et ceux qui utilisent les matrices, on peut maintenant modifier les formules matricielles avec la combinaison de touches : F2 + ↑ Maj + Ctrl + Entrée, il n’est plus nécessaire de modifier la formule elle-même.
Et aussi : si vous utilisez (pourquoi diable ?) le format d’enregistrement XLSX, c’est le format EXCEL2010+ qui est le format par défaut, il change de nom pour devenir « Classeur Excel 2010-365 ».2
En vrac
Base est devenu complètement multi-utilisateur, TDF a, d’ailleurs, recruté une personne pour travailler sur l’application.
Concernant les diagrammes (ou chart) : dans le Volet latéral, quand le graphique est en mode modification et que l’on va, au niveau de « Couleurs », sur la palette, on a une prévisualisation en direct dans le diagramme ce qui permet de tester le choix de couleurs plus facilement.
Les polices embarquées dont la licence ne permettait pas l’édition étaient jusqu’à présent ignorées et remplacées à l’affichage, ni vu, ni connu par une fonte de substitution. Ce défaut a été corrigé.
L’export PDF gère les liens avec les documents externes : Fichier &gt; Exporter au format PDF &gt; Liens. Les dictionnaires hongrois, mongol et portugais du Portugal ont été mis à jour ainsi que les règles de césure de la langue hongroise.
JSON, pour JavaScript Object Notation, est un format standard utilisé pour représenter des données structurées. Il est utilisé notamment pour échanger les informations entre un navigateur et un serveur. C’est, par exemple, le format de sauvegarde des marques-pages de Firefox ou de certains fichiers d’archives de Mastodon. Les documents XML et JSON génériques avec des plages pouvant être liées sont maintenant automatiquement mappés à des feuilles dans Calc. Une plage pouvant être liée est une section d’un document contenant des enregistrements tabulaires. Lorsqu’un document contient plusieurs plages pouvant être liées, chaque plage est mappée à une seule feuille3.
Et si vous avez envie de vous amuser avec les fonctions expérimentales (à activer dansOutils &gt; Options &gt; LibreOffice &gt; Avancé), vous pouvez jouer avec la nouvelle de boite de dialogue « Gestion des macros ».
Pour finir
Cette dépêche a, bien, évidemment, été rédigée avec LibreOffice et, cette fois-ci dans un fichier enregistré en Markdown. Les seules balises que j’ai dû entrer à la main sont celles des images. Kate a l’air de modifier le fichier et, quand je réouvre le .md dans LibreOffice, il y a des styles qui ont sauté mais la mise en forme reste visuellement la même. Kate rajoute aussi des barres obliques devant les « &gt; », aux crochets [ ] et même à certains hyperliens (images). Il y a peut-être des éditeurs de texte plus adaptés ou des réglages à faire.
J’ai rédigé cette dépêche en même temps qu’un article sur LibreOffice 26.2 pour mon site. Si l’article n’est pas vraiment dupliqué, il n’est pas étonnant d’y trouver des morceaux ici. Que tout cela ne nous empêche d’adresser tous nos remerciements à celles et ceux qui font de LibreOffice une suite bureautique si agréable à utiliser et si performante.
Post-scriptum : si vous voulez savoir modifier les couleurs de l’interface comme sur les captures d’écran, ça peut s’envisager, demandez gentiment, avec un peu de chance.
The Document Foundation ou TDF est la fondation de droit allemand qui pilote le projet LibreOffice. Il y a deux formats OOXML différents et donc deux formats XLSX différents, la version 2007 et la version actuelle depuis 2010. S’il vous est vraiment nécessaire d’enregistrer au format XLSX, il faut utiliser la version de 2010. Notes de version. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Fri, 13 Feb 2026 09:09:23 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</guid>
    </item>
    <item>
      <title><![CDATA[Projets Libres saison 4 épisode 11 : PVH éditions, une maison d'édition libérée et dans le Fediverse]]></title>
      <link>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</link>
      <description><![CDATA[Nous avons eu le plaisir de rencontrer Lionel Jeannerat durant les Rencontres Hivernales du libre à Saint-Cergue (VD) en janvier 2026. son parcours
la maison d'édition et ses œuvres
le passage au libre que ce soit pour les licences mais aussi pour leurs outils métiers
Bonne écoute ou lecture lien nᵒ 1 : Lien vers l'épisode
lien nᵒ 2 : S'abonner au podcast
lien nᵒ 3 : Le site de PVH éditions
lien nᵒ 4 : Soutenir le podcast
lien nᵒ 5 : L'épisode traduit en anglais
lien nᵒ 6 : Le site des Rencontres Hivernales du libre Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 07:40:57 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</guid>
    </item>
    <item>
      <title><![CDATA[Les journaux LinuxFr.org les mieux notés de janvier 2026]]></title>
      <link>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</link>
      <description><![CDATA[LinuxFr.org propose des dépêches et articles, soumis par tout un chacun, puis revus et corrigés par l’équipe de modération avant publication. C’est la partie la plus visible de LinuxFr.org, ce sont les dépêches qui sont le plus lues et suivies, sur le site, via Atom/RSS, ou bien via partage par messagerie instantanée, par courriel, ou encore via médias sociaux. Ce que l’on sait moins, c’est que LinuxFr.org vous propose également de publier directement vos propres articles, sans validation a priori de lʼéquipe de modération. Ceux-ci s’appellent des journaux. Voici un florilège d’une dizaine de ces journaux parmi les mieux notés par les utilisateurs et les utilisatrices… qui notent. Lumière sur ceux du mois de janvier passé.
« lecteur mp3 pour personne handicapée mentale » par ChocolatineFlying ;
« À la recherche du Linuxfrien type » par Ysabeau ;
« hacker sa pompe de relevage 3 et fin ! » par ChocolatineFlying ;
« [Hors sujet] Des tablettes lave-vaisselle tout-en-un » par Tanguy Ortolo ;
« Francis Hallé Bronsonisé » par Joris Dedieu ;
« 10 ans après, Modoboa est toujours là pour prendre soin de votre serveur de messagerie » par mirtouf ;
« À table ! » par JaguarWan ;
« Retour d'expérience sur le développement d'une application par l'utilisation d'IA » par phoenix ;
« Algoo lance un bulletin d'information mensuel « veille techno et logiciels libres » » par LeBouquetin ;
« Linux : les planètes s'alignent en 2026 » par vmagnin. lien nᵒ 1 : Participez à l’écriture d’un article
lien nᵒ 2 : Publiez votre journal
lien nᵒ 3 : Proposez une dépêche Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 09:23:50 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[Meilleures contributions LinuxFr.org : les primées de janvier 2026]]></title>
      <link>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</link>
      <description><![CDATA[Nous continuons sur notre lancée de récompenser celles et ceux qui chaque mois contribuent au site LinuxFr.org (dépêches, , logo, journaux, correctifs, etc.). Vous n’êtes pas sans risquer de gagner un livre des éditions Eyrolles, ENI et D-Booker. Voici les gagnants du mois de janvier 2026 :
Stefane Fermigier, pour sa dépêche « Appel à de la Commission "Vers des écosystèmes numériques ouverts européens" » ;
ChocolatineFlying, pour son journal « lecteur mp3 pour personne handicapé mental » ;
YvanM, pour sa dépêche « MeshCentral, alternative à TeamViewer et RustDesk » ;
Christophe Bliard, pour sa dépêche « Sortie de OpenProject 17.0 ».
Les livres gagnés sont détaillés en seconde partie de la dépêche. N’oubliez pas de contribuer, LinuxFr.org vit pour vous et par vous ! lien nᵒ 1 : Contribuez à LinuxFr.org !
lien nᵒ 2 : Tous les moyens (ou presque) de participer
lien nᵒ 3 : Récompenses précédentes (décembre 2025) Les livres sélectionnés
Linux — Maîtrisez l'administration du système — 7e édition. Certaines personnes n’ont pas pu être jointes ou n’ont pas répondu. Les lots ont été réattribués automatiquement. N’oubliez pas de mettre une adresse de courriel valable dans votre compte ou lors de la proposition d’une dépêche. En effet, c’est notre seul moyen de vous contacter, que ce soit pour les lots ou des questions sur votre dépêche lors de sa modération. Tous nos remerciements aux contributeurs du site ainsi qu’aux éditions Eyrolles, ENI et D-Booker. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 07:09:14 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[L'April propose le pacte du logiciel libre à l'occasion des élections municipales et communautaires de 2026]]></title>
      <link>https://linuxfr.org/news/l-april-propose-le-pacte-du-logiciel-libre-a-l-occasion-des-elections-municipales-et-communautaires-de-2026</link>
      <description><![CDATA[À l'occasion des élections municipales et communautaires des 15 et 22 mars 2026, l'April propose aux personnes candidates de signer le Pacte du Logiciel Libre afin de marquer leur engagement, si elles sont élues, à promouvoir et défendre une priorité aux logiciels libres et aux formats ouverts au sein de leurs collectivités.
Le pacte du logiciel libre est une initiative de l'April qui remonte à l'élection présidentielle de 2007. À l'occasion des élections locales à venir, le pacte a évolué pour être plus représentatif des enjeux actuels.
En complément du pacte, l'April propose une liste d'exemples d'actions concrètes que les collectivités peuvent mettre en place dans la poursuite de ces objectifs. lien nᵒ 1 : Site de la campagne
lien nᵒ 2 : Pacte du logiciel libre (PDF)
lien nᵒ 3 : Pacte du logiciel libre (PDF brochure)
lien nᵒ 4 : Pacte du logiciel libre (ODT) Le pacte est disponible sur le site de la campagne.
En plus du format PDF classique, Le pacte est également disponible en mode « brochure », de manière à pouvoir l'imprimer et le plier dans un format 4 pages, par exemple pour être distribué sur les marchés en échange d'un tract électoral ;). Le pacte pour les élections municipales et communautaires de mars 2026 est construit autour de trois objectifs complémentaires :
Donner la priorité aux logiciels libres et aux formats ouverts, qui est l'ambition historique de l'April. Avoir recours à des logiciels privateurs doit rester une exception dûment justifiée, dans le respect d’une stricte procédure de définition des besoins. Une priorité qui est compatible avec le droit de la commande publique – chose confirmée par le Conseil d'État depuis 2011 – et matériellement possible puisqu'il existe à présent des logiciels libres en mesure de répondre à la majorité des besoins des collectivités.
Défendre et promouvoir une informatique émancipatrice. Le logiciel libre participe à la préservation des libertés fondamentales dans une société informatisée, au partage du savoir et à l'accès éclairé au numérique pour toutes et tous. Que ce soit dans les écoles dont elles ont la charge, comme dans l'ensemble des lieux d'accueil du public qu'elles peuvent être amenées à gérer, les collectivités ont un rôle important de sensibilisation et d’accompagnement à exercer.
Contribuer à la pérennité des logiciels libres utilisés. Les collectivités doivent contribuer au maintien, à la documentation et au développement des solutions qu’elles utilisent. Un travail et un investissement mutualisables, notamment avec d'autres collectivités, au bénéfice de toutes et tous, d’autant plus pertinent et durable qu’ils s’inscrivent dans une politique formalisée de contribution et de partage.
Le pacte s'adresse à l'ensemble des personnes candidates qui souhaitent marquer leur attachement à agir pour le logiciel libre au sein de leur collectivité et pour les libertés informatiques des habitantes et habitants. Il s'adresse également aux listes candidates qui souhaitent collectivement marquer, comme élément de leur programme, leur engagement à mettre en œuvre une politique en faveur du logiciel libre si elles obtiennent la majorité.
Nous invitons toutes celles et ceux qui le souhaitent à contacter leurs candidates et candidats, qui ont déjà pu se manifester, pour les encourager à signer le Pacte du Logiciel Libre et profiter de l'occasion pour les sensibiliser aux enjeux des libertés informatiques.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Fri, 06 Feb 2026 20:26:31 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/l-april-propose-le-pacte-du-logiciel-libre-a-l-occasion-des-elections-municipales-et-communautaires-de-2026</guid>
    </item>
    <item>
      <title><![CDATA[Continuous AI in practice: What developers can automate today with agentic CI]]></title>
      <link>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</link>
      <description><![CDATA[Think of Continuous AI as background agents that operate in your repository for tasks that require reasoning.]]></description>
      <pubDate>Thu, 05 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</guid>
    </item>
    <item>
      <title><![CDATA[Setting Docker Hardened Images free]]></title>
      <link>https://changelog.com/podcast/675</link>
      <description><![CDATA[In May of 2025, Docker launched Hardened Images, a secure, minimal, production-ready set of images. In December, they made DHI freely available and open source to everyone who builds software. On this episode, we're joined by Tushar Jain, EVP of Engineering at Docker to learn all about it.]]></description>
      <pubDate>Wed, 04 Feb 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/675</guid>
    </item>
    <item>
      <title><![CDATA[Pick your agent: Use Claude and Codex on Agent HQ]]></title>
      <link>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</link>
      <description><![CDATA[Claude by Anthropic and OpenAI Codex are now available in public preview on GitHub and VS Code with a Copilot Pro+ or Copilot Enterprise subscription. Here's what you need to know and how to get started today.]]></description>
      <pubDate>Wed, 04 Feb 2026 17:00:19 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</guid>
    </item>
    <item>
      <title><![CDATA[What the fastest-growing tools reveal about how software is being built]]></title>
      <link>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</link>
      <description><![CDATA[What languages are growing fastest, and why? What about the projects that people are interested in the most? Where are new developers cutting their teeth? Let’s take a look at Octoverse data to find out.]]></description>
      <pubDate>Tue, 03 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</guid>
    </item>
    <item>
      <title><![CDATA[The state of homelab tech (2026)]]></title>
      <link>https://changelog.com/friends/125</link>
      <description><![CDATA[Techno Tim joins Adam to dive deep into the state of homelab'ing in 2026. Hardware is scarce and expensive due to the AI gold rush, but software has never been better. From unleashing Claude on your UDM Pro to building custom Proxmox CLIs, they explores how AI is transforming what's possible in the homelab. Tim declares 2026 the "Year of Self-Hosted Software" while Adam reveals his homelab's secret weapons: DNSHole (a Pi-hole replacement written in Rust) and PXM (a Proxmox automation CLI).]]></description>
      <pubDate>Sat, 24 Jan 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/125</guid>
    </item>
    <item>
      <title><![CDATA[Very important agents]]></title>
      <link>https://changelog.com/friends/120</link>
      <description><![CDATA[Nick Nisi joins us to dig into the latest trends from this year and how they're impacting his day-to-day coding and Vision Pro wearing. Anthropic's acquisition of Bun, the evolving JavaScript and AI landscape, GitHub's challenges and the Amp/Sourcegraph split. We dive into AI development practices, context management, voice assistants, Home Assistant OS and home automation, the state of the AI browser war, and we close with a prediction from Nick.]]></description>
      <pubDate>Fri, 05 Dec 2025 22:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/120</guid>
    </item>
    <item>
      <title><![CDATA[Vite documentary companion pod]]></title>
      <link>https://changelog.com/podcast/661</link>
      <description><![CDATA[Our friends at Cult.Repo launch their epic Vite documentary on October 9th, 2025! To celebrate, Jerod sat down with Evan You to discuss Vite's adoption story, why he raised money to start VoidZero, how developer documentaries get made, open source sustainability, and more.]]></description>
      <pubDate>Wed, 08 Oct 2025 19:45:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/661</guid>
    </item>
  </channel>
</rss>