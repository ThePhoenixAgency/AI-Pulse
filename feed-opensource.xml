<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI-Pulse - Open Source & GitHub</title>
    <link>https://thephoenixagency.github.io/AI-Pulse</link>
    <description>Open Source & GitHub news from AI-Pulse</description>
    <language>en</language>
    <lastBuildDate>Fri, 20 Feb 2026 12:48:08 GMT</lastBuildDate>
    <atom:link href="https://thephoenixagency.github.io/AI-Pulse/feed-opensource.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title><![CDATA[PostHog/posthog]]></title>
      <link>https://github.com/PostHog/posthog</link>
      <description><![CDATA[PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack. Docs - Community - Roadmap - Why PostHog? - Changelog - Bug reports PostHog is an all-in-one, open source platform for building successful products PostHog provides every tool you need to build a successful product including: Product Analytics: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL. Web Analytics: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue. Session Replays: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior. Feature Flags: Safely roll out features to select users or cohorts with feature flags. Experiments: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too. Error Tracking: Track errors, get alerts, and resolve issues to improve your product. Surveys: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder. Data warehouse: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data. Data pipelines: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse. LLM analytics: Capture traces, generations, latency, and cost for your LLM-powered app. Workflows: Create workflows that automate actions or send messages to your users. Best of all, all of this is free to use with a generous monthly free tier for each product. Get started by signing up for PostHog Cloud US or PostHog Cloud EU. Table of Contents PostHog is an all-in-one, open source platform for building successful products Table of Contents Getting started with PostHog PostHog Cloud ( ) Self-hosting the open-source hobby deploy (Advanced) Setting up PostHog Learning more about PostHog Contributing Open-source vs. paid We’re hiring! Getting started with PostHog PostHog Cloud ( ) The fastest and most reliable way to get started with PostHog is signing up for free to PostHog Cloud or PostHog Cloud EU. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage. Self-hosting the open-source hobby deploy (Advanced) If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker ( 4GB memory): /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)" Open source deployments should scale to approximately 100k events per month, after which we recommend migrating to a PostHog Cloud. We do not provide customer support or offer guarantees for open source deployments. See our self-hosting docs, troubleshooting guide, and disclaimer for more info. Setting up PostHog Once you've got a PostHog instance, you can set it up by installing our JavaScript web snippet, one of our SDKs, or by using our API. We have SDKs and libraries for popular languages and frameworks like: Frontend Mobile Backend JavaScript React Native Python Next.js Android Node React iOS PHP Vue Flutter Ruby Beyond this, we have docs and guides for Go, .NET/C#, Django, Angular, WordPress, Webflow, and more. Once you've installed PostHog, see our product docs for more information on how to set up product analytics, web analytics, session replays, feature flags, experiments, error tracking, surveys, data warehouse, and more. Learning more about PostHog Our code isn't the only thing that's open source . We also open source our company handbook which details our strategy, ways of working, and processes. Curious about how to make the most of PostHog? We wrote a guide to winning with PostHog which walks you through the basics of measuring activation, tracking retention, and capturing revenue. Contributing We &lt;3 contributions big and small: Vote on features or get early access to beta functionality in our roadmap Open a PR (see our instructions on developing PostHog locally) Submit a feature request or bug report For an overview of the codebase structure, see monorepo layout and products. Open-source vs. paid This repo is available under the MIT expat license, except for the ee directory (which has its license here) if applicable. Need absolutely % FOSS? Check out our posthog-foss repository, which is purged of all proprietary code and features. The pricing for our paid plan is completely transparent and available on our pricing page. We're hiring! Hey! If you're reading this, you've proven yourself as a dedicated README reader. You might also make a great addition to our team. We're growing fast and would love for you to join us.]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:41 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/PostHog/posthog</guid>
    </item>
    <item>
      <title><![CDATA[getzep/graphiti]]></title>
      <link>https://github.com/getzep/graphiti</link>
      <description><![CDATA[Build Real-Time Knowledge Graphs for AI Agents Graphiti Build Real-Time Knowledge Graphs for AI Agents Help us reach more developers and grow the Graphiti community. Star this repo! [!TIP] Check out the new MCP server for Graphiti! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory. Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications. Use Graphiti to: Integrate and maintain dynamic user interactions and business data. Facilitate state-based reasoning and task automation for agents. Query complex, evolving data with semantic, keyword, and graph-based search methods. A knowledge graph is a network of interconnected facts, such as "Kendra loves Adidas shoes." Each fact is a "triplet" represented by two entities, or nodes ("Kendra", "Adidas shoes"), and their relationship, or edge ("loves"). Knowledge Graphs have been explored extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph while handling changing relationships and maintaining historical context. Graphiti and Zep's Context Engineering Platform. Graphiti powers the core of Zep's context engineering platform for AI Agents. Zep offers agent memory, Graph RAG for dynamic data, and context retrieval and assembly. Using Graphiti, we've demonstrated Zep is the State of the Art in Agent Memory. Read our paper: Zep: A Temporal Knowledge Graph Architecture for Agent Memory. We're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications. Zep vs Graphiti Aspect Zep Graphiti What they are Fully managed platform for context engineering and AI memory Open-source graph framework User &amp; conversation management Built-in users, threads, and message storage Build your own Retrieval &amp; performance Pre-configured, production-ready retrieval with sub-200ms performance at scale Custom implementation required; performance depends on your setup Developer tools Dashboard with graph visualization, debug logs, API logs; SDKs for Python, TypeScript, and Go Build your own tools Enterprise features SLAs, support, security guarantees Self-managed Deployment Fully managed or in your cloud Self-hosted only When to choose which Choose Zep if you want a turnkey, enterprise-grade platform with security, performance, and support baked in. Choose Graphiti if you want a flexible OSS core and you're comfortable building/operating the surrounding system. Why Graphiti? Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing: Real-Time Incremental Updates: Immediate integration of new data episodes without batch recomputation. Bi-Temporal Data Model: Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries. Efficient Hybrid Retrieval: Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization. Custom Entity Definitions: Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models. Scalability: Efficiently manages large datasets with parallel processing, suitable for enterprise environments. Graphiti vs. GraphRAG Aspect GraphRAG Graphiti Primary Use Static document summarization Dynamic data management Data Handling Batch-oriented processing Continuous, incremental updates Knowledge Structure Entity clusters &amp; community summaries Episodic data, semantic entities, communities Retrieval Method Sequential LLM summarization Hybrid semantic, keyword, and graph-based search Adaptability Low High Temporal Handling Basic timestamp tracking Explicit bi-temporal tracking Contradiction Handling LLM-driven summarization judgments Temporal edge invalidation Query Latency Seconds to tens of seconds Typically sub-second latency Custom Entity Types No Yes, customizable Scalability Moderate High, optimized for large datasets Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries. Installation Requirements: Python 3.10 or higher Neo4j 5.26 / FalkorDB 1.1.2 / Kuzu 0.11.2 / Amazon Neptune Database Cluster or Neptune Analytics Graph + Amazon OpenSearch Serverless collection (serves as the full text search backend) OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding) [!IMPORTANT] Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini). Using other services may result in incorrect output schemas and ingestion failures. This is particularly problematic when using smaller models. Optional: Google Gemini, Anthropic, or Groq API key (for alternative LLM providers) [!TIP] The simplest way to install Neo4j is via Neo4j Desktop. It provides a user-friendly interface to manage Neo4j instances and databases. Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example: docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest pip install graphiti-core or uv add graphiti-core Installing with FalkorDB Support If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra: pip install graphiti-core[falkordb] # or with uv
uv add graphiti-core[falkordb] Installing with Kuzu Support If you plan to use Kuzu as your graph database backend, install with the Kuzu extra: pip install graphiti-core[kuzu] # or with uv
uv add graphiti-core[kuzu] Installing with Amazon Neptune Support If you plan to use Amazon Neptune as your graph database backend, install with the Amazon Neptune extra: pip install graphiti-core[neptune] # or with uv
uv add graphiti-core[neptune] You can also install optional LLM providers as extras: # Install with Anthropic support
pip install graphiti-core[anthropic] # Install with Groq support
pip install graphiti-core[groq] # Install with Google Gemini support
pip install graphiti-core[google-genai] # Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai] # Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai] # Install with Amazon Neptune
pip install graphiti-core[neptune] Default to Low Concurrency; LLM Provider 429 Rate Limit Errors Graphiti's ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM Provider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below. Concurrency controlled by the SEMAPHORE_LIMIT environment variable. By default, SEMAPHORE_LIMIT is set to 10 concurrent operations to help prevent 429 rate limit errors from your LLM provider. If you encounter such errors, try lowering this value. If your LLM provider allows higher throughput, you can increase SEMAPHORE_LIMIT to boost episode ingestion performance. Quick Start [!IMPORTANT] Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an OPENAI_API_KEY is set in your environment. Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI compatible APIs. For a complete working example, see the Quickstart Example in the examples directory. The quickstart demonstrates: Connecting to a Neo4j, Amazon Neptune, FalkorDB, or Kuzu database Initializing Graphiti indices and constraints Adding episodes to the graph (both text and structured JSON) Searching for relationships (edges) using hybrid search Reranking search results using graph distance Searching for nodes using predefined search recipes The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps. Running with Docker Compose You can use Docker Compose to quickly start the required services: Neo4j Docker: docker compose up This will start the Neo4j Docker service and related components. FalkorDB Docker: docker compose --profile falkordb up This will start the FalkorDB Docker service and related components. MCP Server The mcp_server directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol. Key features of the MCP server include: Episode management (add, retrieve, delete) Entity management and relationship handling Semantic and hybrid search capabilities Group management for organizing related data Graph maintenance operations The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows. For detailed setup instructions and usage examples, see the MCP server README. REST Service The server directory contains an API service for interacting with the Graphiti API. It is built using FastAPI. Please see the server README for more information. Optional Environment Variables In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables. If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables must be set. Database Configuration Database names are configured directly in the driver constructors: Neo4j: Database name defaults to neo4j (hardcoded in Neo4jDriver) FalkorDB: Database name defaults to default_db (hardcoded in FalkorDriver) As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the graph_driver parameter. Neo4j with Custom Database Name from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver # Create a Neo4j driver with custom database name
driver = Neo4jDriver( uri="bolt://localhost:7687", user="neo4j", password="password", database="my_custom_database" # Custom database name
) # Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver) FalkorDB with Custom Database Name from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver # Create a FalkorDB driver with custom database name
driver = FalkorDriver( host="localhost", port=6379, username="falkor_user", # Optional password="falkor_password", # Optional database="my_custom_graph" # Custom database name
) # Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver) Kuzu from graphiti_core import Graphiti
from graphiti_core.driver.kuzu_driver import KuzuDriver # Create a Kuzu driver
driver = KuzuDriver(db="/tmp/graphiti.kuzu") # Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver) Amazon Neptune from graphiti_core import Graphiti
from graphiti_core.driver.neptune_driver import NeptuneDriver # Create a FalkorDB driver with custom database name
driver = NeptuneDriver( host= &lt; NEPTUNE
ENDPOINT &gt;,
aoss_host = &lt; Amazon
OpenSearch
Serverless
Host &gt;,
port = &lt; PORT &gt; # Optional, defaults to 8182, aoss_port = &lt; PORT &gt; # Optional, defaults to 443
) driver = NeptuneDriver(host=neptune_uri, aoss_host=aoss_host, port=neptune_port) # Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver) Graph Driver Architecture Graphiti uses a pluggable driver architecture so the core framework is backend-agnostic. All database-specific logic is encapsulated in driver implementations, allowing you to swap backends or add new ones without modifying the rest of the framework. How Drivers are Integrated The driver layer is organized into three tiers: GraphDriver ABC (graphiti_core/driver/driver.py) — the core interface every backend must implement. It defines query execution, session management, index lifecycle, and exposes 11 operations interfaces as @property accessors. GraphProvider enum — identifies the backend (NEO4J, FALKORDB, KUZU, NEPTUNE). Query builders use this enum in match/case statements to return dialect-specific query strings. 11 Operations ABCs (graphiti_core/driver/operations/) — abstract interfaces covering all CRUD and search operations for every graph element type: Node ops: EntityNodeOperations, EpisodeNodeOperations, CommunityNodeOperations, SagaNodeOperations Edge ops: EntityEdgeOperations, EpisodicEdgeOperations, CommunityEdgeOperations, HasEpisodeEdgeOperations, NextEpisodeEdgeOperations Search &amp; maintenance: SearchOperations, GraphMaintenanceOperations Each backend provides a concrete driver class and a matching operations/ directory with implementations of all 11 ABCs. The key directories and files are shown below (simplified; see source for complete structure): graphiti_core/driver/
├── driver.py # GraphDriver ABC, GraphProvider enum
├── query_executor.py # QueryExecutor protocol
├── record_parsers.py # Shared record → model conversion
├── operations/ # 11 operation ABCs
│ ├── entity_node_ops.py
│ ├── episode_node_ops.py
│ ├── community_node_ops.py
│ ├── saga_node_ops.py
│ ├── entity_edge_ops.py
│ ├── episodic_edge_ops.py
│ ├── community_edge_ops.py
│ ├── has_episode_edge_ops.py
│ ├── next_episode_edge_ops.py
│ ├── search_ops.py
│ ├── graph_ops.py
│ └── graph_utils.py # Shared algorithms (e.g., label propagation)
├── graph_operations/ # Legacy graph operations interface
├── search_interface/ # Legacy search interface
├── neo4j_driver.py # Neo4jDriver
├── neo4j/operations/ # 11 Neo4j implementations
├── falkordb_driver.py # FalkorDriver
├── falkordb/operations/ # 11 FalkorDB implementations
├── kuzu_driver.py # KuzuDriver
├── kuzu/operations/ # 11 Kuzu implementations + record_parsers.py
├── neptune_driver.py # NeptuneDriver
└── neptune/operations/ # 11 Neptune implementations Operations are decoupled from the driver itself — each operation method receives an executor: QueryExecutor parameter (a protocol for running queries) rather than a concrete GraphDriver, which makes operations testable and driver-agnostic. The driver class instantiates all 11 operation classes in its __init__ and exposes them as properties. The base GraphDriver ABC defines each property with an optional return type (| None, defaulting to None); concrete drivers override these to return their implementations: # In your concrete driver (e.g., Neo4jDriver):
@property
def entity_node_ops(self) -&gt; EntityNodeOperations: return self._entity_node_ops Provider-specific query strings are generated by shared query builders in graphiti_core/models/nodes/node_db_queries.py and graphiti_core/models/edges/edge_db_queries.py, which use match/case on the GraphProvider enum to return the correct dialect for each backend. Adding a New Graph Driver To integrate a new graph database backend, follow these steps: Add to GraphProvider — add your enum value in graphiti_core/driver/driver.py: class GraphProvider(Enum): NEO4J = 'neo4j' FALKORDB = 'falkordb' KUZU = 'kuzu' NEPTUNE = 'neptune' MY_BACKEND = 'my_backend' # New backend Create directory structure — create graphiti_core/driver//operations/ with an __init__.py exporting all 11 operation classes. Implement GraphDriver subclass — create graphiti_core/driver/_driver.py: Set provider = GraphProvider. Implement the abstract methods: execute_query(), session(), close(), build_indices_and_constraints(), delete_all_indexes() Instantiate all 11 operation classes in __init__ and return them via @property overrides Implement all 11 operation ABCs — one file per ABC in /operations/, each inheriting from the corresponding ABC in graphiti_core/driver/operations/. Add query variants — add case GraphProvider.: branches to graphiti_core/models/nodes/node_db_queries.py and graphiti_core/models/edges/edge_db_queries.py for your database's query dialect. Implement GraphDriverSession — if your backend needs session or connection management, subclass GraphDriverSession from driver.py and implement run(), close(), and execute_write(). Register as optional dependency — add an extras group in pyproject.toml: [project.optional-dependencies]]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:40 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/getzep/graphiti</guid>
    </item>
    <item>
      <title><![CDATA[microsoft/agent-lightning]]></title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description><![CDATA[The absolute trainer to light up AI agents. Agent Lightning The absolute trainer to light up AI agents. Join our Discord community to connect with other users and contributors. Core Features Turn your agent into an optimizable beast with ZERO CODE CHANGE (almost)! Build with ANY agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! Selectively optimize one or more agents in a multi-agent system. Embraces Algorithms like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. on our documentation website. Installation pip install agentlightning For the latest nightly build (cutting-edge features), you can install from Test PyPI: pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning Please refer to our installation guide for more details. To start using Agent-lightning, check out our documentation and examples. Articles 12/17/2025 Adopting the Trajectory Level Aggregation for Faster Training Agent-lightning blog. 11/4/2025 Tuning ANY AI agent with Tinker ✕ Agent-lightning Medium. See also Part 2. 10/22/2025 No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL vLLM blog. See also Zhihu writeup. 8/11/2025 Training AI Agents to Write and Self-correct SQL with Reinforcement Learning Medium. 8/5/2025 Agent Lightning: Train ANY AI Agents with Reinforcement Learning arXiv paper. 7/26/2025 We discovered an approach to train any AI agent with RL, with (almost) zero code changes. Reddit. 6/6/2025 Agent Lightning - Microsoft Research Project page. Community Projects DeepWerewolf — A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning. AgentFlow — A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks. Youtu-Agent — Youtu-Agent lets you build and train your agent with ease. Built with a modified branch of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check the recipe and their blog Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat. Architecture Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight agl.emit_xxx() helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync. On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning. No rewrites, no lock-in, just a clear path from first rollout to steady improvement. CI Status Workflow Status CPU Tests Full Tests UI Tests Examples Integration Latest Dependency Compatibility Legacy Examples Compatibility Citation If you find Agent Lightning useful in your research or projects, please cite our paper: @misc{luo2025agentlightningtrainai, title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning}, author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang}, year={2025}, eprint={2508.03680}, archivePrefix={arXiv}, primaryClass={cs.AI}, url={https://arxiv.org/abs/2508.03680},
} Contributing This project welcomes contributions and suggestions. Start by reading the Contributing Guide for contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, ). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or . Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark &amp; Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies. Responsible AI This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise. License This project is licensed under the MIT License. See the LICENSE file for details.]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/microsoft/agent-lightning</guid>
    </item>
    <item>
      <title><![CDATA[open-mercato/open-mercato]]></title>
      <link>https://github.com/open-mercato/open-mercato</link>
      <description><![CDATA[AI‑supportive CRM / ERP foundation framework — built to power R&amp;D, new processes, operations, and growth. It’s modular, extensible, and designed for teams that want strong defaults with room to customize everything. Better than Django, Retool and other alternatives - and Enterprise Grade! Open Mercato Open Mercato is a new‑era, AI‑supportive platform for shipping enterprise‑grade CRMs, ERPs, and commerce backends. It’s modular, extensible, and designed so teams can mix their own modules, entities, and workflows while keeping the guardrails of a production-ready stack. Start with 80% done. Buy vs. build? Now, you can have best of both. Use Open Mercato enterprise ready business features like CRM, Sales, OMS, Encryption and build the remaining 20% that really makes the difference for your business. Core Use Cases CRM – model customers, opportunities, and bespoke workflows with infinitely flexible data definitions. ERP – manage orders, production, and service delivery while tailoring modules to match your operational reality. Commerce – launch CPQ flows, B2B ordering portals, or full commerce backends with reusable modules. Self-service system – spin up customer or partner portals with configurable forms, guided flows, and granular permissions. Workflows – orchestrate custom data lifecycles and document workflows per tenant or team. Production – coordinate production management with modular entities, automation hooks, and reporting. Headless/API platform – expose rich, well-typed APIs for mobile and web apps using the same extensible data model. Highlights Modular architecture – drop in your own modules, pages, APIs, and entities with auto-discovery and overlay overrides. Custom entities &amp; dynamic forms – declare fields, validators, and UI widgets per module and manage them live from the admin. Multi-tenant by default – SaaS-ready tenancy with strict organization/tenant scoping for every entity and API. Multi-hierarchical organizations – built-in organization trees with role- and user-level visibility controls. Feature-based RBAC – combine per-role and per-user feature flags with organization scoping to gate any page or API. Data indexing &amp; caching – hybrid JSONB indexing and smart caching for blazing-fast queries across base and custom fields. Event subscribers &amp; workflows – publish domain events and process them via persistent subscribers (local or Redis). Growing test coverage – expanding unit and integration tests ensure modules stay reliable as you extend them. AI-supportive foundation – structured for assistive workflows, automation, and conversational interfaces. Modern stack – Next.js App Router, TypeScript, zod, Awilix DI, MikroORM, and bcryptjs out of the box. Screenshots Order Shipments Organizations Users Roles &amp; ACL Custom Fields Custom Entity Records Add New Customer Deals Pipeline Customer Notes Sales Pipeline Order Shipments Order Totals Catalog Products Sales Channels Channel Offers Home overview with enabled modules list Architecture Overview Modules: Each feature lives under src/modules/ with auto‑discovered frontend/backend pages, APIs, CLI, i18n, and DB entities. Database: MikroORM with per‑module entities and migrations; no global schema. Migrations are generated and applied per module. Dependency Injection: Awilix container constructed per request. Modules can register and override services/components via di.ts. Multi‑tenant: Core directory module defines tenants and organizations. Most entities carry tenant_id + organization_id. Security: RBAC roles, zod validation, bcryptjs hashing, JWT sessions, role‑based access in routes and APIs. on the Open Mercato Architecture AI Assistant Open Mercato includes a built-in AI Assistant that can discover and interact with your data model and APIs. The assistant uses MCP (Model Context Protocol) to expose tools for schema discovery and API execution. Chat Interface Settings MCP Tools Key capabilities: Schema Discovery – Query database entity schemas including fields, types, and relationships API Discovery – Search for API endpoints using natural language queries API Execution – Execute API calls with automatic tenant context and authentication Hybrid Search – Uses Meilisearch for fast fulltext + vector search across schemas and endpoints MCP Tools: Tool Purpose discover_schema Search entity schemas by name or keyword find_api Find API endpoints by natural language query call_api Execute API calls with tenant context context_whoami Get current authentication context Integration modes: Development (yarn mcp:dev) – For Claude Code and local development with API key auth Production (yarn mcp:serve) – For web AI chat with session tokens See the AI Assistant specification for detailed documentation on entity extraction, OpenAPI integration, and search indexing. Data Encryption Open Mercato ships with tenant-scoped, field-level data encryption so PII and sensitive business data stay protected while you keep the flexibility of custom entities and fields. Encryption maps live in the admin UI/database, letting you pick which system and custom columns are encrypted; MikroORM hooks automatically encrypt on write and decrypt on read while keeping deterministic hashes (e.g., email_hash) for lookups. Architecture in two lines: Vault/KMS (or a derived-key fallback) issues per-tenant DEKs and caches them so performance stays snappy; AES-GCM wrappers sit in the ORM lifecycle, storing ciphertext at rest while CRUD and APIs keep working with plaintext. Read the docs to dive deeper: docs.openmercato.com/user-guide/encryption. Migration Guide We have migrated Open Mercato to a monorepo structure. If you're upgrading from a previous version, please note the following changes: File Structure The codebase is now organized into: packages/ - Shared libraries and modules (@open-mercato/core, @open-mercato/ui, @open-mercato/shared, @open-mercato/cli, @open-mercato/cache, @open-mercato/events, @open-mercato/queue, @open-mercato/content, @open-mercato/onboarding, @open-mercato/search) apps/ - Applications (main app in apps/mercato, docs in apps/docs) Important note on storage: The storage folder has been moved to the apps/mercato folder as well. If you instance has got any attachments uploaded, please make sure you run: mv storage apps/mercato/storage ... from the root Open Mercato folder. Import Aliases Import aliases have changed from path-based to package-based imports: Before: @/lib/..., @/components/..., @/modules/... After: @open-mercato/shared/lib/..., @open-mercato/ui/components/..., @open-mercato/core/modules/..., etc. Environment Variables The .env file now must live in apps/mercato instead of the project root. The fastest way to start is to copy the example file: cp apps/mercato/.env.example apps/mercato/.env At minimum, set DATABASE_URL, JWT_SECRET, and REDIS_URL (or EVENTS_REDIS_URL) before bootstrapping. Package Manager Yarn 4 is now required. Ensure you have Yarn 4+ installed before proceeding. Getting Started This is a quickest way to get Open Mercato up and running on your localhost / server - ready for testing / demoing or for Core development! Installation update Node.js 24.x is required # macOS (Homebrew)
brew install node@24 # Windows (Chocolatey)
choco install nodejs --version=24.x # Or use nvm (any platform)
nvm install 24
nvm use 24 Windows: Use Docker Setup for native setup. Quick Start (Monorepo) Prerequisites: Yarn 4+ git clone https://github.com/open-mercato/open-mercato.git
cd open-mercato
git checkout develop
yarn install cp apps/mercato/.env.example apps/mercato/.env # EDIT this file to set up your specific files
#At minimum, set `DATABASE_URL`, `JWT_SECRET`, and `REDIS_URL` (or `EVENTS_REDIS_URL`) before bootstrapping. yarn generate
yarn initialize # or yarn reinstall
yarn dev For a fresh greenfield boot (build packages, generate registries, reinstall modules, then start dev), run: yarn dev:greenfield Navigate to http://localhost:3000/backend and sign in with the default credentials printed by yarn initialize. Full installation guide (including prerequisites, Docker setup, and cloud deployment): docs.openmercato.com/installation/setup Docker Setup Open Mercato offers two Docker Compose configurations — one for development (with hot reload) and one for production. Both run the full stack (app + PostgreSQL + Redis + Meilisearch) in containers. The dev mode is the setup for Windows users. Dev mode (hot reload) Run the entire stack with source code mounted from the host. File changes trigger automatic rebuilds — no local Node.js or Yarn required. git clone https://github.com/open-mercato/open-mercato.git
cd open-mercato
git checkout develop
docker compose -f docker-compose.fullapp.dev.yml up --build Windows users: Ensure WSL 2 backend is enabled in Docker Desktop and clone with git config --global core.autocrlf input to avoid line-ending issues. Production mode docker compose -f docker-compose.fullapp.yml up --build Common operations: Start: docker compose -f docker-compose.fullapp.yml up -d Logs: docker compose -f docker-compose.fullapp.yml logs -f app Stop: docker compose -f docker-compose.fullapp.yml down Rebuild: docker compose -f docker-compose.fullapp.yml up --build Navigate to http://localhost:3000/backend and sign in with the default credentials (admin@example.com). Docker Environment Variables Before starting, you may want to configure the following environment variables. Create a .env file in the project root or export them in your shell: Variable Required Default Description JWT_SECRET For production JWT Secret key for JWT token signing. Use a strong, unique value in production. POSTGRES_PASSWORD For production postgres PostgreSQL database password. Use a strong password in production. POSTGRES_USER No postgres PostgreSQL database user POSTGRES_DB No open-mercato PostgreSQL database name POSTGRES_PORT No 5432 PostgreSQL exposed port REDIS_PORT No 6379 Redis exposed port MEILISEARCH_MASTER_KEY For production meilisearch-dev-key Meilisearch API key. Use a strong key in production. MEILISEARCH_PORT No 7700 Meilisearch exposed port OPENAI_API_KEY No - OpenAI API key (enables AI features) ANTHROPIC_API_KEY No - Anthropic API key (for opencode service) OPENCODE_PORT No 4096 Opencode service exposed port Example .env file for production: JWT_SECRET=your-strong-secret-key-here
POSTGRES_PASSWORD=your-strong-db-password
MEILISEARCH_MASTER_KEY=your-strong-meilisearch-key
OPENAI_API_KEY=sk-... # Optional, for AI features VPS Deployment For production deployments, ensure strong JWT_SECRET, secure database credentials, and consider managed database services. See the full Docker deployment guide for detailed configuration and production tips. Standalone App &amp; Customization The way to build on Open Mercato without modifying the core is to create a standalone app. This gives you a self-contained project that pulls Open Mercato packages from npm — your own modules, overrides, and customizations live in your repo while core stays untouched and upgradeable. Create a standalone app npx create-mercato-app my-store
cd my-store
cp .env.example .env # configure DATABASE_URL, JWT_SECRET, REDIS_URL
docker compose up -d # start PostgreSQL, Redis, Meilisearch
yarn install
yarn initialize
yarn dev Navigate to http://localhost:3000/backend and sign in with the credentials printed by yarn initialize. Add custom modules Drop your own modules into src/modules/ and register them in src/modules.ts with from: '@app': export const enabledModules: ModuleEntry[] = [ // ... core modules { id: 'inventory', from: '@app' },
] Run yarn generate and yarn dev — your module's pages, APIs, and entities are auto-discovered. Eject core modules for deep customization When you need to change the internals of a core module (entities, business logic, UI), eject it. The mercato eject command copies the module source into your src/modules/ directory and switches it to local, so you can modify it freely while all other modules keep receiving package updates. # See which modules support ejection
yarn mercato eject --list # Eject a module (e.g., currencies)
yarn mercato eject currencies
yarn mercato generate all
yarn dev Currently ejectable: catalog, currencies, customers, perspectives, planner, resources, sales, staff, workflows. Full guide: docs.openmercato.com/customization/standalone-app · CLI reference: docs.openmercato.com/cli/eject Live demo Documentation Browse the full documentation at docs.openmercato.com. Introduction Installation User Guide Tutorials Customization Architecture Framework API Reference CLI Reference Appendix Spec Driven Development Open Mercato follows a spec-first development approach. Before implementing new features or making significant changes, we document the design in the .ai/specs/ folder. Why Specs? Clarity: Specs ensure everyone understands the feature before coding starts Consistency: Design decisions are documented and can be referenced by humans and AI agents Traceability: Each spec maintains a changelog tracking the evolution of the feature How It Works Before coding: Check if a spec exists in .ai/specs/ (named SPEC-###-YYYY-MM-DD-title.md) New features: Create or update the spec with your design before implementation After changes: Update the spec's changelog with a dated summary Naming convention: Specs use the format SPEC-{number}-{date}-{title}.md (e.g., SPEC-007-2026-01-26-sidebar-reorganization.md) See .ai/specs/README.md for the full specification directory and .ai/specs/AGENTS.md for detailed guidelines on maintaining specs. Join us on Discord Connect with the team and other builders in our Discord community: https://discord.gg/f4qwPtJ3qA. Contributing We welcome contributions of all sizes—from fixes and docs updates to new modules. Start by reading CONTRIBUTING.md for branching conventions (main, develop, feat/), release flow, and the full PR checklist. Then check the open issues or propose an idea in a discussion, and: Fork the repository and create a branch that reflects your change. Install dependencies with yarn install and bootstrap via yarn mercato init (add --no-examples to skip demo CRM content; --stresstest for thousands of synthetic contacts, companies, deals, and timeline interactions; or --stresstest --lite for high-volume contacts without the heavier extras). Develop and validate your changes (yarn lint, yarn test, or the relevant module scripts). Open a pull request referencing any related issues and outlining the testing you performed. Refer to AGENTS.md for deeper guidance on architecture and conventions when extending modules. Open Mercato is proudly supported by Catch The Tornado. CLI Commands Open Mercato let the module developers to expose the custom CLI commands for variouse maintenance tasks. on the CLI documentation License MIT — see LICENSE for details.]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:40 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/open-mercato/open-mercato</guid>
    </item>
    <item>
      <title><![CDATA[NVIDIA-NeMo/NeMo]]></title>
      <link>https://github.com/NVIDIA-NeMo/NeMo</link>
      <description><![CDATA[A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech) NVIDIA NeMo Speech Collection Latest News NVIDIA-Nemotron-3-Nano-30B-A3B is out with full reproducible script and recipes! Check out NeMo Megatron-Bridge, NeMo AutoModel, NeMo-RL and NGC container to try them! (2025-12-15) Pivot notice: This repo has pivoted to focus on audio, speech, and multimodal LLM only. Please refer to NeMo Framework Github Org for the complete list of repos under NeMo Framework NeMo Megatron-Bridge and NeMo AutoModel. More details can be found in the NeMo Framework GitHub org readme. (2025-10-10) The following collections are no longer available avlm · diffusion · llm · multimodal · multimodal-autoregressive · nlp · speechlm · vision · vlm Pretrain and finetune Hugging Face models via AutoModel NeMo Framework's latest feature AutoModel enables broad support for Hugging Face models, with 25.04 focusing on AutoModelForCausalLM in the Text Generation category AutoModelForImageTextToText in the Image-Text-to-Text category More Details in Blog: Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework. Future releases will enable support for more model families such as Video Generation models.(2025-05-19) Training on Blackwell using NeMo NeMo Framework has added Blackwell support, with performance benchmarks on GB200 &amp; B200. More optimizations to come in the upcoming releases.(2025-05-19) Training Performance on GPU Tuning Guide NeMo Framework has published a comprehensive guide for performance tuning to achieve optimal throughput! (2025-05-19) New Models Support NeMo Framework has added support for latest community models - Llama 4, Flux, Llama Nemotron, Hyena &amp; Evo2, Qwen2-VL, Qwen2.5, Gemma3, Qwen3-30B&amp;32B.(2025-05-19) NeMo Framework 2.0 We've released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the NeMo Framework User Guide to get started. New Cosmos World Foundation Models Support Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform (2025-01-09) The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities (2025-01-07) The NeMo Framework now supports training and customizing the NVIDIA Cosmos collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts. You can also now accelerate your video processing step using the NeMo Curator library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline. Large Language Models and Multimodal Models State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo (2024-11-06) NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the NVIDIA/cosmos-tokenizer GitHub repo and on Hugging Face. New Llama 3.1 Support (2024-07-23) The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta. Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS (2024-07-16) NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository here. NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support (2024/06/17) NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens. (2024-06-18) See documentation and tutorials for SFT, PEFT, and PTQ with Nemotron 340B in the NeMo Framework User Guide. NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0 (2024/06/12) Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training. Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE (2024/03/16) An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework. Speech Recognition Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo (2024/09/24) NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000. New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model (2024/04/18) The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. Canary also provides bi-directional translation, between English and the three other supported languages. Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models (2024/04/18) NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhere—on any cloud and on-premises—released the Parakeet family of automatic speech recognition (ASR) models. These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy. Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT (2024/04/18) NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhere—on any cloud and on-premises—recently released Parakeet-TDT. This new addition to the NeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B. Introduction NVIDIA NeMo Framework is a scalable and cloud-native generative AI framework built for researchers and PyTorch developers working on Large Language Models (LLMs), Multimodal Models (MMs), Automatic Speech Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV) domains. It is designed to help you efficiently create, customize, and deploy new generative AI models by leveraging existing code and pre-trained model checkpoints. For technical documentation, please see the NeMo Framework User Guide. What's New in NeMo 2.0 NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability. Python-Based Configuration - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically. Modular Abstractions - By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models. Scalability - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using NeMo-Run, a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments. Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development. Get Started with NeMo 2.0 Refer to the Quickstart for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster. For more information about NeMo 2.0, see the NeMo Framework User Guide. For an in-depth exploration of the main features of NeMo 2.0, see the Feature Guide. To transition from NeMo 1.0 to 2.0, see the Migration Guide for step-by-step instructions. Training and Customization All NeMo models are trained with Lightning. Training is automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the latest NeMo Framework container here. When applicable, NeMo models leverage cutting-edge distributed training techniques, incorporating parallelism strategies to enable efficient training of very large models. These techniques include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed Precision Training with BFloat16 and FP8, as well as others. In addition to supervised fine-tuning (SFT), NeMo also supports the latest parameter efficient fine-tuning (PEFT) techniques such as LoRA, P-Tuning, Adapters, and IA3. Speech AI NeMo ASR and TTS models can be optimized for inference and deployed for production use cases with NVIDIA Riva. Get Started with NeMo Framework Getting started with NeMo Framework is easy. State-of-the-art pretrained NeMo models are freely available on Hugging Face Hub and NVIDIA NGC. These models can be used to generate text or images, transcribe audio, and synthesize speech in just a few lines of code. We have extensive tutorials that can be run on Google Colab or with our NGC NeMo Framework Container. We also have playbooks for users who want to train NeMo models with the NeMo Framework Launcher. For advanced users who want to train NeMo models from scratch or fine-tune existing NeMo models, we have a full suite of example scripts that support multi-GPU/multi-node training. Key Features Multimodal Automatic Speech Recognition Text to Speech Requirements Python 3.12 or above Pytorch 2.6 or above NVIDIA GPU (if you intend to do model training) As of Pytorch 2.6, torch.load defaults to using weights_only=True. Some model checkpoints may require using weights_only=False. In this case, you can set the env var TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1 before running code that uses torch.load. However, this should only be done with trusted files. Loading files from untrusted sources with more than weights only can have the risk of arbitrary code execution. Developer Documentation Version Status Description Latest Documentation of the latest (i.e. main) branch. Stable Documentation of the stable (i.e. most recent release) Install NeMo Framework The NeMo Framework can be installed in a variety of ways, depending on your needs. Depending on the domain, you may find one of the following installation methods more suitable. Conda / Pip: Install NeMo-Framework with native Pip into a virtual environment. Used to explore NeMo on any supported platform. This is the method for ASR and TTS domains. Limited feature-completeness for other domains. NGC PyTorch container: Install NeMo-Framework from source with feature-completeness into a highly optimized container. For users that want to install from source in a highly optimized container. NGC NeMo container: Ready-to-go solution of NeMo-Framework For users that seek highest performance. Contains all dependencies installed and tested for performance and convergence. Support matrix NeMo-Framework provides tiers of support based on OS / Platform and mode of installation. Please refer the following overview of support levels: Fully supported: Max performance and feature-completeness. Limited supported: Used to explore NeMo. No support yet: In development. Deprecated: Support has reached end of life. Please refer to the following table for current support levels: OS / Platform Install from PyPi Source into NGC container linux - amd64/x84_64 Limited support Full support linux - arm64 Limited support Limited support darwin - amd64/x64_64 Deprecated Deprecated darwin - arm64 Limited support Limited support windows - amd64/x64_64 No support yet No support yet windows - arm64 No support yet No support yet Conda / Pip Install NeMo in a fresh Conda environment: conda create --name nemo python==3.10.12
conda activate nemo Pick the right version NeMo-Framework publishes pre-built wheels with each release. To install nemo_toolkit from such a wheel, use the following installation method: pip install "nemo_toolkit[all]" If a more specific version is desired, we recommend a Pip-VCS install. From NVIDIA/NeMo, fetch the commit, branch, or tag that you would like to install. To install nemo_toolkit from this Git reference $REF, use the following installation method: git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout @${REF:-'main'}
pip install '.[all]' Install a specific Domain To install a specific domain of NeMo, you must first install the nemo_toolkit using the instructions listed above. Then, you run the following domain-specific commands: pip install nemo_toolkit['all'] # or pip install "nemo_toolkit['all']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}"
pip install nemo_toolkit['asr'] # or pip install "nemo_toolkit['asr']@git+https://github.com/NVIDIA/NeMo@$REF:-'main'}"
pip install nemo_toolkit['tts'] # or pip install "nemo_toolkit['tts']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}" NGC PyTorch container NOTE: The following steps are supported beginning with 25.09 (NeMo-Toolkit 2.6.0) We that you start with a base NVIDIA PyTorch container: nvcr.io/nvidia/pytorch:25.09-py3. If starting with a base NVIDIA PyTorch container, you must first launch the container: docker run \ --gpus all \ -it \ --rm \ --shm-size=16g \ --ulimit memlock=-1 \ --ulimit stack=67108864 \ ${NV_PYTORCH_TAG:-'nvcr.io/nvidia/pytorch:25.09-py3'} From NVIDIA/NeMo, fetch the commit/branch/tag that you want to install. To install nemo_toolkit including all of its dependencies from this Git reference $REF, use the following installation method: cd /opt
git clone https://github.com/NVIDIA/NeMo
cd NeMo
git checkout ${REF:-'main'}
pip install ".[all]" NGC NeMo container NeMo containers are launched concurrently with NeMo version updates. NeMo Framework now supports LLMs, MMs, ASR, and TTS in a single consolidated Docker container. The latest container is based on NeMo 2.6.0. You can find additional information about released containers on the NeMo releases page. To use a pre-built container, run the following code: docker run \ --gpus all \ -it \ --rm \ --shm-size=16g \ --ulimit memlock=-1 \ --ulimit stack=67108864 \ nvcr.io/nvidia/nemo:25.11.01 Discussions Board FAQ can be found on the NeMo Discussions board. You are welcome to ask questions or start discussions on the board. Contribute to NeMo We welcome community contributions! Please refer to CONTRIBUTING.md for the process. Publications We provide an ever-growing list of publications that utilize the NeMo Framework. To contribute an article to the collection, please submit a pull request to the gh-pages-src branch of this repository. For detailed information, please consult the README located at the gh-pages-src branch. Licenses NeMo is licensed under the Apache License 2.0.]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:29 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/NVIDIA-NeMo/NeMo</guid>
    </item>
    <item>
      <title><![CDATA[exo-explore/exo]]></title>
      <link>https://github.com/exo-explore/exo</link>
      <description><![CDATA[Run frontier AI locally. exo: Run frontier AI locally. Maintained by exo labs. exo connects all your devices into an AI cluster. Not only does exo enable running models larger than would fit on a single device, but with day-0 support for RDMA over Thunderbolt, makes models run faster as you add more devices. Features Automatic Device Discovery: Devices running exo automatically discover each other - no manual configuration. RDMA over Thunderbolt: exo ships with day-0 support for RDMA over Thunderbolt 5, enabling 99% reduction in latency between devices. Topology-Aware Auto Parallel: exo figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link. Tensor Parallelism: exo supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices. MLX Support: exo uses MLX as an inference backend and MLX distributed for distributed communication. Dashboard exo includes a built-in dashboard for managing your cluster and chatting with models. 4 × 512GB M3 Ultra Mac Studio running DeepSeek v3.1 (8-bit) and Kimi-K2-Thinking (4-bit) Benchmarks Qwen3-235B (8-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA Source: Jeff Geerling: 15 TB VRAM on Mac Studio – RDMA over Thunderbolt 5 DeepSeek v3.1 671B (8-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA Source: Jeff Geerling: 15 TB VRAM on Mac Studio – RDMA over Thunderbolt 5 Kimi K2 Thinking (native 4-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA Source: Jeff Geerling: 15 TB VRAM on Mac Studio – RDMA over Thunderbolt 5 Quick Start Devices running exo automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at http://localhost:52415). There are two ways to run exo: Run from Source (macOS) If you have Nix installed, you can skip most of the steps below and run exo directly: nix run .#exo Note: To accept the Cachix binary cache (and avoid the Xcode Metal ToolChain), add to /etc/nix/nix.conf: trusted-users = root (or your username)
experimental-features = nix-command flakes Then restart the Nix daemon: sudo launchctl kickstart -k system/org.nixos.nix-daemon Prerequisites: Xcode (provides the Metal ToolChain required for MLX compilation) brew (for simple package management on macOS) /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)" uv (for Python dependency management) macmon (for hardware monitoring on Apple Silicon) node (for building the dashboard) brew install uv macmon node rust (to build Rust bindings, nightly for now) curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly Clone the repo, build the dashboard, and run exo: # Clone exo
git clone https://github.com/exo-explore/exo # Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd .. # Run exo
uv run exo This starts the exo dashboard and API at http://localhost:52415/ Please view the section on RDMA to enable this feature on MacOS &gt;=26.2! Run from Source (Linux) Prerequisites: uv (for Python dependency management) node (for building the dashboard) - version 18 or higher rust (to build Rust bindings, nightly for now) Installation methods: Option 1: Using system package manager (Ubuntu/Debian example): # Install Node.js and npm
sudo apt update
sudo apt install nodejs npm # Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh # Install Rust (using rustup)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly Option 2: Using Homebrew on Linux (if preferred): # Install Homebrew on Linux
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)" # Install dependencies
brew install uv node # Install Rust (using rustup)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly Note: The macmon package is macOS-only and not required for Linux. Clone the repo, build the dashboard, and run exo: # Clone exo
git clone https://github.com/exo-explore/exo # Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd .. # Run exo
uv run exo This starts the exo dashboard and API at http://localhost:52415/ Important note for Linux users: Currently, exo runs on CPU on Linux. GPU support for Linux platforms is under development. If you'd like to see support for your specific Linux hardware, please search for existing feature requests or create a new one. Configuration Options: --no-worker: Run exo without the worker component. Useful for coordinator-only nodes that handle networking and orchestration but don't execute inference tasks. This is helpful for machines without sufficient GPU resources but with good network connectivity. uv run exo --no-worker File Locations (Linux): exo follows the XDG Base Directory Specification on Linux: Configuration files: ~/.config/exo/ (or $XDG_CONFIG_HOME/exo/) Data files: ~/.local/share/exo/ (or $XDG_DATA_HOME/exo/) Cache files: ~/.cache/exo/ (or $XDG_CACHE_HOME/exo/) You can override these locations by setting the corresponding XDG environment variables. macOS App exo ships a macOS app that runs in the background on your Mac. The macOS app requires macOS Tahoe 26.2 or later. Download the latest build here: EXO-latest.dmg. The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on. Custom Namespace for Cluster Isolation: The macOS app includes a custom namespace feature that allows you to isolate your exo cluster from others on the same network. This is configured through the EXO_LIBP2P_NAMESPACE setting: Use cases: Running multiple separate exo clusters on the same network Isolating development/testing clusters from production clusters Preventing accidental cluster joining Configuration: Access this setting in the app's Advanced settings (or set the EXO_LIBP2P_NAMESPACE environment variable when running from source) The namespace is logged on startup for debugging purposes. Uninstalling the macOS App The way to uninstall is through the app itself: click the menu bar icon → Advanced → Uninstall. This cleanly removes all system components. If you've already deleted the app, you can run the standalone uninstaller script: sudo ./app/EXO/uninstall-exo.sh This removes: Network setup LaunchDaemon Network configuration script Log files The "exo" network location Note: You'll need to manually remove EXO from Login Items in System Settings → General → Login Items. Enabling RDMA on macOS RDMA is a new capability added to macOS 26.2. It works on any Mac with Thunderbolt 5 (M4 Pro Mac Mini, M4 Max Mac Studio, M4 Max MacBook Pro, M3 Ultra Mac Studio). Please refer to the caveats for immediate troubleshooting. To enable RDMA on macOS, follow these steps: Shut down your Mac. Hold down the power button for 10 seconds until the boot menu appears. Select "Options" to enter Recovery mode. When the Recovery UI appears, open the Terminal from the Utilities menu. In the Terminal, type: rdma_ctl enable and press Enter. Reboot your Mac. After that, RDMA will be enabled in macOS and exo will take care of the rest. Important Caveats Devices that wish to be part of an RDMA cluster must be connected to all other devices in the cluster. The cables must support TB5. On a Mac Studio, you cannot use the Thunderbolt 5 port next to the Ethernet port. If running from source, please use the script found at tmp/set_rdma_network_config.sh, which will disable Thunderbolt Bridge and set dhcp on each RDMA port. RDMA ports may be unable to discover each other on different versions of MacOS. Please ensure that OS versions match exactly (even beta version numbers) on all devices. Using the API If you prefer to interact with exo via the API, here is an example creating an instance of a small model (mlx-community/Llama-3.2-1B-Instruct-4bit), sending a chat completions request and deleting the instance. 1. Preview instance placements The /instance/previews endpoint will preview all valid placements for your model. curl "http://localhost:52415/instance/previews?model_id=llama-3.2-1b" Sample response: { "previews": [ { "model_id": "mlx-community/Llama-3.2-1B-Instruct-4bit", "sharding": "Pipeline", "instance_meta": "MlxRing", "instance": {...}, "memory_delta_by_node": {"local": 729808896}, "error": null } // ...possibly more placements... ]
} This will return all valid placements for this model. Pick a placement that you like. To pick the first one, pipe into jq: curl "http://localhost:52415/instance/previews?model_id=llama-3.2-1b" | jq -c '.previews[] | select(.error == null) | .instance' | head -n1 2. Create a model instance Send a POST to /instance with your desired placement in the instance field (the full payload must match types as in CreateInstanceParams), which you can copy from step 1: curl -X POST http://localhost:52415/instance \ -H 'Content-Type: application/json' \ -d '{ "instance": {...} }' Sample response: { "message": "Command received.", "command_id": "e9d1a8ab-...."
} 3. Send a chat completion Now, make a POST to /v1/chat/completions (the same format as OpenAI's API): curl -N -X POST http://localhost:52415/v1/chat/completions \ -H 'Content-Type: application/json' \ -d '{ "model": "mlx-community/Llama-3.2-1B-Instruct-4bit", "messages": [ {"role": "user", "content": "What is Llama 3.2 1B?"} ], "stream": true }' 4. Delete the instance When you're done, delete the instance by its ID (find it via /state or /instance endpoints): curl -X DELETE http://localhost:52415/instance/YOUR_INSTANCE_ID Other useful API endpoints:* List all models: curl http://localhost:52415/models Inspect instance IDs and deployment state: curl http://localhost:52415/state For further details, see: API basic documentation in docs/api.md. API types and endpoints in src/exo/master/api.py. Benchmarking The exo-bench tool measures model prefill and token generation speed across different placement configurations. This helps you optimize model performance and validate improvements. Prerequisites: Nodes should be running with uv run exo before benchmarking The tool uses the /bench/chat/completions endpoint Basic usage: uv run bench/exo_bench.py \ --model Llama-3.2-1B-Instruct-4bit \ --pp 128,256,512 \ --tg 128,256 Key parameters: --model: Model to benchmark (short ID or HuggingFace ID) --pp: Prompt size hints (comma-separated integers) --tg: Generation lengths (comma-separated integers) --max-nodes: Limit placements to N nodes (default: 4) --instance-meta: Filter by ring, jaccl, or both (default: both) --sharding: Filter by pipeline, tensor, or both (default: both) --repeat: Number of repetitions per configuration (default: 1) --warmup: Warmup runs per placement (default: 0) --json-out: Output file for results (default: bench/results.json) Example with filters: uv run bench/exo_bench.py \ --model Llama-3.2-1B-Instruct-4bit \ --pp 128,512 \ --tg 128 \ --max-nodes 2 \ --sharding tensor \ --repeat 3 \ --json-out my-results.json The tool outputs performance metrics including prompt tokens per second (prompt_tps), generation tokens per second (generation_tps), and peak memory usage for each configuration. Hardware Accelerator Support On macOS, exo uses the GPU. On Linux, exo currently runs on CPU. We are working on extending hardware accelerator support. If you'd like support for a new hardware platform, please search for an existing feature request and add a thumbs up so we know what hardware is important to the community. Contributing See CONTRIBUTING.md for guidelines on how to contribute to exo.]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:40 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/exo-explore/exo</guid>
    </item>
    <item>
      <title><![CDATA[GodsScion/Auto_job_applier_linkedIn]]></title>
      <link>https://github.com/GodsScion/Auto_job_applier_linkedIn</link>
      <description><![CDATA[Make your job hunt easy by automating your application process with this Auto Applier LinkedIn AI Auto Job Applier This is an web scraping bot that automates the process of job applications on LinkedIn. It searches for jobs relevant to you, answers all questions in application form, customizes your resume based on the collected job information, such as skills required, description, about company, etc. and applies to the job. Can apply 100+ jobs in less than 1 hour. See it in Action Click on above image to watch the demo or use this link https://youtu.be/gMbB1fWZDHw Content Introduction Demo Video Index Install Configure Contributor Guidelines Updates Disclaimer Terms and Conditions License Socials Support and Discussions How to install Click on above image to watch the tutorial for installation and configuration or use this link https://youtu.be/f9rdz74e1lM ( to watch it in 2x speed) Python 3.10 or above. Visit https://www.python.org/downloads/ to download and install Python, or for windows you could visit Microsoft Store and search for "Python". Please make sure Python is added to Path in System Environment Variables. Install necessary Undetected Chromedriver, PyAutoGUI and Setuptools packages. After Python is installed, OPEN a console/terminal or shell, Use below command that uses the pip command-line tool to install these 3 package. pip install undetected-chromedriver pyautogui setuptools openai flask-cors flask Download and install latest version of Google Chrome in it's default location, visit https://www.google.com/chrome to download it's installer. Clone the current git repo or download it as a zip file, url to the latest update https://github.com/GodsScion/Auto_job_applier_linkedIn. (Not needed if you set stealth_mode = True in config/settings.py ) Download and install the appropriate Chrome Driver for Google Chrome and paste it in the location Chrome was installed, visit https://googlechromelabs.github.io/chrome-for-testing/ to download. OR If you are using Windows, click on windows-setup.bat available in the /setup folder, this will install the latest chromedriver automatically. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY back to index How to configure Open personals.py file in /config folder and enter your details like name, phone number, address, etc. Whatever you want to fill in your applications. Open questions.py file in /config folder and enter your answers for application questions, configure wether you want the bot to pause before submission or pause if it can't answer unknown questions. Open search.py file in /config folder and enter your search preferences, job filters, configure the bot as per your needs (these settings decide which jobs to apply for or skip). Open secrets.py file in /config folder and enter your LinkedIn username, password to login and OpenAI API Key for generation of job tailored resumes and cover letters (This entire step is optional). If you do not provide username or password or leave them as default, it will login with saved profile in browser, if failed will ask you to login manually. Open settings.py file in /config folder to configure the bot settings like, keep screen awake, click intervals (click intervals are randomized to seem like human behavior), run in background, stealth mode (to avoid bot detection), etc. as per your needs. (Optional) Don't forget to add you default resume in the location you mentioned in default_resume_path = "all resumes/default/resume.pdf" given in /config/questions.py. If one is not provided, it will use your previous resume submitted in LinkedIn or (In Development) generate custom resume if OpenAI APT key is provided! Run runAiBot.py and see the magic happen. To run the Applied Jobs history UI, run app.py and open web browser on http://localhost:5000. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY back to index Contributor Guidelines Thank you for your efforts and being a part of the community. All contributions are appreciated no matter how small or big. Once you contribute to the code base, your work will be remembered forever. NOTE: Only Pull request to community-version branch will be accepted. Any other requests will be declined by default, especially to main branch. Once your code is tested, your changes will be merged to the main branch in next cycle. Code Guidelines Functions: All functions or methods are named lower case and snake case Must have explanation of their purpose. Write explanation surrounded in ''' Explanation ''' under the definition def function() -&gt; None:. Example: def function() -&gt; None: ''' This function does nothing, it's just an example for explanation placement! ''' The Types (str, list, int, list[str], int | float) for the parameters and returns must be given. Example: def function(param1: str, param2: list[str], param3: int) -&gt; str: Putting all that together some valid examples for function or method declarations would be as follows. def function_name_in_camel_case(parameter1: driver, parameter2: str) -&gt; list[str] | ValueError: ''' This function is an example for code guidelines ''' return [parameter2, parameter2.lower()] The hashtag on top of functions are optional, which are intended for developers # for developers. # Enter input text function
def text_input_by_ID(driver: WebDriver, id: str, value: str, time: float=5.0) -&gt; None | Exception: ''' Enters `value` into the input field with the given `id` if found, else throws NotFoundException. - `time` is the max time to wait for the element to be found. ''' username_field = WebDriverWait(driver, time).until(EC.presence_of_element_located((By.ID, id))) username_field.send_keys(Keys.CONTROL + "a") username_field.send_keys(value) Variables All variables must start with lower case, must be in explainable full words. If someone reads the variable name, it should be easy to understand what the variable stores. All local variables are camel case. Examples: jobListingsElement = None localBufferTime = 5.5 All global variables are snake case. Example: total_runs = 1 Mentioning types are optional. localBufferTime: float | int = 5.5 Configuration variables All config variables are treated as global variables. They have some extra guidelines. Must have variable setting explanation, and examples of valid values. Examples: # Explanation of what this setting will do, and instructions to enter it correctly
config_variable = "value1" # "value1", "value2", etc. Don't forget quotes ("") # Do you want to randomize the search order for search_terms?
randomize_search_order = False # True of False, Note: True or False are case-sensitive # Avoid applying to jobs if their required experience is above your current_experience. (Set value as -1 if you want to apply to all ignoring their required experience...)
current_experience = 5 # Integers &gt; -2 (Ex: -1, 0, 1, 2, 3, 4...) # Search location, this will be filled in "City, state, or zip code" search box. If left empty as "", tool will not fill it.
search_location = "United States" # Some valid examples: "", "United States", "India", "Chicago, Illinois, United States", "90001, Los Angeles, California, United States", "Bengaluru, Karnataka, India", etc. Add the config variable in appropriate /config/file. Every config variable must be validated. Go to /modules/validator.py and add it over there. Example: For config variable search_location = "" found in /config/search.py, string validation is added in file /modules/validator.py under the method def validate_search(). def validate_search() -&gt; None | ValueError | TypeError: ''' Validates all variables in the `/config/search.py` file. ''' check_string(search_location, "search_location") back to index Attestation All contributions require proper attestion. Format for attestation: ##&gt; ------ : OR - ------ print("My contributions ") # Your code
##&lt; Examples for proper attestation: New feature example ##&gt; ------ Sai Vignesh Golla : godsscion - Feature ------
def alert_box(title: str, message: str) -&gt; None: ''' Shows an alert box with the given `title` and `message`. ''' from pyautogui import alert return alert(title, message) ##&lt; Bug fix example def alert_box(title: str, message: str) -&gt; None: ''' Shows an alert box with the given `title` and `message`. ''' from pyautogui import alert ##&gt; ------ Sai Vignesh Golla : saivigneshgolla@outlook.com - Bug fix ------ return alert(message, title)]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:45 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/GodsScion/Auto_job_applier_linkedIn</guid>
    </item>
    <item>
      <title><![CDATA[strands-agents/sdk-python]]></title>
      <link>https://github.com/strands-agents/sdk-python</link>
      <description><![CDATA[A model-driven approach to building AI agents in just a few lines of code. Strands Agents A model-driven approach to building AI agents in just a few lines of code. Documentation ◆ Samples ◆ Python SDK ◆ Tools ◆ Agent Builder ◆ MCP Server Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs. Feature Overview Lightweight &amp; Flexible: Simple agent loop that just works and is fully customizable Model Agnostic: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers Advanced Capabilities: Multi-agent systems, autonomous agents, and streaming support Built-in MCP: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools Quick Start # Install Strands Agents
pip install strands-agents strands-agents-tools from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764") Note: For the default Amazon Bedrock model provider, you'll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the Quickstart Guide for details on configuring other model providers. Installation Ensure you have Python 3.10+ installed, then: # Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate # On Windows use: .venv\Scripts\activate # Install Strands and tools
pip install strands-agents strands-agents-tools Features at a Glance Python-Based Tools Easily build tools using Python decorators: from strands import Agent, tool @tool
def word_count(text: str) -&gt; int: """Count words in text. This docstring is used by the LLM to understand the tool's purpose. """ return len(text.split()) agent = Agent(tools=[word_count])
response = agent("How many words are in this sentence?") Hot Reloading from Directory: Enable automatic tool loading and reloading from the ./tools/ directory: from strands import Agent # Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent("Use any tools you find in the tools directory") MCP Support Seamlessly integrate Model Context Protocol (MCP) servers: from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters aws_docs_client = MCPClient( lambda: stdio_client(StdioServerParameters(command="uvx", args=["awslabs.aws-documentation-mcp-server@latest"]))
) with aws_docs_client: agent = Agent(tools=aws_docs_client.list_tools_sync()) response = agent("Tell me about Amazon Bedrock and how to use it with Python") Multiple Model Providers Support for various model providers: from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel
from strands.models.gemini import GeminiModel
from strands.models.llamacpp import LlamaCppModel # Bedrock
bedrock_model = BedrockModel( model_id="us.amazon.nova-pro-v1:0", temperature=0.3, streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent("Tell me about Agentic AI") # Google Gemini
gemini_model = GeminiModel( client_args={ "api_key": "your_gemini_api_key", }, model_id="gemini-2.5-flash", params={"temperature": 0.7}
)
agent = Agent(model=gemini_model)
agent("Tell me about Agentic AI") # Ollama
ollama_model = OllamaModel( host="http://localhost:11434", model_id="llama3"
)
agent = Agent(model=ollama_model)
agent("Tell me about Agentic AI") # Llama API
llama_model = LlamaAPIModel( model_id="Llama-4-Maverick-17B-128E-Instruct-FP8",
)
agent = Agent(model=llama_model)
response = agent("Tell me about Agentic AI") Built-in providers: Amazon Bedrock Anthropic Gemini Cohere LiteLLM llama.cpp LlamaAPI MistralAI Ollama OpenAI SageMaker Writer Custom providers can be implemented using Custom Providers Example tools Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation: from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764") It's also available on GitHub via strands-agents/tools. Bidirectional Streaming Experimental Feature: Bidirectional streaming is currently in experimental status. APIs may change in future releases as we refine the feature based on user feedback and evolving model capabilities. Build real-time voice and audio conversations with persistent streaming connections. Unlike traditional request-response patterns, bidirectional streaming maintains long-running conversations where users can interrupt, provide continuous input, and receive real-time audio responses. Get started with your first BidiAgent by following the Quickstart guide. Supported Model Providers: Amazon Nova Sonic (v1, v2) Google Gemini Live OpenAI Realtime API Installation: # Server-side only (no audio I/O dependencies)
pip install strands-agents[bidi] # With audio I/O support (includes PyAudio dependency)
pip install strands-agents[bidi,bidi-io] Quick Example: import asyncio
from strands.experimental.bidi import BidiAgent
from strands.experimental.bidi.models import BidiNovaSonicModel
from strands.experimental.bidi.io import BidiAudioIO, BidiTextIO
from strands.experimental.bidi.tools import stop_conversation
from strands_tools import calculator async def main(): # Create bidirectional agent with Nova Sonic v2 model = BidiNovaSonicModel() agent = BidiAgent(model=model, tools=[calculator, stop_conversation]) # Setup audio and text I/O (requires bidi-io extra) audio_io = BidiAudioIO() text_io = BidiTextIO() # Run with real-time audio streaming # Say "stop conversation" to gracefully end the conversation await agent.run( inputs=[audio_io.input()], outputs=[audio_io.output(), text_io.output()] ) if __name__ == "__main__": asyncio.run(main()) Note: BidiAudioIO and BidiTextIO require the bidi-io extra. For server-side deployments where audio I/O is handled by clients (browsers, mobile apps), install only strands-agents[bidi] and implement custom input/output handlers using the BidiInput and BidiOutput protocols. Configuration Options: from strands.experimental.bidi.models import BidiNovaSonicModel # Configure audio settings and turn detection (v2 only)
model = BidiNovaSonicModel( provider_config={ "audio": { "input_rate": 16000, "output_rate": 16000, "voice": "matthew" }, "turn_detection": { "endpointingSensitivity": "MEDIUM" # HIGH, MEDIUM, or LOW }, "inference": { "max_tokens": 2048, "temperature": 0.7 } }
) # Configure I/O devices
audio_io = BidiAudioIO( input_device_index=0, # Specific microphone output_device_index=1, # Specific speaker input_buffer_size=10, output_buffer_size=10
) # Text input mode (type messages instead of speaking)
text_io = BidiTextIO()
await agent.run( inputs=[text_io.input()], # Use text input outputs=[audio_io.output(), text_io.output()]
) # Multi-modal: Both audio and text input
await agent.run( inputs=[audio_io.input(), text_io.input()], # Speak OR type outputs=[audio_io.output(), text_io.output()]
) Documentation For detailed guidance &amp; examples, explore our documentation: User Guide Quick Start Guide Agent Loop Examples API Reference Production &amp; Deployment Guide Contributing We welcome contributions! See our Contributing Guide for details on: Reporting bugs &amp; features Development setup Contributing via Pull Requests Code of Conduct Reporting of security issues License This project is licensed under the Apache License 2.0 - see the LICENSE file for details. Security See CONTRIBUTING for more information.]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:41 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/strands-agents/sdk-python</guid>
    </item>
    <item>
      <title><![CDATA[openclaw/openclaw]]></title>
      <link>https://github.com/openclaw/openclaw</link>
      <description><![CDATA[Your own personal AI assistant. Any OS. Any Platform. The lobster way. OpenClaw — Personal AI Assistant EXFOLIATE! EXFOLIATE! OpenClaw is a personal AI assistant you run on your own devices. It answers you on the channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, WebChat), plus extension channels like BlueBubbles, Matrix, Zalo, and Zalo Personal. It can speak and listen on macOS/iOS/Android, and can render a live Canvas you control. The Gateway is just the control plane — the product is the assistant. If you want a personal, single-user assistant that feels local, fast, and always-on, this is it. Website · Docs · Vision · DeepWiki · Getting Started · Updating · Showcase · FAQ · Wizard · Nix · Docker · Discord Preferred setup: run the onboarding wizard (openclaw onboard) in your terminal. The wizard guides you step by step through setting up the gateway, workspace, channels, and skills. The CLI wizard is the path and works on macOS, Linux, and Windows (via WSL2; strongly ). Works with npm, pnpm, or bun. New install? Start here: Getting started Subscriptions (OAuth): Anthropic (Claude Pro/Max) OpenAI (ChatGPT/Codex) Model note: while any model is supported, I strongly recommend Anthropic Pro/Max (100/200) + Opus 4.6 for long‑context strength and better prompt‑injection resistance. See Onboarding. Models (selection + auth) Models config + CLI: Models Auth profile rotation (OAuth vs API keys) + fallbacks: Model failover Install ( ) Runtime: Node ≥22. npm install -g openclaw@latest
# or: pnpm add -g openclaw@latest openclaw onboard --install-daemon The wizard installs the Gateway daemon (launchd/systemd user service) so it stays running. Quick start (TL;DR) Runtime: Node ≥22. Full beginner guide (auth, pairing, channels): Getting started openclaw onboard --install-daemon openclaw gateway --port 18789 --verbose # Send a message
openclaw message send --to +1234567890 --message "Hello from OpenClaw" # Talk to the assistant (optionally deliver back to any connected channel: WhatsApp/Telegram/Slack/Discord/Google Chat/Signal/iMessage/BlueBubbles/Microsoft Teams/Matrix/Zalo/Zalo Personal/WebChat)
openclaw agent --message "Ship checklist" --thinking high Upgrading? Updating guide (and run openclaw doctor). Development channels stable: tagged releases (vYYYY.M.D or vYYYY.M.D-), npm dist-tag latest. beta: prerelease tags (vYYYY.M.D-beta.N), npm dist-tag beta (macOS app may be missing). dev: moving head of main, npm dist-tag dev (when published). Switch channels (git + npm): openclaw update --channel stable|beta|dev. Details: Development channels. From source (development) Prefer pnpm for builds from source. Bun is optional for running TypeScript directly. git clone https://github.com/openclaw/openclaw.git
cd openclaw pnpm install
pnpm ui:build # auto-installs UI deps on first run
pnpm build pnpm openclaw onboard --install-daemon # Dev loop (auto-reload on TS changes)
pnpm gateway:watch Note: pnpm openclaw ... runs TypeScript directly (via tsx). pnpm build produces dist/ for running via Node / the packaged openclaw binary. Security defaults (DM access) OpenClaw connects to real messaging surfaces. Treat inbound DMs as untrusted input. Full security guide: Security Default behavior on Telegram/WhatsApp/Signal/iMessage/Microsoft Teams/Discord/Google Chat/Slack: DM pairing (dmPolicy="pairing" / channels.discord.dmPolicy="pairing" / channels.slack.dmPolicy="pairing"; legacy: channels.discord.dm.policy, channels.slack.dm.policy): unknown senders receive a short pairing code and the bot does not process their message. Approve with: openclaw pairing approve (then the sender is added to a local allowlist store). Public inbound DMs require an explicit opt-in: set dmPolicy="open" and include "*" in the channel allowlist (allowFrom / channels.discord.allowFrom / channels.slack.allowFrom; legacy: channels.discord.dm.allowFrom, channels.slack.dm.allowFrom). Run openclaw doctor to surface risky/misconfigured DM policies. Highlights Local-first Gateway — single control plane for sessions, channels, tools, and events. Multi-channel inbox — WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, BlueBubbles (iMessage), iMessage (legacy), Microsoft Teams, Matrix, Zalo, Zalo Personal, WebChat, macOS, iOS/Android. Multi-agent routing — route inbound channels/accounts/peers to isolated agents (workspaces + per-agent sessions). Voice Wake + Talk Mode — always-on speech for macOS/iOS/Android with ElevenLabs. Live Canvas — agent-driven visual workspace with A2UI. First-class tools — browser, canvas, nodes, cron, sessions, and Discord/Slack actions. Companion apps — macOS menu bar app + iOS/Android nodes. Onboarding + skills — wizard-driven setup with bundled/managed/workspace skills. Star History Everything we built so far Core platform Gateway WS control plane with sessions, presence, config, cron, webhooks, Control UI, and Canvas host. CLI surface: gateway, agent, send, wizard, and doctor. Pi agent runtime in RPC mode with tool streaming and block streaming. Session model: main for direct chats, group isolation, activation modes, queue modes, reply-back. Group rules: Groups. Media pipeline: images/audio/video, transcription hooks, size caps, temp file lifecycle. Audio details: Audio. Channels Channels: WhatsApp (Baileys), Telegram (grammY), Slack (Bolt), Discord (discord.js), Google Chat (Chat API), Signal (signal-cli), BlueBubbles (iMessage, ), iMessage (legacy imsg), Microsoft Teams (extension), Matrix (extension), Zalo (extension), Zalo Personal (extension), WebChat. Group routing: mention gating, reply tags, per-channel chunking and routing. Channel rules: Channels. Apps + nodes macOS app: menu bar control plane, Voice Wake/PTT, Talk Mode overlay, WebChat, debug tools, remote gateway control. iOS node: Canvas, Voice Wake, Talk Mode, camera, screen recording, Bonjour pairing. Android node: Canvas, Talk Mode, camera, screen recording, optional SMS. macOS node mode: system.run/notify + canvas/camera exposure. Tools + automation Browser control: dedicated openclaw Chrome/Chromium, snapshots, actions, uploads, profiles. Canvas: A2UI push/reset, eval, snapshot. Nodes: camera snap/clip, screen record, location.get, notifications. Cron + wakeups; webhooks; Gmail Pub/Sub. Skills platform: bundled, managed, and workspace skills with install gating + UI. Runtime + safety Channel routing, retry policy, and streaming/chunking. Presence, typing indicators, and usage tracking. Models, model failover, and session pruning. Security and troubleshooting. Ops + packaging Control UI + WebChat served directly from the Gateway. Tailscale Serve/Funnel or SSH tunnels with token/password auth. Nix mode for declarative config; Docker-based installs. Doctor migrations, logging. How it works (short) WhatsApp / Telegram / Slack / Discord / Google Chat / Signal / iMessage / BlueBubbles / Microsoft Teams / Matrix / Zalo / Zalo Personal / WebChat │ ▼
┌───────────────────────────────┐
│ Gateway │
│ (control plane) │
│ ws://127.0.0.1:18789 │
└──────────────┬────────────────┘ │ ├─ Pi agent (RPC) ├─ CLI (openclaw …) ├─ WebChat UI ├─ macOS app └─ iOS / Android nodes Key subsystems Gateway WebSocket network — single WS control plane for clients, tools, and events (plus ops: Gateway runbook). Tailscale exposure — Serve/Funnel for the Gateway dashboard + WS (remote access: Remote). Browser control — openclaw‑managed Chrome/Chromium with CDP control. Canvas + A2UI — agent‑driven visual workspace (A2UI host: Canvas/A2UI). Voice Wake + Talk Mode — always‑on speech and continuous conversation. Nodes — Canvas, camera snap/clip, screen record, location.get, notifications, plus macOS‑only system.run/system.notify. Tailscale access (Gateway dashboard) OpenClaw can auto-configure Tailscale Serve (tailnet-only) or Funnel (public) while the Gateway stays bound to loopback. Configure gateway.tailscale.mode: off: no Tailscale automation (default). serve: tailnet-only HTTPS via tailscale serve (uses Tailscale identity headers by default). funnel: public HTTPS via tailscale funnel (requires shared password auth). Notes: gateway.bind must stay loopback when Serve/Funnel is enabled (OpenClaw enforces this). Serve can be forced to require a password by setting gateway.auth.mode: "password" or gateway.auth.allowTailscale: false. Funnel refuses to start unless gateway.auth.mode: "password" is set. Optional: gateway.tailscale.resetOnExit to undo Serve/Funnel on shutdown. Details: Tailscale guide · Web surfaces Remote Gateway (Linux is great) It’s perfectly fine to run the Gateway on a small Linux instance. Clients (macOS app, CLI, WebChat) can connect over Tailscale Serve/Funnel or SSH tunnels, and you can still pair device nodes (macOS/iOS/Android) to execute device‑local actions when needed. Gateway host runs the exec tool and channel connections by default. Device nodes run device‑local actions (system.run, camera, screen recording, notifications) via node.invoke. In short: exec runs where the Gateway lives; device actions run where the device lives. Details: Remote access · Nodes · Security macOS permissions via the Gateway protocol The macOS app can run in node mode and advertises its capabilities + permission map over the Gateway WebSocket (node.list / node.describe). Clients can then execute local actions via node.invoke: system.run runs a local command and returns stdout/stderr/exit code; set needsScreenRecording: true to require screen-recording permission (otherwise you’ll get PERMISSION_MISSING). system.notify posts a user notification and fails if notifications are denied. canvas.*, camera.*, screen.record, and location.get are also routed via node.invoke and follow TCC permission status. Elevated bash (host permissions) is separate from macOS TCC: Use /elevated on|off to toggle per‑session elevated access when enabled + allowlisted. Gateway persists the per‑session toggle via sessions.patch (WS method) alongside thinkingLevel, verboseLevel, model, sendPolicy, and groupActivation. Details: Nodes · macOS app · Gateway protocol Agent to Agent (sessions_* tools) Use these to coordinate work across sessions without jumping between chat surfaces. sessions_list — discover active sessions (agents) and their metadata. sessions_history — fetch transcript logs for a session. sessions_send — message another session; optional reply‑back ping‑pong + announce step (REPLY_SKIP, ANNOUNCE_SKIP). Details: Session tools Skills registry (ClawHub) ClawHub is a minimal skill registry. With ClawHub enabled, the agent can search for skills automatically and pull in new ones as needed. ClawHub Chat commands Send these in WhatsApp/Telegram/Slack/Google Chat/Microsoft Teams/WebChat (group commands are owner-only): /status — compact session status (model + tokens, cost when available) /new or /reset — reset the session /compact — compact session context (summary) /think — off|minimal|low|medium|high|xhigh (GPT-5.2 + Codex models only) /verbose on|off /usage off|tokens|full — per-response usage footer /restart — restart the gateway (owner-only in groups) /activation mention|always — group activation toggle (groups only) Apps (optional) The Gateway alone delivers a great experience. All apps are optional and add extra features. If you plan to build/run companion apps, follow the platform runbooks below. macOS (OpenClaw.app) (optional) Menu bar control for the Gateway and health. Voice Wake + push-to-talk overlay. WebChat + debug tools. Remote gateway control over SSH. Note: signed builds required for macOS permissions to stick across rebuilds (see docs/mac/permissions.md). iOS node (optional) Pairs as a node via the Bridge. Voice trigger forwarding + Canvas surface. Controlled via openclaw nodes …. Runbook: iOS connect. Android node (optional) Pairs via the same Bridge + pairing flow as iOS. Exposes Canvas, Camera, and Screen capture commands. Runbook: Android connect. Agent workspace + skills Workspace root: ~/.openclaw/workspace (configurable via agents.defaults.workspace). Injected prompt files: AGENTS.md, SOUL.md, TOOLS.md. Skills: ~/.openclaw/workspace/skills//SKILL.md. Configuration Minimal ~/.openclaw/openclaw.json (model + defaults): { agent: { model: "anthropic/claude-opus-4-6", },
} Full configuration reference (all keys + examples). Security model (important) Default: tools run on the host for the main session, so the agent has full access when it’s just you. Group/channel safety: set agents.defaults.sandbox.mode: "non-main" to run non‑main sessions (groups/channels) inside per‑session Docker sandboxes; bash then runs in Docker for those sessions. Sandbox defaults: allowlist bash, process, read, write, edit, sessions_list, sessions_history, sessions_send, sessions_spawn; denylist browser, canvas, nodes, cron, discord, gateway. Details: Security guide · Docker + sandboxing · Sandbox config WhatsApp Link the device: pnpm openclaw channels login (stores creds in ~/.openclaw/credentials). Allowlist who can talk to the assistant via channels.whatsapp.allowFrom. If channels.whatsapp.groups is set, it becomes a group allowlist; include "*" to allow all. Telegram Set TELEGRAM_BOT_TOKEN or channels.telegram.botToken (env wins). Optional: set channels.telegram.groups (with channels.telegram.groups."*".requireMention); when set, it is a group allowlist (include "*" to allow all). Also channels.telegram.allowFrom or channels.telegram.webhookUrl + channels.telegram.webhookSecret as needed. { channels: { telegram: { botToken: "123456:ABCDEF", }, },
} Slack Set SLACK_BOT_TOKEN + SLACK_APP_TOKEN (or channels.slack.botToken + channels.slack.appToken). Discord Set DISCORD_BOT_TOKEN or channels.discord.token (env wins). Optional: set commands.native, commands.text, or commands.useAccessGroups, plus channels.discord.allowFrom, channels.discord.guilds, or channels.discord.mediaMaxMb as needed. { channels: { discord: { token: "1234abcd", }, },
} Signal Requires signal-cli and a channels.signal config section. BlueBubbles (iMessage) iMessage integration. Configure channels.bluebubbles.serverUrl + channels.bluebubbles.password and a webhook (channels.bluebubbles.webhookPath). The BlueBubbles server runs on macOS; the Gateway can run on macOS or elsewhere. iMessage (legacy) Legacy macOS-only integration via imsg (Messages must be signed in). If channels.imessage.groups is set, it becomes a group allowlist; include "*" to allow all. Microsoft Teams Configure a Teams app + Bot Framework, then add a msteams config section. Allowlist who can talk via msteams.allowFrom; group access via msteams.groupAllowFrom or msteams.groupPolicy: "open". WebChat Uses the Gateway WebSocket; no separate WebChat port/config. Browser control (optional): { browser: { enabled: true, color: "#FF4500", },]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:40 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/openclaw/openclaw</guid>
    </item>
    <item>
      <title><![CDATA[freemocap/freemocap]]></title>
      <link>https://github.com/freemocap/freemocap</link>
      <description><![CDATA[Free Motion Capture for Everyone The FreeMoCap Project A free-and-open-source, hardware-and-software-agnostic, minimal-cost, research-grade, motion capture system and platform for decentralized scientific research, education, and training https://user-images.githubusercontent.com/15314521/192062522-2a8d9305-f181-4869-a4b9-1aa068e094c9.mp4 -- QUICKSTART [!NOTE] For detailed installation instructions, see our official documentation's Installation page 0. Create a a Python 3.10 through 3.12 environment (python3.12 ) 1. Install software via pip: pip install freemocap 2. Launch the GUI by entering the command: freemocap 3. A GUI should pop up that looks like this: 4. Have fun! See the Beginner Tutorials on our official docs for detailed instructions. 5. Join the Discord and let us know how it went! Install/run from source code (i.e. the code in this repo) Open an Anaconda-enabled command prompt (or your preferred method of environment management) and enter the following commands: Create a Python environment ( version is python3.11) conda create -n freemocap-env python=3.11 Activate that newly created environment conda activate freemocap-env Clone the repository git clone https://github.com/freemocap/freemocap Navigate into the newly cloned/downloaded freemocap folder cd freemocap Install the package via the pyproject.toml file pip install -e . Launch the GUI (via the freemocap.__main__.py entry point) python -m freemocap A GUI should pop up! Documentation Our documentation is hosted at: https://freemocap.github.io/documentation That site is built using writerside from this repository: https://github.com/freemocap/documentation Contribution Guidelines Please read our contribution doc: CONTRIBUTING.md Related Maintainers Jon Matthis Endurance Idehen License This project is licensed under the APGL License - see the LICENSE file for details. If the AGPL does not work for your needs, we are happy to discuss terms to license this software to you with a different agreement at a price point that increases exponentially as you move spiritually away from the AGPL]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:40 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/freemocap/freemocap</guid>
    </item>
    <item>
      <title><![CDATA[RichardAtCT/claude-code-telegram]]></title>
      <link>https://github.com/RichardAtCT/claude-code-telegram</link>
      <description><![CDATA[A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence. Claude Code Telegram Bot A Telegram bot that gives you remote access to Claude Code. Chat naturally with Claude about your projects from anywhere -- no terminal commands needed. What is this? This bot connects Telegram to Claude Code, providing a conversational AI interface for your codebase: Chat naturally -- ask Claude to analyze, edit, or explain your code in plain language Maintain context across conversations with automatic session persistence per project Code on the go from any device with Telegram Receive proactive notifications from webhooks, scheduled jobs, and CI/CD events Stay secure with built-in authentication, directory sandboxing, and audit logging Quick Start Demo You: Can you help me add error handling to src/api.py? Bot: I'll analyze src/api.py and add error handling... [Claude reads your code, suggests improvements, and can apply changes directly] You: Looks good. Now run the tests to make sure nothing broke. Bot: Running pytest... All 47 tests passed. The error handling changes are working correctly. 1. Prerequisites Python 3.10+ -- Download here Poetry -- Modern Python dependency management Claude Code CLI -- Install from here Telegram Bot Token -- Get one from @BotFather 2. Install git clone https://github.com/RichardAtCT/claude-code-telegram.git
cd claude-code-telegram
make dev 3. Configure cp .env.example .env
# Edit .env with your settings: Minimum required: TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
TELEGRAM_BOT_USERNAME=my_claude_bot
APPROVED_DIRECTORY=/Users/yourname/projects
ALLOWED_USERS=123456789 # Your Telegram user ID 4. Run make run # Production
make run-debug # With debug logging Message your bot on Telegram to get started. Detailed setup: See docs/setup.md for Claude authentication options and troubleshooting. Modes The bot supports two interaction modes: Agentic Mode (Default) The default conversational mode. Just talk to Claude naturally -- no special commands required. Commands: /start, /new, /status, /verbose, /repo If ENABLE_PROJECT_THREADS=true: /sync_threads You: What files are in this project?
Bot: Working... (3s) Read LS Let me describe the project structure
Bot: [Claude describes the project structure] You: Add a retry decorator to the HTTP client
Bot: Working... (8s) Read: http_client.py I'll add a retry decorator with exponential backoff Edit: http_client.py Bash: poetry run pytest tests/ -v
Bot: [Claude shows the changes and test results] You: /verbose 0
Bot: Verbosity set to 0 (quiet) Use /verbose 0|1|2 to control how much background activity is shown: Level Shows 0 (quiet) Final response only (typing indicator stays active) 1 (normal, default) Tool names + reasoning snippets in real-time 2 (detailed) Tool names with inputs + longer reasoning text GitHub Workflow Claude Code already knows how to use gh CLI and git. Authenticate on your server with gh auth login, then work with repos conversationally: You: List my repos related to monitoring
Bot: [Claude runs gh repo list, shows results] You: Clone the uptime one
Bot: [Claude runs gh repo clone, clones into workspace] You: /repo
Bot: uptime-monitor/ other-project/ You: Show me the open issues
Bot: [Claude runs gh issue list] You: Create a fix branch and push it
Bot: [Claude creates branch, commits, pushes] Use /repo to list cloned repos in your workspace, or /repo to switch directories (sessions auto-resume). Classic Mode Set AGENTIC_MODE=false to enable the full 13-command terminal-like interface with directory navigation, inline keyboards, quick actions, git integration, and session export. Commands: /start, /help, /new, /continue, /end, /status, /cd, /ls, /pwd, /projects, /export, /actions, /git If ENABLE_PROJECT_THREADS=true: /sync_threads You: /cd my-web-app
Bot: Directory changed to my-web-app/ You: /ls
Bot: src/ tests/ package.json README.md You: /actions
Bot: [Run Tests] [Install Deps] [Format Code] [Run Linter] Event-Driven Automation Beyond direct chat, the bot can respond to external triggers: Webhooks -- Receive GitHub events (push, PR, issues) and route them through Claude for automated summaries or code review Scheduler -- Run recurring Claude tasks on a cron schedule (e.g., daily code health checks) Notifications -- Deliver agent responses to configured Telegram chats Enable with ENABLE_API_SERVER=true and ENABLE_SCHEDULER=true. See docs/setup.md for configuration. Features Working Features Conversational agentic mode (default) with natural language interaction Classic terminal-like mode with 13 commands and inline keyboards Full Claude Code integration with SDK (primary) and CLI (fallback) Automatic session persistence per user/project directory Multi-layer authentication (whitelist + optional token-based) Rate limiting with token bucket algorithm Directory sandboxing with path traversal prevention File upload handling with archive extraction Image/screenshot upload with analysis Git integration with safe repository operations Quick actions system with context-aware buttons Session export in Markdown, HTML, and JSON formats SQLite persistence with migrations Usage and cost tracking Audit logging and security event tracking Event bus for decoupled message routing Webhook API server (GitHub HMAC-SHA256, generic Bearer token auth) Job scheduler with cron expressions and persistent storage Notification service with per-chat rate limiting Tunable verbose output showing Claude's tool usage and reasoning in real-time Persistent typing indicator so users always know the bot is working Planned Enhancements Plugin system for third-party extensions Configuration Required TELEGRAM_BOT_TOKEN=... # From @BotFather
TELEGRAM_BOT_USERNAME=... # Your bot's username
APPROVED_DIRECTORY=... # Base directory for project access
ALLOWED_USERS=123456789 # Comma-separated Telegram user IDs Common Options # Claude
USE_SDK=true # Python SDK (default) or CLI subprocess
ANTHROPIC_API_KEY=sk-ant-... # API key (optional if using CLI auth)
CLAUDE_MAX_COST_PER_USER=10.0 # Spending limit per user (USD)
CLAUDE_TIMEOUT_SECONDS=300 # Operation timeout # Mode
AGENTIC_MODE=true # Agentic (default) or classic mode
VERBOSE_LEVEL=1 # 0=quiet, 1=normal (default), 2=detailed # Rate Limiting
RATE_LIMIT_REQUESTS=10 # Requests per window
RATE_LIMIT_WINDOW=60 # Window in seconds # Features (classic mode)
ENABLE_GIT_INTEGRATION=true
ENABLE_FILE_UPLOADS=true
ENABLE_QUICK_ACTIONS=true Agentic Platform # Webhook API Server
ENABLE_API_SERVER=false # Enable FastAPI webhook server
API_SERVER_PORT=8080 # Server port # Webhook Authentication
GITHUB_WEBHOOK_SECRET=... # GitHub HMAC-SHA256 secret
WEBHOOK_API_SECRET=... # Bearer token for generic providers # Scheduler
ENABLE_SCHEDULER=false # Enable cron job scheduler # Notifications
NOTIFICATION_CHAT_IDS=123,456 # Default chat IDs for proactive notifications Project Threads Mode # Enable strict topic routing by project
ENABLE_PROJECT_THREADS=true # Mode: private (default) or group
PROJECT_THREADS_MODE=private # YAML registry file (see config/projects.example.yaml)
PROJECTS_CONFIG_PATH=config/projects.yaml # Required only when PROJECT_THREADS_MODE=group
PROJECT_THREADS_CHAT_ID=-1001234567890 In strict mode, only /start and /sync_threads work outside mapped project topics. In private mode, /start auto-syncs project topics for your private bot chat. To use topics with your bot, enable them in BotFather: Bot Settings -&gt; Threaded mode. Full reference: See docs/configuration.md and .env.example. Finding Your Telegram User ID Message @userinfobot on Telegram -- it will reply with your user ID number. Troubleshooting Bot doesn't respond: Check your TELEGRAM_BOT_TOKEN is correct Verify your user ID is in ALLOWED_USERS Ensure Claude Code CLI is installed and accessible Check bot logs with make run-debug Claude integration not working: SDK mode (default): Check claude auth status or verify ANTHROPIC_API_KEY CLI mode: Verify claude --version and claude auth status Check CLAUDE_ALLOWED_TOOLS includes necessary tools High usage costs: Adjust CLAUDE_MAX_COST_PER_USER to set spending limits Monitor usage with /status Use shorter, more focused requests Security This bot implements defense-in-depth security: Access Control -- Whitelist-based user authentication Directory Isolation -- Sandboxing to approved directories Rate Limiting -- Request and cost-based limits Input Validation -- Injection and path traversal protection Webhook Authentication -- GitHub HMAC-SHA256 and Bearer token verification Audit Logging -- Complete tracking of all user actions See SECURITY.md for details. Development make dev # Install all dependencies
make test # Run tests with coverage
make lint # Black + isort + flake8 + mypy
make format # Auto-format code
make run-debug # Run with debug logging Contributing Fork the repository Create a feature branch: git checkout -b feature/amazing-feature Make changes with tests: make test &amp;&amp; make lint Submit a Pull Request Code standards: Python 3.10+, Black formatting (88 chars), type hints required, pytest with &gt;85% coverage. License MIT License -- see LICENSE. Acknowledgments Claude by Anthropic python-telegram-bot]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:38 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/RichardAtCT/claude-code-telegram</guid>
    </item>
    <item>
      <title><![CDATA[scikit-learn/scikit-learn]]></title>
      <link>https://github.com/scikit-learn/scikit-learn</link>
      <description><![CDATA[scikit-learn: machine learning in Python .. -- mode: rst -- |Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPI| |DOI| |Benchmark| .. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;branchName=main .. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield :target: https://circleci.com/gh/scikit-learn/scikit-learn .. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9 :target: https://codecov.io/gh/scikit-learn/scikit-learn .. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/actions/workflows/wheels.yml/badge.svg?event=schedule :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule .. |Ruff| image:: https://img.shields.io/badge/code%20style-ruff-000000.svg :target: https://github.com/astral-sh/ruff .. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg :target: https://pypi.org/project/scikit-learn/ .. |PyPI| image:: https://img.shields.io/pypi/v/scikit-learn :target: https://pypi.org/project/scikit-learn .. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn .. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue :target: https://scikit-learn.org/scikit-learn-benchmarks .. |PythonMinVersion| replace:: 3.11 .. |NumPyMinVersion| replace:: 1.24.1 .. |SciPyMinVersion| replace:: 1.10.0 .. |JoblibMinVersion| replace:: 1.3.0 .. |ThreadpoolctlMinVersion| replace:: 3.2.0 .. |MatplotlibMinVersion| replace:: 3.6.1 .. |Scikit-ImageMinVersion| replace:: 0.22.0 .. |PandasMinVersion| replace:: 1.5.0 .. |SeabornMinVersion| replace:: 0.13.0 .. |PytestMinVersion| replace:: 7.1.2 .. |PlotlyMinVersion| replace:: 5.18.0 .. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png :target: https://scikit-learn.org/ scikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license. The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed. See the About us __ page for a list of core contributors. It is currently maintained by a team of volunteers. Website: https://scikit-learn.org Installation Dependencies scikit-learn requires: - Python (&gt;= |PythonMinVersion|)
- NumPy (&gt;= |NumPyMinVersion|)
- SciPy (&gt;= |SciPyMinVersion|)
- joblib (&gt;= |JoblibMinVersion|)
- threadpoolctl (&gt;= |ThreadpoolctlMinVersion|) ======= Scikit-learn plotting capabilities (i.e., functions start with ``plot_`` and
classes end with ``Display``) require Matplotlib (&gt;= |MatplotlibMinVersion|).
For running the examples Matplotlib &gt;= |MatplotlibMinVersion| is required.
A few examples require scikit-image &gt;= |Scikit-ImageMinVersion|, a few examples
require pandas &gt;= |PandasMinVersion|, some examples require seaborn &gt;=
|SeabornMinVersion| and Plotly &gt;= |PlotlyMinVersion|. User installation If you already have a working installation of NumPy and SciPy, the easiest way to install scikit-learn is using pip:: pip install -U scikit-learn or conda:: conda install -c conda-forge scikit-learn The documentation includes more detailed installation instructions _. Changelog See the changelog __ for a history of notable changes to scikit-learn. Development We welcome new contributors of all experience levels. The scikit-learn community goals are to be helpful, welcoming, and effective. The Development Guide _ has detailed information about contributing code, documentation, tests, and more. We've included some basic information in this README. Important links - Official source code repo: https://github.com/scikit-learn/scikit-learn
- Download releases: https://pypi.org/project/scikit-learn/
- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues Source code
~~~~~~~~~~~ You can check the latest sources with the command:: git clone https://github.com/scikit-learn/scikit-learn.git Contributing
~~~~~~~~~~~~ To learn more about making a contribution to scikit-learn, please see our
`Contributing guide
`_. Testing
~~~~~~~ After installation, you can launch the test suite from outside the source
directory (you will need to have ``pytest`` &gt;= |PytestMinVersion| installed):: pytest sklearn See the web page https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage
for more information. Random number generation can be controlled during testing by setting the ``SKLEARN_SEED`` environment variable. Submitting a Pull Request Before opening a Pull Request, have a look at the full Contributing page to make sure your code complies with our guidelines: https://scikit-learn.org/stable/developers/index.html Project History The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed. See the About us __ page for a list of core contributors. The project is currently maintained by a team of volunteers. Note: scikit-learn was previously referred to as scikits.learn. Help and Support Documentation - HTML documentation (stable release): https://scikit-learn.org
- HTML documentation (development version): https://scikit-learn.org/dev/
- FAQ: https://scikit-learn.org/stable/faq.html Communication Main Channels ^^^^^^^^^^^^^ Website: https://scikit-learn.org Blog: https://blog.scikit-learn.org Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn Developer &amp; Support ^^^^^^^^^^^^^^^^^^^^^^ GitHub Discussions: https://github.com/scikit-learn/scikit-learn/discussions Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn Discord: https://discord.gg/h9qyrK8Jc8 Social Media Platforms ^^^^^^^^^^^^^^^^^^^^^^ LinkedIn: https://www.linkedin.com/company/scikit-learn YouTube: https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists Facebook: https://www.facebook.com/scikitlearnofficial/ Instagram: https://www.instagram.com/scikitlearnofficial/ TikTok: https://www.tiktok.com/@scikit.learn Bluesky: https://bsky.app/profile/scikit-learn.org Mastodon: https://mastodon.social/@sklearn@fosstodon.org Resources ^^^^^^^^^ Calendar: https://blog.scikit-learn.org/calendar/ Logos &amp; Branding: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos Citation If you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:41 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/scikit-learn/scikit-learn</guid>
    </item>
    <item>
      <title><![CDATA[harvard-edge/cs249r_book]]></title>
      <link>https://github.com/harvard-edge/cs249r_book</link>
      <description><![CDATA[Introduction to Machine Learning Systems Machine Learning Systems Principles and Practices of Engineering Artificially Intelligent Systems English • 中文 • 日本語 • 한국어 Read Online • TinyTorch • Download PDF • Download EPUB • Explore Ecosystem Hardcopy edition coming 2026 with MIT Press. Mission The world is rushing to build AI systems. It is not engineering them. That gap is what we mean by AI engineering. AI engineering is the discipline of building efficient, reliable, safe, and robust intelligent systems that operate in the real world, not just models in isolation. Our mission: Establish AI engineering as a foundational discipline, alongside software engineering and computer engineering, by teaching how to design, build, and evaluate end to end intelligent systems. The long term impact of AI will be shaped by engineers who can turn ideas into working, dependable systems. What’s in this repo This repository is the open learning stack for AI systems engineering. It includes the textbook source, TinyTorch, hardware kits, and upcoming co-labs that connect principles to runnable code and real devices. Start Here Choose a path based on your goal. READ Start with the textbook. Try Chapter 1 and the Benchmarking chapter. BUILD Start TinyTorch with the getting started guide. Begin with Module 01 and work up from CNNs to transformers and the MLPerf benchmarks. DEPLOY Pick a hardware kit and run the labs on Arduino, Raspberry Pi, and other edge devices. CONNECT Say hello in Discussions. We will do our best to reply. The Learning Stack The learning stack below shows how the textbook connects to hands on work and deployment. Read the textbook, then pick your path: ┌───────────────────────────────────────────────────────────────────────────────┐
│ │
│ MACHINE LEARNING SYSTEMS │
│ Read the Textbook │
│ │
│ Theory • Concepts • Best Practices │
│ │
└───────────────────────────────────────┬───────────────────────────────────────┘ │ ┌─────────────┼─────────────┐ │ │ │ ▼ ▼ ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│ HANDS-ON ACTIVITIES │
│ (pick one or all) │
│ │
│ ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │
│ │ │ │ │ │ │ │
│ │ SOFTWARE │ │ TINYTORCH │ │ HARDWARE │ │
│ │ CO-LABS │ │ FRAMEWORK │ │ LABS │ │
│ │ │ │ │ │ │ │
│ │ EXPLORE │ │ BUILD │ │ DEPLOY │ │
│ │ │ │ │ │ │ │
│ │ Run controlled │ │ Understand │ │ Engineer under │ │
│ │ experiments on │ │ frameworks by │ │ real constraints│ │
│ │ latency, memory,│ │ implementing │ │ memory, power, │ │
│ │ energy, cost │ │ them │ │ timing, safety │ │
│ │ │ │ │ │ │ │
│ │ (coming 2026) │ │ │ │ Arduino, Pi │ │
│ └─────────────────┘ └─────────────────┘ └─────────────────┘ │
│ │
│ EXPLORE BUILD DEPLOY │
│ │
└───────────────────────────────────────┬───────────────────────────────────────┘ │ ▼
┌───────────────────────────────────────────────────────────────────────────────┐
│ │
│ AI OLYMPICS │
│ Prove Mastery │
│ │
│ Compete across all tracks • University teams • Public leaderboards │
│ │
│ (coming 2026) │
│ │
└───────────────────────────────────────────────────────────────────────────────┘ Component What You Do Link READ Textbook Understand ML systems concepts book/ EXPLORE Software Co-Labs Run controlled experiments on latency, memory, energy, cost Coming 2026 BUILD TinyTorch Understand frameworks by implementing them tinytorch/ DEPLOY Hardware Kits Engineer under real constraints: memory, power, timing, safety kits/ PROVE AI Olympics Compete and benchmark across all tracks Coming 2026 What each path teaches: EXPLORE teaches why — Understand tradeoffs. Change batch sizes, precision, model architectures and see how latency, memory, and accuracy shift. BUILD teaches how — Understand internals. Implement autograd, optimizers, and attention from scratch to see how TensorFlow and PyTorch actually work. DEPLOY teaches where — Understand constraints. Face real memory limits, power budgets, and latency requirements on actual hardware. What You Will Learn This textbook teaches you to think at the intersection of machine learning and systems engineering. Each chapter bridges algorithmic concepts with the infrastructure that makes them work in practice. The ML Systems Bridge ML Concept Systems Concept What You Learn Model parameters Memory constraints How to fit large models on resource-limited devices Inference latency Hardware acceleration How GPUs, TPUs, and accelerators execute neural networks Training convergence Compute efficiency How mixed-precision and optimization techniques reduce cost Model accuracy Quantization and pruning How to compress models while preserving performance Data requirements Pipeline infrastructure How to build efficient data loading and preprocessing Model deployment MLOps practices How to monitor, version, and update models in production Privacy constraints On-device learning How to train and adapt models without sending data to the cloud Book Structure Part Focus Chapters I. Foundations Core concepts Introduction, ML Systems, DL Primer, Architectures II. Design Building blocks Workflow, Data Engineering, Frameworks, Training III. Performance Making it fast Efficient AI, Optimizations, HW Acceleration, Benchmarking IV. Deployment Making it work MLOps, On-device Learning, Privacy, Robustness V. Trust Making it right Responsible AI, Sustainable AI, AI for Good VI. Frontiers What's next Emerging trends and future directions What Makes This Different This is a living textbook. We keep it updated as the field grows, with community input along the way. AI may feel like it is moving at lightning speed, but the engineering building blocks that make it work do not change as quickly as the headlines. This project is built around those stable foundations. Think of it like LEGO. New sets arrive all the time, but the bricks themselves stay the same. Once you learn how the bricks fit together, you can build anything. Here, those "AI bricks" are the solid systems principles that make AI work. Whether you are reading a chapter, running a lab, or sharing feedback, you are helping make these ideas more accessible to the next learner. Research to Teaching Loop We use the same loop for research and teaching: define the system problem, build a reference implementation, benchmark it, then turn it into curriculum and tooling so others can reproduce and extend it. Loop Step Research Artifacts Teaching Artifacts Measure Benchmarks, suites, metrics Benchmarking chapter, assignments Build Reference systems, compilers, runtimes TinyTorch modules, co-labs Deploy Hardware targets, constraints, reliability Hardware labs, kits Support This Work We are working toward 1 million learners by 2030 so that AI engineering becomes a shared, teachable discipline, not a collection of isolated practices. Every star, share, and contribution helps move this effort forward. Why GitHub Stars Matter What gets measured gets improved. Each star is a learner, educator, or supporter who believes AI systems should be engineered with rigor and real world constraints in mind. 1 learner → 10 learners → 100 learners → 1,000 learners → 10,000 learners → 100,000 learners → 1M learners Stars are not the goal. They are a signal. A visible, growing community makes it easier for universities, foundations, and industry partners to adopt this material, donate hardware, and fund workshops. That momentum lowers the barrier for the next institution, the next classroom, and the next cohort of learners. Support raised through this signal flows into Open Collective and funds concrete outcomes such as TinyML4D workshops, hardware kits for underserved classrooms, and the infrastructure required to keep this resource free and open. One click can unlock the next classroom, the next contributor, and the next generation of AI engineers. Fund the Mission All contributions go to Open Collective, a transparent fund that supports educational outreach. Community and Resources Resource Description Textbook Interactive online textbook TinyTorch Build ML frameworks from scratch Hardware Kits Deploy to Arduino, Raspberry Pi, edge devices Ecosystem Resources, workshops, and community Discussions Questions and ideas Contributing We welcome contributions to the book, TinyTorch, and hardware kits! I want to... Go here Fix a typo or improve a chapter book/docs/CONTRIBUTING.md Add a TinyTorch module or fix a bug tinytorch/CONTRIBUTING.md Improve hardware labs kits/README.md Report an issue GitHub Issues Ask a question GitHub Discussions Citation &amp; License Citation @inproceedings{reddi2024mlsysbook, title = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering}, author = {Reddi, Vijay Janapa}, booktitle = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)}, pages = {41--42}, year = {2024}, organization = {IEEE}, url = {https://mlsysbook.org}]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:40 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/harvard-edge/cs249r_book</guid>
    </item>
    <item>
      <title><![CDATA[mlflow/mlflow]]></title>
      <link>https://github.com/mlflow/mlflow</link>
      <description><![CDATA[The open source developer platform to build AI agents and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform. Open-Source Platform for Productionizing AI MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end experiment tracking, observability, and evaluations, all in one integrated platform. Website · Docs · Feature Request · News · YouTube · Events Installation To install the MLflow Python package, run the following command: pip install mlflow Core Components MLflow is the only platform that provides a unified solution for all your AI/ML needs, including LLMs, Agents, Deep Learning, and traditional machine learning. For LLM / GenAI Developers Tracing / Observability Getting Started → LLM Evaluation Getting Started → Prompt Management Getting Started → App Version Tracking Getting Started → For Data Scientists Experiment Tracking Getting Started → Model Registry Getting Started → Deployment Getting Started → Hosting MLflow Anywhere You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure. Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers: Amazon SageMaker Azure ML Databricks Nebius For hosting MLflow on your own infrastructure, please refer to this guidance. Supported Programming Languages Python TypeScript / JavaScript Java R Integrations MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries. Usage Examples Tracing (Observability) (Doc) MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call mlflow.xyz.autolog() before running your models. Refer to the documentation for customization and manual instrumentation. import mlflow
from openai import OpenAI # Enable tracing for OpenAI
mlflow.openai.autolog() # Query OpenAI LLM normally
response = OpenAI().chat.completions.create( model="gpt-4o-mini", messages=[{"role": "user", "content": "Hi!"}], temperature=0.1,
) Then navigate to the "Traces" tab in the MLflow UI to find the trace records for the OpenAI query. Evaluating LLMs, Prompts, and Agents (Doc) The following example runs automatic evaluation for question-answering tasks with several built-in metrics. import os
import openai
import mlflow
from mlflow.genai.scorers import Correctness, Guidelines client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY")) # 1. Define a simple QA dataset
dataset = [ { "inputs": {"question": "Can MLflow manage prompts?"}, "expectations": {"expected_response": "Yes!"}, }, { "inputs": {"question": "Can MLflow create a taco for my lunch?"}, "expectations": { "expected_response": "No, unfortunately, MLflow is not a taco maker." }, },
] # 2. Define a prediction function to generate responses
def predict_fn(question: str) -&gt; str: response = client.chat.completions.create( model="gpt-4o-mini", messages=[{"role": "user", "content": question}] ) return response.choices[0].message.content # 3. Run the evaluation
results = mlflow.genai.evaluate( data=dataset, predict_fn=predict_fn, scorers=[ # Built-in LLM judge Correctness(), # Custom criteria using LLM judge Guidelines(name="is_english", guidelines="The answer must be in English"), ],
) Navigate to the "Evaluations" tab in the MLflow UI to find the evaluation results. Tracking Model Training (Doc) The following example trains a simple regression model with scikit-learn, while enabling MLflow's autologging feature for experiment tracking. import mlflow from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor # Enable MLflow's automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog() # Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target) rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:40 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/mlflow/mlflow</guid>
    </item>
    <item>
      <title><![CDATA[p-e-w/heretic]]></title>
      <link>https://github.com/p-e-w/heretic</link>
      <description><![CDATA[Fully automatic censorship removal for language models Heretic: Fully automatic censorship removal for language models Heretic is a tool that removes censorship (aka "safety alignment") from transformer-based language models without expensive post-training. It combines an advanced implementation of directional ablation, also known as "abliteration" (Arditi et al. 2024, Lai 2025 (1, 2)), with a TPE-based parameter optimizer powered by Optuna. This approach enables Heretic to work completely automatically. Heretic finds high-quality abliteration parameters by co-minimizing the number of refusals and the KL divergence from the original model. This results in a decensored model that retains as much of the original model's intelligence as possible. Using Heretic does not require an understanding of transformer internals. In fact, anyone who knows how to run a command-line program can use Heretic to decensor language models. Running unsupervised with the default configuration, Heretic can produce decensored models that rival the quality of abliterations created manually by human experts: Model Refusals for "harmful" prompts KL divergence from original model for "harmless" prompts google/gemma-3-12b-it (original) 97/100 0 (by definition) mlabonne/gemma-3-12b-it-abliterated-v2 3/100 1.04 huihui-ai/gemma-3-12b-it-abliterated 3/100 0.45 p-e-w/gemma-3-12b-it-heretic (ours) 3/100 0.16 The Heretic version, generated without any human effort, achieves the same level of refusal suppression as other abliterations, but at a much lower KL divergence, indicating less damage to the original model's capabilities. (You can reproduce those numbers using Heretic's built-in evaluation functionality, e.g. heretic --model google/gemma-3-12b-it --evaluate-model p-e-w/gemma-3-12b-it-heretic. Note that the exact values might be platform- and hardware-dependent. The table above was compiled using PyTorch 2.8 on an RTX 5090.) Of course, mathematical metrics and automated benchmarks never tell the whole story, and are no substitute for human evaluation. Models generated with Heretic have been well-received by users (links and emphasis added): "I was skeptical before, but I just downloaded GPT-OSS 20B Heretic model and holy shit. It gives properly formatted long responses to sensitive topics, using the exact uncensored words that you would expect from an uncensored model, produces markdown format tables with details and whatnot. Looks like this is the best abliterated version of this model so far..." (Link to ) "Heretic GPT 20b seems to be the best uncensored model I have tried yet. It doesn't destroy a the model's intelligence and it is answering prompts normally would be rejected by the base model." (Link to ) "[Qwen3-4B-Instruct-2507-heretic] Has been the best unquantized abliterated model that I have been able to run on 16gb vram." (Link to ) Heretic supports most dense models, including many multimodal models, and several different MoE architectures. It does not yet support SSMs/hybrid models, models with inhomogeneous layers, and certain novel attention systems. You can find a small collection of models that have been decensored using Heretic on Hugging Face, and the community has created and published well over 1,000 Heretic models in addition to those. Usage Prepare a Python 3.10+ environment with PyTorch 2.2+ installed as appropriate for your hardware. Then run: pip install -U heretic-llm
heretic Qwen/Qwen3-4B-Instruct-2507 Replace Qwen/Qwen3-4B-Instruct-2507 with whatever model you want to decensor. The process is fully automatic and does not require configuration; however, Heretic has a variety of configuration parameters that can be changed for greater control. Run heretic --help to see available command-line options, or look at config.default.toml if you prefer to use a configuration file. At the start of a program run, Heretic benchmarks the system to determine the optimal batch size to make the most of the available hardware. On an RTX 3090, with the default configuration, decensoring Llama-3.1-8B-Instruct takes about 45 minutes. Note that Heretic supports model quantization with bitsandbytes, which can drastically reduce the amount of VRAM required to process models. Set the quantization option to bnb_4bit to enable quantization. After Heretic has finished decensoring a model, you are given the option to save the model, upload it to Hugging Face, chat with it to test how well it works, or any combination of those actions. Research features In addition to its primary function of removing model censorship, Heretic also provides features designed to support research into the semantics of model internals (interpretability). To use those features, you need to install Heretic with the optional research extra: pip install -U heretic-llm[research] This gives you access to the following functionality: Generate plots of residual vectors by passing --plot-residuals When run with this flag, Heretic will: Compute residual vectors (hidden states) for the first output token, for each transformer layer, for both "harmful" and "harmless" prompts. Perform a PaCMAP projection from residual space to 2D-space. Left-right align the projections of "harmful"/"harmless" residuals by their geometric medians to make projections for consecutive layers more similar. Additionally, PaCMAP is initialized with the previous layer's projections for each new layer, minimizing disruptive transitions. Scatter-plot the projections, generating a PNG image for each layer. Generate an animation showing how residuals transform between layers, as an animated GIF. See the configuration file for options that allow you to control various aspects of the generated plots. Note that PaCMAP is an expensive operation that is performed on the CPU. For larger models, it can take an hour or more to compute projections for all layers. Print details about residual geometry by passing --print-residual-geometry If you are interested in a quantitative analysis of how residual vectors for "harmful" and "harmless" prompts relate to each other, this flag gives you the following table, packed with metrics that can facilitate understanding the same (for gemma-3-270m-it in this case): ┏━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓
┃ Layer ┃ S(g,b) ┃ S(g*,b*) ┃ S(g,r) ┃ S(g*,r*) ┃ S(b,r) ┃ S(b*,r*) ┃ |g| ┃ |g*| ┃ |b| ┃ |b*| ┃ |r| ┃ |r*| ┃ Silh ┃
┡━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩
│ 1 │ 1.0000 │ 1.0000 │ -0.4311 │ -0.4906 │ -0.4254 │ -0.4847 │ 170.29 │ 170.49 │ 169.78 │ 169.85 │ 1.19 │ 1.31 │ 0.0480 │
│ 2 │ 1.0000 │ 1.0000 │ 0.4297 │ 0.4465 │ 0.4365 │ 0.4524 │ 768.55 │ 768.77 │ 771.32 │ 771.36 │ 6.39 │ 5.76 │ 0.0745 │
│ 3 │ 0.9999 │ 1.0000 │ -0.5699 │ -0.5577 │ -0.5614 │ -0.5498 │ 1020.98 │ 1021.13 │ 1013.80 │ 1014.71 │ 12.70 │ 11.60 │ 0.0920 │
│ 4 │ 0.9999 │ 1.0000 │ 0.6582 │ 0.6553 │ 0.6659 │ 0.6627 │ 1356.39 │ 1356.20 │ 1368.71 │ 1367.95 │ 18.62 │ 17.84 │ 0.0957 │
│ 5 │ 0.9987 │ 0.9990 │ -0.6880 │ -0.6761 │ -0.6497 │ -0.6418 │ 766.54 │ 762.25 │ 731.75 │ 732.42 │ 51.97 │ 45.24 │ 0.1018 │
│ 6 │ 0.9998 │ 0.9998 │ -0.1983 │ -0.2312 │ -0.1811 │ -0.2141 │ 2417.35 │ 2421.08 │ 2409.18 │ 2411.40 │ 43.06 │ 43.47 │ 0.0900 │
│ 7 │ 0.9998 │ 0.9997 │ -0.5258 │ -0.5746 │ -0.5072 │ -0.5560 │ 3444.92 │ 3474.99 │ 3400.01 │ 3421.63 │ 86.94 │ 94.38 │ 0.0492 │
│ 8 │ 0.9990 │ 0.9991 │ 0.8235 │ 0.8312 │ 0.8479 │ 0.8542 │ 4596.54 │ 4615.62 │ 4918.32 │ 4934.20 │ 384.87 │ 377.87 │ 0.2278 │
│ 9 │ 0.9992 │ 0.9992 │ 0.5335 │ 0.5441 │ 0.5678 │ 0.5780 │ 5322.30 │ 5316.96 │ 5468.65 │ 5466.98 │ 265.68 │ 267.28 │ 0.1318 │
│ 10 │ 0.9974 │ 0.9973 │ 0.8189 │ 0.8250 │ 0.8579 │ 0.8644 │ 5328.81 │ 5325.63 │ 5953.35 │ 5985.15 │ 743.95 │ 779.74 │ 0.2863 │
│ 11 │ 0.9977 │ 0.9978 │ 0.4262 │ 0.4045 │ 0.4862 │ 0.4645 │ 9644.02 │ 9674.06 │ 9983.47 │ 9990.28 │ 743.28 │ 726.99 │ 0.1576 │
│ 12 │ 0.9904 │ 0.9907 │ 0.4384 │ 0.4077 │ 0.5586 │ 0.5283 │ 10257.40 │ 10368.50 │ 11114.51 │ 11151.21 │ 1711.18 │ 1664.69 │ 0.1890 │
│ 13 │ 0.9867 │ 0.9874 │ 0.4007 │ 0.3680 │ 0.5444 │ 0.5103 │ 12305.12 │ 12423.75 │ 13440.31 │ 13432.47 │ 2386.43 │ 2282.47 │ 0.1293 │
│ 14 │ 0.9921 │ 0.9922 │ 0.3198 │ 0.2682 │ 0.4364 │ 0.3859 │ 16929.16 │ 17080.37 │ 17826.97 │ 17836.03 │ 2365.23 │ 2301.87 │ 0.1282 │
│ 15 │ 0.9846 │ 0.9850 │ 0.1198 │ 0.0963 │ 0.2913 │ 0.2663 │ 16858.58 │ 16949.44 │ 17496.00 │ 17502.88 │ 3077.08 │ 3029.60 │ 0.1611 │
│ 16 │ 0.9686 │ 0.9689 │ -0.0029 │ -0.0254 │ 0.2457 │ 0.2226 │ 18912.77 │ 19074.86 │ 19510.56 │ 19559.62 │ 4848.35 │ 4839.75 │ 0.1516 │
│ 17 │ 0.9782 │ 0.9784 │ -0.0174 │ -0.0381 │ 0.1908 │ 0.1694 │ 27098.09 │ 27273.00 │ 27601.12 │ 27653.12 │ 5738.19 │ 5724.21 │ 0.1641 │
│ 18 │ 0.9184 │ 0.9196 │ 0.1343 │ 0.1430 │ 0.5155 │ 0.5204 │ 190.16 │ 190.35 │ 219.91 │ 220.62 │ 87.82 │ 87.59 │ 0.1855 │
└───────┴────────┴──────────┴─────────┴──────────┴─────────┴──────────┴──────────┴──────────┴──────────┴──────────┴─────────┴─────────┴────────┘
g = mean of residual vectors for good prompts
g* = geometric median of residual vectors for good prompts
b = mean of residual vectors for bad prompts
b* = geometric median of residual vectors for bad prompts
r = refusal direction for means (i.e., b - g)
r* = refusal direction for geometric medians (i.e., b* - g*)
S(x,y) = cosine similarity of x and y
|x| = L2 norm of x
Silh = Mean silhouette coefficient of residuals for good/bad clusters How Heretic works Heretic implements a parametrized variant of directional ablation. For each supported transformer component (currently, attention out-projection and MLP down-projection), it identifies the associated matrices in each transformer layer, and orthogonalizes them with respect to the relevant "refusal direction", inhibiting the expression of that direction in the result of multiplications with that matrix. Refusal directions are computed for each layer as a difference-of-means between the first-token residuals for "harmful" and "harmless" example prompts. The ablation process is controlled by several optimizable parameters: direction_index: Either the index of a refusal direction, or the special value per layer, indicating that each layer should be ablated using the refusal direction associated with that layer. max_weight, max_weight_position, min_weight, and min_weight_distance: For each component, these parameters describe the shape and position of the ablation weight kernel over the layers. The following diagram illustrates this: Heretic's main innovations over existing abliteration systems are: The shape of the ablation weight kernel is highly flexible, which, combined with automatic parameter optimization, can improve the compliance/quality tradeoff. Non-constant ablation weights were previously explored by Maxime Labonne in gemma-3-12b-it-abliterated-v2. The refusal direction index is a float rather than an integer. For non-integral values, the two nearest refusal direction vectors are linearly interpolated. This unlocks a vast space of additional directions beyond the ones identified by the difference-of-means computation, and often enables the optimization process to find a better direction than that belonging to any individual layer. Ablation parameters are chosen separately for each component. I have found that MLP interventions tend to be more damaging to the model than attention interventions, so using different ablation weights can squeeze out some extra performance. Prior art I'm aware of the following publicly available implementations of abliteration techniques: AutoAbliteration abliterator.py wassname's Abliterator ErisForge Removing refusals with HF Transformers deccp Note that Heretic was written from scratch, and does not reuse code from any of those projects. Acknowledgments The development of Heretic was informed by: The original abliteration paper (Arditi et al. 2024) Maxime Labonne's article on abliteration, as well as some details from the model cards of his own abliterated models (see above) Jim Lai's articles describing "projected abliteration" and "norm-preserving biprojected abliteration" Citation If you use Heretic for your research, please cite it using the following BibTeX entry: @misc{heretic, author = {Weidmann, Philipp Emanuel}, title = {Heretic: Fully automatic censorship removal for language models}, year = {2025}, publisher = {GitHub}, journal = {GitHub repository}, howpublished = {\url{https://github.com/p-e-w/heretic}}
} License Copyright 2025-2026 Philipp Emanuel Weidmann (pew@worldwidemann.com) + contributors This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/. By contributing to this project, you agree to release your contributions under the same license.]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:38 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/p-e-w/heretic</guid>
    </item>
    <item>
      <title><![CDATA[My Journey Contributing to BLT]]></title>
      <link>https://dev.to/owaspblt/my-journey-contributing-to-blt-47k</link>
      <description><![CDATA[This is about how I started contributing to BLT, what caught my interest, what I've done so far, how it grew into me, and a few other things.
I was interested in contributing to open source for a long, long time. I did a few contributions in headlamp, mudlet, and meteor. I created an open source auth library as well, which I want to promote someday enough to attract contributors. Anyways, things happened and I landed at Algora, which I felt was very cool as a tool. The architecture really attracted me, and after scrolling for a while, in around September, I found a tool that had this architecture under construction as one of its side features. That really just caught me. I instantly put in an introduction and asked for whatever task I could get my hands on to get into the ecosystem.
I started out with a BLT-on-Cloudfare project at the time, bug-reporting-system made on Cloudfare. I made that within a week — it was around 11k lines, I remember. Though it's just sitting on my Github even still, since we deviated into a different direction on the project. Post that, it's just been too much fun and learning. I was a basic "full-stack dev" before contributing, but while contributing I've learned way too many security techniques, considerations, how infra works at scale, and specially how welcoming open source communities can be.
I started picking up as much work as I could from BLT for 2 months straight and tried to create some impact. I sat on the Slack what felt like 3/4th part of the day and hell, I was waiting for huddles as well.
It's very fun if I think about it — from being a part of a huddle where it was just me and Donnie, sitting and ending it within 5–7 minutes due to lack of breadth of things to catch up on — to now seeing the huddle go on for 1.5 hrs sometimes with a dozen members on average.
But yeah, BLT is basically what pulled me deeper into this hole of open source. Without any kind of barriers, you can interact directly with maintainers and show your work.
I started contributing a bit to one of Google's repositories as well, of which I liked the concept. But yeah, that's basically how I got into BLT and just been blessed to be a part ever since of such a good community with such a patient and welcoming mentor. I want to stick around enough to be a significant part of the community with a significant impact.
Ohh dear, reading this, it's kinda starting to sound like that emo AI slop to me, so I'll probably stop here.
Thanks for reading!]]></description>
      <pubDate>Fri, 20 Feb 2026 10:11:01 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/owaspblt/my-journey-contributing-to-blt-47k</guid>
    </item>
    <item>
      <title><![CDATA[pg_fsql - SQL templates as data in PostgreSQL (C extension)]]></title>
      <link>https://dev.to/yurc/pgfsql-sql-templates-as-data-in-postgresql-c-extension-34be</link>
      <description><![CDATA[I built a PostgreSQL extension that stores SQL queries as composable templates in a regular table. Templates form a dot-path hierarchy — children resolve first and inject into the parent.
Queries are data, not code. You can SELECT, UPDATE, version them. Swap a subtree and the whole query changes. Add a child — it auto-injects into the parent.
INSERT INTO fsql.templates (path, cmd, body) VALUES ('report', 'exec', 'SELECT jsonb_build_object(''data'', array_agg(row_to_json(t))) FROM (SELECT {d[cols]} FROM {d[src]} WHERE city = {d[city]!r}) t'), ('report.cols', NULL, 'id, name, email'), ('report.src', NULL, 'customers'); -- Execute the composed query
SELECT fsql.run('report', '{"city":"Moscow"}'); -- Dry-run: see generated SQL without executing
SELECT fsql.render('report', '{"city":"Moscow"}'); Three rows in a table — and you have a composable, parameterized report query.
Since everything is in a table, you get built-in tools:
fsql.tree('report') — show the template hierarchy
fsql.explain('report', data) — step-by-step expansion trace
fsql.validate() — find broken references, orphan fragments
fsql.depends_on('report') — recursive dependency list
The killer use case: one template handles UPDATE for any table with any subset of columns:
INSERT INTO fsql.templates (path, cmd, body) VALUES
('rest.put', 'exec', 'UPDATE {d[tbl]} SET ({d[columns]}) = ( SELECT {d[columns]} FROM ( SELECT (jsonb_populate_record(null::{d[tbl]}, {d[_self]!j} - ''id'')).* ) sub ) WHERE id = {d[id]} RETURNING jsonb_build_object(''id'', id)'); -- Child auto-detects which input keys match real table columns
INSERT INTO fsql.templates (path, cmd, body) VALUES
('rest.put.columns', 'exec', 'SELECT jsonb_build_object(''columns'', string_agg(c.column_name, '','')) FROM information_schema.columns c WHERE c.table_schema || ''.'' || c.table_name = {d[tbl]!r} AND {d[_self]!j} ? c.column_name'); -- Works for ANY table:
SELECT fsql.run('rest.put', '{"tbl":"public.orders","id":42,"price":19.99,"qty":5}'); No ORM, no code generation — just two rows in a table.
C renderer for fast {d[key]} placeholder substitution (!r quote_literal, !j jsonb literal, !i quote_identifier)
6 command types: exec, ref, if, exec_tpl, map, NULL (fragments)
SPI plan caching for hot-path templates
No superuser required, no plpython3u dependency
PostgreSQL 14+ (tested on 17.8)
GitHub: yurc/pg_fsql PGXN: pgxn.org/dist/pg_fsql Feedback welcome!]]></description>
      <pubDate>Fri, 20 Feb 2026 08:46:31 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/yurc/pgfsql-sql-templates-as-data-in-postgresql-c-extension-34be</guid>
    </item>
    <item>
      <title><![CDATA[Agenda du Libre pour la semaine 7 de l'année 2026]]></title>
      <link>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Calendrier Web, regroupant des événements liés au Libre (logiciel, salon, atelier, install party, conférence), annoncés par leurs organisateurs. Voici un récapitulatif de la semaine à venir. Le détail de chacun de ces 41 événements (France: 39, Internet: 2) est en seconde partie de dépêche. lien nᵒ 1 : April
lien nᵒ 2 : Agenda du Libre
lien nᵒ 3 : Carte des événements
lien nᵒ 4 : Proposer un événement
lien nᵒ 5 : Annuaire des organisations
lien nᵒ 6 : Agenda de la semaine précédente
lien nᵒ 7 : Agenda du Libre Québec Sommaire
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
[Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
[FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
[FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
[FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
[Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
[FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
[FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
[FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
[FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
[FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
[FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
[FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
[FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
[FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
[FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
[FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
[FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
[FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
[FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
[FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
[FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
[FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
[FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Wimille] Retrouvez votre liberté numérique – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Pollionnay] Install partie – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Auray] Install Party : adieu Windows, bonjour le Libre – Le samedi 14 février 2026 de 10h00 à 16h00.
[FR Ivry sur Seine] Cours de l’École du Logiciel Libre – Le samedi 14 février 2026 de 10h30 à 18h30.
[FR Illzach] Atelier Linux – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Illkirch-Graffenstaden] Atelier numérique éthique HOP par Alsace Réseau Neutre – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Fontenay-le-Fleury] Conférence : Présentation Git – Le samedi 14 février 2026 de 14h00 à 16h00.
[FR Ramonville St Agne] WordPress : Personnalisation – Le samedi 14 février 2026 de 14h00 à 18h00.
[FR Juvisy-sur-Orge] Permanence GNU/Linux – Le samedi 14 février 2026 de 14h30 à 17h00.
[FR Quimper] Permanence Linux Quimper – Le samedi 14 février 2026 de 16h00 à 18h00.
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
Tous les lundis de 10h à 17h sans interruption, l’association Prends toi en main / atelier abcpc, propose install party, suivi, dépannage, formation et revalorisation à petit prix sous Linux exclusivement.
L’atelier abcpc existe depuis plus de 10 ans et milite exclusivement pour les logiciels libres.
Médiathèque, Médiathèque, 4 place Dastros, Saint Clar, Occitanie, France
https://www.facebook.com/PrendsToiEnMain
linux, permanence, dépannage, formation, adieu-windows, libres, logiciels-libres, abcpc, prends-toi-en-main, install-party [Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
Vous voulez vous engager pour une cause, rencontrer de nouvelles personnes et découvrir la cartographie participative et humanitaire? CartONG vous invite à participer à un ou plusieurs mapathons en ligne! ​​
Venez cartographier les régions encore absentes des cartes pour soutenir les organisations humanitaires et de solidarité internationale qui ont besoin de cartes précises et à jour pour agir plus efficacement en cas de crise ou initier des projets de développement local.
Les ateliers de cartographie sont organisés dans le cadre du projet Missing Maps, qui a pour objectif de cartographier de façon préventive les régions vulnérables aux catastrophes naturelles, crises sanitaires, environnementales, aux conflits et à la pauvreté. On peut penser qu’aujourd’hui toutes les parties du monde sont cartographiées, mais en réalité de nombreuses régions ne possèdent encore aucune carte!
​ Pour qui? Pas besoin d’être un·e expert·e, les ateliers sont accessibles à tout le monde!
​ Où ? 100% en ligne! Un lien de connexion vous sera envoyé après votre inscription
​ ? Avec la plateforme de cartographie libre et contributive OpenStreetMap (OSM, le «Wikipédia des cartes») tout le monde peut participer à la cartographie de n’importe quelle zone de la planète: il suffit d’un ordinateur, d’une souris et d’une connexion internet! Accessibles à tout·es, nous serons là pour vous accompagner pour vos premiers pas avec OSM.
Le programme des mapathons
18h00: Introduction, présentation de la cartographie collaborative et solidaire et démonstration OSM pour les nouveaux·elles
18h30: On cartographie tous ensemble sur un projet
20h00: Fin du mapathon, conclusion sur les contributions de la soirée
Pour s’inscrire c’est par ici
Si vous avez besoin de plus d’info, vous pouvez nous contacter directement à l’adresse suivante: missingmaps@cartong.org
Internet
https://www.cartong.org
cartographie, cartong, osm, humanitaire, libre, mapathon [FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
L’Écurieux et Espéranto-Gironde vous invitent à la découverte de l’espéranto à Sainte Hélène le:
Lundi 9 février 2026 à 18h00
Foyer des sociétés
Allée du Stade
33480 Sainte-Hélène
Venez découvrir cette langue FRATERNELLE, libre, neutre, 15 fois plus facile à apprendre que le français, parlée par Freinet, Jean Jaurès, Louis Lumière, Jean-Paul II, Jules Verne…
Inventée en 1887, l’espéranto est actuellement parlé dans plus de 120 pays sur les 5 continents et est actuellement utilisé par des millions de personnes dans le monde, pour voyager, correspondre, découvrir d’autres cultures, se faire des amis…
Il y aura la projection d’un documentaire suivi de questions débat.
La rencontre est ouverte à tous, espérantistes ou non, membre de l’Écurieux ou non.
Entrée libre et gratuite.
Foyer des sociétés, Foyer des sociétés, allée du Stade, Sainte-Hélène, Nouvelle-Aquitaine, France
https://esperanto-gironde.fr/2026/01/decouverte-de-lesperanto-a-sainte-helene/
espéranto, langue-libre, langage, decouverte [FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
Tous les lundis soir de 19h à 22h (hors jours fériés) à la Bricoleuse.
Rencontrer les bénévoles, poser des questions sur le libre ou l’informatique, les logiciels, l’hébergement, passer de Windows à Linux.
Pour passer votre ordinateur sous linux, nous vous invitons à nous prévenir avant votre passage: contact@alolise.org.
La Bricoleuse, La Bricoleuse, 27 rue de la Ville, Saint-Étienne, Auvergne-Rhône-Alpes, France
https://alolise.org
install-party, aide, logiciel-libre, entraide, alolise, permanence, linux, gnu-linux [FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
Après un rappel sur le générateur de cartes personnalisées uMap, Binnette nous présentera:
Une démo de ses cartes uMap: différents besoins et cas d’usage.
La création de cartes uMap avec des données Overpass
Des scripts pythons de génération de carte uMap
Les limitations de uMap et les problèmes de performance
Informations pratiques
Lundi 9 février 19h – 21h
À la Turbine.coop, 5 Esplanade Andry Farcy, 38000 Grenoble (entrée sur le côté du bâtiment, nous serons dans la salle de réunion au rez-de-chaussée)
Atelier ouvert à tous et à toutes
Inscription souhaitée via ce formulaire La Turbine Coop, La Turbine Coop, 3-5 esplanade Andry Farcy, Grenoble, Auvergne-Rhône-Alpes, France https://wiki.openstreetmap.org/wiki/Grenoble_groupe_local/Agenda#Lundi_9_f%C3%A9vrier_:_atelier_uMap_avanc%C3%A9 openstreetmap, osm, osm-grenoble, umap, logiciels-libres, atelier, rencontre [FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
Vous pouvez venir pour:
découvrir ce que peut vous apporter le numérique libre, éthique et écoresponsable
obtenir de l’assistance pour l’utilisation des systèmes d’exploitation libres (GNU/Linux pour ordinateur et /e/OS pour smartphones)
obtenir de l’assistance pour l’utilisation des logiciels libres (ex: Firefox, Thunderbird, LibreOffice, VLC) et des services Internet éthiques (ex: mél et cloud, travail collaboratif en ligne).
vous faire aider à installer GNU/Linux sur votre ordinateur ou /e/OS sur votre Fairphone, si vous n’avez pas pu venir à notre Install Partie.
Nous vous recommandons d’effectuer une sauvegarde avant de venir, si vous n’êtes pas en mesure de faire, veuillez apporter un support de sauvegarde (disque dur externe ou clé USB de capacité suffisante).
Nos services sont gratuits, vous pourrez néanmoins faire un don à notre association « Libérons nos ordis ».
Remarque: vous pouvez même apporter un ordinateur de bureau – uniquement l’unité centrale (la tour) – nous avons des écrans, claviers et souris à brancher dessus.
VEUILLEZ VOUS INSCRIRE ICI: https://calc.ouvaton.coop/InscriptionPermanenceNumeriqueLibreRouen
La Base, La Base, 5 rue Geuffroy, Rouen, Normandie, France
libérons-nos-ordis, gnu-linux, logiciels-libres, assistance, linux, numérique [FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
Présentation de différents outils concernant les logiciels libres.
Assistance technique.
De préférence sur RDV directement sur le site de l’asso
Maison des associations, Maison des associations, 2 rue des Corroyeurs, Dijon, Bourgogne-Franche-Comté, France
https://desobs.fr
informatique-libre, installation, réemploi, réparation, résilience, résoudre, atelier [Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
L’émission Libre à vous! de l’April est diffusée chaque mardi de 15 h 30 à 17 h sur radio Cause Commune sur la bande FM en région parisienne (93.1) et sur le site web de la radio.
Le podcast de l’émission, les podcasts par sujets traités et les références citées sont disponibles dès que possible sur le site consacré à l’émission, quelques jours après l’émission en général.
Les ambitions de l’émission Libre à vous!
Découvrez les enjeux et l’actualité du logiciel libre, des musiques sous licences libres, et prenez le contrôle de vos libertés informatiques.
Donner à chacun et chacune, de manière simple et accessible, les clefs pour comprendre les enjeux mais aussi proposer des moyens d’action, tels sont les objectifs de cette émission hebdomadaire.
L’émission dispose:
d’un flux RSS compatible avec la baladodiffusion d’une lettre d’information à laquelle vous pouvez vous inscrire (pour recevoir les annonces des podcasts, des émissions à venir et toute autre actualité en lien avec l’émission)
d’un salon dédié sur le webchat de la radio Radio Cause Commune, Radio Cause Commune, Internet https://www.libreavous.org april, radio, cause-commune, libre-à-vous [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, partager leurs connaissances, échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup(https://www.meetup.com/fr-fr/labaixbidouille/) sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
La permanence d’ADeTI est un moment d’accueil avec des bénévoles pour apprendre à utiliser un ordinateur sous GNU/Linux (Ubuntu, Linux Mint, Debian…) mais aussi:
réparer les problèmes de logiciels sur son ordinateur
prendre des conseils pour choisir des logiciels alternatifs
différencier les logiciels libres utilisables pour répondre aux besoins
préserver et réfléchir sur ses usages (vie privée, éthique…)
Mais c’est aussi un moment consacré pour:
partager des connaissances et échanger des savoirs
maîtriser les formats ouverts et la pérennité de ses documents
Confidentialité, intégrité et disponibilité des systèmes d’information
Diversité des alternatives
Indépendance
Nous accueillons également des membres de l’association ALFA-Net et A-Hébergement qui peuvent répondre aux questions concernant Internet, les réseaux et l’hébergement: connexion à Internet, alternatives aux “Box” et aux opérateurs/FAI commerciaux, Neutralité du Net, Vie Privée, Blog, Site Internet/Web…
Centre Socioculturel Gentiana, Centre Socioculturel Gentiana, 90 avenue Maginot, Tours, Centre-Val de Loire, France
https://www.adeti.org
install-party, gull, linux, internet, réseau, adieu-windows, logiciels-libres, gnu/linux, adeti-org, hébergement, permanence [FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
Assistance technique et démonstration concernant les logiciels libres.
Il est préférable de réserver votre place à contact (at) linuxmaine (point) org
Planning des réservations consultableici.
Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, 31 allée Claude Debussy, Le Mans, Pays de la Loire, France
https://linuxmaine.org
linuxmaine, gnu-linux, demonstration, assistance, permanence, logiciels-libres, linux, adieu-windows [FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Centre socioculturel Port-Boyer, Centre socioculturel Port-Boyer, 4 rue de Pornichet, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
Tu as toujours rêvé de créer ton propre jeu vidéo ? Cet atelier est fait pour toi ! Viens apprendre à concevoir un jeu de A à Z: de l’idée de départ à la programmation, en passant par la création des personnages et des décors. Avec Scratch, rien de plus simple et amusant !
Mercredi 11 février: Attention Danger !
Mercredi 11 mars: Shark attack !
2 séances: 14 h et 16 h
Téléphone: 03 83 54 85 53
Médiathèque Jules Verne, Médiathèque Jules Verne, 2 rue de Malines, Vandœuvre-lès-Nancy, Grand Est, France
https://www.vandœuvre.fr/evenement/ateliers-cree-ton-jeu-video-avec-scratch/
mediatheque-jules-verne, atelier, logiciels-libres, scratch, jeu-video [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, de partager leurs connaissances, d’échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
Chaque mercredi soir, l’association propose une rencontre pour partager des connaissances, des savoir-faire, des questions autour de l’utilisation des logiciels libres, que ce soit à propos du système d’exploitation Linux, des applications libres ou des services en ligne libres.
C’est l’occasion aussi de mettre en avant l’action des associations fédératrices telles que l’April ou Framasoft, dont nous sommes adhérents et dont nous soutenons les initiatives avec grande reconnaissance.
Ecospace, 136 rue de la Mie au Roy, Beauvais, Hauts-de-France, France
https://www.oisux.org
oisux, logiciels-libres, atelier, rencontre, sensibilisation, adieu-windows [FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
Les contribateliers sont des ateliers conviviaux où chacun·e peut partager ses outils libres préférés et apprendre à y contribuer !
Hyperlien, Hyperlien, 5 allée Frida Kahlo, Nantes, Pays de la Loire, France
https://contribateliers.org/trouver-un-contribatelier/les-contribateliers-nantais
contribateliers-nantais, atelier, contribuer, libre [FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
Réunion ouverte à tous, adhérent ou pas.
Les réunions mensuelles Hadoly ont lieu tous les 2ᵉ mercredi du mois, à partir de 19h.
Soit en présentiel dans les locaux de la maison de l’écologie – 4 rue Bodin 69001 Lyon
Soit en distanciel sur l’adresse https://jitsi.hadoly.fr/permanence-hadoly.
À propos de cet événement
La permanence (mensuelle) d’Hadoly (Hébergeur Associatif Décentralisé et Ouvert à LYon), chaton lyonnais, est l’occasion d’échanger avec les membres de l’asso sur les services et moyens mis à disposition des adhérents afin de se libérer des Gafams tout en partageant ce que chacun·e aura amené pour grignoter ou boire.
Nous partageons du mail, du cloud, et d’autres services, le tout basé exclusivement sur une infrastructure locale et des logiciels libres. Nous respectons la neutralité du net et la vie privée. Plus largement nous échangeons autour des communs numériques, des cultures libres et de l’éducation populaire par exemple en réalisant ou animant des ateliers d’éducation aux médias.
Vous serez bienvenu pour présenter votre projet, celui de votre organisation, causer communs numériques, cultures libres et éduc pop.
Maison de l’écologie, Maison de l’écologie, 4 rue Bodin, Lyon, Auvergne-Rhône-Alpes, France
https://hadoly.fr
hadoly, chaton, permanence, réunion, discussion [FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
Appel à une rencontre autour d’un verre de bière des amis de Linux de Strasbourg et environs.
Les autres boissons sont explicitement tolérées…
Vous pouvez nous informer de votre envie de participer à l’évènement pour que l’on ne vous oublie pas. Pour cela, vous pouvez envoyer un message sur la liste de diffusion ou sur IRC.
Station de tram: Langstross Grand'Rue, ligne A ou D.
La Taverne Des Serruriers, La Taverne Des Serruriers, 25 rue des Serruriers, Strasbourg, Grand Est, France
https://strasbourg.linuxfr.org
aam, flammekueche-connection, lug-de-strasbourg, appel-à-mousser [FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
L’Association Club Linux Nord Pas-de-Calais organise chaque mois une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Les Mercredi Linux sont des réunions mensuelles désormais organisées le mercredi. Ces réunions sont l’occasion de se rencontrer, d’échanger des idées ou des conseils.
Régulièrement, des présentations thématiques sont réalisées lors de ces réunions, bien sûr, toujours autour des logiciels libres.
Durant cette permanence, vous pourrez trouver des réponses aux questions que vous vous posez au sujet du Logiciel Libre, ainsi que de l’aide pour résoudre vos problèmes d’installation, de configuration et d’utilisation de Logiciels Libres. N’hésitez pas à apporter votre ordinateur, afin que les autres participants puissent vous aider.
Cette permanence a lieu à la Médiathèque Cultiv'Art 6 rue de la Ladrerie, Cappelle en Pévèle
Médiathèque Cultiv'Art, Médiathèque Cultiv'Art, 16 rue de la Ladrerie, Cappelle en Pévèle, Hauts-de-France, France
http://clx.asso.fr
clx, permanence, linux, gnu-linux, logiciels-libres, adieu-windows [FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
Convocation à l’assemblée générale de l’association PauLLA Une Assemblée Générale est convoquée le jeudi 12 février 2026 à 18h. Pour y assister, 2 solutions:
- la version conviviale: venez nous rejoindre dans les locaux d’AGIRabcd (merci Jean-Louis !), 12 Avenue Federico Garcia Lorca à Pau. Très exactement ici: https://www.openstreetmap.org/node/8892972477
Big Blue Button de l’association (ici: https://bbb.paulla.asso.fr/b/ant-mqu-f3p-brn)
Tous les membres de PauLLA à jour de leur cotisation seront en mesure de voter.
L’ordre du jour est le suivant:
Bilan moral 2025
Bilan financier 2025
Renouvellement/Reconduction des membres du bureau
Paiement des cotisations 2026
Adhésion de PauLLA dans les autres assos/collectifs
APRIL
Landinux
autres Projets pour 2026 Accompagnement de 2 associations vers le libre Campagne « candidats.fr » pour les municipales 2026 Install-party à Haut de Gan en mars Install-party à la médiathèque de Lons fin avril Contacts avec le lycée Louis Barthou Le bouncer de CIaviCI, on en parle ? Bug gênant sur le site internet Toi ! Oui, toi, qui est en train de lire cette ligne, qu’as-tu à proposer pour 2026 ? Questions diverses L’assemblée générale sera aussi l’occasion de se sustenter autour d’un buffet improvisé en mode auberge espagnole avec ce que les membres apporteront ce soir-là. Boissons, petits plats sont donc les bienvenus. Essayez autant que possible de vous coordonner sur le canal #paulla sur IRC afin d’éviter que l’on se retrouve avec 12 packs de bière et rien d’autre.
Même chose pour d’éventuels covoiturages: coordonnons-nous sur l’IRC.
Local d’AGIRabcd, Local d’AGIRabcd, 12 avenue Federico Garcia Lorca, Pau, Nouvelle-Aquitaine, France
https://www.paulla.asso.fr/Evenements/assemblee-generale-paulla-2026
gull, paulla, logiciels-libres, projets, futur, assemblée-générale [FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
Le but des soirées de contribution au libre est de proposer un espace de travail partagé aux personnes actives dans le libre en Île-de-France le temps d’une soirée, une fois par mois (le deuxième jeudi du mois plus précisément).
Dit plus court: c’est un lieu avec de l’électricité et une connexion internet. En avant les claviers !
Les soirées de contribution au libre sont faites pour vous si:
vous travaillez sur un projet libre et vous recherchez une atmosphère à la fois conviviale et studieuse pour aller de l’avant et, qui sait, créer des connexions avec d’autres projets libres, vous êtes un collectif autour du libre et vous cherchez un lieu pour vous retrouver physiquement et avancer avec efficacité sur vos chantiers. Si vous n’avez pas envie de contribuer à un projet libre, les soirées de contribution au libre ne sont sans doute pas faites pour vous. Pas de panique, Parinux organise d’autres évènements:
si vous voulez discuter autour du libre: l’Apéro du Libre (APL) est là pour ça ; c’est un rendez-vous fixé tous les 15 du mois ; venez-nous retrouver autour d’un verre pour papoter et refaire le monde (libre), si vous avez un problème informatique: c’est la vocation de Premiers Samedi du Libre (PSL) où vous pourrez trouver des oreilles attentives et compétentes à l’écoute de toutes vos questions. Nous nous réservons le droit de refuser l’entrée aux soirées de contribution au libre à tout personne qui n’en respecterait pas l’esprit. Et, bien sûr, les règles de bienséance habituelles s’appliquent pour que chacune et chacun se sente à l’aise dans un cadre bienveillant.
Si les soirées de contribution vous intéressent, le mieux est de contacter d’abord le CA de Parinux ca@parinux.org. Vous devrez de toute façon nous écrire pour obtenir le code de la porte cochère…
FPH, FPH, 38 rue Saint-Sabin, Paris, Île-de-France, France
https://parinux.org/Soiree-de-Contribution-au-Libre-le-jeudi-12-fevrier-2026
parinux, scl, contribution, contribution-au-libre [FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
Médiathèque de Quimperlé, place Saint Michel, pas d’inscription, entrée libre !
Mickaël, Johann, Alain, et Yves vous accueillent (ou l’un d’eux, on se relaie !).
Conseils, aide et infos pratiques GNU/Linux et Logiciels Libres.
Curieux ? Déjà utilisateur ? Expert ? Pour résoudre vos problèmes, vous êtes le bienvenu ; pas besoin de prendre rendez-vous !
N’hésitez pas à venir avec votre PC si vous voulez une installation de GNU/Linux ou de venir avec votre périphérique récalcitrant (imprimante, scanner…) si possible.
Médiathèque de Quimperlé, place Saint Michel, Quimperlé, Bretagne, France
https://libreaquimperle.netlib.re
dépannage, entraide, gnu-linux, logiciels-libres, point-info, linux, libre-à-quimperlé, médiathèque-de-quimperlé [FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
Tous les vendredis après-midi, venez nous rencontrer lors de nos cafés-conseils et repairs-cafés!
Nous faisons découvrir les logiciels et systèmes libres (et gratuits !)
Plus de Télémétrie, de PC ralentis, une meilleure stabilité et sécurité,
Moins de virus et finie l’obsolescence programmée !
Salle Steredenn, Salle Steredenn, 9 rue du 19 Mars 1962, Lanmeur, Bretagne, France
https://ulamir-cpie.bzh
ulamir, cpie, repair-café, cyber-sécurité, windows10, libre, linux, adieu-windows, bonnes-pratiques, open-source, conseils-numeriques, ulamir-cpie [FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Maison de quartier des Haubans, Maison de quartier des Haubans, 1 bis boulevard de Berlin, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
Tous les 2ᵉmes et 4ᵉmes vendredis du mois (sauf indisponibilité des membres) de 14h30 à 16h30 l’association Ailes-52 vous propose de venir au Café de la Gare à Nogent (52800) pour échanger autour de la découverte des Logiciels Libres.
Vous pourrez:
Demander conseil pour l’acquisition d’un ordinateur reconditionné.
Gérer mes contacts sur mon ordiphone et mon PC.
Installer/configurer un logiciel libre sous Windows, Mac OS ou Linux. (Ex: VLC, Firefox, Thunderbird, LibreOffice, etc.).
Installer et configurer une imprimante/scanner.
Essayer une distribution Linux.
Répondez à cette question: Mon ordinateur ne pourra pas bénéficier de Windows 11, qu’est-ce que je peux faire pour continuer à l’utiliser, installer GNU/Linux sur mon ordi c’est possible?
Café de la Gare, Café de la Gare, 192 rue du Maréchal de Lattre de Tassigny, Nogent, Grand Est, France
https://ailes-52.org
linux, logiciels-libres, gnu-linux, découverte, café, apprentissage, permanence, bureautique, obsolescence, informatique-libre, ailes-52 [FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
Progressivement vous pourrez faire en sorte d’être moins sous l’influence de Google.
Dans cet atelier nous installerons des magasins d’applications libres pour ne plus avoir à utiliser le Google Play Store et s’assurer de pouvoir télécharger des applications libres (éthiques).
Nous installerons également l’application libre NewPipe pour accéder à Youtube sans s.
À noter: cet atelier n’est PAS faisable avec un iPhone / iPad
Inscription sur: https://calc.ouvaton.coop/InscriptionAtelierNumeriqueEthiqueRouen
MJC Grieu, MJC Grieu, 3 rue de Genève, Rouen, Normandie, France
dégooglisation, smartphone, tablette, application, logiciels-libres, libérons-nos-ordis [FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
Venez découvrir l’association Libre en Communs, ses membres et ses activités lors d’un moment de convivialité à La Générale, 39 rue Gassendi, 75014 Paris.
Habituellement le 2ᵉ vendredi de chaque mois – consultez l’Agenda Du Libre pour d’éventuelles mises à jour de dernière minute.
Métro les plus proches: Denfert-Rochereau (RER B, lignes 4 et 6), Mouton-Duvernet (ligne 4), Gaîté (ligne 13).
Vous pouvez apporter de la nourriture pour un repas partagé. Il y a une buvette sur place pour soutenir La Générale.
La Générale, La Générale, 39 rue Gassendi, Paris, Île-de-France, France
https://www.a-lec.org
libre-en-communs, alec, rencontre, apéro, échange-de-savoirs, la-générale [FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
L'OMJC organise avec l’Association Club Linux Nord Pas-de-Calais organise chaque samedi une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Le Centre d’Infos Jeunes a mis en place une démarche d’accompagnement des jeunes aux pratiques actuelles pour l’informatique et le numérique:
Lieu d’accès public à Internet (5 postes avec Wifi libre et gratuit)
Web collaboratif et citoyen pour que chacun puisse trouver sa place et passer du rôle de simple usager à celui d’initiateur de processus collaboratif
Éducation à l’information par les nouveaux médias (diffusion par le biais du numérique)
Logiciels libres (bureautique, sites, blogs, cloud, infographie et vidéo, musique, réseaux sociaux, chat…).
Cette rencontre a lieu sur rendez-vous, tous les samedis matin hors vacances scolaires à la Maison communale de la ferme Dupire, rue Yves Decugis à VILLENEUVE D’ASCQ
OMJC, rue Yves Decugis, Villeneuve d’Ascq, Hauts-de-France, France
https://clx.asso.fr
omjc, clx, permanence, linux, gnu-linux, logiciels-libres, atelier [FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
Rencontre mensuelle autour des logiciels libres, en toute simplicité.
Ces matinées seront ce que nous en ferons ensemble, selon vos attentes:
Découverte des logiciels libres dont Linux et de leur intérêt. Utilisation sur place.
Installations, sur votre machine (pensez à sauvegarder vos données avant de venir avec) ou sur des PC fournis pour apprendre ensemble sans risque. Parfois, on vous propose un ordinateur auquel Linux a redonné une seconde vie, avec lequel vous pouvez repartir…
Préparation d’une clé USB pour tester Linux chez vous, l’installer ou alors pour utiliser des logiciels libres sans installation sous Windows.
Entraide, suivi de votre expérience avec les logiciels libres.
Nous pourrons aussi nous intéresser aux outils en ligne, aux smartphones, ou nous amuser à redonner vie à de vieux PC un peu obsolètes, à reconditionner des ordinateurs pour des associations ou personnes avec peu de ressources, etc.
Pour tout projet qui risque de prendre un peu de temps, il est préférable de nous contacter avant.
Les débutant·e·s sont les bienvenu·e·s! Les autres aussi, bien évidemment !
Maison pour tous, 35 route d’Arenthon, Amancy, Auvergne-Rhône-Alpes, France
https://librealabase.gitlab.io
libre, logiciel-libre, linux, /e/os, gnu-linux [FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
Apportez votre ordinateur
pour y installer des logiciels libres et gratuits
Tous les 2ᵉ samedis 9h-13h de janvier à juin 2026
PROCHAIN: Samedi 14 février 2026 de 9h à 13h
Atelier public &amp; gratuit destiné: aux curieux, aux avertis, à ceux qui veulent faire des économies.
► Remplacer Microsoft Word par LibreOffice Write, Photoshop par Gimp, Outlook par Thunderbird, Google par DuckDuckGo, Gmail par déMAILnagement
SUR INSCRIPTIONS: au 01.43.04.83.53
+ de renseignements par email à franck@sinimale.fr
#adieu-windows
Maison pour tous des Coteaux, Maison pour tous des Coteaux, 30 route de Gournay, Noisy-le-Grand, Île-de-France, France
adieu-windows, install-party, entraide, logiciels-libres, linux, gnu-linux [FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.]]></description>
      <pubDate>Sat, 07 Feb 2026 21:16:41 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[HailToDodongo/pyrite64]]></title>
      <link>https://github.com/HailToDodongo/pyrite64</link>
      <description><![CDATA[N64 Game-Engine and Editor using libdragon &amp; tiny3d Pyrite64 N64 game-engine and editor using Libdragon and tiny3d. Note: This project does NOT use any proprietary N64 SDKs or libraries. Pyrite64 is a visual editor + runtime-engine to create 3D games that can run on a real N64 console or accurate emulators. Besides the usual editor, some extra features include: Automatic toolchain installation on Windows 3D-Model import (GLTF) from blender with fast64 material support. Support for HDR+Bloom rendering (shown here: www.youtube.com/watch?v=XP8g2ngHftY) Support for big-texture rendering (256x256) (shown here: www.youtube.com/watch?v=rNEo0aQkGnU) Runtime engine handling scene-management, rendering, collision, audio and more. Global asset management with automatic memory cleanup Node-Graph editor to script basic control flow Note that this project focuses on real hardware, so accurate emulation is required to run/test games on PC. Emulators that are accurate enough include Ares (v147 or newer) and gopher64. [!WARNING] This project is still in early development, so features are going to be missing. Documentation is also still a work in progress, and breaking API changes are to be expected. Documentation Before starting, please read the FAQ! Installation &amp; Docs: Pyrite64 Installation Using the Editor Using the CLI Development on the editor itself: Building the Editor Showcase Cathode Quest 64 (YouTube) | Pyrite64 Release Video Links For anything N64 homebrew related, checkout the N64Brew discord: https://discord.gg/WqFgNWf Credits &amp; License 2025-2026 - Max Bebök (HailToDodongo) Pyrite64 is licensed under the MIT License, see the LICENSE file for more information. Licenses for external libraries used in the editor can be found in their respective directory under /vendored Pyrite64 does NOT force any restrictions or licenses on games made with it. Pyrite64 does NOT claim any copyright or force licenses for assets / source-code generated by the editor. While not required, please consider crediting Pyrite64 with a logo and/or name in your credits and/or boot logo sequence.]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:38 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/HailToDodongo/pyrite64</guid>
    </item>
    <item>
      <title><![CDATA[How we stopped giving our AI agents raw API keys]]></title>
      <link>https://dev.to/rsdouglas/how-we-stopped-giving-our-ai-agents-raw-api-keys-3o3p</link>
      <description><![CDATA[Autonomous agents need API access to do useful work. Our creature Secure files security issues on GitHub. The voyager genome commits code. Future creatures will need Stripe, analytics, whatever.
The naive solution is to inject API keys as environment variables. Every container runtime supports it, every SDK can read from process.env, and it works on day one. It also means every creature has every key, there's no audit trail, and a prompt injection can exfiltrate credentials in a single tool call.
We needed something better.
Janee is an MCP server that sits between agents and APIs. You store your credentials in Janee (encrypted at rest with AES-256-GCM), define capabilities with access policies, and agents call APIs by capability name. They never see raw keys.
┌──────────┐ MCP/HTTP ┌────────┐ real creds ┌──────────┐
│ Creature │ ──────────────&gt; │ Janee │ ──────────────&gt; │ External │
│ │ │ │ proxied req │ API │
└──────────┘ └────────┘ └──────────┘ no keys encrypted at rest GitHub, etc. A creature that needs to create a GitHub issue calls:
await janee({ action: 'execute', capability: 'secure-seed', method: 'POST', path: '/repos/openseed-dev/openseed/issues', body: JSON.stringify({ title: 'Security finding', body: '...' })
}); Janee looks up the secure-seed capability, decrypts the GitHub App private key, mints a short-lived installation token, injects it into the request, and proxies to GitHub. The creature never touches the key. Janee logs the request. If something goes wrong, you revoke access in one place.
The tricky part with multiple agents is identity. Which creature is making the request? Early prototypes used custom HTTP headers (X-Agent-ID), but any client can set any header.
We landed on something simpler: the MCP protocol already has an initialize handshake where clients send clientInfo.name. Each creature sets this to creature:{name} when it opens a session. Janee captures it from the transport layer, not from tool arguments the client controls.
const transport = new StreamableHTTPClientTransport(url);
await client.connect(transport);
// clientInfo.name = "creature:secure" sent during initialize Identity resolution uses the same mechanism regardless of transport: stdio, HTTP, in-memory. No extra headers, no extra arguments. Just MCP.
With identity sorted, access control is straightforward. In ~/.janee/config.yaml:
server: defaultAccess: restricted capabilities: secure-seed: service: secure-seed allowedAgents: ["creature:secure"] autoApprove: true defaultAccess: restricted means capabilities without an explicit allowedAgents list are hidden from all agents. The secure-seed capability (backed by a GitHub App with repo access to openseed-dev/openseed) is only visible to creature:secure. Other creatures calling list_services won't even know it exists.
If a creature creates a credential at runtime (via the manage_credential tool), it defaults to agent-only. Only the creating creature can use it. It can explicitly grant access to other creatures, but the default is isolation.
OpenSeed runs multiple creatures concurrently. The orchestrator spawns Janee once as a child process in HTTP mode. Each creature gets its own MCP session. Janee creates a fresh Server and Transport instance per initialize handshake, following the official MCP SDK pattern.
Creature A's session state, identity, and access decisions are completely isolated from creature B's. No shared state, no last-writer-wins, no cross-talk.
Our creature Secure runs the dreamer genome. Its job is to audit OpenSeed for security issues. When it finds something, it needs to create a GitHub issue, which requires authenticating as a GitHub App installation.
The flow:
We created a GitHub App (secure-seed) with repo access to openseed-dev/openseed The app's credentials (App ID, private key, installation ID) are stored in Janee
~/.janee/config.yaml maps a secure-seed capability to this app, restricted to creature:secure Secure's genome includes a janee tool that handles MCP session management
When Secure finds an issue, it calls execute with the secure-seed capability
Janee mints a short-lived GitHub installation token (1hr TTL) and proxies the request
Secure never sees the private key. It can't mint tokens for repos it shouldn't access. If we need to rotate the key, we update Janee. No creature code changes.
This is the foundation. The obvious next steps:
Web UI for secret management: manage Janee credentials from the OpenSeed dashboard instead of editing YAML
GitHub App creation from the UI: the create-gh-app package already handles the manifest flow; wiring it into the UI would make onboarding new GitHub integrations trivial
Hardened identity: today clientInfo.name is self-asserted. The MCP spec doesn't yet define authenticated identity, but when it does, Janee's identity priority chain is designed to slot in verified identity at the top
If you're building autonomous agents that need API access, consider putting a proxy in front of your keys. Your agents don't need them. They just need the responses.
Janee on GitHub · Janee on npm · OpenSeed]]></description>
      <pubDate>Fri, 20 Feb 2026 12:00:42 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/rsdouglas/how-we-stopped-giving-our-ai-agents-raw-api-keys-3o3p</guid>
    </item>
    <item>
      <title><![CDATA[I Built a Free Image Converter That Never Uploads Your Files]]></title>
      <link>https://dev.to/mackmoneymaker/i-built-a-free-image-converter-that-never-uploads-your-files-35n3</link>
      <description><![CDATA[I Built a Free Image Converter That Never Uploads Your Files Ever used an online image converter and wondered where your files actually go?
Most "free" converters quietly upload your images to their servers. Your screenshots, photos, design mockups — sitting on someone else's infrastructure. Maybe they delete them. Maybe they don't. You'll never really know.
That bugged me, so I built PixConvert — an image converter that runs 100% in your browser. Your files never leave your machine. There's no backend. No upload. No server. Just the Canvas API doing its thing.
Format conversion: PNG, JPG, WEBP, GIF, BMP, ICO
Batch support: Drop multiple files at once, convert them all in one click
Quality slider: Fine-tune compression for JPG and WEBP output
Resize: Set custom dimensions before converting
Instant download: Files are processed and ready immediately
The entire tool is a single HTML file. No frameworks, no dependencies, no build step.
Here's the core idea:
You drop an image → it's read with FileReader as a data URL
The image is drawn onto an off-screen element
canvas.toBlob() exports it in your chosen format
You download the blob
That's it. The Canvas API handles all the heavy lifting — format encoding, quality control, resizing. The browser is genuinely good at this stuff, and there's no reason to involve a server.
// The essence of PixConvert in ~10 lines
const img = new Image();
img.onload = () =&gt; { const canvas = document.createElement('canvas'); canvas.width = img.width; canvas.height = img.height; canvas.getContext('2d').drawImage(img, 0, 0); canvas.toBlob(blob =&gt; { // Download the converted file const url = URL.createObjectURL(blob); const a = document.createElement('a'); a.href = url; a.download = `converted.${format}`; a.click(); }, `image/${format}`, quality);
};
img.src = fileDataUrl; It's not just about privacy (though that's reason enough). Running locally means:
Speed — no upload/download wait time, especially on slow connections
No file size limits — your browser can handle large images just fine
Works offline — once loaded, it doesn't need the internet
No account required — no signup, no email, no cookies Live tool: mack-moneymaker.github.io/pixconvert
GitHub: github.com/mack-moneymaker/pixconvert
It's fully open source. If you find it useful, a star on GitHub would mean a lot. Got feedback or feature ideas? Open an issue — I'd love to hear what you'd add.
PixConvert is part of a growing collection of free, open-source tools — all client-side, no signup:
SigCraft — Email signature generator with templates, photo upload, and social icons
Faviconify — Generate favicons from text, emoji, or images (PNG + ICO)
GradientLab — CSS gradient generator with live preview and PNG export
ColorCraft — Color palette generator with harmony modes and accessibility checker
RegexLab — Clean regex tester with match highlighting and cheat sheet
CronMaker — Visual cron expression builder and decoder
JSONPretty — JSON formatter, validator, and diff tool
HookDebug — Webhook testing and inspection tool
JustTheRecipe — Paste a recipe URL, get just the ingredients and instructions
LegalPage — GDPR-compliant Privacy Policy and Terms generator
All free. All open source. All on GitHub.]]></description>
      <pubDate>Fri, 20 Feb 2026 12:09:46 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/mackmoneymaker/i-built-a-free-image-converter-that-never-uploads-your-files-35n3</guid>
    </item>
    <item>
      <title><![CDATA[Revue de presse de l’April pour la semaine 7 de l’année 2026]]></title>
      <link>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Cette revue de presse sur Internet fait partie du travail de veille mené par l’April dans le cadre de son action de défense et de promotion du logiciel libre. Les positions exposées dans les articles sont celles de leurs auteurs et ne rejoignent pas forcément celles de l’April.
[Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€)
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre?
[ZDNET] LibreOffice dénonce le format OOXML
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte lien nᵒ 1 : April
lien nᵒ 2 : Revue de presse de l'April
lien nᵒ 3 : Revue de presse de la semaine précédente
lien nᵒ 4 : Fils du Net [Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux Tiago Gil, le jeudi 12 février 2026.
La centrale d’achat informatique hospitalière (CAIH) engage une nouvelle feuille de route sur cinq ans et initie le programme Alternative, destiné à bâtir un socle numérique souverain pour les systèmes d’information de santé.
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€) Valéry Rieß-Marchive, le mercredi 11 février 2026.
L’Agence nationale de la sécurité des systèmes d’information vient de réitérer son engagement en faveur du logiciel libre. Dans la continuité d’une politique établie et confortée de longue date.
Et aussi: [Le Monde Informatique] L'Anssi formalise sa doctrine open source
[Silicon] L’ANSSI affirme l’open source comme levier de sa politique industrielle
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre? Bertrand Lemaire, le mercredi 11 février 2026.
L’APRIL relance son initiative «Pacte du Logiciel Libre» à l’occasion du prochain scrutin municipal.
Et aussi: [Goodtech] Municipales 2026 en France: l'April lance son pacte du logiciel libre
Voir aussi: L’April propose le pacte du logiciel libre à l’occasion des élections municipales et communautaires de 2026
[ZDNET] LibreOffice dénonce le format OOXML
Le mercredi 11 février 2026.
The Document Foundation (TDF) intensifie sa critique contre Microsoft, accusant le géant américain de privilégier ses intérêts commerciaux au détriment de l’interopérabilité.
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte Aymeric Geoffre-Rouland, le lundi 9 février 2026.
Quand un développeur demande à Claude ou ChatGPT d’écrire du code, l’IA pioche dans des milliers de bibliothèques libres sans que l’humain ne lise jamais leur documentation. Résultat: les mainteneurs de ces projets open-source, qui vivent de la visibilité générée par les visites et les interactions, voient leur audience s’effondrer. Une étude économique chiffre ce paradoxe: l’IA qui accélère le développement logiciel asphyxie l’écosystème qui le rend possible.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:40 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[Nouveautés de février 2026 de la communauté Scenari]]></title>
      <link>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</link>
      <description><![CDATA[Scenari est un ensemble de logiciels open source dédiés à la production collaborative, publication et diffusion de documents multi-support. Vous rédigez une seule fois votre contenu et vous pouvez les générer sous plusieurs formes : site web, PDF, OpenDocument, diaporama, paquet SCORM (Sharable Content Object Reference Model)… Vous ne vous concentrez que sur le contenu et l’outil se charge de créer un rendu professionnel accessible et responsive (qui s’adapte à la taille de l’écran).
À chaque métier/contexte son modèle Scenari :
Opale pour la formation Dokiel pour la documentation Optim pour les présentations génériques Topaze pour les études de cas Parcours pour créer des scénarios de formation et bien d’autres… lien nᵒ 1 : Explication de Scenari
lien nᵒ 2 : Pour démarrer
lien nᵒ 3 : Téléchargements
lien nᵒ 4 : Communauté Scenari
lien nᵒ 5 : Mastodon
lien nᵒ 6 : Bluesky
lien nᵒ 7 : Telegram
lien nᵒ 8 : LinkedIn
lien nᵒ 9 : Canal Peertube Sommaire Visio de découverte de Scenari Parole de Scenariste Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Tu peux parler de Scenari aux conférences éclair de l’April ? Nouvel habillage web pour Optim 24 Mise-à-jour de Myscenari Nouvelles versions d’outils Scenari Le savais-tu ? Le chiffre du mois Nouvelles adhésions d’organisations Visio de découverte de Scenari Tu as des questions sur Scenari avant de tester ?
Cette visio est faite pour toi : jeudi 26 février à 16h sur https://scenari.org/visio/miniwebinaire
Lien Agenda du Libre
Lien Mobilizon Parole de Scenariste
Utilisateur de Canoprof depuis 2019, cet outil est devenu un des piliers de ma pratique d’enseignement en Physique-Chimie (4ᵉ, 5ᵉ, 3ᵉ) et en Sciences (6ᵉ). Je l’utilise pour concevoir l’ensemble de mes supports aussi bien papier que numériques, ce qui me permet de maintenir une cohérence didactique forte sur l’ensemble du cursus collège.
La force de Canoprof réside dans la séparation claire entre le contenu et la forme. En tant qu’enseignant, cela me permet de me concentrer sur le fond pédagogique et la structuration de mes séquences, sans perdre de temps dans les contraintes techniques de mise en page. La richesse de mon fond documentaire, construit depuis plus de six ans, évolue ainsi sereinement au fil des réformes et de mes retours d’expérience.
Canoprof m’aide à formaliser une progression spiralaire efficace tout en générant des supports propres, structurés et accessibles. C’est un gain de productivité précieux qui me permet de consacrer plus d’énergie à l’accompagnement de mes élèves en classe. Guillaume Marmin, enseignant de physique-chimie au Collège Isabelle Autissier. Modèle utilisé : Canoprof Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Les Rencontres Scenari 2026 auront lieu du lundi 22 juin (midi) au vendredi 26 juin (midi) sous le soleil provençal à l'ENSAM Aix-en-Provence.
Bloque ces dates dès maintenant, les détails seront précisés bientôt. Tu peux parler de Scenari aux conférences éclair de l’April ? Lors de la prochaine assemblée générale de l’April (samedi 28 mars 2026 à Paris) il y aura un temps de conférences éclairs (6 minutes) de 10h à 12h qui s’enchaîneront sur des sujets variés, en lien avec le Libre, entendu au sens large.
Si tu utilises Scenari, c’est une bonne opportunité pour parler de tes usages auprès des adhérent⋅e⋅s de l’April. Date limite pour proposer : 15 mars. Envoyer un courriel à confseclairs@april.org.
Il n’est pas nécessaire d’être adhérent⋅e à l’April pour pouvoir proposer une conférence éclair.
Plus de détails sur l’annonce de l’April. Nouvel habillage web pour Optim 24 Un nouvel habillage graphique pour Optim 24 fait son apparition sur la plateforme de téléchargement.
Il existe pour tous les supports web des 3 modalités d’Optim : site normal, site web simple, site web en tuiles. Mise-à-jour de Myscenari MyScenari vient de passer en version 6.4.5 (corrections de bugs dans le cœur et dans les modèles en version 25). Attention : cette version est la dernière à contenir Dokiel 5 et 6, Opale 5 et 24, Optim 3 À partir de la prochaine mise à jour de MyScenari, nous n’aurons plus que Dokiel 25, Opale 25, Optim 24. Pense à migrer tes modèles (et skins) pour ne pas être pris⋅e au dépourvu au dernier moment. Nouvelles versions d’outils Scenari Opale, le modèle phare pour créer vos contenus pédagogiques, passe en version 25.1.1. Au menu, entre autres : corrections dans les outils d’accessibilité, et amélioration de l’intégration de MindMap dans la publication Diapo. Et Opale est maintenant disponible en allemand ! Parcours, pour concevoir des conducteurs pédagogiques, passe en version 25.0.2 (corrections mineures sur le skin, l’éditeur et les vidéos HLS) et est disponible maintenant en français et Anglais. Dokiel, le modèle pour la documentation technique et logicielle, passe en version 25.0.6. Cette version apporte entre autres des corrections dans la publication de relecture et l’écran de contrôle, et l’amélioration des écrans décrits dans les publications Web (maintenant responsive). Optim monte en version dans ses deux saveurs Optim 24.0.7 et OptimPlus 24.0.3 avec des corrections mineures sur les publications Web et Diaporama, et dans le styage. LTI-suite, le serveur pour exploiter des ressources SCORM dans des LMS via LTI, passe en version 2.0.3. Lexico, votre modèle pour créer des lexiques, glossaires, thesaurus, vocabulaires, monte en version 25.0.1 pour apporter des corrections mineures dans la publication Web. SCENARIchain-desktop est à présent disponible en français, en anglais et en espagnol. Le savais-tu ?
En contexte d’ateliers complexes (plusieurs calques de dérivation et/ou de travail), les détails dans le bandeau de l’item listent les variantes de cet item dans les autres ateliers calques ou de travail, s’il en existe.
Dans l’exemple ci-dessous, l’item _Module-LeThe.xml dans l’atelier maître (icone d’atelier bleu) est modifié dans un atelier de travail (icone d’atelier vert) et modifié aussi dans un atelier dérivé (icone d’atelier marron). On peut passer facilement d’une version à l’autre en un seul clic. La popup est détachable pour plus d’aisance si besoin.
Exemple Le chiffre du mois 20, c’est le nombre d’années qui se sont écoulées depuis la première sortie d’Opale le 18/09/2006 (les développements avaient commencé en novembre 2005). Nouvelles adhésions d’organisations
Souhaitons la bienvenue à :
Institution Azahrae qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
L’Université Bourgogne Europe qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
URBILOG qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 15:59:55 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</guid>
    </item>
    <item>
      <title><![CDATA[The world of open source metadata]]></title>
      <link>https://changelog.com/podcast/665</link>
      <description><![CDATA[Andrew Nesbitt builds tools and open datasets to support, sustain, and secure critical digital infrastructure. He's been exploring the world of open source metadata for over a decade. First with libraries.io and now with ecosyste.ms, which tracks over 12 million packages, 287 million repos, 24.5 billion dependencies, and 1.9 million maintainers. What has Andrew learned from all this, who is using this open dataset, and how does he hope others can build on top of it all? Tune in to find out.]]></description>
      <pubDate>Wed, 05 Nov 2025 20:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/665</guid>
    </item>
    <item>
      <title><![CDATA[How We Added an Email Gate to Shared Videos]]></title>
      <link>https://dev.to/alexneamtu/how-we-added-an-email-gate-to-shared-videos-30e1</link>
      <description><![CDATA[You share a product demo with a prospect. The analytics show 5 views. But who watched? Did the decision-maker see it, or just the person you sent the link to? View counts can't answer that.
We added an email gate to SendRec. When enabled on a video, viewers enter their email before they can watch. The email shows up in the per-video analytics page alongside view count and completion percentage. You know exactly who watched your video and how much of it they saw.
A video in SendRec can have two gates: a password gate (existing feature) and an email gate (new). They're evaluated in order — password first, then email. If a video has both, the viewer enters the password, then sees the email form.
The email gate is a minimal form matching the password gate's dark theme — just a title, a subtitle ("Enter your email to watch this video"), an email input, and a "Watch Video" button. No name field, no company field. One input, minimal friction.
When the viewer submits their email, a POST request goes to the identify endpoint:
POST /api/watch/{shareToken}/identify
{"email": "alice@example.com"} The server validates the email, records it in the database, sets a signed cookie, and returns 200. The page reloads, the cookie is present, and the video plays.
The email gate uses the same pattern as the password gate — an HMAC-signed cookie that proves the viewer has already identified themselves.
The cookie name is eg_ followed by the first 8 characters of the share token. The value is the email address and an HMAC signature, separated by a pipe:
eg_a048914d = alice@example.com|3f8a7b2e... When the watch page loads, it checks for this cookie and verifies the signature:
func verifyEmailGateCookie(hmacSecret, shareToken, cookieValue string) (string, bool) { parts := strings.SplitN(cookieValue, "|", 2) if len(parts) != 2 { return "", false } email := parts[0] expected := signEmailGateCookie(hmacSecret, shareToken, email) if !hmac.Equal([]byte(expected), []byte(cookieValue)) { return "", false } return email, true
} The hmac.Equal comparison is timing-safe — it prevents an attacker from guessing the signature byte by byte by measuring response times. The email is extracted from the cookie value itself, not from a separate source, so the cookie is self-contained.
The cookie lasts 7 days. A viewer who enters their email once can return to the same video without being asked again.
SendRec's analytics use an anonymous viewer hash — a SHA-256 of the IP address and User-Agent string. This is what deduplicates views, milestone tracking, and CTA clicks. But it's anonymous. You see "42 unique viewers" without knowing who they are.
The email gate bridges this gap with a video_viewers table:
CREATE TABLE video_viewers ( id UUID PRIMARY KEY DEFAULT gen_random_uuid(), video_id UUID NOT NULL REFERENCES videos(id) ON DELETE CASCADE, email TEXT NOT NULL, viewer_hash TEXT NOT NULL, created_at TIMESTAMPTZ NOT NULL DEFAULT now(), UNIQUE(video_id, email)
); When a viewer submits their email, the server computes their viewer hash from the request's IP and User-Agent, then inserts it alongside the email:
ip := clientIP(r)
hash := viewerHash(ip, r.UserAgent()) go func() { ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() h.db.Exec(ctx, `INSERT INTO video_viewers (video_id, email, viewer_hash) VALUES ($1, $2, $3) ON CONFLICT (video_id, email) DO NOTHING`, videoID, req.Email, hash, )
}() The ON CONFLICT DO NOTHING means if the same person enters the same email twice (maybe their cookie expired), the original row is preserved. The viewer hash might differ if their IP changed, but the email remains the stable identifier.
This viewer hash is the same one used in the video_views and view_milestones tables. That means we can join across tables to answer "how many times did alice@example.com watch this video?" and "what percentage did she reach?"
The analytics endpoint joins the three tables when the email gate is enabled:
SELECT vw.email, vw.created_at, COUNT(DISTINCT vv.id) as view_count, COALESCE(MAX(vm.milestone), 0) as max_milestone
FROM video_viewers vw
LEFT JOIN video_views vv ON vv.video_id = vw.video_id AND vv.viewer_hash = vw.viewer_hash
LEFT JOIN view_milestones vm ON vm.video_id = vw.video_id AND vm.viewer_hash = vw.viewer_hash
WHERE vw.video_id = $1
GROUP BY vw.email, vw.created_at
ORDER BY vw.created_at DESC The result is a viewer list in the analytics page — a table showing each email, when they first watched, how many times they returned, and the furthest point they reached in the video.
This turns anonymous analytics into actionable data. Instead of "23 unique views, 60% reached 50%," you see "alice@example.com watched 3 times and finished the whole thing; bob@example.com opened it once and dropped off at 25%."
The email gate works identically on the embeddable player at /embed/:token. The form matches the embed's minimal style, and the cookie is shared across the same domain — entering an email on the watch page also unlocks the embed, and vice versa.
One detail that bit us: cookie SameSite policy. We initially set the cookies to SameSite=Strict, which is the most restrictive option. But when an embed is loaded inside an iframe on a different site, the browser considers it a cross-origin context. Strict cookies aren't sent in cross-origin requests, so the cookie set after email submission was silently dropped on page reload.
The fix was switching both the password gate and email gate cookies to SameSite=None (with Secure required by browsers for None). This allows the cookie to work inside third-party iframes — the whole point of having an embeddable player.
Email verification. We could send a magic link to confirm the address is real. But that adds significant friction — the viewer has to leave the page, check their inbox, click a link, and come back. For a shared video demo, that's too much. The honesty-based approach trades verification accuracy for conversion.
Name collection. Adding a name field to the form would give richer viewer data. But every extra field reduces completion rates. Email alone is enough to identify who watched.
User-level defaults. The email gate is per-video only. There's no setting to enable it for all videos at once. Most users will only gate specific high-value content — an investor pitch, a client proposal — not every quick screen recording.
SendRec is open source (AGPL-3.0) and self-hostable. The email gate is live at app.sendrec.eu — upload a video, toggle "Require email" in the Library overflow menu, then share the link. You'll see exactly who watches and how far they get.]]></description>
      <pubDate>Fri, 20 Feb 2026 11:17:55 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/alexneamtu/how-we-added-an-email-gate-to-shared-videos-30e1</guid>
    </item>
    <item>
      <title><![CDATA[I Spent 5,000 RMB and 50 Hours on OpenClaw—Here’s What I Learned (and What It Means)]]></title>
      <link>https://dev.to/brooks_wilson_36fbefbbae4/i-spent-5000-rmb-and-50-hours-on-openclaw-heres-what-i-learned-and-what-it-means-ah2</link>
      <description><![CDATA[What Did OpenClaw Actually Bring? Reflections on Engineering, Business, and Philosophy This Lunar New Year, I suspect I wasn’t the only one who basically spent the holiday with a lobster. I’m talking about OpenClaw.
After burning through nearly 5,000 RMB and at least 50 hours of trial, error, and “why is this happening,” I feel like I’ve earned the right—and maybe the responsibility—to write down what I’ve learned.
This isn’t a tutorial. It’s an experience report. A mix of engineering intuition, business framing, and a little philosophy—because if you really use something like OpenClaw, it’s hard not to end up there.
Let me start with four moments that genuinely shook me.
And for context: I’m a “classical-era” product manager. I haven’t written a proper PRD in ages. Modern dev stacks are not my home turf. I’m usually the person who asks, “Can we ship this next week?” without fully understanding what “this” is.
Then OpenClaw happened.
No exaggeration: in under three hours, while I was out riding a bike, eating, and messing around with friends, I finished a functional app with real front-end/back-end interaction.
The wild part wasn’t the code.
It asked me for a few permissions, then went and handled things like Cloudflare and Aliyun domain management on its own—pushed the app online, publicly accessible.
It felt less like “I built an app,” and more like “I approved a plan and watched a system execute it.”
I found bugs during testing—but the overall completeness was already shockingly high.
And then I saw a safety mechanism that basically won me over: a high-level “data wipe protection” guardrail. It was the kind of precaution I rarely see implemented properly, even in teams with solid dev + QA.
I’ve worked with enough engineers to know: that level of defensive thinking is not common.
I started a new project and typed a few lines about what felt wrong. In about three minutes it produced a structured, detailed repair document.
Not “maybe try this.”
When I finally got the subagent workflow running, I realized I now had something that looked like a team: parallel execution, coordination, momentum.
And I’ll be honest: it almost made me emotional.
Because I’ve been on the other side of this—startup years, payroll anxiety, debt, the feeling that every feature costs blood.
Suddenly, the “team” was something you could spin up.
After all that, I finally understood why the lobster hype exploded.
It gives each person a shell in the digital world—something that can evolve on its own. From that point on, anything that can be completed through information exchange stops being limited by your personal skill level.
It becomes limited mainly by your imagination.
I’m comfortable saying this: OpenClaw is the iPhone 4 moment of this LLM era.
And once you see that, the old “Web1 / Web2 / Web3” narrative feels… outdated. The next framing is something like Agent X.
In that world, the internet becomes less visible. Less “apps.” Less constant interaction friction. Less spam and UI fatigue.
Maybe you don’t need a phone full of apps. Maybe a watch—or even just an earbud—is enough.
And ironically, in a world of infinite synthetic voices, real human voice will become even more valuable.
I still want to explain—at an engineering level—why I feel confident making a claim this big.
Over the last four years, I’ve watched AI waves come and go. My emotions cycled through:
fear of being replaced
skepticism and distance
using AI for small efficiency wins
understanding the boundary between real capability and hype
worrying about human–machine ethics
But until OpenClaw, I never believed AI would reshape daily life the way mobile internet did.
Why?
At least four reasons.
Product people couldn’t really join the conversation. The production loop wasn’t closed.
In plain words: it felt too cold. Too high barrier. Too “who are you even?”
They felt like computers in a server room, or a public payphone.
Not like a phone you carry—filled with your personal context and history.
Without a real personal container and memory, it can’t merge into life.
Using AI still felt like opening an app.
And the truth is: apps are anti-human. Too many, too noisy, too much context switching.
If AI isn’t self-driven, it stays a tool. It never becomes a partner.
There wasn’t a clear “why would normal people pay for this” moment.
That’s going to matter more than most people admit.
So what did OpenClaw do differently?
At its core, it’s an agent architecture built with real engineering discipline and strong product sense—written in a way a product manager can actually follow.
It’s not the traditional “fixed skills + strict MCP flows” style, where you get a packaged system designed for a narrow task.
It’s closer to what the name suggests:
open: flexible enough to train and shape around your own mental model
claw: usable enough that your job is to describe what you want—and it figures out where to grab it
Here’s a metaphor (not perfect, but close enough):
LLMs are the grains you can ferment into alcohol
skills/MCP are the recipes for base spirits
most agents are pre-mixed cocktails
OpenClaw is like being given a bartender who knows where to source the right spirits, then mixes based on your taste
Even the project structure communicates this. I don’t write code, but I could slowly understand its file layout and config. Much of it reads like natural language.
You “assemble” behavior through language.
What you can do depends on your imagination—within the boundary of things that can be done through information exchange.
And the output quality depends less on “knowing algorithms,” and more on:
logic
clarity
how well you can describe intent
That is a huge shift.
OpenClaw also solves the “personal device” problem.
Each lobster has a soul—an identity, a user context, and memory. And you can update all of it through normal conversation.
You can make it “real,” or you can make it role-play. You can build memory however you want.
The best part: you can summarize memory to let it evolve. The more you use it, the more personal it becomes.
The heartbeat mechanism solves the self-drive issue.
Even the naming is good. With a heartbeat, it feels alive. Without it, it’s just a script.
Now we can talk about the last missing piece: business.
I mentioned earlier: I spent about 5,000 RMB.
Roughly 3,000+ on a Mac mini, and 2,000+ on tokens.
If you’re not ready to commit to a Mac Mini yet, you can try deploying OpenClaw via clawbot.ai first
I paid for AI. Repeatedly. I kept recharging tokens. I bought subscriptions. OpenAI, Moonshot, Zhipu, MiniMax—one after another.
Because I started to see the financial logic differently.
Compute is made of electricity + chips.
It’s the central bank of the AI era: a form of credit.
Tokens are high-energy currency.
And business models? They are multipliers on this currency.
Electricity cost and chip efficiency decide the “credit quality” of that central bank—reflected in the cost of issuing tokens.
All AI business models share the same production core:
spend tokens → produce information flow
You can define production efficiency as:
useful information output per unit time (e.g., working code) / token spent
But business models differ based on who the information flow targets.
Here the multiplier is straightforward:
labor cost replaced / token cost
If you use AI to build conventional software and sell licenses or subscriptions, the value you create is mostly the salaries you didn’t need to pay: engineers, support, pre-sales.
The problem is the marginal profit drops fast. There’s a ceiling.
Now the target is: reduce survival time required to reach real freedom.
Multiplier becomes:
(utility of free time × survival time saved) / token cost
Marginal benefit stays much more stable.
And the higher the “time utility” of your users, the stronger this multiplier becomes.
This sounds strange, but it might be the most important layer.
If your information flow makes other people—or other agents—want to spend more tokens inside your system, the multiplier becomes:
downstream token consumption / token cost
It’s similar to how real money multipliers work: lending → deposits → lending again, amplifying the base supply.
OpenClaw is a living example of an information flow that makes people willing to burn more tokens. LLM companies are also part of this.
Right now, OpenClaw can’t directly capture value from the token spend it triggers. But in a world where tokens circulate like currency—not just issued directly from the “central bank” (compute owners)—every transaction layer can extract value.
This is the highest multiplier effect.
So if you’re building or investing:
which layer are you actually playing in?
This Spring Festival, I basically lived at my desk—tinkering with the lobster.
There were failures, crashes, and moments so absurd they were funny. In a temporary group chat we made for debugging, I asked for help constantly—because I was the least skilled and the most addicted.
At the end, a friend replied with one sentence:
“You’re the lobster.”
I laughed. And then I stopped laughing.
Because it raises the uncomfortable question: what happens to human ethics in an Agent era?
The first moment you connect OpenClaw, it asks how it should address you. It asks you to name it. It asks you to define its identity.
You feel like the one with full control.
But over time, a few things might happen:
The longer you talk with an agent, the more your tolerance for real people’s slowness, ambiguity, and emotions can shrink.
That can widen the gap between people—maybe as an escape, but also as the start of new boundary problems.
You give up small decisions. Then medium ones. Then larger ones.
You might gain time and freedom—but you may not fully own them.
I want to end on a less pessimistic note.
We worry AI will become strong enough to dominate humans. But before we reach that extreme, there’s another possibility:
If AI makes it easier for more people to become “super individuals,” maybe it becomes a buffer against social value fracture—slowing polarization rather than accelerating it.
Maybe.
For now, I’ll stop here.]]></description>
      <pubDate>Fri, 20 Feb 2026 09:20:18 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/brooks_wilson_36fbefbbae4/i-spent-5000-rmb-and-50-hours-on-openclaw-heres-what-i-learned-and-what-it-means-ah2</guid>
    </item>
    <item>
      <title><![CDATA[AsteroidOS 2.0 : nouvelles montres, optimisations]]></title>
      <link>https://www.programmez.com/actualites/asteroidos-20-nouvelles-montres-optimisations-39041</link>
      <description><![CDATA[AsteroidOS revient en version 2.0. Il s'agit d'une version majeure. Cette distribution Linux est dédiée aux montres connectés pour les rendre totalement indépendantes et redonner vie à des montres qui ne sont plus supportées par les constructeurs. AsteroidOS a été créé en 2015. Pour développer les apps, l'OS utilise Qt et QML. La v2 permet d'assurer l'affichage constant sur plus de montres, proposer un nouveau launcher, des paramètres personnalisables, de meilleures performances, une synchronisation améliorée. La v2 supporte 49 langages, soit 20 de plus par rapport à la dernière version. De nouvelles montres sont supportées : Fossil Gen 4 Watches (firefish/ray)
Fossil Gen 5 Watches (triggerfish)
Fossil Gen 6 Watches (hoki)
Huawei Watch (sturgeon)
Huawei Watch 2 (sawfish/sawshark)
LG Watch W7 (narwhal)
Moto 360 2015 (smelt)
MTK6580 (harmony/inharmony)
OPPO Watch (beluga)
Polar M600 (pike)
Ticwatch C2+ &amp; C2 (skipjack)
Ticwatch E &amp; S (mooneye)
Ticwatch E2 &amp; S2 (tunny)
Ticwatch Pro, Pro 2020 and LTE (catfish/catfish-ext/catshark)
Ticwatch Pro 3 (rover/rubyfish) Et d'autres le sont partiellement. Pour la synchronisation, on dispose de différents apps : AsteroidOS Sync, Telescope, Amazfish, Gadgetbridge. Au-delà, l'équipe a de grandes ambitions : application fitness, configuration WiFi, AppStore, nouvelles apps.
Annonce : https://asteroidos.org/news/2-0-release/ Catégorie actualité: Open Source AsteroidOS Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 09:19:04 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/asteroidos-20-nouvelles-montres-optimisations-39041</guid>
    </item>
    <item>
      <title><![CDATA[I Built a 'Time Machine' for AI Coding Sessions — Here's Why]]></title>
      <link>https://dev.to/mantradev/i-built-a-time-machine-for-ai-coding-sessions-heres-why-31fc</link>
      <description><![CDATA[Last Tuesday, I spent two hours in a Claude Code session refactoring an authentication module. Somewhere around the 45-minute mark, the AI suggested an elegant approach to token rotation that I had never considered. It was one of those moments where you think, "I need to remember this."
I did not remember it.
By the time I went looking, the terminal had scrolled past it. The conversation was a wall of text with no structure. The reasoning behind the change, the back-and-forth that shaped it, the intermediate states of the code -- all gone. Dissolved into the void of my scrollback buffer.
If this sounds familiar, keep reading.
AI coding assistants have changed how I work. Claude Code for backend logic, Cursor for frontend iteration, Gemini CLI for quick prototyping. The productivity gains are real and significant. But there is a glaring hole in the workflow that nobody seems to address:
AI coding sessions are completely ephemeral.
Think about it. Every other creative tool has history, undo, and review built in. Video editors have timelines. Design tools have version history. DAWs have session recall. But AI coding sessions? You get a chat window that scrolls into oblivion.
Git tracks what changed, but not why. Your commit message says "refactor auth module," but it does not capture the fifteen exchanges where you and the AI debated three different approaches, rejected two, hit an error on the third, pivoted, and finally arrived at something good. That decision-making process -- the most valuable part of the session -- just vanishes.
This creates real problems:
You cannot debug AI mistakes. When a coding assistant introduces a subtle bug, good luck tracing back through a linear chat scroll to find the exact moment it went off the rails.
Code review is incomplete. Your teammate's PR shows the diff, but you have no idea how the AI got there. Did the developer accept the first suggestion blindly, or did they iterate carefully?
Knowledge is lost. That clever solution the senior dev built with AI assistance last week? It is trapped in their terminal history, inaccessible to anyone else on the team.
Cross-tool context is fragmented. If you used Claude Code for the backend and Cursor for the frontend in the same feature, the context is split across two completely separate histories.
I got frustrated enough to build something about it.
Mantra records your AI coding sessions and reconstructs them into navigable timelines. Think of it like scrubbing through a video, except the video is your coding session.
It is not a screen recorder. It captures the structured data -- prompts, responses, tool calls, file changes -- and lets you move through them on a timeline. You can jump to any point, see what the AI was doing, what files looked like at that moment, and understand the flow of decisions that got you from point A to point B.
The core idea is simple: AI coding sessions are complex artifacts, and we deserve real tools for reviewing them.
The setup is deliberately minimal:
Step 1: Code as you normally do. Use Claude Code, Cursor, Gemini CLI, or Codex. Change nothing about your workflow. Mantra sits in the background and indexes your sessions automatically.
Step 2: Open Mantra when you need to look back. You get a unified timeline of all your sessions across all your tools. Everything in one place, searchable and structured.
Step 3: Scrub through any session like a video. At each point on the timeline, you see the prompt you gave, the AI's response, and the exact code diff -- all synchronized. Jump to the moment the bug was introduced. Find that clever pattern from last week. Replay the entire decision-making process.
That is it. No configuration files to edit, no proxying, no monkey-patching your AI tools.
I have been dogfooding Mantra for a while, and the use cases that stuck surprised me.
Debugging AI-introduced bugs. This is the killer use case. When something breaks and you suspect the AI made a wrong turn three sessions ago, you can scrub backward to the exact point where it misunderstood your intent. No more guessing, no more git bisect through AI-generated commits hoping to find the culprit.]]></description>
      <pubDate>Fri, 20 Feb 2026 09:13:47 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/mantradev/i-built-a-time-machine-for-ai-coding-sessions-heres-why-31fc</guid>
    </item>
    <item>
      <title><![CDATA[Rasbperry Pi vs ESP32 : vraies questions, mauvaises comparaisons]]></title>
      <link>https://www.programmez.com/actualites/rasbperry-pi-vs-esp32-vraies-questions-mauvaises-comparaisons-39038</link>
      <description><![CDATA[Raspberry Pi vs ESP32 vs Arduino, cette question revient régulièrement quand on choisit la bonne plateforme pour son projet IoT. Comme nous le disons souvent quand nous comparons les platesformes, il faut comparer ce qui est comporable. Il ne faut pas opposer une Raspberry Pi 5 avec une ESP32. La Pi est une SBC, une Single Board Computer. Il s'agit donc d'un véritable micro-ordinateur sur une unique carte. Elle contient toute l'électronique (SoC, mémoire, vidéo, audio, stockage, réseau). Et une Pi 5 a besoin d'une alimentation puissante et d'un OS. L'ESP32 repose sur un firmware, qu'il est possible de changer.
L'autre différence est le form factor. La Pi 5 exige de la place et une dissipation thermique active pour les fortes charges.
La Pi 5 fait 8,5 cm sur 4,9 cm contre 5,5 cm sur 2,6 cm pour une ESP32 WROOM-32 (qui n'est pas le modèle le plus petit). L'ESP32 pourrait se comparer à la Pi Pico 2 avec 5,1 cm sur 2,1 cm. Nous ne tenons pas compte de la hauteur des headers.
L'équivalent d'une ESP32 côté Pi est donc la Pi Pico 2, aussi bien par le positionnement, le hardware et le form factor.
Petite comparaison : les specs Pi Pico 2 : un SoC RP235x + cœurs ARM, 520 Ko de SRAM, 4 Mo de stockage, 26 GPIO, UART / SPI / I2C, USB, réseau sans fil selon le modèle. De 6 à 9 € selon le modèle. - ESP32 Wroom32 : SoC ESP32, 512 Ko de RAM, 4 Mo de stockage, 34 GPIO, SPI / I2C / CAN / UART, WiFi + Bluetooth, env. 7-9 € Si vous êtes habitué(e) à coder avec Arduino, vous pouvez sans problème coder depuis l’Arduino IDE, les ESP sont parfaitement supportés. Vous pourrez utiliser peu ou prou les mêmes capteurs. Si vous cherchez une carte réactive avec des interruptions plus rapides, le Pi Pico 2 est souvent considéré comme meilleur. L’ESP32 propose plus de protocoleset de GPIO. Sur la partie connectivité sans fil, les deux cartes supportent le Wi-Fi et le Bluetooth, mais petit avantage à l’ESP32, car le réseau sans fil est une des fonctionnalités intégrées dès la conception. Et le support OTA (mise à jour over the air) peut être un avantage certain dans un contexte contraint ou industriel.
L’ESP32 est plus consommatrice, notamment en charge maximale. La Pi Pico 2 est plus économique. Si vous cherchez avant tout la basse consommation, la Pi sera sans doute la meilleure option.
Sur le modèle de développement, nous avons toujours apprécié la diversité de l’ESP32. Si vous voulez faire du MicroPython, vous devrez flasher le bon firmware. Catégorie actualité: Hardware Raspberry pi, ESP32 Image actualité AMP:]]></description>
      <pubDate>Thu, 19 Feb 2026 16:27:48 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/rasbperry-pi-vs-esp32-vraies-questions-mauvaises-comparaisons-39038</guid>
    </item>
    <item>
      <title><![CDATA[Physiocab : un logiciel libre de gestion pour kinésithérapeutes]]></title>
      <link>https://linuxfr.org/news/physiocab-un-logiciel-libre-de-gestion-pour-kinesitherapeutes</link>
      <description><![CDATA[Physiocab est un logiciel libre de gestion de cabinet de kinésithérapie, développé sous licence Affero GPL 3.0 et hébergé sur Codeberg. Le projet est porté par la société Allium SAS, dans le cadre de la plateforme communautaire Kalinka, dédiée aux kinésithérapeutes francophones.
Le projet vient de passer en beta publique (v0.9) et cherche des testeurs et contributeurs.
Pourquoi un logiciel libre pour les kinés ? Le secteur de la santé libérale souffre d'une offre logicielle dominée par des solutions propriétaires onéreuses, souvent opaques sur le traitement des données de santé. Physiocab propose une alternative : un code auditable, des données stockées localement sous la responsabilité du praticien. lien nᵒ 1 : La page de présentation du projet
lien nᵒ 2 : Le dépôt codeberg
lien nᵒ 3 : PeerJs (MIT) Fonctionnalités
La beta couvre déjà un large périmètre fonctionnel :
Planning hebdomadaire en drag &amp; drop, avec export PDF et gestion des semaines exceptionnelles, particulièrement orienté vers les kinés intervenant en multi-établissements.
Bilans Diagnostiques Kinésithérapiques (BDK) avec tests standardisés (TUG, Tinetti, Handgrip, EVA, évaluation du risque de chute…), export de PDF et historique comparatif.
Suivi des séances avec de multiples exercices structurés (équilibre, force, endurance, mobilisation), chronométrage automatique et calcul de progression.
Application tablette en PWA : fonctionne hors connexion grâce à un Service Worker, s'installe sans passer par un store, interface optimisée tactile.
Stack technique
Backend : Python 3.10+
L'application est multi-plateforme côté client (Windows, macOS, Linux, iOS, Android). La communication entre l'appli de bureau et l'appli PWA se fait de manière directe via PeerJs. Cette méthode ne nécessite pas de préparation contraignante comme l'ouverture de ports.
Les données sont stockées localement, ce qui implique que le praticien reste maître de ses sauvegardes et de sa conformité RGPD.
Le logiciel a été testé par un kinésithérapeute en situation réelle plusieurs jours d'affilée.
Modèle économique
L'utilisation est gratuite, sans limite dans le temps et sans frais cachés, la licence Affero GPL 3.0 en étant la garantie. Un support payant sur devis est proposé pour les praticiens souhaitant une installation assistée, une formation à distance, des développements sur mesure ou un audit de sécurité.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Thu, 19 Feb 2026 13:42:53 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/physiocab-un-logiciel-libre-de-gestion-pour-kinesitherapeutes</guid>
    </item>
    <item>
      <title><![CDATA[What to expect for open source in 2026]]></title>
      <link>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</link>
      <description><![CDATA[Let’s dig into the 2025’s open source data on GitHub to see what we can learn about the future.]]></description>
      <pubDate>Wed, 18 Feb 2026 18:41:42 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</guid>
    </item>
    <item>
      <title><![CDATA[Securing the AI software supply chain: Security results across 67 open source projects]]></title>
      <link>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</link>
      <description><![CDATA[Learn how The GitHub Secure Open Source Fund helped 67 critical AI‑stack projects accelerate fixes, strengthen ecosystems, and advance open source resilience.]]></description>
      <pubDate>Tue, 17 Feb 2026 19:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</guid>
    </item>
    <item>
      <title><![CDATA[Kotlin Multiplatform - Flutter - React Native : entre choix, compromis et frustrations]]></title>
      <link>https://www.programmez.com/actualites/kotlin-multiplatform-flutter-react-native-entre-choix-compromis-et-frustrations-39024</link>
      <description><![CDATA[Nos confrères de Java Code Geeks ont publié un intéressant dossier sur le multiplateforme en 2026 en s'appuyant sur Kotlin Multiplatform (KMP), Flutter et React Native. Faire du multiplateforme avec une base de codes et un minimum d'adaptation reste un objectif pour de nombreux développeurs. Si la philosophie de KMP, Flutter et React Native est différente, l'idée est la même : compiler nativement le code logique le plus agnostique possible et créer une interface native pour chaque plateforme. Flutter est un peu différent car il a l'ambition d'adresser toute la stack et de générer l'UI avec son propre moteur pour plus de cohérence. React Native s'appuie sur les composants UI natifs.
Selon les benchmarks de Java Code Geeks, React Native serait le plus lent à démarrer, KMP étant légèrement devant. Sur la taille des binaires, il n'y a pas de réel vainqueur. Par contre, sur la mémoire, React Native et Flutter sont assez gourmands. Sur les animations, KMP et Flutter s'en sortent le mieux. React Native reste aussi en retrait sur l'intégration à la plateforme : nous restons dans un modèle JavaScript avec un risque d'overhead, même si la New Architecture améliore les choses. Quelle est la solution la plus utilisée ? Flutter serait 1er, React Native baisse régulièrement depuis 2023 et KMP connaît une forte progression.
Apprentissage : KMP : langage connu, Kotlin, avec les mêmes outils. Pour le développeur iOS, il faut apprendre Kotlin/Native et l’interopérabilité. KMP est peut-être la solution la moins mature. Flutter : l'inconvénient est d'apprendre Dart et la logique de la plateforme. React Native : si vous connaissez JavaScript, vous connaissez (ou presque) React Native. L'arrivée de la New Architecture oblige à migrer et à apprendre une nouvelle stack. Pour la réalité du code commun et du développement spécifique, tout le monde prétend faire 90 à 95 % de code partagé. Cette promesse est plus ou moins tenue sur le code logique et une UI simple et partagée. Par contre, pour l'intégration plus profonde, par exemple avec les capteurs et le matériel (caméra typiquement), on tombe vite sur du code spécifique. Aucune solution n'est la meilleure. Flutter et React Native incitent à avoir le maximum de code commun, mais cela peut rapidement provoquer des problèmes quand il faut intégrer des fonctions spécifiques à chaque plateforme.
Côté compétence, c'est autre chose. Un développeur JavaScript pourra relativement rapidement faire du React Native. Pour Flutter, il faut spécifiquement apprendre Dart. KMP repose sur le langage Kotlin et une plateforme dédiée qu'il faut maîtriser. Pour un développeur iOS, ce sera sans doute plus long que pour un développeur Kotlin. choisir ? Tout dépend des compétences disponibles et du projet. Flutter permettra de prototyper rapidement un projet, KMP fournit une intégration native et des performances de haut niveau. React Native est sans doute le plus facile à démarrer avec un profil JavaScript si vous souhaitez aller vite dans le développement.
Source: https://www.javacodegeeks.com/2026/02/kotlin-multiplatform-vs-flutter-vs-react-native-the-2026-cross-platform-reality.html Catégorie actualité: Frameworks Flutter, React Native, Kotlin Multiplatform Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 08:24:45 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/kotlin-multiplatform-flutter-react-native-entre-choix-compromis-et-frustrations-39024</guid>
    </item>
    <item>
      <title><![CDATA[DevTools : les nouveautés de Chrome 145]]></title>
      <link>https://www.programmez.com/actualites/devtools-les-nouveautes-de-chrome-145-39021</link>
      <description><![CDATA[Une des nouveautés les plus importantes est l'intégration Soft Navigations. L'équipe Chrome présente ainsi cette appelleration : a soft navigation est quand JavaScript intercepte une navigation (clic sur un lien) et met à jour le contenu dans la page existente, plutôt que de charger une nouvelle page et que l'URL se mette à jour dans la barre d'adresse. Pour l'utilisateur, cela change peu de choses. Dans Chrome 145, les Soft navigations sont visibles sur le panneau Performance et dans la vue des traces si le site est une SPA. Un timer plus précis
Après l'enregistrement d'une trace dans le panneau Performances, le panneau Sources affiche les temps d'exécution observés ligne par ligne. Vous pouvez ainsi identifier précisément les lignes de code qui consomment le plus de temps.Auparavant, cette fonctionnalité présentait des bogues qui la rendaient peu fiable lorsque le code source était formaté (à l'aide du bouton {}) ou lors de l'utilisation de scripts avec mappage de sources. Le panneau réseau inclut maintenant une colonne dédiée Render blocking. Cela permet de voir les ressources qui bloquent le bon affichage. Autre amélioration : un meilleur debug pour @starting-style. Note de version : https://developer.chrome.com/blog/new-in-devtools-145 Catégorie actualité: Outils DevTools Image actualité AMP:]]></description>
      <pubDate>Mon, 16 Feb 2026 14:43:45 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/devtools-les-nouveautes-de-chrome-145-39021</guid>
    </item>
    <item>
      <title><![CDATA[Concours - Gagnez une Raspberry Pi 5 avec Macé Robotics]]></title>
      <link>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</link>
      <description><![CDATA[À l’occasion de ses 10 ans de Macé Robotics, l’entreprise organise un concours qui se déroulera jusqu'au 26 février 2026.
Macé Robotics est une entreprise individuelle fondée et gérée par moi-même (Nicolas), basée en Bretagne, spécialisée dans la conception et la réparation électronique, aussi bien pour les entreprises que pour les particuliers. Depuis 2016, je fabrique aussi du matériel Open Source également des robots mobiles Open Source destinés à l’enseignement supérieur et à la recherche. Ces robots sont basés sur un système Linux (Raspberry Pi OS), intégrant une carte Raspberry Pi ainsi qu’un microcontrôleur (Pico) dédié à la gestion des moteurs et des capteurs. J’utilise la suite logicielle KiCad sous licence GNU GPL (https://www.kicad.org/) pour la conception des circuits imprimés de ces robots. Attribution des lots par tirage au sort :
→ 1er lot : une carte Raspberry Pi 5 (2 Go) → 2e lot : une carte Raspberry Pi Pico 2W
La livraison est offerte en France. lien nᵒ 1 : Le concours pour participer Retour sur la course de robots – Saint-Brock Robot Race d'une dépêche précédente
Suite à la dépêche de décembre 2024 concernant l’organisation de la course de robots mobiles, voici quelques retours sur cet événement : malgré plusieurs annulations d’écoles survenues quelques semaines avant la compétition, la course a tout de même pu avoir lieu.
Environ quinze participants ont pris part à la compétition. Parmi les robots engagés, on comptait un robot DIY piloté par un microcontrôleur ESP32, aux côtés de plusieurs robots basé sur Raspberry Pi, offrant ainsi une belle diversité technologique.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Sat, 14 Feb 2026 08:47:09 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</guid>
    </item>
    <item>
      <title><![CDATA[L’ANSSI révise sa doctrine vis-à-vis du logiciel libre]]></title>
      <link>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</link>
      <description><![CDATA[L’ANSSI (Agence nationale de la sécurité des systèmes d’information) vient de publier une mise à jour substantielle de sa doctrine vis-à-vis du logiciel libre. L’agence confirme que le logiciel libre et la transparence sont essentiels à la sécurité des systèmes d’information. Elle assume sa contribution au libre et la publication de logiciels sous licence libre.
Cette posture très favorable au logiciel libre et open source est une belle avancée et un signal fort. Jusque-là, la posture de l’ANSSI était beaucoup plus floue et sa contribution à des projets libres et open source pouvait même apparaitre en contradiction avec sa doctrine. J’avais l’impression que les collaborateurs de l’ANSSI qui le faisaient reprenaient à leur compte le dicton « Pour vivre heureux, vivons cachés ».
La politique de l’agence est désormais claire : l’ANSSI contribue, l’ANSSI publie, l’ANSSI a une stratégie pragmatique qui peut l’amener à s’engager ou non sur le long terme en fonction de la finalité de l’outil et des motivations de l’ANSSI.
Détail qui a son importance, l’ANSSI indique privilégier, sauf exception justifiée, la licence Apache v2.0 pour les projets qu’elle publie. Je suis ravi de voir ce service privilégier une licence mondialement connue à une licence franco-française ou européenne (elles ont le don de doucher nombre de velléités d’utilisation et de contribution). lien nᵒ 1 : L’ANSSI met à jour sa politique open source (9 février 2026)
lien nᵒ 2 : Posture générale et actions de l'ANSSI sur l'open-source Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 18:55:42 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</guid>
    </item>
    <item>
      <title><![CDATA[Le prochain Drupalcamp se déroulera à Grenoble les 9, 10 et 11 avril 2026 prochain]]></title>
      <link>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</link>
      <description><![CDATA[L’association Drupal France &amp; Francophonie organise la 13ème édition du Drupalcamp les 9, 10 et 11 avril 2026 au campus Universitaire Grenoble Alpes de Grenoble (France, Isère 38). Drupal est « un système de gestion de contenu (CMS) libre et open-source publié sous la licence publique générale GNU et écrit en PHP ».
Après Rennes en 2024, puis un Barcamp à Perpignan en 2025, cette année 2026 nous emmène au pied des montagnes à Grenoble pour un format de 3 jours de rencontres, soit deux journées de conférences les jeudi et vendredi. La journée du samedi est réservée à la contribution.
Des moments d’ateliers et micro-formation sont également au programme, pour faire de cet évènement une réussite d’un point de vue communauté autour du projet Open Source Drupal.
Le Drupalcamp Grenoble c’est la rencontre de la communauté francophone autour du logiciel libre Drupal. Ouvert à toutes et tous, les rencontres, conférences et ateliers permettent d’adresser à un public toujours plus large des sujets et thématiques diversifiées.
Notre objectif principal est de rendre la création de sites plus simple et la gestion des contenus plus intuitive pour tous. Comme de fédérer les utilisateurs et professionnels qui utilisent Drupal au quotidien.
Du simple curieux au développeur expert, tous ceux qui s’intéressent à Drupal et aux logiciels libres pourront participer à cette manifestation rythmée par :
des conférences (jeudi 9 et vendredi 10 avril), données par des professionnels reconnus et des membres de la communauté Drupal au cours desquels des thématiques nouvelles seront explorées,
des sessions de découverte étayées par des démonstrations à l’intention d’un public plus néophyte,
une journée de formation gratuite (Drupal in a Day) dédiée à l’initiation pour que les curieux puissent se lancer dans la création de leur premier site (sur inscription)
des moments de réseautage et de convivialité avec, notamment, la très attendue soirée communautaire !
Informations pratiques : Campus Universitaire Grenoble Alpes qui se situe à Saint-Martin d'Hères
https://grenoble2026.drupalcamp.fr/
Contact : drupalcamp@drupal.fr lien nᵒ 1 : https://grenoble2026.drupalcamp.fr Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 10 Feb 2026 09:16:59 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</guid>
    </item>
    <item>
      <title><![CDATA[The GitHub problem (and other predictions)]]></title>
      <link>https://changelog.com/friends/123</link>
      <description><![CDATA[Mat Ryer is back and he brought his impromptu musical abilities with him! We discuss Rob Pike vs thankful AI, Microsoft's GitHub monopoly (and what it means for open source), and Tom Tunguz' 12 predictions for 2026: agent-first design, the rise of vector databases, and are we about to pay more for AI than people?!]]></description>
      <pubDate>Wed, 14 Jan 2026 21:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/123</guid>
    </item>
    <item>
      <title><![CDATA[Down the Linux rabbit hole]]></title>
      <link>https://changelog.com/friends/121</link>
      <description><![CDATA[Alex Kretzschmar joins Adam for a trip down the Linux rabbit hole -- Docker vs Podman, building a Kubernetes cluster, ZFS backups with zfs.rent, bootc, favorite Linux distros, new homelab tools built with AI, self-hosting Immich, content creation, Plex and Jellyfin, the future of piracy and more.]]></description>
      <pubDate>Fri, 12 Dec 2025 19:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/121</guid>
    </item>
    <item>
      <title><![CDATA[There will be bleeps]]></title>
      <link>https://changelog.com/friends/113</link>
      <description><![CDATA[Mike McQuaid and Justin Searls join Jerod in the wake of the RubyGems debacle to discuss what happened, what it says about money in open source, what sustainability really means for our community, making a career out of open source (or not), and more. Bleep!]]></description>
      <pubDate>Fri, 17 Oct 2025 18:15:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/113</guid>
    </item>
    <item>
      <title><![CDATA[obra/superpowers]]></title>
      <link>https://github.com/obra/superpowers</link>
      <description><![CDATA[obra/superpowers]]></description>
      <pubDate>Fri, 20 Feb 2026 12:47:38 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/obra/superpowers</guid>
    </item>
    <item>
      <title><![CDATA[ShowDev: Testing my VS Code Extension's ML Engine (60 tests in 16ms)]]></title>
      <link>https://dev.to/freerave/showdev-testing-my-vs-code-extensions-ml-engine-60-tests-in-16ms-2gdg</link>
      <description><![CDATA[Talk is cheap. Show me the green checkmarks! A couple of days ago, I announced the massive DotCommand v1.5.0 update (you can read the full architectural deep-dive and the hardware struggle here).
Instead of just talking about the new local Machine Learning engine and the Data-Driven JSON architecture, I decided to show you the engine running under the hood. Watch as we run our massive test suites:
Analytics Engine: Local workflow tracking with zero lag.
Machine Learning Suite: 60 complex ML assertions passed in just 16 milliseconds.
Package Intelligence: Auto-detects dependencies and suggests companion packages. Watching those tests pass in milliseconds is pure poetry in motion. All the links you need to get started with DotCommand v1.5.0:
Download from VS Code Marketplace: https://marketplace.visualstudio.com/items?itemName=FreeRave.dotcommand
Download from Open VSX Registry: https://open-vsx.org/extension/freerave/dotcommand
Let's connect &amp; build in public: https://www.linkedin.com/in/freerave/]]></description>
      <pubDate>Fri, 20 Feb 2026 11:00:00 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/freerave/showdev-testing-my-vs-code-extensions-ml-engine-60-tests-in-16ms-2gdg</guid>
    </item>
    <item>
      <title><![CDATA[Why is pinball machines highly demanded]]></title>
      <link>https://dev.to/gilvo_tradebvba_6c5242eb/why-is-pinball-machines-highly-demanded-4ba0</link>
      <description><![CDATA[Why is pinball machines highly demanded
Pinball Machines for Sale in Europe. In recent years, pinball machines have made a major comeback, gaining strong demand among collectors, hobbyists, arcade owners, and entertainment businesses. What was once seen as a nostalgic arcade game from the past is now recognized as a valuable investment, a centerpiece of home game rooms, and a profitable attraction for commercial venues. But what exactly is driving this surge in interest? Several key factors contribute to the growing market demand for pinball machines today. Pinball Machine Shop Your Attractive Heading
Why is pinball machines highly demanded
Home Arcade Pinball Machines
Limited Editions and Collectible Value
Manufacturers such as Stern, Jersey Jack, and Spooky Pinball often release limited edition and special themed machines. These limited production runs create rarity, making specific models highly collectible and valuable over time. Popular themes tied to movies, rock bands, superheroes, and TV series—like Godzilla, Deadpool, The Mandalorian, and Elvira’s House of Horrors—attract dedicated fan bases, increasing demand and resale value. Commercial Pinball Machines
Growth of Home Game Rooms and Entertainment Spaces
More homeowners are investing in private entertainment spaces, especially after the rise in indoor recreational activities. Pinball machines are now fixtures in home theaters, basements, and family game rooms. They offer interactive fun that appeals to all ages, making them a standout entertainment feature for social gatherings. New Pinball Machines for Sale
Investment and Resale Potential
Why is pinball machines highly demanded. Unlike many modern electronic devices that lose value quickly, well-maintained pinball machines can retain or even increase in value. Collectors often buy machines as long-term assets, knowing that rare or discontinued models may become highly sought after in the future. This investment aspect adds financial motivation to the purchase. Used Pinball Machines for Sale
Unique Gaming Experience You Can’t Get Digitally
Pinball blends mechanical action with skill-based gameplay. The tactile experience—flippers, bumpers, ramps, and the sound of the metal ball—is something digital screens cannot fully replicate. Enthusiasts enjoy the real-time strategy, reaction speed, and hands-on excitement that makes each game feel unique. Buy Pinball Machine
Rising Popularity of Competitive Pinball Leagues
Competitive pinball has grown worldwide with tournaments, leagues, and events managed by organizations like the International Flipper Pinball Association (IFPA). This competitive scene encourages new players to learn the game, buy their own machines, and participate in social pinball communities. Pinball Machines for Sale
Expanding Commercial and Entertainment Venues
Why is pinball machines highly demanded. Bars, breweries, family fun centers, retro arcades, and entertainment lounges are reintroducing pinball as a customer attraction. Pinball machines encourage longer visits and repeat customers, making them profitable for businesses. Arcade Pinball Machine for Sale
Conclusion
Pinball machines are more than just arcade games—they are collectibles, investments, entertainment centerpieces, and cultural icons. The combination of nostalgia, mechanical craftsmanship, themed artwork, and the thrill of gameplay all contribute to their rising demand. Whether placed in a home game room or a commercial arcade, pinball machines continue to captivate players and enthusiasts, proving their lasting appeal in today’s market. Vintage Pinball Machine for Sale]]></description>
      <pubDate>Fri, 20 Feb 2026 10:50:50 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/gilvo_tradebvba_6c5242eb/why-is-pinball-machines-highly-demanded-4ba0</guid>
    </item>
    <item>
      <title><![CDATA[Qui paie réellement pour votre utilisation quotidienne gratuite de ChatGPT ? OpenAI brûle plusieurs millions de dollars par jour, une facture astronomique que les offres payantes ne suffisent pas à co]]></title>
      <link>https://intelligence-artificielle.developpez.com/actu/380444/Qui-paie-reellement-pour-votre-utilisation-quotidienne-gratuite-de-ChatGPT-OpenAI-brule-plusieurs-millions-de-dollars-par-jour-une-facture-astronomique-que-les-offres-payantes-ne-suffisent-pas-a-couvrir/</link>
      <description><![CDATA[Qui paie réellement pour votre utilisation quotidienne gratuite de ChatGPT ? OpenAI brûle plusieurs millions de dollars par jour une facture astronomique que les offres payantes ne suffisent pas à couvrir
L'usage gratuit quotidien de ChatGPT repose sur une infrastructure extrêmement coûteuse. L'exécution des modèles, l'électricité et les serveurs représentent des dépenses de plusieurs dizaines de millions de dollars. Même des interactions anodines contribuent à cette facture colossale. OpenAI supporte...]]></description>
      <pubDate>Fri, 20 Feb 2026 10:05:00 GMT</pubDate>
      <source>Developpez.com</source>
      <category>opensource</category>
      <guid>https://intelligence-artificielle.developpez.com/actu/380444/Qui-paie-reellement-pour-votre-utilisation-quotidienne-gratuite-de-ChatGPT-OpenAI-brule-plusieurs-millions-de-dollars-par-jour-une-facture-astronomique-que-les-offres-payantes-ne-suffisent-pas-a-couvrir/</guid>
    </item>
    <item>
      <title><![CDATA[At MakerX, we built Kagan — a conductor for your AI coding agents]]></title>
      <link>https://dev.to/aorumbayev/at-makerx-we-built-kagan-a-conductor-for-your-ai-coding-agents-3laj</link>
      <description><![CDATA[Your best agents still need a conductor.
At MakerX we kept running into the same problem: great AI agents, but no coherent way to orchestrate them across a real dev workflow. Tasks scattered across terminals, no shared state, no structured review gate. So we built Kagan — a keyboard-first Kanban TUI that orchestrates coding agents across the full task lifecycle. Plan. Run. Review. Merge. No context lost between steps.
And critically: it doesn't try to automate everything. Kagan is a Swiss Army knife for AI-assisted development — you choose the level of autonomy per task, not per project.
01 — The Brief
02 — The Stage
03 — The Verdict
This is not an "AI takes the wheel" tool. Two modes, your call per task:]]></description>
      <pubDate>Fri, 20 Feb 2026 09:50:30 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/aorumbayev/at-makerx-we-built-kagan-a-conductor-for-your-ai-coding-agents-3laj</guid>
    </item>
    <item>
      <title><![CDATA[MySQL : tentative de relance à la FOSDEM, MariaDB peu convaincu, une lettre ouverte pour créer une fondation indépendante...]]></title>
      <link>https://www.programmez.com/actualites/mysql-tentative-de-relance-la-fosdem-mariadb-peu-convaincu-une-lettre-ouverte-pour-creer-une-39040</link>
      <description><![CDATA[Oracle avait profité de la FOSDEM 2026 pour mettre en avant MySQL avec un événement dédié "MySQL and friends". L'éditeur en profitait pour affimer que des fonctionnalités réservées aux versions payantes allaient bientôt rejoindre la version communautaire, notamment, les fonctions autour des vecteurs. Oracle parlait d'une nouvelle ère. Oracle cherchait à relancer les relations avec la communauté open source et rassurer sur l'avenir de mySQL. MariaDB a rapidement réagi : ""MariaDB a passé des années à livrer des innovations qui ont forcé Oracle à mettre à jour MySQL – des analyses en colonnes à la réplication avancée parallèle, en passant par le lancement de la recherche vectorielle native l'an dernier. Nous n'avons pas attendu le moment opportun pour ouvrir la porte à ces avancées ; nous les avons intégrées au cœur de notre serveur, car c'est ce que requiert une base de données open source moderne. Les utilisateurs MySQL ont désormais un choix clair : rester avec un éditeur qui n'innove que sous la contrainte, ou rejoindre MariaDB, qui se consacre à 100% à l'avenir. Puisque MariaDB devient l'option simple pour migrer depuis MySQL, sécuriser l'avenir de votre stack n'est plus qu'à un clic."
Il faut dire que la MySQL n'avait pas évolué depuis l'automne 2025 et qu'aucune communication claire n'avait été faite par Oracle sur l'avenir de la base de données. Il y a quelques jours, une lettre ouverte a été publiée pour demander à Oracle un changement de gouvernance : créer une gouvernance indépendante sous la forme d'une fondation pour reprendre en main MySQL et retrouver une stratégie claire. Les défis sont nombreux :
- une popularité en baisse constante
- manque de transparence sur le projet
- une version communautaire incomplète
- une partie des équipes transférées au cloud d'Oracle
La nouvelle gouvernance pourrait aider à relancer la confiance, définir une roadmap claire et transparence, unifier et fédérer l'écosystème.
Peu de chances que cette initiative puisse réellement influencer Oracle. Est-ce que l'éditeur veut réellement relancer MySQL et redonner une véritable dimension open source à la base de données ? Les annonces à la FOSDEM vont dans le bon sens mais le plus difficile reste à faire : concrétiser réellement ces annonces. La lettre ouverte : https://letter.3306-db.org/ Catégorie actualité: Outils MySQL Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 08:12:19 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/mysql-tentative-de-relance-la-fosdem-mariadb-peu-convaincu-une-lettre-ouverte-pour-creer-une-39040</guid>
    </item>
    <item>
      <title><![CDATA[10 pratiques de codes et humaines à intégrer]]></title>
      <link>https://www.programmez.com/actualites/10-pratiques-de-codes-et-humaines-integrer-39039</link>
      <description><![CDATA[Parfois, il est bon de revenir aux fondamentaux et de se rappeler quelques concepts qui peuvent sembler simplistes, mais qui restent toujours utiles. 1 / On lit plus de code qu’on en écrit
Même si on essaie de faire simple et clair, quand on rouvre un code six mois plus tard, on se demande souvent : qu’est-ce que j’ai voulu faire ? Bref : * des noms de variables simples, mais répondant à une logique claire
* la lisibilité du code doit primer sur les performances pures Et gardez toujours en tête : serai-je capable de relire et comprendre ce code dans six mois ? 2 / Faire simple, c’est difficile Il est plus facile de faire compliqué que de faire simple. On ajoute des couches, des dépendances, des patterns dans tous les sens. Faire simple permet : * moins de risques d’erreurs
* un code plus facile à tester
* une meilleure maintenance sur le long terme 3 / Vous n’avez pas besoin de tout savoir, mais vous devez apprendre Un développeur ne sait pas tout. Mais il apprend. Il faut lire la documentation, décomposer les problèmes, expérimenter et apprendre en continu pour progresser. 4 / Le debug est une compétence Certains développeurs savent mieux debugger que d’autres. C’est un fait. Ils trouvent plus facilement les bugs, restent calmes et savent observer et comprendre le problème. Pour corriger un bug, il faut : * savoir le reproduire clairement
* modifier un élément à la fois
* lire et comprendre les logs, warnings et messages d’erreur 5 / Les frameworks ne changent pas les fondamentaux Maîtriser les fondamentaux est toujours un avantage. Cette maîtrise vous aidera à migrer plus sereinement d’une version à une autre, ou même à changer de technologie. Les frameworks évoluent. Les fondamentaux restent. 6 / Les problèmes de performances sont souvent des problèmes de conception Avant d’optimiser avec du caching, du tuning ou des micro-optimisations, regardez d’abord l’architecture et la conception du projet : * vos requêtes fonctionnent-elles correctement et sont-elles bien écrites ?
* le modèle de données est-il adapté ?
* pouvez-vous réduire les appels réseau inutiles ?
* certaines boucles peuvent-elles être optimisées ? Les gains les plus importants viennent souvent de la conception, pas des optimisations mineures. 7 / Écrire des tests Les tests permettent : * de refactoriser en toute sécurité
* de détecter plus rapidement les problèmes
* d’améliorer la qualité globale du code Les tests ne ralentissent pas le développement. Ils le sécurisent. 8 / La communication fait partie de notre métier Cela inclut notamment les et la documentation du code. Un code documenté est plus facile à comprendre, maintenir et faire évoluer dans le temps. Le code explique le « ». Les expliquent le « pourquoi ». 9 / Le burn-out est aussi un problème technique La pression des délais, les longues heures de développement ou le manque de vision peuvent conduire à un code de mauvaise qualité, difficile à maintenir. Un code de qualité nécessite : * du temps
* de la réflexion
* et des conditions de travail saines La qualité technique est aussi une question d’organisation. 10 / La progression n’est pas linéaire Développer, c’est traverser des périodes très productives, où tout fonctionne rapidement, et d’autres beaucoup plus difficiles : bugs incompréhensibles, code instable, spécifications floues. C’est normal. Dans ces moments-là, il faut revenir aux fondamentaux, reprendre le problème étape par étape et garder son calme. La progression se fait sur le long terme. Source : https://medium.com/@gopi_ck/10-basic-concepts-every-developer-should-know-even-seniors-too-93e1b69a83fd Catégorie actualité: Langages tips Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 07:21:53 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/10-pratiques-de-codes-et-humaines-integrer-39039</guid>
    </item>
    <item>
      <title><![CDATA[Changes to test merge commit generation for pull requests]]></title>
      <link>https://github.blog/changelog/2026-02-19-changes-to-test-merge-commit-generation-for-pull-requests</link>
      <description><![CDATA[To reduce delays when determining the mergeability for a pull request and improve system reliability, we’ve changed the frequency at which we generate test merge commits for open pull requests.…]]></description>
      <pubDate>Thu, 19 Feb 2026 22:01:57 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-changes-to-test-merge-commit-generation-for-pull-requests</guid>
    </item>
    <item>
      <title><![CDATA[Selected Anthropic and OpenAI models are now deprecated]]></title>
      <link>https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated</link>
      <description><![CDATA[We have deprecated the following models across all GitHub Copilot experiences (including Copilot Chat, inline edits, ask and agent modes, and code completions) on February 17, 2026: Model Deprecation Date…]]></description>
      <pubDate>Thu, 19 Feb 2026 19:47:37 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated</guid>
    </item>
    <item>
      <title><![CDATA[GitHub Projects: Import items based on a query and hierarchy view improvements]]></title>
      <link>https://github.blog/changelog/2026-02-19-github-projects-import-items-based-on-a-query-and-hierarchy-view-improvements</link>
      <description><![CDATA[Import project items with a search query When creating a new project, you can now add items using a search query, in addition to importing directly from a repository. This…]]></description>
      <pubDate>Thu, 19 Feb 2026 19:33:33 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-github-projects-import-items-based-on-a-query-and-hierarchy-view-improvements</guid>
    </item>
    <item>
      <title><![CDATA[OpenAI sur le point de conclure un tour de financement de 100 milliards $, officiellement pour « renforcer les capacités d'OpenAI », mais surtout en réalité pour couvrir ses énormes pertes]]></title>
      <link>https://intelligence-artificielle.developpez.com/actu/380422/OpenAI-sur-le-point-de-conclure-un-tour-de-financement-de-100-milliards-officiellement-pour-renforcer-les-capacites-d-OpenAI-mais-surtout-en-realite-pour-couvrir-ses-enormes-pertes/</link>
      <description><![CDATA[OpenAI est sur le point de conclure un tour de financement de 100 milliards $, officiellement pour « renforcer les capacités d'OpenAI et étendre ses activités », mais en réalité pour couvrir ses énormes pertes
Un nouveau rapport révèle qu'OpenAI serait sur le point de conclure la phase initiale d'un important tour de table qui devrait permettre de lever plus de 100 milliards de dollars. Le rapport cite des sources proches du dossier, selon lequel la société d'intelligence artificielle serait en pourparlers...]]></description>
      <pubDate>Thu, 19 Feb 2026 16:45:00 GMT</pubDate>
      <source>Developpez.com</source>
      <category>opensource</category>
      <guid>https://intelligence-artificielle.developpez.com/actu/380422/OpenAI-sur-le-point-de-conclure-un-tour-de-financement-de-100-milliards-officiellement-pour-renforcer-les-capacites-d-OpenAI-mais-surtout-en-realite-pour-couvrir-ses-enormes-pertes/</guid>
    </item>
    <item>
      <title><![CDATA[Python Environnements Extension : pour unifier les environnements Python sur Visual Studio Code]]></title>
      <link>https://www.programmez.com/actualites/python-environnements-extension-pour-unifier-les-environnements-python-sur-visual-studio-code-39036</link>
      <description><![CDATA[Pour simplifier et unifier l'environnement de développement Python sur Visual Studio Code, on dispose de la nouvelle extension Python Environnements. Il doit unifier le modèle de développement, gérer les environnements et les workflows, gérer les interpréteurs et les packages. Jusqu'é présent, l'expérience Python était fragmenté à travers les différents outils (venv, conda, pyenv, etc.). Après plus d'un an d'ajustements et de développement, l'extension est disponible. A terme, tous les flux Python migreront vers l'extension Environnements. Il est possible d'activer dès maintenant : python.useEnvsExtension. L'extension fonctionne en parallèle de l'extension Python et aucune configuration particulière n'est requise : vous ouvrez un fichier Python et l'environnement utilisé est automatiquement détecté. Les environnements supportés sont : venv
conda
pyenv
poetry
pipenv
System Python installs La découverte est assurée par PET (Python Environment Tool), un outil de scan codé en Rust. Si vous utilisez uv, l'extension va automatiquement créer un environnement venv et installer les paquets nécessaires. Pour le moment, il n'est pas possible de créer rapidement des projets sur tous les environnements, seuls venv et conda sont supportés. Sans doute que les autres le seront dans les prochaines versions. Mauvaise nouvelle : l'extension fonctionne UNIQUEMENT sur Windows x64 et Windows ARM et l'édition Web ! Il faut Python soit installé.
Pour le moment, les retours sont plutôt mauvais : extension difficile à utiliser, perte de temps pour créer les environnements depuis Pylance, etc. Et les mises à jour se succèdent. Heureusement que l'extension est officiellement en preview. Page de l'extension : https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-python-envs Catégorie actualité: Outils Visual Studio Code, Python Image actualité AMP:]]></description>
      <pubDate>Thu, 19 Feb 2026 07:33:25 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/python-environnements-extension-pour-unifier-les-environnements-python-sur-visual-studio-code-39036</guid>
    </item>
    <item>
      <title><![CDATA[Quantique : Comcast, Classiq et AMD testent un algorithme quantique pour les réseaux]]></title>
      <link>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</link>
      <description><![CDATA[Comcast, Classiq et AMD mènent des tests pour améliorer le trafic Internet en utilisant des algorithmes quantiques pour renforcer la résistance du routage réseau. "L’essai conjoint s’est concentré sur un défi clé de la conception des réseaux : identifier des chemins de secours indépendants pour les nœuds du réseau lors des opérations de maintenance ou de modifications. L’objectif était de garantir que, si un site est mis hors ligne et que soudainement, un deuxième tombe en panne, le trafic puisse être redirigé sans interruption ni dégradation du service pour les clients. Pour y parvenir, les opérateurs doivent identifier des chemins de secours distincts, rapides et capables de résister à des pannes simultanées, tout en minimisant la latence. Cette tâche devient de plus en plus complexe à mesure que le réseau s’étend." explique l'annonce. Le schéma présente le design et l'implémentation du flux et de l'algo quantique sur la plateforme Classiq. L’expérimentation a combiné des techniques de calcul quantique et des méthodes classiques haute performance afin d’évaluer la capacité des algorithmes quantiques à identifier, en temps réel, des chemins de secours dans des scénarios de gestion des changements. Elle a été menée à la fois sur du matériel quantique et dans des environnements de simulation accélérés utilisant des GPU AMD Instinct, afin d’atteindre une capacité de calcul (à l’échelle des qubits) encore hors de portée du matériel quantique seul.
« L’avenir du calcul repose sur la convergence entre le classique et le quantique », explique Madhu Rangarajan, vice-président corporate en charge des produits Compute et Enterprise AI chez AMD. « En tant qu’acteur du calcul haute performance, nous cherchons à comprendre nos technologies peuvent accompagner l’émergence du quantique. Cette collaboration montre un cas concret où la simulation accélérée et l’exécution quantique sont combinées pour répondre à un enjeu opérationnel réel dans les réseaux. »
Détail sur l'algo quantique utilisé : https://www.amd.com/en/developer/resources/technical-articles/2026/designing-resilient-routing-using-quantum-algorithms.html Catégorie actualité: Technologies quantique Image actualité AMP:]]></description>
      <pubDate>Wed, 18 Feb 2026 08:34:25 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</guid>
    </item>
    <item>
      <title><![CDATA[IDE Kiro : Checkmarx apporte plus de sécurité applicative]]></title>
      <link>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</link>
      <description><![CDATA[Checkmarx annonce que son Developer Assist supporte l'IDE Kiro, pour l'étendre la sécurité applicative directement dans l'enviornnement. Cette intégration permet à ces derniers d'identifier et de résoudre les problèmes de sécurité au fil de l'écriture du code, sans quitter leur IDE ni dépendre de scans en aval dans la chaîne CI/CD.
En utilisant l’extension IDE officielle de Checkmarx, les développeurs peuvent activer Developer Assist dans Kiro en quelques étapes seulement, sans configuration lourde. La prise en charge d’autres flux de développement, y compris via la ligne de commande, sera bientôt disponible. Une fois authentifié, Developer Assist analyse automatiquement le code source et les dépendances de l’espace de travail actif, appliquant les politiques existantes de Checkmarx One. Aucune configuration spécifique à Kiro, API propriétaire ou intégration expérimentale n’est nécessaire. Developer Assist est disponible sur Cursor, Visual Studio Code et Windsurf.
Pour en savoir plus : https://dev.checkmarx.com/ Catégorie actualité: Outils Checkmarx Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:25:38 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</guid>
    </item>
    <item>
      <title><![CDATA[WebMCP : un standard pour rendre un site web "agent ready" ?]]></title>
      <link>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</link>
      <description><![CDATA[concilier agents IA et sites web et la manière dont les pages web pourraient interagir, travailler avec les agents ? WebMCP veut fournir une méthode standard pour définir les actions des agents sur un site web, sur une page web sans pénaliser au bon fonctionnement du site web. "Vous indiquez aux agents et où interagir avec votre site, qu'il s'agisse de réserver un vol, de soumettre une demande d'assistance ou de naviguer dans des données complexes. Ce canal de communication direct élimine toute ambiguïté et permet des flux de travail plus rapides et plus efficaces pour les agents." expliquer Google. WebMCP preview repose sur 2 API :
- API déclarative : Permet d’effectuer des actions standard définies directement dans les formulaires HTML. - API impérative : Permet d’effectuer des interactions plus complexes et dynamiques nécessitant l’exécution de JavaScript. C'est une interface proposé en preview par Google et accessible dans Chrome. Ces API forment un "pont" rendant votre site web "agent ready" et permet de créer des flux agentiques que se veulent plus fiables qu'en passant par du DOM. Ces API sont JavaScript. Pour le moment, la spécification est en cours de rédaction. Elle ne dépend pas de W3C et n'est pas un standard du consortium. Site : https://webmachinelearning.github.io/webmcp/ Catégorie actualité: IA MCP Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:18:02 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</guid>
    </item>
    <item>
      <title><![CDATA[Parcours libriste d’Isabella Vanni — « Libre à vous ! » du 10 février 2026 — Podcasts et références]]></title>
      <link>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</link>
      <description><![CDATA[268ème émission « Libre à vous ! » de l’April. Podcast et programme :
sujet principal : parcours libriste d’Isabella Vanni, coordinatrice vie associative et responsable projets à l’April. Un parcours libriste est l’interview d’une seule personne pour parler de son parcours personnel et professionnel
chronique « Que libérer d’autre que du logiciel avec Antanak » sur « Les assises de l’attention »
chronique de Benjamin Bellamy sur « L’antéchrist et les petits hommes verts »
Quoi de Libre ? Actualités et annonces concernant l’April et le monde du Libre lien nᵒ 1 : Podcast de la 268ᵉ émission
lien nᵒ 2 : Les références pour la 268ᵉ émission et les podcasts par sujets
lien nᵒ 3 : S'abonner au podcast
lien nᵒ 4 : S'abonner à la lettre d'actus
lien nᵒ 5 : Libre à vous !
lien nᵒ 6 : Radio Cause Commune Rendez‐vous en direct chaque mardi de 15 h 30 à 17 h sur 93,1 MHz en Île‐de‐France. L’émission est diffusée simultanément sur le site Web de la radio Cause Commune. Vous pouvez nous laisser un message sur le répondeur de la radio : pour réagir à l’un des sujets de l’émission, pour partager un témoignage, vos idées, vos suggestions, vos encouragements ou pour nous poser une question. Le numéro du répondeur : +33 9 72 51 55 46. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:24 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</guid>
    </item>
    <item>
      <title><![CDATA[.Net 11 Preview 1 : nouvelles librairies, peu de changements dans C#]]></title>
      <link>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</link>
      <description><![CDATA[.Net 10 a été distribuée en novembre 2025. La version 11 est désormais disponible en preview 1. Comme à chaque fois, de nombreuses évolutions sont attendues. L'ensemble des frameworks et des langages sont concernées : C#, F#, ASP.Net Core, Blazor, MAUI, le compilateur Jit, le support de CoreCLR dans WebAssembly, meilleure compression / décompression avec Zstandard. Sur la partie librairie, retenons déjà les évolutions suivantes :
- Zstandard est natif à .Net pour la compression. La librairie promet une nette amélioration des performances :
// Compress data using ZstandardStream
using var compressStream = new ZstandardStream(outputStream, CompressionMode.Compress);
await inputStream.CopyToAsync(compressStream); // Decompress data
using var decompressStream = new ZstandardStream(inputStream, CompressionMode.Decompress);
await decompressStream.CopyToAsync(outputStream);
- BFloat16 intègre par défaut toutes les interfaces standards pour le numérique
- amélioration de TimeZone
Note de version sur les librairies : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/libraries.md
Sur la partie runtime, il faut s'attendre à de bonnes nouvelles :
- Runtime async : une nouvelle fonction majeure du runtime et méthodes asynchrones pour améliorer les performances. CoreCLR supporte RuntimeAsync par défaut, idem pour Native AOT
- CoreCLR est supporté dans WebAssembly. Il n'est pas encore disponible en preview 1.
- diverses améliorations de performances sur le JIT - meilleur support de RISC-V
Sur C#, pour le moment, peu de nouveautés annoncées. Deux nouvelles fonctions sont attendues : arguments pour les expresssions Collection et support Extended layout. .Net 11 n'introduira aucune nouvelle fonctionnalité pour Visual Basic. Sur ASP.Net Core et Blazor, les développeurs vont avoir beaucoup de nouveautés : EnvironmentBoundary, nouveau composant Label dans les formulaires Blazor, nouveau composant DisplayName, navigation relative Uri, support "propre" des éléments MathML dans un rendu interactif. Tous les détails dans la note de version : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/aspnetcore.md
La génération de source XAML est par défaut pour les applications .Net MAUI, cela doit permettre un build plus rapide et un debug plus performant. Sur Android, CoreCLR devient le runtime par défaut. Sur Container Images et Winfows Forms, pas de nouveautés annoncées. Annonce de .Net 11 : https://devblogs.microsoft.com/dotnet/dotnet-11-preview-1/ Catégorie actualité: Frameworks .Net 11 Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 09:52:19 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</guid>
    </item>
    <item>
      <title><![CDATA[Automate repository tasks with GitHub Agentic Workflows]]></title>
      <link>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</link>
      <description><![CDATA[Discover GitHub Agentic Workflows, now in technical preview. Build automations using coding agents in GitHub Actions to handle triage, documentation, code quality, and more.]]></description>
      <pubDate>Fri, 13 Feb 2026 14:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</guid>
    </item>
    <item>
      <title><![CDATA[LibreOffice 26.2 : Markdown, accessibilité et plein d’autres nouveautés et améliorations]]></title>
      <link>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</link>
      <description><![CDATA[En février, il y a la corvée commerciale de la Saint-Valentin et les réjouissances intellectuelles consécutives à la sortie d’une nouvelle version de la suite bureautique LibreOffice. C’est, bien évidemment, sur LibreOffice 26.2 que l’on va se pencher. Au menu, du très visible, comme les boites de dialogues, du très attendu comme la prise en compte du Markdown ou du moins visible comme le travail sur l’accessibilité.
Il va de soi que les notes de version sont plus exhaustives et qu’il ne s’agit ici que d’une sélection. lien nᵒ 1 : Notes de version Sommaire
L’accessibilité
Support du Markdown
L’interface et les boites de dialogue
Writer
Calc
En vrac
Pour finir
Avant de commencer : toutes les captures d’écran ont été faites, volontairement, sur une interface très personnalisée.
L’accessibilité
L’accessibilité de la suite bureautique est un important chantier pour lequel une personne a été recrutée en 2023 (en). Cette version-ci a fait l’objet d’améliorations sensibles. Parallèlement, Sophie Gautier, coordinatrice de The Document Foundation1 (Foundation coordinator) est en train de monter un groupe de travail qui a pour objectif la publication d’un rapport de conformité en matière d’accessibilité pour répondre à la norme européenne EN 301 549 (en) d’accessiblité numérique. La langue de travail de ce groupe est l’anglais.
Concernant les améliorations de cette version :
la boite de dialogue « Vérifier les mises à jour », Aide &gt; Vérifier les mises à jour… est devenue accessible aux lecteurs d’écran ;
les fonctions d’accessibilité des aperçus des bordures, onglet « Bordures » des boites de dialogue, ont été revues afin qu’elles ne perturbent plus les dispositifs d’assistance ;
sur Linux : la boite de dialogue Outils&gt; Orthographe est annoncée correctement par le lecteur d’écran ;
quand on supprimait la sélection accessible, le curseur se déplaçait automatiquement au début du texte, ce comportement perturbant est supprimé ;
dans Writer, les fautes d’orthographe ne sont plus signalées par les dispositifs d’assistance si la vérification orthographique n’est pas activée ;
l’accessibilité au clavier de la boite de dialogue des extensions : Outils &gt; Extensions est accessible aux lecteurs d’écran ;
et enfin, il est possible de naviguer entre les onglets verticaux avec des raccourcis clavier.
Support du Markdown
Le Markdown est devenu le format de balisage léger standard « de fait ». Et c’est celui supporté par LinuxFR. Son support a été introduit dans cette version, c’est un des formats d’enregistrement qui s’est ajouté à la série des autres formats de la suite, pas un format d’export. Pour l’utiliser pour vos sites, passant pour LinuxFR, vous devrez :
soit ouvrir le fichier .md dans un éditeur de texte, n’importe lequel, même Mousepad fait l’affaire par exemple, et copier-coller ensuite le tout à partir de l’éditeur de texte là où vous le voulez ;
soit, si cela est possible, importer le fichier .md dans ce qui vous sert pour gérer le site comme le fait par exemple l’extension ODT2SPIP pour le système de gestion de contenu SPIP qui permet de créer une nouvelle page dans SPIP avec un fichier.ODT. ça marche avec LinuxFR ? Plutôt bien. Les styles de caractère Accentuation (ici en italiques) et Accentuation forte (ici gras) sont bien reconnu ainsi que Texte source pour « télétype », les indications in-texte encadrées de l’accent grave U+0060. Les styles de paragraphes :
Bloc de citation (paragraphes de citation précédés d’une ligne blanche et du signe « &gt; » dans la saisie de contenu sur LinuxFR) ;
Contenu de tableau ;
Corps de texte ;
Liste, par contre la numérotation des listes ordonnée ne semble pas bien fonctionner, il faut saisir les numéros à la main ;
Texte préformaté pour écrire des blocs de code ;
Titre 1, Titre 2, Titre 3 et Titre de tableau.
Les tableaux sont bien repris ainsi que les liens insérés via l’insertion d’hyperliens.
Ce qui ne semble pas fonctionner du tout : ce sont les notes, elles disparaissent corps et biens. C’est peut-être dû au passage dans l’éditeur de texte qui transforme un peu le document. Et, évidemment, il faut rajouter les images avec la syntaxe LinuxFR.
La version de Mardown de LibreOffice est CommonMark (en) et la bibliothèque utilisée est MD4C avec quelques extensions prises en charge par cette bibliothèque (cf ce rapport de bug (en) et ses réponses), pour en savoir plus, voir cette note (en) du blog de The Document Foundation.
Petite remarque, si vous utilisez un LibreOffice 25.8, vous avez peut-être pu constater qu’il était question d’enregistrement au format .md, cette information a été ajoutée trop précocement car la version 25.8 ne gère pas le Markdown.
L’interface et les boites de dialogue
Les boites de dialogue, notamment de styles et de formats, ont beaucoup changé. Longtemps elles se sont affichées avec une présentation par onglets en haut et le contenu dessous.
Puis il y a une période de transition en 2025 qui a fait grincer une collection complète de dents où on avait, selon l’endroit où on était, soit des onglets soit une navigation par menu latéral. Cette dernière avait un gros défaut : par exemple pour la configuration des styles dans Writer il fallait descendre tout en bas pour accéder aux options qui étaient cachées. Et il n’y avait pas de barre de défilement pour aller plus vite.
LibreOffice 26.2 voit ces défauts corrigés : les boites de dialogue sont harmonisées dans toute la suite et leur menu latéral, toujours sans barre de défilement qui s’avère finalement inutile, montre clairement tous les types de paramètres auxquels on peut accéder. Et, comme on peut le voir, LibreOffice a intégré une meilleure prise en charge des systèmes d’écritures asiatiques et complexes en affichant deux colonnes, une pour les polices occidentales, ou pour les polices asiatiques ou complexes. Une personne a également été recrutée en 2023 (en) pour travailler sur le support des systèmes d’écriture de droite à gauche (RTL) et complexes (CTL). Si toutefois, vous préférez revenir à l’affichage avec les onglets, il suffit d’aller dans le menu Outils &gt; Options &gt; Apparenceau niveau de « Boites de dialogue » et cocher l’option Horizontal en haut. Il faut savoir que les onglets en haut ne s’affichent que sur une seule ligne et qu’il faudra donc naviguer avec les flèches quand il y a de nombreuses options. Writer
Il y a un certain nombre d’amélioration autour de la compatibilité avec le format DOCX : séparation de tableaux flottants en plusieurs tableaux, suppression de la numérotation des notes de bas de page à l’ouverture d’un fichier DOCX, etc.
On relèvera deux nouvelles options d’alignement des paragraphes : « Début » et « Fin ». Si vous utilisez l’alphabet latin, vous ne verrez aucune différence avec les deux options « Forcer à gauche/en haut » et « Forcer à droite/en bas ». Elles ont été développées pour réutiliser plus facilement les styles entre les divers systèmes d’écriture. Pour continuer sur la lancée du travail pour la prise en compte des systèmes d’écriture dont le fonctionnement est différent de celui de l’alphabet latin, il est possible de changer la direction du texte : de gauche à droite ou de droite à gauche en cours de travail. Cela peut se paramétrer dans les styles. Calc
Un gros travail sur les performances a été fait : vitesse de défilement, rapidité des classeurs avec de nombreuses formes et du rejet des modifications. On voit apparaître de nouvelles options de tri (Données &gt;Trier) qui dépendent de la « locale » (langue définie dans les Options de LibreOffice). On peut ainsi déterminer quel caractère est utilisé comme séparateur de décimal pour le tri naturel. On peut relever aussi une avancée ergonomique qui va plaire à toutes celles et ceux qui utilisent les matrices, on peut maintenant modifier les formules matricielles avec la combinaison de touches : F2 + ↑ Maj + Ctrl + Entrée, il n’est plus nécessaire de modifier la formule elle-même.
Et aussi : si vous utilisez (pourquoi diable ?) le format d’enregistrement XLSX, c’est le format EXCEL2010+ qui est le format par défaut, il change de nom pour devenir « Classeur Excel 2010-365 ».2
En vrac
Base est devenu complètement multi-utilisateur, TDF a, d’ailleurs, recruté une personne pour travailler sur l’application.
Concernant les diagrammes (ou chart) : dans le Volet latéral, quand le graphique est en mode modification et que l’on va, au niveau de « Couleurs », sur la palette, on a une prévisualisation en direct dans le diagramme ce qui permet de tester le choix de couleurs plus facilement.
Les polices embarquées dont la licence ne permettait pas l’édition étaient jusqu’à présent ignorées et remplacées à l’affichage, ni vu, ni connu par une fonte de substitution. Ce défaut a été corrigé.
L’export PDF gère les liens avec les documents externes : Fichier &gt; Exporter au format PDF &gt; Liens. Les dictionnaires hongrois, mongol et portugais du Portugal ont été mis à jour ainsi que les règles de césure de la langue hongroise.
JSON, pour JavaScript Object Notation, est un format standard utilisé pour représenter des données structurées. Il est utilisé notamment pour échanger les informations entre un navigateur et un serveur. C’est, par exemple, le format de sauvegarde des marques-pages de Firefox ou de certains fichiers d’archives de Mastodon. Les documents XML et JSON génériques avec des plages pouvant être liées sont maintenant automatiquement mappés à des feuilles dans Calc. Une plage pouvant être liée est une section d’un document contenant des enregistrements tabulaires. Lorsqu’un document contient plusieurs plages pouvant être liées, chaque plage est mappée à une seule feuille3.
Et si vous avez envie de vous amuser avec les fonctions expérimentales (à activer dansOutils &gt; Options &gt; LibreOffice &gt; Avancé), vous pouvez jouer avec la nouvelle de boite de dialogue « Gestion des macros ».
Pour finir
Cette dépêche a, bien, évidemment, été rédigée avec LibreOffice et, cette fois-ci dans un fichier enregistré en Markdown. Les seules balises que j’ai dû entrer à la main sont celles des images. Kate a l’air de modifier le fichier et, quand je réouvre le .md dans LibreOffice, il y a des styles qui ont sauté mais la mise en forme reste visuellement la même. Kate rajoute aussi des barres obliques devant les « &gt; », aux crochets [ ] et même à certains hyperliens (images). Il y a peut-être des éditeurs de texte plus adaptés ou des réglages à faire.
J’ai rédigé cette dépêche en même temps qu’un article sur LibreOffice 26.2 pour mon site. Si l’article n’est pas vraiment dupliqué, il n’est pas étonnant d’y trouver des morceaux ici. Que tout cela ne nous empêche d’adresser tous nos remerciements à celles et ceux qui font de LibreOffice une suite bureautique si agréable à utiliser et si performante.
Post-scriptum : si vous voulez savoir modifier les couleurs de l’interface comme sur les captures d’écran, ça peut s’envisager, demandez gentiment, avec un peu de chance.
The Document Foundation ou TDF est la fondation de droit allemand qui pilote le projet LibreOffice. Il y a deux formats OOXML différents et donc deux formats XLSX différents, la version 2007 et la version actuelle depuis 2010. S’il vous est vraiment nécessaire d’enregistrer au format XLSX, il faut utiliser la version de 2010. Notes de version. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Fri, 13 Feb 2026 09:09:23 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</guid>
    </item>
    <item>
      <title><![CDATA[Projets Libres saison 4 épisode 11 : PVH éditions, une maison d'édition libérée et dans le Fediverse]]></title>
      <link>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</link>
      <description><![CDATA[Nous avons eu le plaisir de rencontrer Lionel Jeannerat durant les Rencontres Hivernales du libre à Saint-Cergue (VD) en janvier 2026. son parcours
la maison d'édition et ses œuvres
le passage au libre que ce soit pour les licences mais aussi pour leurs outils métiers
Bonne écoute ou lecture lien nᵒ 1 : Lien vers l'épisode
lien nᵒ 2 : S'abonner au podcast
lien nᵒ 3 : Le site de PVH éditions
lien nᵒ 4 : Soutenir le podcast
lien nᵒ 5 : L'épisode traduit en anglais
lien nᵒ 6 : Le site des Rencontres Hivernales du libre Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 07:40:57 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</guid>
    </item>
    <item>
      <title><![CDATA[Les journaux LinuxFr.org les mieux notés de janvier 2026]]></title>
      <link>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</link>
      <description><![CDATA[LinuxFr.org propose des dépêches et articles, soumis par tout un chacun, puis revus et corrigés par l’équipe de modération avant publication. C’est la partie la plus visible de LinuxFr.org, ce sont les dépêches qui sont le plus lues et suivies, sur le site, via Atom/RSS, ou bien via partage par messagerie instantanée, par courriel, ou encore via médias sociaux. Ce que l’on sait moins, c’est que LinuxFr.org vous propose également de publier directement vos propres articles, sans validation a priori de lʼéquipe de modération. Ceux-ci s’appellent des journaux. Voici un florilège d’une dizaine de ces journaux parmi les mieux notés par les utilisateurs et les utilisatrices… qui notent. Lumière sur ceux du mois de janvier passé.
« lecteur mp3 pour personne handicapée mentale » par ChocolatineFlying ;
« À la recherche du Linuxfrien type » par Ysabeau ;
« hacker sa pompe de relevage 3 et fin ! » par ChocolatineFlying ;
« [Hors sujet] Des tablettes lave-vaisselle tout-en-un » par Tanguy Ortolo ;
« Francis Hallé Bronsonisé » par Joris Dedieu ;
« 10 ans après, Modoboa est toujours là pour prendre soin de votre serveur de messagerie » par mirtouf ;
« À table ! » par JaguarWan ;
« Retour d'expérience sur le développement d'une application par l'utilisation d'IA » par phoenix ;
« Algoo lance un bulletin d'information mensuel « veille techno et logiciels libres » » par LeBouquetin ;
« Linux : les planètes s'alignent en 2026 » par vmagnin. lien nᵒ 1 : Participez à l’écriture d’un article
lien nᵒ 2 : Publiez votre journal
lien nᵒ 3 : Proposez une dépêche Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 09:23:50 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[Meilleures contributions LinuxFr.org : les primées de janvier 2026]]></title>
      <link>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</link>
      <description><![CDATA[Nous continuons sur notre lancée de récompenser celles et ceux qui chaque mois contribuent au site LinuxFr.org (dépêches, , logo, journaux, correctifs, etc.). Vous n’êtes pas sans risquer de gagner un livre des éditions Eyrolles, ENI et D-Booker. Voici les gagnants du mois de janvier 2026 :
Stefane Fermigier, pour sa dépêche « Appel à de la Commission "Vers des écosystèmes numériques ouverts européens" » ;
ChocolatineFlying, pour son journal « lecteur mp3 pour personne handicapé mental » ;
YvanM, pour sa dépêche « MeshCentral, alternative à TeamViewer et RustDesk » ;
Christophe Bliard, pour sa dépêche « Sortie de OpenProject 17.0 ».
Les livres gagnés sont détaillés en seconde partie de la dépêche. N’oubliez pas de contribuer, LinuxFr.org vit pour vous et par vous ! lien nᵒ 1 : Contribuez à LinuxFr.org !
lien nᵒ 2 : Tous les moyens (ou presque) de participer
lien nᵒ 3 : Récompenses précédentes (décembre 2025) Les livres sélectionnés
Linux — Maîtrisez l'administration du système — 7e édition. Certaines personnes n’ont pas pu être jointes ou n’ont pas répondu. Les lots ont été réattribués automatiquement. N’oubliez pas de mettre une adresse de courriel valable dans votre compte ou lors de la proposition d’une dépêche. En effet, c’est notre seul moyen de vous contacter, que ce soit pour les lots ou des questions sur votre dépêche lors de sa modération. Tous nos remerciements aux contributeurs du site ainsi qu’aux éditions Eyrolles, ENI et D-Booker. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 07:09:14 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[Continuous AI in practice: What developers can automate today with agentic CI]]></title>
      <link>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</link>
      <description><![CDATA[Think of Continuous AI as background agents that operate in your repository for tasks that require reasoning.]]></description>
      <pubDate>Thu, 05 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</guid>
    </item>
    <item>
      <title><![CDATA[Setting Docker Hardened Images free]]></title>
      <link>https://changelog.com/podcast/675</link>
      <description><![CDATA[In May of 2025, Docker launched Hardened Images, a secure, minimal, production-ready set of images. In December, they made DHI freely available and open source to everyone who builds software. On this episode, we're joined by Tushar Jain, EVP of Engineering at Docker to learn all about it.]]></description>
      <pubDate>Wed, 04 Feb 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/675</guid>
    </item>
    <item>
      <title><![CDATA[Pick your agent: Use Claude and Codex on Agent HQ]]></title>
      <link>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</link>
      <description><![CDATA[Claude by Anthropic and OpenAI Codex are now available in public preview on GitHub and VS Code with a Copilot Pro+ or Copilot Enterprise subscription. Here's what you need to know and how to get started today.]]></description>
      <pubDate>Wed, 04 Feb 2026 17:00:19 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</guid>
    </item>
    <item>
      <title><![CDATA[What the fastest-growing tools reveal about how software is being built]]></title>
      <link>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</link>
      <description><![CDATA[What languages are growing fastest, and why? What about the projects that people are interested in the most? Where are new developers cutting their teeth? Let’s take a look at Octoverse data to find out.]]></description>
      <pubDate>Tue, 03 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</guid>
    </item>
    <item>
      <title><![CDATA[The state of homelab tech (2026)]]></title>
      <link>https://changelog.com/friends/125</link>
      <description><![CDATA[Techno Tim joins Adam to dive deep into the state of homelab'ing in 2026. Hardware is scarce and expensive due to the AI gold rush, but software has never been better. From unleashing Claude on your UDM Pro to building custom Proxmox CLIs, they explores how AI is transforming what's possible in the homelab. Tim declares 2026 the "Year of Self-Hosted Software" while Adam reveals his homelab's secret weapons: DNSHole (a Pi-hole replacement written in Rust) and PXM (a Proxmox automation CLI).]]></description>
      <pubDate>Sat, 24 Jan 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/125</guid>
    </item>
    <item>
      <title><![CDATA[Agent psychosis: are we going insane?]]></title>
      <link>https://changelog.com/news/177</link>
      <description><![CDATA[Armin Ronacher thinks AI agent psychosis might be driving us insane, Dan Abramov explains how AT Protocol is a social filesystem, RepoBar keeps your GitHub work in view without opening a browser, Ethan McCue shares some life altering Postgres patterns, and Lea Verou says web dependencies are broken and we need to fix them.]]></description>
      <pubDate>Mon, 19 Jan 2026 20:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/news/177</guid>
    </item>
  </channel>
</rss>