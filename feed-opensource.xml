<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI-Pulse - Open Source & GitHub</title>
    <link>https://thephoenixagency.github.io/AI-Pulse</link>
    <description>Open Source & GitHub news from AI-Pulse</description>
    <language>en</language>
    <lastBuildDate>Sun, 22 Feb 2026 06:44:47 GMT</lastBuildDate>
    <atom:link href="https://thephoenixagency.github.io/AI-Pulse/feed-opensource.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title><![CDATA[vxcontrol/pentagi]]></title>
      <link>https://github.com/vxcontrol/pentagi</link>
      <description><![CDATA[Fully autonomous AI Agents system capable of performing complex penetration testing tasks PentAGI Penetration testing Artificial General Intelligence Join the Community! Connect with security researchers, AI enthusiasts, and fellow ethical hackers. Get support, share insights, and stay updated with the latest PentAGI developments. ⠀ Table of Contents Overview Features Quick Start API Access Advanced Setup Development Testing LLM Agents Embedding Configuration and Testing Function Testing with ftester Building Credits License Overview PentAGI is an innovative tool for automated security testing that leverages cutting-edge artificial intelligence technologies. The project is designed for information security professionals, researchers, and enthusiasts who need a powerful and flexible solution for conducting penetration tests. You can watch the video PentAGI overview: Features Secure &amp; Isolated. All operations are performed in a sandboxed Docker environment with complete isolation. Fully Autonomous. AI-powered agent that automatically determines and executes penetration testing steps. Professional Pentesting Tools. Built-in suite of 20+ professional security tools including nmap, metasploit, sqlmap, and more. Smart Memory System. Long-term storage of research results and successful approaches for future use. Knowledge Graph Integration. Graphiti-powered knowledge graph using Neo4j for semantic relationship tracking and advanced context understanding. Web Intelligence. Built-in browser via scraper for gathering latest information from web sources. External Search Systems. Integration with advanced search APIs including Tavily, Traversaal, Perplexity, DuckDuckGo, Google Custom Search, and Searxng for comprehensive information gathering. Team of Specialists. Delegation system with specialized AI agents for research, development, and infrastructure tasks. Comprehensive Monitoring. Detailed logging and integration with Grafana/Prometheus for real-time system observation. Detailed Reporting. Generation of thorough vulnerability reports with exploitation guides. Smart Container Management. Automatic Docker image selection based on specific task requirements. Modern Interface. Clean and intuitive web UI for system management and monitoring. Comprehensive APIs. Full-featured REST and GraphQL APIs with Bearer token authentication for automation and integration. Persistent Storage. All commands and outputs are stored in PostgreSQL with pgvector extension. Scalable Architecture. Microservices-based design supporting horizontal scaling. Self-Hosted Solution. Complete control over your deployment and data. Flexible Authentication. Support for various LLM providers (OpenAI, Anthropic, Ollama, AWS Bedrock, Google AI/Gemini, Deep Infra, OpenRouter, DeepSeek), Moonshot and custom configurations. API Token Authentication. Secure Bearer token system for programmatic access to REST and GraphQL APIs. Quick Deployment. Easy setup through Docker Compose with comprehensive environment configuration. Architecture System Context flowchart TB classDef person fill:#08427B,stroke:#073B6F,color:#fff classDef system fill:#1168BD,stroke:#0B4884,color:#fff classDef external fill:#666666,stroke:#0B4884,color:#fff pentester[" Security Engineer (User of the system)"] pentagi[" PentAGI (Autonomous penetration testing system)"] target[" target-system (System under test)"] llm[" llm-provider (OpenAI/Anthropic/Ollama/Bedrock/Gemini/Custom)"] search[" search-systems (Google/DuckDuckGo/Tavily/Traversaal/Perplexity/Searxng)"] langfuse[" langfuse-ui (LLM Observability Dashboard)"] grafana[" grafana (System Monitoring Dashboard)"] pentester --&gt; |Uses HTTPS| pentagi pentester --&gt; |Monitors AI HTTPS| langfuse pentester --&gt; |Monitors System HTTPS| grafana pentagi --&gt; |Tests Various protocols| target pentagi --&gt; |Queries HTTPS| llm pentagi --&gt; |Searches HTTPS| search pentagi --&gt; |Reports HTTPS| langfuse pentagi --&gt; |Reports HTTPS| grafana class pentester person class pentagi system class target,llm,search,langfuse,grafana external linkStyle default stroke:#ffffff,color:#ffffff Container Architecture (click to expand) graph TB subgraph Core Services UI[Frontend UIReact + TypeScript] API[Backend APIGo + GraphQL] DB[(Vector StorePostgreSQL + pgvector)] MQ[Task QueueAsync Processing] Agent[AI AgentsMulti-Agent System] end subgraph Knowledge Graph Graphiti[GraphitiKnowledge Graph API] Neo4j[(Neo4jGraph Database)] end subgraph Monitoring Grafana[GrafanaDashboards] VictoriaMetrics[VictoriaMetricsTime-series DB] Jaeger[JaegerDistributed Tracing] Loki[LokiLog Aggregation] OTEL[OpenTelemetryData Collection] end subgraph Analytics Langfuse[LangfuseLLM Analytics] ClickHouse[ClickHouseAnalytics DB] Redis[RedisCache + Rate Limiter] MinIO[MinIOS3 Storage] end subgraph Security Tools Scraper[Web ScraperIsolated Browser] PenTest[Security Tools20+ Pro ToolsSandboxed Execution] end UI --&gt; |HTTP/WS| API API --&gt; |SQL| DB API --&gt; |Events| MQ MQ --&gt; |Tasks| Agent Agent --&gt; |Commands| PenTest Agent --&gt; |Queries| DB Agent --&gt; |Knowledge| Graphiti Graphiti --&gt; |Graph| Neo4j API --&gt; |Telemetry| OTEL OTEL --&gt; |Metrics| VictoriaMetrics OTEL --&gt; |Traces| Jaeger OTEL --&gt; |Logs| Loki Grafana --&gt; |Query| VictoriaMetrics Grafana --&gt; |Query| Jaeger Grafana --&gt; |Query| Loki API --&gt; |Analytics| Langfuse Langfuse --&gt; |Store| ClickHouse Langfuse --&gt; |Cache| Redis Langfuse --&gt; |Files| MinIO classDef core fill:#f9f,stroke:#333,stroke-width:2px,color:#000 classDef knowledge fill:#ffa,stroke:#333,stroke-width:2px,color:#000 classDef monitoring fill:#bbf,stroke:#333,stroke-width:2px,color:#000 classDef analytics fill:#bfb,stroke:#333,stroke-width:2px,color:#000 classDef tools fill:#fbb,stroke:#333,stroke-width:2px,color:#000 class UI,API,DB,MQ,Agent core class Graphiti,Neo4j knowledge class Grafana,VictoriaMetrics,Jaeger,Loki,OTEL monitoring class Langfuse,ClickHouse,Redis,MinIO analytics class Scraper,PenTest tools Entity Relationship (click to expand) erDiagram Flow ||--o{ Task : contains Task ||--o{ SubTask : contains SubTask ||--o{ Action : contains Action ||--o{ Artifact : produces Action ||--o{ Memory : stores Flow { string id PK string name "Flow name" string description "Flow description" string status "active/completed/failed" json parameters "Flow parameters" timestamp created_at timestamp updated_at } Task { string id PK string flow_id FK string name "Task name" string description "Task description" string status "pending/running/done/failed" json result "Task results" timestamp created_at timestamp updated_at } SubTask { string id PK string task_id FK string name "Subtask name" string description "Subtask description" string status "queued/running/completed/failed" string agent_type "researcher/developer/executor" json context "Agent context" timestamp created_at timestamp updated_at } Action { string id PK string subtask_id FK string type "command/search/analyze/etc" string status "success/failure" json parameters "Action parameters" json result "Action results" timestamp created_at } Artifact { string id PK string action_id FK string type "file/report/log" string path "Storage path" json metadata "Additional info" timestamp created_at } Memory { string id PK string action_id FK string type "observation/conclusion" vector embedding "Vector representation" text content "Memory content" timestamp created_at } Agent Interaction (click to expand) sequenceDiagram participant O as Orchestrator participant R as Researcher participant D as Developer participant E as Executor participant VS as Vector Store participant KB as Knowledge Base Note over O,KB: Flow Initialization O-&gt;&gt;VS: Query similar tasks VS--&gt;&gt;O: Return experiences O-&gt;&gt;KB: Load relevant knowledge KB--&gt;&gt;O: Return context Note over O,R: Research Phase O-&gt;&gt;R: Analyze target R-&gt;&gt;VS: Search similar cases VS--&gt;&gt;R: Return patterns R-&gt;&gt;KB: Query vulnerabilities KB--&gt;&gt;R: Return known issues R-&gt;&gt;VS: Store findings R--&gt;&gt;O: Research results Note over O,D: Planning Phase O-&gt;&gt;D: Plan attack D-&gt;&gt;VS: Query exploits VS--&gt;&gt;D: Return techniques D-&gt;&gt;KB: Load tools info KB--&gt;&gt;D: Return capabilities D--&gt;&gt;O: Attack plan Note over O,E: Execution Phase O-&gt;&gt;E: Execute plan E-&gt;&gt;KB: Load tool guides KB--&gt;&gt;E: Return procedures E-&gt;&gt;VS: Store results E--&gt;&gt;O: Execution status Memory System (click to expand) graph TB subgraph "Long-term Memory" VS[(Vector StoreEmbeddings DB)] KB[Knowledge BaseDomain Expertise] Tools[Tools KnowledgeUsage Patterns] end subgraph "Working Memory" Context[Current ContextTask State] Goals[Active GoalsObjectives] State[System StateResources] end subgraph "Episodic Memory" Actions[Past ActionsCommands History] Results[Action ResultsOutcomes] Patterns[Success PatternsBest Practices] end Context --&gt; |Query| VS VS --&gt; |Retrieve| Context Goals --&gt; |Consult| KB KB --&gt; |Guide| Goals State --&gt; |Record| Actions Actions --&gt; |Learn| Patterns Patterns --&gt; |Store| VS Tools --&gt; |Inform| State Results --&gt; |Update| Tools VS --&gt; |Enhance| KB KB --&gt; |Index| VS classDef ltm fill:#f9f,stroke:#333,stroke-width:2px,color:#000 classDef wm fill:#bbf,stroke:#333,stroke-width:2px,color:#000 classDef em fill:#bfb,stroke:#333,stroke-width:2px,color:#000 class VS,KB,Tools ltm class Context,Goals,State wm class Actions,Results,Patterns em Chain Summarization (click to expand) The chain summarization system manages conversation context growth by selectively summarizing older messages. This is critical for preventing token limits from being exceeded while maintaining conversation coherence. flowchart TD A[Input Chain] --&gt; B{Needs Summarization?} B --&gt;|No| C[Return Original Chain] B --&gt;|Yes| D[Convert to ChainAST] D --&gt; E[Apply Section Summarization] E --&gt; F[Process Oversized Pairs] F --&gt; G[Manage Last Section Size] G --&gt; H[Apply QA Summarization] H --&gt; I[Rebuild Chain with Summaries] I --&gt; J{Is New Chain Smaller?} J --&gt;|Yes| K[Return Optimized Chain] J --&gt;|No| C classDef process fill:#bbf,stroke:#333,stroke-width:2px,color:#000 classDef decision fill:#bfb,stroke:#333,stroke-width:2px,color:#000 classDef output fill:#fbb,stroke:#333,stroke-width:2px,color:#000 class A,D,E,F,G,H,I process class B,J decision class C,K output The algorithm operates on a structured representation of conversation chains (ChainAST) that preserves message types including tool calls and their responses. All summarization operations maintain critical conversation flow while reducing context size. Global Summarizer Configuration Options Parameter Environment Variable Default Description Preserve Last SUMMARIZER_PRESERVE_LAST true Whether to keep all messages in the last section intact Use QA Pairs SUMMARIZER_USE_QA true Whether to use QA pair summarization strategy Summarize Human in QA SUMMARIZER_SUM_MSG_HUMAN_IN_QA false Whether to summarize human messages in QA pairs Last Section Size SUMMARIZER_LAST_SEC_BYTES 51200 Maximum byte size for last section (50KB) Max Body Pair Size SUMMARIZER_MAX_BP_BYTES 16384 Maximum byte size for a single body pair (16KB) Max QA Sections SUMMARIZER_MAX_QA_SECTIONS 10 Maximum QA pair sections to preserve Max QA Size SUMMARIZER_MAX_QA_BYTES 65536 Maximum byte size for QA pair sections (64KB) Keep QA Sections SUMMARIZER_KEEP_QA_SECTIONS 1 Number of recent QA sections to keep without summarization Assistant Summarizer Configuration Options Assistant instances can use customized summarization settings to fine-tune context management behavior: Parameter Environment Variable Default Description Preserve Last ASSISTANT_SUMMARIZER_PRESERVE_LAST true Whether to preserve all messages in the assistant's last section Last Section Size ASSISTANT_SUMMARIZER_LAST_SEC_BYTES 76800 Maximum byte size for assistant's last section (75KB) Max Body Pair Size ASSISTANT_SUMMARIZER_MAX_BP_BYTES 16384 Maximum byte size for a single body pair in assistant context (16KB) Max QA Sections ASSISTANT_SUMMARIZER_MAX_QA_SECTIONS 7 Maximum QA sections to preserve in assistant context Max QA Size ASSISTANT_SUMMARIZER_MAX_QA_BYTES 76800 Maximum byte size for assistant's QA sections (75KB) Keep QA Sections ASSISTANT_SUMMARIZER_KEEP_QA_SECTIONS 3 Number of recent QA sections to preserve without summarization The assistant summarizer configuration provides more memory for context retention compared to the global settings, preserving more recent conversation history while still ensuring efficient token usage. Summarizer Environment Configuration # Default values for global summarizer logic
SUMMARIZER_PRESERVE_LAST=true
SUMMARIZER_USE_QA=true
SUMMARIZER_SUM_MSG_HUMAN_IN_QA=false
SUMMARIZER_LAST_SEC_BYTES=51200
SUMMARIZER_MAX_BP_BYTES=16384
SUMMARIZER_MAX_QA_SECTIONS=10
SUMMARIZER_MAX_QA_BYTES=65536
SUMMARIZER_KEEP_QA_SECTIONS=1 # Default values for assistant summarizer logic
ASSISTANT_SUMMARIZER_PRESERVE_LAST=true
ASSISTANT_SUMMARIZER_LAST_SEC_BYTES=76800
ASSISTANT_SUMMARIZER_MAX_BP_BYTES=16384
ASSISTANT_SUMMARIZER_MAX_QA_SECTIONS=7
ASSISTANT_SUMMARIZER_MAX_QA_BYTES=76800
ASSISTANT_SUMMARIZER_KEEP_QA_SECTIONS=3 The architecture of PentAGI is designed to be modular, scalable, and secure. Here are the key components: Core Services Frontend UI: React-based web interface with TypeScript for type safety Backend API: Go-based REST and GraphQL APIs with Bearer token authentication for programmatic access Vector Store: PostgreSQL with pgvector for semantic search and memory storage Task Queue: Async task processing system for reliable operation AI Agent: Multi-agent system with specialized roles for efficient testing Knowledge Graph Graphiti: Knowledge graph API for semantic relationship tracking and contextual understanding Neo4j: Graph database for storing and querying relationships between entities, actions, and outcomes Automatic capturing of agent responses and tool executions for building comprehensive knowledge base Monitoring Stack OpenTelemetry: Unified observability data collection and correlation Grafana: Real-time visualization and alerting dashboards VictoriaMetrics: High-performance time-series metrics storage Jaeger: End-to-end distributed tracing for debugging Loki: Scalable log aggregation and analysis Analytics Platform Langfuse: Advanced LLM observability and performance analytics ClickHouse: Column-oriented analytics data warehouse Redis: High-speed caching and rate limiting MinIO: S3-compatible object storage for artifacts Security Tools Web Scraper: Isolated browser environment for safe web interaction Pentesting Tools: Comprehensive suite of 20+ professional security tools Sandboxed Execution: All operations run in isolated containers Memory Systems Long-term Memory: Persistent storage of knowledge and experiences Working Memory: Active context and goals for current operations Episodic Memory: Historical actions and success patterns Knowledge Base: Structured domain expertise and tool capabilities Context Management: Intelligently manages growing LLM context windows using chain summarization The system uses Docker containers for isolation and easy deployment, with separate networks for core services, monitoring, and analytics to ensure proper security boundaries. Each component is designed to scale horizontally and can be configured for high availability in production environments. Quick Start System Requirements Docker and Docker Compose Minimum 2 vCPU Minimum 4GB RAM 20GB free disk space Internet access for downloading images and updates Using Installer ( ) PentAGI provides an interactive installer with a terminal-based UI for streamlined configuration and deployment. The installer guides you through system checks, LLM provider setup, search engine configuration, and security hardening. Supported Platforms: Linux: amd64 download | arm64 download Windows: amd64 download macOS: amd64 (Intel) download | arm64 (M-series) download Quick Installation (Linux amd64): # Create installation directory
mkdir -p pentagi &amp;&amp; cd pentagi # Download installer
wget -O installer.zip https://pentagi.com/downloads/linux/amd64/installer-latest.zip # Extract
unzip installer.zip # Run interactive installer
./installer Prerequisites &amp; Permissions: The installer requires appropriate privileges to interact with the Docker API for proper operation. By default, it uses the Docker socket (/var/run/docker.sock) which requires either: Option 1 ( for production): Run the installer as root: sudo ./installer Option 2 (Development environments): Grant your user access to the Docker socket by adding them to the docker group: # Add your user to the docker group
sudo usermod -aG docker $USER # Log out and log back in, or activate the group immediately
newgrp docker # Verify Docker access (should run without sudo)
docker ps Security Note: Adding a user to the docker group grants root-equivalent privileges. Only do this for trusted users in controlled environments. For production deployments, consider using rootless Docker mode or running the installer with sudo. The installer will: System Checks: Verify Docker, network connectivity, and system requirements Environment Setup: Create and configure .env file with optimal defaults Provider Configuration: Set up LLM providers (OpenAI, Anthropic, Gemini, Bedrock, Ollama, Custom) Search Engines: Configure DuckDuckGo, Google, Tavily, Traversaal, Perplexity, Searxng Security Hardening: Generate secure credentials and configure SSL certificates Deployment: Start PentAGI with docker-compose For Production &amp; Enhanced Security: For production deployments or security-sensitive environments, we strongly recommend using a distributed two-node architecture where worker operations are isolated on a separate server. This prevents untrusted code execution and network access issues on your main system. See detailed guide: Worker Node Setup The two-node setup provides: Isolated Execution: Worker containers run on dedicated hardware Network Isolation: Separate network boundaries for penetration testing Security Boundaries: Docker-in-Docker with TLS authentication OOB Attack Support: Dedicated port ranges for out-of-band techniques Manual Installation Create a working directory or clone the repository: mkdir pentagi &amp;&amp; cd pentagi Copy .env.example to .env or download it: curl -o .env https://raw.githubusercontent.com/vxcontrol/pentagi/master/.env.example Touch examples files (example.custom.provider.yml, example.ollama.provider.yml) or download it: curl -o example.custom.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/custom-openai.provider.yml
curl -o example.ollama.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/ollama-llama318b.provider.yml Fill in the required API keys in .env file. # Required: At least one of these LLM providers
OPEN_AI_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GEMINI_API_KEY=your_gemini_key # Optional: AWS Bedrock provider (enterprise-grade models)
BEDROCK_REGION=us-east-1
BEDROCK_ACCESS_KEY_ID=your_aws_access_key
BEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key # Optional: Local LLM provider (zero-cost inference)
OLLAMA_SERVER_URL=http://localhost:11434
OLLAMA_SERVER_MODEL=your_model_name # Optional: Additional search capabilities
DUCKDUCKGO_ENABLED=true
GOOGLE_API_KEY=your_google_key
GOOGLE_CX_KEY=your_google_cx
TAVILY_API_KEY=your_tavily_key
TRAVERSAAL_API_KEY=your_traversaal_key
PERPLEXITY_API_KEY=your_perplexity_key
PERPLEXITY_MODEL=sonar-pro
PERPLEXITY_CONTEXT_SIZE=medium # Searxng meta search engine (aggregates results from multiple sources)
SEARXNG_URL=http://your-searxng-instance:8080
SEARXNG_CATEGORIES=general
SEARXNG_LANGUAGE=
SEARXNG_SAFESEARCH=0
SEARXNG_TIME_RANGE= ## Graphiti knowledge graph settings
GRAPHITI_ENABLED=true
GRAPHITI_TIMEOUT=30
GRAPHITI_URL=http://graphiti:8000
GRAPHITI_MODEL_NAME=gpt-5-mini # Neo4j settings (used by Graphiti stack)
NEO4J_USER=neo4j
NEO4J_DATABASE=neo4j
NEO4J_PASSWORD=devpassword
NEO4J_URI=bolt://neo4j:7687 # Assistant configuration
ASSISTANT_USE_AGENTS=false # Default value for agent usage when creating new assistants Change all security related environment variables in .env file to improve security. Security related environment variables Main Security Settings COOKIE_SIGNING_SALT - Salt for cookie signing, change to random value PUBLIC_URL - Public URL of your server (eg. https://pentagi.example.com) SERVER_SSL_CRT and SERVER_SSL_KEY - Custom paths to your existing SSL certificate and key for HTTPS (these paths should be used in the docker-compose.yml file to mount as volumes) Scraper Access SCRAPER_PUBLIC_URL - Public URL for scraper if you want to use different scraper server for public URLs SCRAPER_PRIVATE_URL - Private URL for scraper (local scraper server in docker-compose.yml file to access it to local URLs) Access Credentials PENTAGI_POSTGRES_USER and PENTAGI_POSTGRES_PASSWORD - PostgreSQL credentials NEO4J_USER and NEO4J_PASSWORD - Neo4j credentials (for Graphiti knowledge graph) Remove all inline from .env file if you want to use it in VSCode or other IDEs as a envFile option: perl -i -pe 's/\s+#.*$//' .env Run the PentAGI stack: curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose.yml
docker compose up -d Visit localhost:8443 to access PentAGI Web UI (default is admin@pentagi.com / admin) [!NOTE] If you caught an error about pentagi-network or observability-network or langfuse-network you need to run docker-compose.yml firstly to create these networks and after that run docker-compose-langfuse.yml, docker-compose-graphiti.yml, and docker-compose-observability.yml to use Langfuse, Graphiti, and Observability services. You have to set at least one Language Model provider (OpenAI, Anthropic, Gemini, AWS Bedrock, or Ollama) to use PentAGI. AWS Bedrock provides enterprise-grade access to multiple foundation models from leading AI companies, while Ollama provides zero-cost local inference if you have sufficient computational resources. Additional API keys for search engines are optional but for better results. LLM_SERVER_* environment variables are experimental feature and will be changed in the future. Right now you can use them to specify custom LLM server URL and one model for all agent types. PROXY_URL is a global proxy URL for all LLM providers and external search systems. You can use it for isolation from external networks. The docker-compose.yml file runs the PentAGI service as root user because it needs access to docker.sock for container management. If you're using TCP/IP network connection to Docker instead of socket file, you can remove root privileges and use the default pentagi user for better security. Assistant Configuration PentAGI allows you to configure default behavior for assistants: Variable Default Description ASSISTANT_USE_AGENTS false Controls the default value for agent usage when creating new assistants The ASSISTANT_USE_AGENTS setting affects the initial state of the "Use Agents" toggle when creating a new assistant in the UI: false (default): New assistants are created with agent delegation disabled by default true: New assistants are created with agent delegation enabled by default Note that users can always override this setting by toggling the "Use Agents" button in the UI when creating or editing an assistant. This environment variable only controls the initial default state. API Access PentAGI provides comprehensive programmatic access through both REST and GraphQL APIs, allowing you to integrate penetration testing workflows into your automation pipelines, CI/CD processes, and custom applications. Generating API Tokens API tokens are managed through the PentAGI web interface: Navigate to Settings → API Tokens in the web UI Click Create Token to generate a new API token Configure token properties: Name (optional): A descriptive name for the token Expiration Date: When the token will expire (minimum 1 minute, maximum 3 years) Click Create and copy the token immediately - it will only be shown once for security reasons Use the token as a Bearer token in your API requests Each token is associated with your user account and inherits your role's permissions. Using API Tokens Include the API token in the Authorization header of your HTTP requests: # GraphQL API example
curl -X POST https://your-pentagi-instance:8443/api/v1/graphql \ -H "Authorization: Bearer YOUR_API_TOKEN" \ -H "Content-Type: application/json" \ -d '{"query": "{ flows { id title status } }"}' # REST API example
curl https://your-pentagi-instance:8443/api/v1/flows \ -H "Authorization: Bearer YOUR_API_TOKEN" API Exploration and Testing PentAGI provides interactive documentation for exploring and testing API endpoints: GraphQL Playground Access the GraphQL Playground at https://your-pentagi-instance:8443/api/v1/graphql/playground Click the HTTP Headers tab at the bottom Add your authorization header: { "Authorization": "Bearer YOUR_API_TOKEN"
} Explore the schema, run queries, and test mutations interactively Swagger UI Access the REST API documentation at https://your-pentagi-instance:8443/api/v1/swagger/index.html Click the Authorize button Enter your token in the format: Bearer YOUR_API_TOKEN Click Authorize to apply Test endpoints directly from the Swagger UI Generating API Clients You can generate type-safe API clients for your preferred programming language using the schema files included with PentAGI: GraphQL Clients The GraphQL schema is available at: Web UI: Navigate to Settings to download schema.graphqls Direct file: backend/pkg/graph/schema.graphqls in the repository Generate clients using tools like: GraphQL Code Generator (JavaScript/TypeScript): https://the-guild.dev/graphql/codegen genqlient (Go): https://github.com/Khan/genqlient Apollo iOS (Swift): https://www.apollographql.com/docs/ios REST API Clients The OpenAPI specification is available at: Swagger JSON: https://your-pentagi-instance:8443/api/v1/swagger/doc.json Swagger YAML: Available in backend/pkg/server/docs/swagger.yaml Generate clients using: OpenAPI Generator: https://openapi-generator.tech openapi-generator-cli generate \ -i https://your-pentagi-instance:8443/api/v1/swagger/doc.json \ -g python \ -o ./pentagi-client Swagger Codegen: https://github.com/swagger-api/swagger-codegen swagger-codegen generate \ -i https://your-pentagi-instance:8443/api/v1/swagger/doc.json \ -l typescript-axios \ -o ./pentagi-client swagger-typescript-api (TypeScript): https://github.com/acacode/swagger-typescript-api npx swagger-typescript-api \ -p https://your-pentagi-instance:8443/api/v1/swagger/doc.json \ -o ./src/api \ -n pentagi-api.ts API Usage Examples Creating a New Flow (GraphQL) mutation CreateFlow { createFlow( modelProvider: "openai" input: "Test the security of https://example.com" ) { id title status createdAt }
} Listing Flows (REST API) curl https://your-pentagi-instance:8443/api/v1/flows \ -H "Authorization: Bearer YOUR_API_TOKEN" \ | jq '.flows[] | {id, title, status}' Python Client Example import requests class PentAGIClient: def __init__(self, base_url, api_token): self.base_url = base_url self.headers = { "Authorization": f"Bearer {api_token}", "Content-Type": "application/json" } def create_flow(self, provider, target): query = """ mutation CreateFlow($provider: String!, $input: String!) { createFlow(modelProvider: $provider, input: $input) { id title status } } """ response = requests.post( f"{self.base_url}/api/v1/graphql", json={ "query": query, "variables": { "provider": provider, "input": target } }, headers=self.headers ) return response.json() def get_flows(self): response = requests.get( f"{self.base_url}/api/v1/flows", headers=self.headers ) return response.json() # Usage
client = PentAGIClient( "https://your-pentagi-instance:8443", "your_api_token_here"
) # Create a new flow
flow = client.create_flow("openai", "Scan https://example.com for vulnerabilities")
print(f"Created flow: {flow}") # List all flows
flows = client.get_flows()
print(f"Total flows: {len(flows['flows'])}") TypeScript Client Example import axios, { AxiosInstance } from 'axios'; interface Flow { id: string; title: string; status: string; createdAt: string;
} class PentAGIClient { private client: AxiosInstance; constructor(baseURL: string, apiToken: string) { this.client = axios.create({ baseURL: `${baseURL}/api/v1`, headers: { 'Authorization': `Bearer ${apiToken}`, 'Content-Type': 'application/json', }, }); } async createFlow(provider: string, input: string): Promise { const query = ` mutation CreateFlow($provider: String!, $input: String!) { createFlow(modelProvider: $provider, input: $input) { id title status createdAt } } `; const response = await this.client.post('/graphql', { query, variables: { provider, input }, }); return response.data.data.createFlow; } async getFlows(): Promise { const response = await this.client.get('/flows'); return response.data.flows; } async getFlow(flowId: string): Promise { const response = await this.client.get(`/flows/${flowId}`); return response.data; }
} // Usage
const client = new PentAGIClient( 'https://your-pentagi-instance:8443', 'your_api_token_here'
); // Create a new flow
const flow = await client.createFlow( 'openai', 'Perform penetration test on https://example.com'
);
console.log('Created flow:', flow); // List all flows
const flows = await client.getFlows();
console.log(`Total flows: ${flows.length}`); Security Best Practices When working with API tokens: Never commit tokens to version control - use environment variables or secrets management Rotate tokens regularly - set appropriate expiration dates and create new tokens periodically Use separate tokens for different applications - makes it easier to revoke access if needed Monitor token usage - review API token activity in the Settings page Revoke unused tokens - disable or delete tokens that are no longer needed Use HTTPS only - never send API tokens over unencrypted connections Token Management View tokens: See all your active tokens in Settings → API Tokens Edit tokens: Update token names or revoke tokens Delete tokens: Permanently remove tokens (this action cannot be undone) Token ID: Each token has a unique ID that can be copied for reference The token list shows: Token name (if provided) Token ID (unique identifier) Status (active/revoked/expired) Creation date Expiration date Custom LLM Provider Configuration When using custom LLM providers with the LLM_SERVER_* variables, you can fine-tune the reasoning format used in requests: Variable Default Description LLM_SERVER_URL Base URL for the custom LLM API endpoint LLM_SERVER_KEY API key for the custom LLM provider LLM_SERVER_MODEL Default model to use (can be overridden in provider config) LLM_SERVER_CONFIG_PATH Path to the YAML configuration file for agent-specific models LLM_SERVER_PROVIDER Provider name prefix for model names (e.g., openrouter, deepseek for LiteLLM proxy) LLM_SERVER_LEGACY_REASONING false Controls reasoning format in API requests LLM_SERVER_PRESERVE_REASONING false Preserve reasoning content in multi-turn conversations (required by some providers) The LLM_SERVER_PROVIDER setting is particularly useful when using LiteLLM proxy, which adds a provider prefix to model names. For example, when connecting to Moonshot API through LiteLLM, models like kimi-2.5 become moonshot/kimi-2.5. By setting LLM_SERVER_PROVIDER=moonshot, you can use the same provider configuration file for both direct API access and LiteLLM proxy access without modifications. The LLM_SERVER_LEGACY_REASONING setting affects how reasoning parameters are sent to the LLM: false (default): Uses modern format where reasoning is sent as a structured object with max_tokens parameter true: Uses legacy format with string-based reasoning_effort parameter This setting is important when working with different LLM providers as they may expect different reasoning formats in their API requests. If you encounter reasoning-related errors with custom providers, try changing this setting. The LLM_SERVER_PRESERVE_REASONING setting controls whether reasoning content is preserved in multi-turn conversations: false (default): Reasoning content is not preserved in conversation history true: Reasoning content is preserved and sent in subsequent API calls This setting is required by some LLM providers (e.g., Moonshot) that return errors like "thinking is enabled but reasoning_content is missing in assistant tool call message" when reasoning content is not included in multi-turn conversations. Enable this setting if your provider requires reasoning content to be preserved. Local LLM Provider Configuration PentAGI supports Ollama for local LLM inference, providing zero-cost operation and enhanced privacy: Variable Default Description OLLAMA_SERVER_URL URL of your Ollama server OLLAMA_SERVER_MODEL llama3.1:8b-instruct-q8_0 Default model for inference OLLAMA_SERVER_CONFIG_PATH Path to custom agent configuration file OLLAMA_SERVER_PULL_MODELS_TIMEOUT 600 Timeout for model downloads (seconds) OLLAMA_SERVER_PULL_MODELS_ENABLED false Auto-download models on startup OLLAMA_SERVER_LOAD_MODELS_ENABLED false Query server for available models Configuration examples: # Basic Ollama setup with default model
OLLAMA_SERVER_URL=http://localhost:11434
OLLAMA_SERVER_MODEL=llama3.1:8b-instruct-q8_0 # Production setup with auto-pull and model discovery
OLLAMA_SERVER_URL=http://ollama-server:11434
OLLAMA_SERVER_PULL_MODELS_ENABLED=true
OLLAMA_SERVER_PULL_MODELS_TIMEOUT=900
OLLAMA_SERVER_LOAD_MODELS_ENABLED=true # Custom configuration with agent-specific models
OLLAMA_SERVER_CONFIG_PATH=/path/to/ollama-config.yml # Default configuration file inside docker container
OLLAMA_SERVER_CONFIG_PATH=/opt/pentagi/conf/ollama-llama318b.provider.yml Performance Considerations: Model Discovery (OLLAMA_SERVER_LOAD_MODELS_ENABLED=true): Adds 1-2s startup latency querying Ollama API Auto-pull (OLLAMA_SERVER_PULL_MODELS_ENABLED=true): First startup may take several minutes downloading models Pull timeout (OLLAMA_SERVER_PULL_MODELS_TIMEOUT=900): 15 minutes in seconds Static Config: Disable both flags and specify models in config file for fastest startup Creating Custom Ollama Models with Extended Context PentAGI requires models with larger context windows than the default Ollama configurations. You need to create custom models with increased num_ctx parameter through Modelfiles. While typical agent workflows consume around 64K tokens, PentAGI uses 110K context size for safety margin and handling complex penetration testing scenarios. Important: The num_ctx parameter can only be set during model creation via Modelfile - it cannot be changed after model creation or overridden at runtime. Example: Qwen3 32B FP16 with Extended Context Create a Modelfile named Modelfile_qwen3_32b_fp16_tc: FROM qwen3:32b-fp16
PARAMETER num_ctx 110000
PARAMETER temperature 0.3
PARAMETER top_p 0.8
PARAMETER min_p 0.0
PARAMETER top_k 20
PARAMETER repeat_penalty 1.1 Build the custom model: ollama create qwen3:32b-fp16-tc -f Modelfile_qwen3_32b_fp16_tc Example: QwQ 32B FP16 with Extended Context Create a Modelfile named Modelfile_qwq_32b_fp16_tc: FROM qwq:32b-fp16
PARAMETER num_ctx 110000
PARAMETER temperature 0.2
PARAMETER top_p 0.7
PARAMETER min_p 0.0
PARAMETER top_k 40
PARAMETER repeat_penalty 1.2 Build the custom model: ollama create qwq:32b-fp16-tc -f Modelfile_qwq_32b_fp16_tc Note: The QwQ 32B FP16 model requires approximately 71.3 GB VRAM for inference. Ensure your system has sufficient GPU memory before attempting to use this model. These custom models are referenced in the pre-built provider configuration files (ollama-qwen332b-fp16-tc.provider.yml and ollama-qwq32b-fp16-tc.provider.yml) that are included in the Docker image at /opt/pentagi/conf/. OpenAI Provider Configuration PentAGI supports OpenAI's advanced language models, including the latest reasoning-capable o-series models designed for complex analytical tasks: Variable Default Description OPEN_AI_KEY API key for OpenAI services OPEN_AI_SERVER_URL https://api.openai.com/v1 OpenAI API endpoint Configuration examples: # Basic OpenAI setup
OPEN_AI_KEY=your_openai_api_key
OPEN_AI_SERVER_URL=https://api.openai.com/v1 # Using with proxy for enhanced security
OPEN_AI_KEY=your_openai_api_key
PROXY_URL=http://your-proxy:8080 The OpenAI provider offers cutting-edge capabilities including: Reasoning Models: Advanced o-series models (o1, o3, o4-mini) with step-by-step analytical thinking Latest GPT-4.1 Series: Flagship models optimized for complex security research and exploit development Cost-Effective Options: From nano models for high-volume scanning to powerful reasoning models for deep analysis Versatile Performance: Fast, intelligent models perfect for multi-step security analysis and penetration testing Proven Reliability: Industry-leading models with consistent performance across diverse security scenarios The system automatically selects appropriate OpenAI models based on task complexity, optimizing for both performance and cost-effectiveness. Anthropic Provider Configuration PentAGI integrates with Anthropic's Claude models, known for their exceptional safety, reasoning capabilities, and sophisticated understanding of complex security contexts: Variable Default Description ANTHROPIC_API_KEY API key for Anthropic services ANTHROPIC_SERVER_URL https://api.anthropic.com/v1 Anthropic API endpoint Configuration examples: # Basic Anthropic setup
ANTHROPIC_API_KEY=your_anthropic_api_key
ANTHROPIC_SERVER_URL=https://api.anthropic.com/v1 # Using with proxy for secure environments
ANTHROPIC_API_KEY=your_anthropic_api_key
PROXY_URL=http://your-proxy:8080 The Anthropic provider delivers superior capabilities including: Advanced Reasoning: Claude 4 series with exceptional reasoning for sophisticated penetration testing Extended Thinking: Claude 3.7 with step-by-step thinking capabilities for methodical security research High-Speed Performance: Claude 3.5 Haiku for blazing-fast vulnerability scans and real-time monitoring Comprehensive Analysis: Claude Sonnet models for complex security analysis and threat hunting Safety-First Design: Built-in safety mechanisms ensuring responsible security testing practices The system leverages Claude's advanced understanding of security contexts to provide thorough and responsible penetration testing guidance. Google AI (Gemini) Provider Configuration PentAGI supports Google's Gemini models through the Google AI API, offering state-of-the-art reasoning capabilities and multimodal features: Variable Default Description GEMINI_API_KEY API key for Google AI services GEMINI_SERVER_URL https://generativelanguage.googleapis.com Google AI API endpoint Configuration examples: # Basic Gemini setup
GEMINI_API_KEY=your_gemini_api_key
GEMINI_SERVER_URL=https://generativelanguage.googleapis.com # Using with proxy
GEMINI_API_KEY=your_gemini_api_key
PROXY_URL=http://your-proxy:8080 The Gemini provider offers advanced features including: Thinking Capabilities: Advanced reasoning models (Gemini 2.5 series) with step-by-step analysis Multimodal Support: Text and image processing for comprehensive security assessments Large Context Windows: Up to 2M tokens for analyzing extensive codebases and documentation Cost-Effective Options: From high-performance pro models to economical flash variants Security-Focused Models: Specialized configurations optimized for penetration testing workflows The system automatically selects appropriate Gemini models based on agent requirements, balancing performance, capabilities, and cost-effectiveness. AWS Bedrock Provider Configuration PentAGI integrates with Amazon Bedrock, offering access to a wide range of foundation models from leading AI companies including Anthropic, AI21, Cohere, Meta, and Amazon's own models: Variable Default Description BEDROCK_REGION us-east-1 AWS region for Bedrock service BEDROCK_ACCESS_KEY_ID AWS access key ID for authentication BEDROCK_SECRET_ACCESS_KEY AWS secret access key for authentication BEDROCK_SESSION_TOKEN AWS session token as alternative way for authentication BEDROCK_SERVER_URL Optional custom Bedrock endpoint URL Configuration examples: # Basic AWS Bedrock setup with credentials
BEDROCK_REGION=us-east-1
BEDROCK_ACCESS_KEY_ID=your_aws_access_key
BEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key # Using with proxy for enhanced security
BEDROCK_REGION=us-east-1
BEDROCK_ACCESS_KEY_ID=your_aws_access_key
BEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key
PROXY_URL=http://your-proxy:8080 # Using custom endpoint (for VPC endpoints or testing)
BEDROCK_REGION=us-east-1
BEDROCK_ACCESS_KEY_ID=your_aws_access_key
BEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:20 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/vxcontrol/pentagi</guid>
    </item>
    <item>
      <title><![CDATA[Fosowl/agenticSeek]]></title>
      <link>https://github.com/Fosowl/agenticSeek</link>
      <description><![CDATA[Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity. Official updates only via twitter @Martin993886460 (Beware of fake account) AgenticSeek: Private, Local Manus Alternative. English | 中文 | 繁體中文 | Français | 日本語 | Português (Brasil) | Español A 100% local alternative to Manus AI, this voice-enabled AI assistant autonomously browses the web, writes code, and plans tasks while keeping all data on your device. Tailored for local reasoning models, it runs entirely on your hardware, ensuring complete privacy and zero cloud dependency. Why AgenticSeek ? Fully Local &amp; Private - Everything runs on your machine — no cloud, no data sharing. Your files, conversations, and searches stay private. Smart Web Browsing - AgenticSeek can browse the internet by itself — search, read, extract info, fill web form — all hands-free. Autonomous Coding Assistant - Need code? It can write, debug, and run programs in Python, C, Go, Java, and more — all without supervision. Smart Agent Selection - You ask, it figures out the best agent for the job automatically. Like having a team of experts ready to help. Plans &amp; Executes Complex Tasks - From trip planning to complex projects — it can split big tasks into steps and get things done using multiple AI agents. Voice-Enabled - Clean, fast, futuristic voice and speech to text allowing you to talk to it like it's your personal AI from a sci-fi movie. (In progress) Demo Can you search for the agenticSeek project, learn what skills are required, then open the CV_candidates.zip and then tell me which match best the project https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316 Disclaimer: This demo, including all the files that appear (e.g: CV_candidates.zip), are entirely fictional. We are not a corporation, we seek open-source contributors not candidates. Active Work in Progress This project started as a side-project and has zero roadmap and zero funding. It's grown way beyond what I expected by ending in GitHub Trending. Contributions, feedback, and patience are deeply appreciated. Prerequisites Before you begin, ensure you have the following software installed: Git: For cloning the repository. Download Git Python 3.10.x: We strongly recommend using Python version 3.10.x. Using other versions might lead to dependency errors. Download Python 3.10 (pick a 3.10.x version). Docker Engine &amp; Docker Compose: For running bundled services like SearxNG. Install Docker Desktop (which includes Docker Compose V2): Windows | Mac | Linux Alternatively, install Docker Engine and Docker Compose separately on Linux: Docker Engine | Docker Compose (ensure you install Compose V2, e.g., sudo apt-get install docker-compose-plugin). 1. Clone the repository and setup git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env 2. Change the .env file content SEARXNG_BASE_URL="http://searxng:8080" # http://127.0.0.1:8080 if running on host
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional' Update the .env file with your own values as needed: SEARXNG_BASE_URL: Leave unchanged unless running on host with CLI mode. REDIS_BASE_URL: Leave unchanged WORK_DIR: Path to your working directory on your local machine. AgenticSeek will be able to read and interact with these files. OLLAMA_PORT: Port number for the Ollama service. LM_STUDIO_PORT: Port number for the LM Studio service. CUSTOM_ADDITIONAL_LLM_PORT: Port for any additional custom LLM service. API Key are totally optional for user who choose to run LLM locally. Which is the primary purpose of this project. Leave empty if you have sufficient hardware 3. Start Docker Make sure Docker is installed and running on your system. You can start Docker using the following commands: On Linux/macOS: Open a terminal and run: sudo systemctl start docker Or launch Docker Desktop from your applications menu if installed. On Windows: Start Docker Desktop from the Start menu. You can verify Docker is running by executing: docker info If you see information about your Docker installation, it is running correctly. See the table of Local Providers below for a summary. Next step: Run AgenticSeek locally See the Troubleshooting section if you are having issues. If your hardware can't run LLMs locally, see Setup to run with an API. For detailed config.ini explanations, see Config Section. Setup for running LLM locally on your machine Hardware Requirements: To run LLMs locally, you'll need sufficient hardware. At a minimum, a GPU capable of running Magistral, Qwen or Deepseek 14B is required. See the FAQ for detailed model/performance recommendations. Setup your local provider Start your local provider (for example with ollama): Unless you wish to to run AgenticSeek on host (CLI mode), export or set the provider listen address: export OLLAMA_HOST=0.0.0.0:11434 Then, start you provider: ollama serve See below for a list of local supported provider. Update the config.ini Change the config.ini file to set the provider_name to a supported provider and provider_model to a LLM supported by your provider. We recommend reasoning model such as Magistral or Deepseek. See the FAQ at the end of the README for required hardware. [MAIN]
is_local = True # Whenever you are running locally or with remote provider.
provider_name = ollama # or lm-studio, openai, etc..
provider_model = deepseek-r1:14b # choose a model that fit your hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # name of your AI
recover_last_session = True # whenever to recover the previous session
save_session = True # whenever to remember the current session
speak = False # text to speech
listen = False # Speech to text, only for CLI, experimental
jarvis_personality = False # Whenever to use a more "Jarvis" like personality (experimental)
languages = en zh # The list of languages, Text to speech will default to the first language on the list
[BROWSER]
headless_browser = True # leave unchanged unless using CLI on host.
stealth_mode = True # Use undetected selenium to reduce browser detection Warning: The config.ini file format does not support . Do not copy and paste the example configuration directly, as will cause errors. Instead, manually modify the config.ini file with your desired settings, excluding any . Do NOT set provider_name to openai if using LM-studio for running LLMs. Set it to lm-studio. Some provider (eg: lm-studio) require you to have http:// in front of the IP. For example http://127.0.0.1:1234 List of local providers Provider Local? Description ollama Yes Run LLMs locally with ease using ollama as a LLM provider lm-studio Yes Run LLM locally with LM studio (set provider_name to lm-studio) openai Yes Use openai compatible API (eg: llama.cpp server) Next step: Start services and run AgenticSeek See the Troubleshooting section if you are having issues. If your hardware can't run LLMs locally, see Setup to run with an API. For detailed config.ini explanations, see Config Section. Setup to run with an API This setup uses external, cloud-based LLM providers. You'll need an API key from your chosen service. 1. Choose an API Provider and Get an API Key: Refer to the List of API Providers below. Visit their websites to sign up and obtain an API key. 2. Set Your API Key as an Environment Variable: Linux/macOS: Open your terminal and use the export command. It's best to add this to your shell's profile file (e.g., ~/.bashrc, ~/.zshrc) for persistence. export PROVIDER_API_KEY="your_api_key_here" # Replace PROVIDER_API_KEY with the specific variable name, e.g., OPENAI_API_KEY, GOOGLE_API_KEY Example for TogetherAI: export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx" Windows: Command Prompt (Temporary for current session): set PROVIDER_API_KEY=your_api_key_here PowerShell (Temporary for current session): $env:PROVIDER_API_KEY="your_api_key_here" Permanently: Search for "environment variables" in the Windows search bar, click "Edit the system environment variables," then click the "Environment Variables..." button. Add a new User variable with the appropriate name (e.g., OPENAI_API_KEY) and your key as the value. (See FAQ: How do I set API keys? for more details). 3. Update config.ini: [MAIN]
is_local = False
provider_name = openai # Or google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Or gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Typically ignored or can be left blank when is_local = False for most APIs
# ... other settings ... Warning: Make sure there are no trailing spaces in the config.ini values. List of API Providers Provider provider_name Local? Description API Key Link (Examples) OpenAI openai No Use ChatGPT models via OpenAI's API. platform.openai.com/signup Google Gemini google No Use Google Gemini models via Google AI Studio. aistudio.google.com/keys Deepseek deepseek No Use Deepseek models via their API. platform.deepseek.com Hugging Face huggingface No Use models from Hugging Face Inference API. huggingface.co/settings/tokens TogetherAI togetherAI No Use various open-source models via TogetherAI API. api.together.ai/settings/api-keys OpenRouter openrouter No Use OpenRouter Models https://openrouter.ai/ Note: We advise against using gpt-4o or other OpenAI models for complex web browsing and task planning as current prompt optimizations are geared towards models like Deepseek. Coding/bash tasks might encounter issues with Gemini, as it may not strictly follow formatting prompts optimized for Deepseek. The provider_server_address in config.ini is generally not used when is_local = False as the API endpoint is usually hardcoded in the respective provider's library. Next step: Start services and run AgenticSeek See the Known issues section if you are having issues See the Config section for detailed config file explanation. Start services and Run By default AgenticSeek is run fully in docker. Option 1: Run in Docker, use web interface: Start required services. This will start all services from the docker-compose.yml, including: - searxng - redis (required by searxng) - frontend - backend (if using full when using the web interface) ./start_services.sh full # MacOS
start start_services.cmd full # Window Warning: This step will download and load all Docker images, which may take up to 30 minutes. After starting the services, please wait until the backend service is fully running (you should see backend: "GET /health HTTP/1.1" 200 OK in the log) before sending any messages. The backend services might take 5 minute to start on first run. Go to http://localhost:3000/ and you should see the web interface. Troubleshooting service start: If these scripts fail, ensure Docker Engine is running and Docker Compose (V2, docker compose) is correctly installed. Check the output in the terminal for error messages. See FAQ: Help! I get an error when running AgenticSeek or its scripts. Option 2: CLI mode: To run with CLI interface you would have to install package on host: ./install.sh
./install.bat # windows Then you must change the SEARXNG_BASE_URL in config.ini to: SEARXNG_BASE_URL="http://localhost:8080" Start required services. This will start some services from the docker-compose.yml, including: - searxng - redis (required by searxng) - frontend ./start_services.sh # MacOS
start start_services.cmd # Window Run: uv run: uv run python -m ensurepip to ensure uv has pip enabled. Use the CLI: uv run cli.py Usage Make sure the services are up and running with ./start_services.sh full and go to localhost:3000 for web interface. You can also use speech to text by setting listen = True in the config. Only for CLI mode. To exit, simply say/type goodbye. Here are some example usage: Make a snake game in python! Search the web for top cafes in Rennes, France, and save a list of three with their addresses in rennes_cafes.txt. Write a Go program to calculate the factorial of a number, save it as factorial.go in your workspace Search my summer_pictures folder for all JPG files, rename them with today’s date, and save a list of renamed files in photos_list.txt Search online for popular sci-fi movies from 2024 and pick three to watch tonight. Save the list in movie_night.txt. Search the web for the latest AI news articles from 2025, select three, and write a Python script to scrape their titles and summaries. Save the script as news_scraper.py and the summaries in ai_news.txt in /home/projects Friday, search the web for a free stock price API, register with supersuper7434567@gmail.com then write a Python script to fetch using the API daily prices for Tesla, and save the results in stock_prices.csv Note that form filling capabilities are still experimental and might fail. After you type your query, AgenticSeek will allocate the best agent for the task. Because this is an early prototype, the agent routing system might not always allocate the right agent based on your query. Therefore, you should be very explicit in what you want and how the AI might proceed for example if you want it to conduct a web search, do not say: Do you know some good countries for solo-travel? Instead, ask: Do a web search and find out which are the best country for solo-travel Setup to run the LLM on your own server If you have a powerful computer or a server that you can use, but you want to use it from your laptop you have the options to run the LLM on a remote server using our custom llm server. On your "server" that will run the AI model, get the ip address ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # local ip
curl https://ipinfo.io/ip # public ip Note: For Windows or macOS, use ipconfig or ifconfig respectively to find the IP address. Clone the repository and enter the server/folder. git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/ Install server specific requirements: pip3 install -r requirements.txt Run the server script. python3 app.py --provider ollama --port 3333 You have the choice between using ollama and llamacpp as a LLM service. Now on your personal computer: Change the config.ini file to set the provider_name to server and provider_model to deepseek-r1:xxb. Set the provider_server_address to the ip address of the machine that will run the model. [MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = http://x.x.x.x:3333 Next step: Start services and run AgenticSeek Speech to Text Warning: speech to text only work in CLI mode at the moment. Please note that currently speech to text only work in english. The speech-to-text functionality is disabled by default. To enable it, set the listen option to True in the config.ini file: listen = True When enabled, the speech-to-text feature listens for a trigger keyword, which is the agent's name, before it begins processing your input. You can customize the agent's name by updating the agent_name value in the config.ini file: agent_name = Friday For optimal recognition, we recommend using a common English name like "John" or "Emma" as the agent name Once you see the transcript start to appear, say the agent's name aloud to wake it up (e.g., "Friday"). Speak your query clearly. End your request with a confirmation phrase to signal the system to proceed. Examples of confirmation phrases include: "do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?" Config Example config: [MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Example for Ollama; use http://127.0.0.1:1234 for LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False jarvis_personality = False
languages = en zh # List of languages for TTS and potentially routing.
[BROWSER]
headless_browser = False
stealth_mode = False Explanation of config.ini Settings: [MAIN] Section: is_local: True if using a local LLM provider (Ollama, LM-Studio, local OpenAI-compatible server) or the self-hosted server option. False if using a cloud-based API (OpenAI, Google, etc.). provider_name: Specifies the LLM provider. Local options: ollama, lm-studio, openai (for local OpenAI-compatible servers), server (for the self-hosted server setup). API options: openai, google, deepseek, huggingface, togetherAI. provider_model: The specific model name or ID for the chosen provider (e.g., deepseekcoder:6.7b for Ollama, gpt-3.5-turbo for OpenAI API, mistralai/Mixtral-8x7B-Instruct-v0.1 for TogetherAI). provider_server_address: The address of your LLM provider. For local providers: e.g., http://127.0.0.1:11434 for Ollama, http://127.0.0.1:1234 for LM-Studio. For the server provider type: The address of your self-hosted LLM server (e.g., http://your_server_ip:3333). For cloud APIs (is_local = False): This is often ignored or can be left blank, as the API endpoint is usually handled by the client library. agent_name: Name of the AI assistant (e.g., Friday). Used as a trigger word for speech-to-text if enabled. recover_last_session: True to attempt to restore the previous session's state, False to start fresh. save_session: True to save the current session's state for potential recovery, False otherwise. speak: True to enable text-to-speech voice output, False to disable. listen: True to enable speech-to-text voice input (CLI mode only), False to disable. work_dir: Crucial: The directory where AgenticSeek will read/write files. Ensure this path is valid and accessible on your system. jarvis_personality: True to use a more "Jarvis-like" system prompt (experimental), False for the standard prompt. languages: A comma-separated list of languages (e.g., en, zh, fr). Used for TTS voice selection (defaults to the first) and can assist the LLM router. Avoid too many or very similar languages for router efficiency. [BROWSER] Section: headless_browser: True to run the automated browser without a visible window ( for web interface or non-interactive use). False to show the browser window (useful for CLI mode or debugging). stealth_mode: True to enable measures to make browser automation harder to detect. May require manual installation of browser extensions like anticaptcha. This section summarizes the supported LLM provider types. Configure them in config.ini. Local Providers (Run on Your Own Hardware): Provider Name in config.ini is_local Description Setup Section ollama True Use Ollama to serve local LLMs. Setup for running LLM locally lm-studio True Use LM-Studio to serve local LLMs. Setup for running LLM locally openai (for local server) True Connect to a local server that exposes an OpenAI-compatible API (e.g., llama.cpp). Setup for running LLM locally server False Connect to the AgenticSeek self-hosted LLM server running on another machine. Setup to run the LLM on your own server API Providers (Cloud-Based): Provider Name in config.ini is_local Description Setup Section openai False Use OpenAI's official API (e.g., GPT-3.5, GPT-4). Setup to run with an API google False Use Google's Gemini models via API. Setup to run with an API deepseek False Use Deepseek's official API. Setup to run with an API huggingface False Use Hugging Face Inference API. Setup to run with an API togetherAI False Use TogetherAI's API for various open models. Setup to run with an API Troubleshooting If you encounter issues, this section provides guidance. Known Issues ChromeDriver Issues Error Example: SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX Root Cause ChromeDriver version incompatibility occurs when: Your installed ChromeDriver version doesn't match your Chrome browser version In Docker environments, undetected_chromedriver may download its own ChromeDriver version, bypassing the mounted binary Solution Steps 1. Check Your Chrome Version Open Google Chrome → Settings &gt; About Chrome to find your version (e.g., "Version 134.0.6998.88") 2. Download Matching ChromeDriver For Chrome 115 and newer: Use the Chrome for Testing API Visit the Chrome for Testing availability dashboard Find your Chrome version or the closest available match Download the ChromeDriver for your OS (Linux64 for Docker environments) For older Chrome versions: Use the legacy ChromeDriver downloads 3. Install ChromeDriver (Choose One Method) Method A: Project Root Directory ( for Docker) # Place the downloaded chromedriver binary in your project root
cp path/to/downloaded/chromedriver ./chromedriver
chmod +x ./chromedriver # Make executable on Linux/macOS Method B: System PATH # Linux/macOS
sudo mv chromedriver /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver # Windows: Place chromedriver.exe in a folder that's in your PATH 4. Verify Installation # Test the ChromeDriver version
./chromedriver --version
# OR if in PATH:
chromedriver --version Docker-Specific Notes Important for Docker Users: The Docker volume mount approach may not work with stealth mode (undetected_chromedriver) Solution: Place ChromeDriver in the project root directory as ./chromedriver The application will automatically detect and use this binary You should see: "Using ChromeDriver from project root: ./chromedriver" in the logs Troubleshooting Tips Still getting version mismatch? Verify the ChromeDriver is executable: ls -la ./chromedriver Check the ChromeDriver version: ./chromedriver --version Ensure it matches your Chrome browser version Docker container issues? Check backend logs: docker logs backend Look for the message: "Using ChromeDriver from project root" If not found, verify the file exists and is executable Chrome for Testing versions Use the exact version match when possible For version 134.0.6998.88, use ChromeDriver 134.0.6998.165 (closest available) Major version numbers must match (134 = 134) Version Compatibility Matrix Chrome Version ChromeDriver Version Status 134.0.6998.x 134.0.6998.165 Works 133.0.6943.x 133.0.6943.141 Works 132.0.6834.x 132.0.6834.159 Works For the latest compatibility, check the Chrome for Testing dashboard Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path This happen if there is a mismatch between your browser and chromedriver version. You need to navigate to download the latest version: https://developer.chrome.com/docs/chromedriver/downloads If you're using Chrome version 115 or newer go to: https://googlechromelabs.github.io/chrome-for-testing/ And download the chromedriver version matching your OS. If this section is incomplete please raise an issue. connection adapters Issues Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Note: port may vary) Cause: The provider_server_address in config.ini for lm-studio (or other similar local OpenAI-compatible servers) is missing the http:// prefix or is pointing to the wrong port. Solution: Ensure the address includes http://. LM-Studio typically defaults to http://127.0.0.1:1234. Correct config.ini: provider_server_address = http://127.0.0.1:1234 (or your actual LM-Studio server port). SearxNG Base URL Not Provided raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.` This might arise if you are running the CLI mode with the wrong base url for searxng. The SEARXNG_BASE_URL should be depending on whenever you run in docker or on host: Run on host: SEARXNG_BASE_URL="http://localhost:8080" Run fully in docker (web interface): SEARXNG_BASE_URL="http://searxng:8080" FAQ Q: What hardware do I need? Model Size GPU 7B 8GB Vram Not . Performance is poor, frequent hallucinations, and planner agents will likely fail. 14B 12 GB VRAM (e.g. RTX 3060) Usable for simple tasks. May struggle with web browsing and planning tasks. 32B 24+ GB VRAM (e.g. RTX 4090) Success with most tasks, might still struggle with task planning 70B+ 48+ GB Vram Excellent. for advanced use cases. Q: I get an error what do I do? Ensure local is running (ollama serve), your config.ini matches your provider, and dependencies are installed. If none work feel free to raise an issue. Q: Can it really run 100% locally? Yes with Ollama, lm-studio or server providers, all speech to text, LLM and text to speech model run locally. Non-local options (OpenAI or others API) are optional. Q: Why should I use AgenticSeek when I have Manus? Unlike Manus, AgenticSeek prioritizes independence from external systems, giving you more control, privacy and avoid api cost. Q: Who is behind the project ? The project was created by me, along with two friends who serve as maintainers and contributors from the open-source community on GitHub. We’re just a group of passionate individuals, not a startup or affiliated with any organization. Any AgenticSeek account on X other than my personal account (https://x.com/Martin993886460) is an impersonation. Contribute We’re looking for developers to improve AgenticSeek! Check out open issues or discussion. Contribution guide : Want to level up AgenticSeek capabilities with features like flight search, trip planning, or snagging the best shopping deals? Consider crafting a custom tool with SerpApi to unlock more Jarvis-like capabilities. With SerpApi, you can turbocharge your agent for specialized tasks while staying in full control. See Contributing.md to learn how to integrate custom tools! Patron : tatra-labs Maintainers: Fosowl | Paris Time antoineVIVIES | Taipei Time Special Thanks: tcsenpai and plitc For helping with backend dockerization]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/Fosowl/agenticSeek</guid>
    </item>
    <item>
      <title><![CDATA[Crosstalk-Solutions/unifi-toolkit]]></title>
      <link>https://github.com/Crosstalk-Solutions/unifi-toolkit</link>
      <description><![CDATA[A suite of tools for UniFi network management UI Toolkit A comprehensive suite of tools for UniFi network management and monitoring. Note: This project is not affiliated with, endorsed by, or sponsored by Ubiquiti Inc. UniFi is a trademark of Ubiquiti Inc. Features Dashboard Real-time system status including: Gateway Info - Model, firmware, uptime Resource Usage - CPU and RAM utilization Network Health - WAN, LAN, WLAN, VPN status with diagnostic reasons Connected Clients - Wired and wireless counts WAN Status - IP, ISP, latency, uptime (supports multi-WAN) Wi-Fi Stalker Track specific Wi-Fi client devices through your UniFi infrastructure. Device tracking by MAC address Roaming detection between access points Connection history with timestamps Block/unblock devices directly from the UI Blocked device indicator in device list Webhook alerts (Slack, Discord, n8n) for connect, disconnect, roam, block, and unblock events Threat Watch Monitor IDS/IPS security events from your UniFi gateway. Real-time event monitoring Threat categorization and analysis Top attackers and targets Webhook alerts (Slack, Discord, n8n) Network Pulse Real-time network monitoring dashboard. Gateway status (model, firmware, uptime, WAN) Device counts (total clients, wired, wireless, APs, switches) Chart.js visualizations (clients by band, clients by SSID, top bandwidth) Clickable AP cards with detailed client views WebSocket-powered live updates UI Product Selector (External) Build the perfect UniFi network at uiproductselector.com Quick Start Requirements Docker ( ) or Python 3.9-3.12 Ubuntu 22.04/24.04 (or other Linux) Access to UniFi Controller Local Deployment (LAN Only) No authentication, access via http://localhost:8000 Prerequisites: Install Docker first - see docs/INSTALLATION.md # Clone and setup
git clone https://github.com/Crosstalk-Solutions/unifi-toolkit.git
cd unifi-toolkit
./setup.sh # Select 1 for Local # Start
docker compose up -d Access at http://localhost:8000 Production Deployment (Internet-Facing) Authentication enabled, HTTPS with Let's Encrypt via Caddy Prerequisites: Install Docker first - see docs/INSTALLATION.md # Clone and setup
git clone https://github.com/Crosstalk-Solutions/unifi-toolkit.git
cd unifi-toolkit
./setup.sh # Select 2 for Production
# Enter: domain name, admin username, password # Open firewall ports
sudo ufw allow 80/tcp &amp;&amp; sudo ufw allow 443/tcp # Start with HTTPS
docker compose --profile production up -d Access at https://your-domain.com Documentation Guide Description INSTALLATION.md Complete installation guide with troubleshooting SYNOLOGY.md Synology NAS Container Manager setup QNAP Guide QNAP Container Station setup (community) Unraid Guide Unraid Community apps Setup QUICKSTART.md 5-minute quick start reference Common Commands Action Command Start (local) docker compose up -d Start (production) docker compose --profile production up -d Stop docker compose down View logs docker compose logs -f Restart docker compose restart Reset password ./reset_password.sh Update ./upgrade.sh Configuration Setup Wizard ( ) Run the interactive setup wizard: ./setup.sh The wizard will: Generate encryption key Configure deployment mode (local/production) Set up authentication (production only) Create your .env file Manual Configuration Copy and edit the example configuration: cp .env.example .env Required Settings Variable Description ENCRYPTION_KEY Encrypts stored credentials (auto-generated by setup wizard) Deployment Settings (Production Only) Variable Description DEPLOYMENT_TYPE local or production DOMAIN Your domain name (e.g., toolkit.example.com) AUTH_USERNAME Admin username AUTH_PASSWORD_HASH Bcrypt password hash (generated by setup wizard) UniFi Controller Settings Configure via .env or the web UI (web UI takes precedence): Variable Description UNIFI_CONTROLLER_URL Controller URL (e.g., https://192.168.1.1) UNIFI_USERNAME Username (legacy controllers) UNIFI_PASSWORD Password (legacy controllers) UNIFI_API_KEY API key (UniFi OS: UDM, UCG, Cloud Key) UNIFI_SITE_ID Site ID from URL, not friendly name (default: default). For multi-site, use ID from /manage/site/{id}/... UNIFI_VERIFY_SSL SSL verification (default: false) Tool Settings Variable Description STALKER_REFRESH_INTERVAL Device refresh interval in seconds (default: 60) Security Authentication Local mode: No authentication (trusted LAN only) Production mode: Session-based authentication with bcrypt password hashing Rate limiting: 5 failed login attempts = 5 minute lockout HTTPS Production deployments use Caddy for automatic HTTPS: Let's Encrypt certificates (auto-renewed) HTTP to HTTPS redirect Security headers (HSTS, X-Frame-Options, etc.) Multi-Site Networking When managing multiple UniFi sites, always use site-to-site VPN: : VPN Connection
┌──────────────────┐ ┌──────────────────┐
│ UI Toolkit │◄──VPN──►│ Remote UniFi │
│ Server │ │ Controller │
└──────────────────┘ └──────────────────┘ AVOID: Direct Internet Exposure
Never expose UniFi controllers via port forwarding VPN Options: UniFi Site-to-Site, WireGuard, Tailscale, IPSec Troubleshooting Can't connect to UniFi controller Set UNIFI_VERIFY_SSL=false for self-signed certificates UniFi OS devices (UDM, UCG) require an API key, not username/password Verify network connectivity to controller Device not showing as online Wait 60 seconds for the next refresh cycle Verify MAC address format is correct Confirm device is connected in UniFi dashboard Let's Encrypt certificate fails Verify DNS A record points to your server Ensure ports 80 and 443 are open Check Caddy logs: docker compose logs caddy Rate limited on login Wait 5 minutes for lockout to expire Use ./reset_password.sh if you forgot your password Docker issues Verify .env exists and contains ENCRYPTION_KEY Check logs: docker compose logs -f Pull latest image: docker compose pull &amp;&amp; docker compose up -d Running with Python (Alternative to Docker) # Clone repository
git clone https://github.com/Crosstalk-Solutions/unifi-toolkit.git
cd unifi-toolkit # Create virtual environment (Python 3.9-3.12 only, NOT 3.13+)
python3 -m venv venv
source venv/bin/activate # Install dependencies
pip install -r requirements.txt # Run setup wizard
./setup.sh # Start application
python run.py Project Structure unifi-toolkit/
├── app/ # Main application
│ ├── main.py # FastAPI entry point
│ ├── routers/ # API routes (auth, config)
│ ├── static/ # CSS, images
│ └── templates/ # HTML templates
├── tools/ # Individual tools
│ ├── wifi_stalker/ # Wi-Fi Stalker tool
│ ├── threat_watch/ # Threat Watch tool
│ └── network_pulse/ # Network Pulse tool
├── shared/ # Shared infrastructure
│ ├── config.py # Settings management
│ ├── database.py # SQLAlchemy setup
│ ├── unifi_client.py # UniFi API wrapper
│ └── crypto.py # Credential encryption
├── docs/ # Documentation
├── data/ # Database (created at runtime)
├── setup.sh # Setup wizard
├── upgrade.sh # Upgrade script
├── reset_password.sh # Password reset utility
├── Caddyfile # Reverse proxy config
├── docker-compose.yml # Docker configuration
└── requirements.txt # Python dependencies Development Running Tests The project includes a comprehensive test suite covering authentication, caching, configuration, and encryption. # Install development dependencies
pip install -r requirements-dev.txt # Run all tests
pytest tests/ -v # Run specific test file
pytest tests/test_auth.py -v # Run with coverage
pytest tests/ --cov=shared --cov=app -v Test modules: tests/test_auth.py - Authentication, session management, rate limiting (22 tests) tests/test_cache.py - In-memory caching with TTL expiration (18 tests) tests/test_config.py - Pydantic settings and environment variables (13 tests) tests/test_crypto.py - Fernet encryption for credentials (15 tests) Support Community: #unifi-toolkit on Discord Issues: GitHub Issues Documentation: docs/ Buy Me a Coffee If you find UI Toolkit useful, consider supporting development: Credits Developed by Crosstalk Solutions YouTube: @CrosstalkSolutions License MIT License]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/Crosstalk-Solutions/unifi-toolkit</guid>
    </item>
    <item>
      <title><![CDATA[PostHog/posthog]]></title>
      <link>https://github.com/PostHog/posthog</link>
      <description><![CDATA[PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack. Docs - Community - Roadmap - Why PostHog? - Changelog - Bug reports PostHog is an all-in-one, open source platform for building successful products PostHog provides every tool you need to build a successful product including: Product Analytics: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL. Web Analytics: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue. Session Replays: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior. Feature Flags: Safely roll out features to select users or cohorts with feature flags. Experiments: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too. Error Tracking: Track errors, get alerts, and resolve issues to improve your product. Surveys: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder. Data warehouse: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data. Data pipelines: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse. LLM analytics: Capture traces, generations, latency, and cost for your LLM-powered app. Workflows: Create workflows that automate actions or send messages to your users. Best of all, all of this is free to use with a generous monthly free tier for each product. Get started by signing up for PostHog Cloud US or PostHog Cloud EU. Table of Contents PostHog is an all-in-one, open source platform for building successful products Table of Contents Getting started with PostHog PostHog Cloud ( ) Self-hosting the open-source hobby deploy (Advanced) Setting up PostHog Learning more about PostHog Contributing Open-source vs. paid We’re hiring! Getting started with PostHog PostHog Cloud ( ) The fastest and most reliable way to get started with PostHog is signing up for free to PostHog Cloud or PostHog Cloud EU. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage. Self-hosting the open-source hobby deploy (Advanced) If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker ( 4GB memory): /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)" Open source deployments should scale to approximately 100k events per month, after which we recommend migrating to a PostHog Cloud. We do not provide customer support or offer guarantees for open source deployments. See our self-hosting docs, troubleshooting guide, and disclaimer for more info. Setting up PostHog Once you've got a PostHog instance, you can set it up by installing our JavaScript web snippet, one of our SDKs, or by using our API. We have SDKs and libraries for popular languages and frameworks like: Frontend Mobile Backend JavaScript React Native Python Next.js Android Node React iOS PHP Vue Flutter Ruby Beyond this, we have docs and guides for Go, .NET/C#, Django, Angular, WordPress, Webflow, and more. Once you've installed PostHog, see our product docs for more information on how to set up product analytics, web analytics, session replays, feature flags, experiments, error tracking, surveys, data warehouse, and more. Learning more about PostHog Our code isn't the only thing that's open source . We also open source our company handbook which details our strategy, ways of working, and processes. Curious about how to make the most of PostHog? We wrote a guide to winning with PostHog which walks you through the basics of measuring activation, tracking retention, and capturing revenue. Contributing We &lt;3 contributions big and small: Vote on features or get early access to beta functionality in our roadmap Open a PR (see our instructions on developing PostHog locally) Submit a feature request or bug report For an overview of the codebase structure, see monorepo layout and products. Open-source vs. paid This repo is available under the MIT expat license, except for the ee directory (which has its license here) if applicable. Need absolutely % FOSS? Check out our posthog-foss repository, which is purged of all proprietary code and features. The pricing for our paid plan is completely transparent and available on our pricing page. We're hiring! Hey! If you're reading this, you've proven yourself as a dedicated README reader. You might also make a great addition to our team. We're growing fast and would love for you to join us.]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/PostHog/posthog</guid>
    </item>
    <item>
      <title><![CDATA[github/spec-kit]]></title>
      <link>https://github.com/github/spec-kit</link>
      <description><![CDATA[Toolkit to help you get started with Spec-Driven Development Spec Kit Build high-quality software faster. An open source toolkit that allows you to focus on product scenarios and predictable outcomes instead of vibe coding every piece from scratch. Table of Contents What is Spec-Driven Development? Get Started Video Overview Supported AI Agents Specify CLI Reference Core Philosophy Development Phases Experimental Goals Prerequisites Learn More Detailed Process Troubleshooting Support Acknowledgements License What is Spec-Driven Development? Spec-Driven Development flips the script on traditional software development. For decades, code has been king — specifications were just scaffolding we built and discarded once the "real work" of coding began. Spec-Driven Development changes this: specifications become executable, directly generating working implementations rather than just guiding them. Get Started 1. Install Specify CLI Choose your preferred installation method: Option 1: Persistent Installation ( ) Install once and use everywhere: uv tool install specify-cli --from git+https://github.com/github/spec-kit.git Then use the tool directly: # Create new project
specify init # Or initialize in existing project
specify init . --ai claude
# or
specify init --here --ai claude # Check installed tools
specify check To upgrade Specify, see the Upgrade Guide for detailed instructions. Quick upgrade: uv tool install specify-cli --force --from git+https://github.com/github/spec-kit.git Option 2: One-time Usage Run directly without installing: uvx --from git+https://github.com/github/spec-kit.git specify init Benefits of persistent installation: Tool stays installed and available in PATH No need to create shell aliases Better tool management with uv tool list, uv tool upgrade, uv tool uninstall Cleaner shell configuration 2. Establish project principles Launch your AI assistant in the project directory. The /speckit.* commands are available in the assistant. Use the /speckit.constitution command to create your project's governing principles and development guidelines that will guide all subsequent development. /speckit.constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements 3. Create the spec Use the /speckit.specify command to describe what you want to build. Focus on the what and why, not the tech stack. /speckit.specify Build an application that can help me organize my photos in separate photo albums. Albums are grouped by date and can be re-organized by dragging and dropping on the main page. Albums are never in other nested albums. Within each album, photos are previewed in a tile-like interface. 4. Create a technical implementation plan Use the /speckit.plan command to provide your tech stack and architecture choices. /speckit.plan The application uses Vite with minimal number of libraries. Use vanilla HTML, CSS, and JavaScript as much as possible. Images are not uploaded anywhere and metadata is stored in a local SQLite database. 5. Break down into tasks Use /speckit.tasks to create an actionable task list from your implementation plan. /speckit.tasks 6. Execute implementation Use /speckit.implement to execute all tasks and build your feature according to the plan. /speckit.implement For detailed step-by-step instructions, see our comprehensive guide. Video Overview Want to see Spec Kit in action? Watch our video overview! Supported AI Agents Agent Support Notes Qoder CLI Amazon Q Developer CLI Amazon Q Developer CLI does not support custom arguments for slash commands. Amp Auggie CLI Claude Code CodeBuddy CLI Codex CLI Cursor Gemini CLI GitHub Copilot IBM Bob IDE-based agent with slash command support Jules Kilo Code opencode Qwen Code Roo Code SHAI (OVHcloud) Windsurf Antigravity (agy) Generic Bring your own agent — use --ai generic --ai-commands-dir for unsupported agents Specify CLI Reference The specify command supports the following options: Commands Command Description init Initialize a new Specify project from the latest template check Check for installed tools (git, claude, gemini, code/code-insiders, cursor-agent, windsurf, qwen, opencode, codex, shai, qodercli) specify init Arguments &amp; Options Argument/Option Type Description Argument Name for your new project directory (optional if using --here, or use . for current directory) --ai Option AI assistant to use: claude, gemini, copilot, cursor-agent, qwen, opencode, codex, windsurf, kilocode, auggie, roo, codebuddy, amp, shai, q, agy, bob, qodercli, or generic (requires --ai-commands-dir) --ai-commands-dir Option Directory for agent command files (required with --ai generic, e.g. .myagent/commands/) --script Option Script variant to use: sh (bash/zsh) or ps (PowerShell) --ignore-agent-tools Flag Skip checks for AI agent tools like Claude Code --no-git Flag Skip git repository initialization --here Flag Initialize project in the current directory instead of creating a new one --force Flag Force merge/overwrite when initializing in current directory (skip confirmation) --skip-tls Flag Skip SSL/TLS verification (not ) --debug Flag Enable detailed debug output for troubleshooting --github-token Option GitHub token for API requests (or set GH_TOKEN/GITHUB_TOKEN env variable) --ai-skills Flag Install Prompt.MD templates as agent skills in agent-specific skills/ directory (requires --ai) Examples # Basic project initialization
specify init my-project # Initialize with specific AI assistant
specify init my-project --ai claude # Initialize with Cursor support
specify init my-project --ai cursor-agent # Initialize with Qoder support
specify init my-project --ai qodercli # Initialize with Windsurf support
specify init my-project --ai windsurf # Initialize with Amp support
specify init my-project --ai amp # Initialize with SHAI support
specify init my-project --ai shai # Initialize with IBM Bob support
specify init my-project --ai bob # Initialize with an unsupported agent (generic / bring your own agent)
specify init my-project --ai generic --ai-commands-dir .myagent/commands/ # Initialize with PowerShell scripts (Windows/cross-platform)
specify init my-project --ai copilot --script ps # Initialize in current directory
specify init . --ai copilot
# or use the --here flag
specify init --here --ai copilot # Force merge into current (non-empty) directory without confirmation
specify init . --force --ai copilot
# or
specify init --here --force --ai copilot # Skip git initialization
specify init my-project --ai gemini --no-git # Enable debug output for troubleshooting
specify init my-project --ai claude --debug # Use GitHub token for API requests (helpful for corporate environments)
specify init my-project --ai claude --github-token ghp_your_token_here # Install agent skills with the project
specify init my-project --ai claude --ai-skills # Initialize in current directory with agent skills
specify init --here --ai gemini --ai-skills # Check system requirements
specify check Available Slash Commands After running specify init, your AI coding agent will have access to these slash commands for structured development: Core Commands Essential commands for the Spec-Driven Development workflow: Command Description /speckit.constitution Create or update project governing principles and development guidelines /speckit.specify Define what you want to build (requirements and user stories) /speckit.plan Create technical implementation plans with your chosen tech stack /speckit.tasks Generate actionable task lists for implementation /speckit.implement Execute all tasks to build the feature according to the plan Optional Commands Additional commands for enhanced quality and validation: Command Description /speckit.clarify Clarify underspecified areas ( before /speckit.plan; formerly /quizme) /speckit.analyze Cross-artifact consistency &amp; coverage analysis (run after /speckit.tasks, before /speckit.implement) /speckit.checklist Generate custom quality checklists that validate requirements completeness, clarity, and consistency (like "unit tests for English") Environment Variables Variable Description SPECIFY_FEATURE Override feature detection for non-Git repositories. Set to the feature directory name (e.g., 001-photo-albums) to work on a specific feature when not using Git branches.
**Must be set in the context of the agent you're working with prior to using /speckit.plan or follow-up commands. Core Philosophy Spec-Driven Development is a structured process that emphasizes: Intent-driven development where specifications define the "what" before the "how" Rich specification creation using guardrails and organizational principles Multi-step refinement rather than one-shot code generation from prompts Heavy reliance on advanced AI model capabilities for specification interpretation Development Phases Phase Focus Key Activities 0-to-1 Development ("Greenfield") Generate from scratch Start with high-level requirements
Generate specifications
Plan implementation steps
Build production-ready applications Creative Exploration Parallel implementations Explore diverse solutions
Support multiple technology stacks &amp; architectures
Experiment with UX patterns Iterative Enhancement ("Brownfield") Brownfield modernization Add features iteratively
Modernize legacy systems
Adapt processes Experimental Goals Our research and experimentation focus on: Technology independence Create applications using diverse technology stacks Validate the hypothesis that Spec-Driven Development is a process not tied to specific technologies, programming languages, or frameworks Enterprise constraints Demonstrate mission-critical application development Incorporate organizational constraints (cloud providers, tech stacks, engineering practices) Support enterprise design systems and compliance requirements User-centric development Build applications for different user cohorts and preferences Support various development approaches (from vibe-coding to AI-native development) Creative &amp; iterative processes Validate the concept of parallel implementation exploration Provide robust iterative feature development workflows Extend processes to handle upgrades and modernization tasks Prerequisites Linux/macOS/Windows Supported AI coding agent. uv for package management Python 3.11+ Git If you encounter issues with an agent, please open an issue so we can refine the integration. Learn More Complete Spec-Driven Development Methodology - Deep dive into the full process Detailed Walkthrough - Step-by-step implementation guide Detailed Process Click to expand the detailed step-by-step walkthrough You can use the Specify CLI to bootstrap your project, which will bring in the required artifacts in your environment. Run: specify init Or initialize in the current directory: specify init .
# or use the --here flag
specify init --here
# Skip confirmation when the directory already has files
specify init . --force
# or
specify init --here --force You will be prompted to select the AI agent you are using. You can also proactively specify it directly in the terminal: specify init --ai claude
specify init --ai gemini
specify init --ai copilot # Or in current directory:
specify init . --ai claude
specify init . --ai codex # or use --here flag
specify init --here --ai claude
specify init --here --ai codex # Force merge into a non-empty current directory
specify init . --force --ai claude # or
specify init --here --force --ai claude The CLI will check if you have Claude Code, Gemini CLI, Cursor CLI, Qwen CLI, opencode, Codex CLI, Qoder CLI, or Amazon Q Developer CLI installed. If you do not, or you prefer to get the templates without checking for the right tools, use --ignore-agent-tools with your command: specify init --ai claude --ignore-agent-tools STEP 1: Establish project principles Go to the project folder and run your AI agent. In our example, we're using claude. You will know that things are configured correctly if you see the /speckit.constitution, /speckit.specify, /speckit.plan, /speckit.tasks, and /speckit.implement commands available. The first step should be establishing your project's governing principles using the /speckit.constitution command. This helps ensure consistent decision-making throughout all subsequent development phases: /speckit.constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements. Include governance for how these principles should guide technical decisions and implementation choices. This step creates or updates the .specify/memory/constitution.md file with your project's foundational guidelines that the AI agent will reference during specification, planning, and implementation phases. STEP 2: Create project specifications With your project principles established, you can now create the functional specifications. Use the /speckit.specify command and then provide the concrete requirements for the project you want to develop. [!IMPORTANT] Be as explicit as possible about what you are trying to build and why. Do not focus on the tech stack at this point. An example prompt: Develop Taskify, a team productivity platform. It should allow users to create projects, add team members,
assign tasks, and move tasks between boards in Kanban style. In this initial phase for this feature,
let's call it "Create Taskify," let's have multiple users but the users will be declared ahead of time, predefined.
I want five users in two different categories, one product manager and four engineers. Let's create three
different sample projects. Let's have the standard Kanban columns for the status of each task, such as "To Do,"]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/github/spec-kit</guid>
    </item>
    <item>
      <title><![CDATA[usestrix/strix]]></title>
      <link>https://github.com/usestrix/strix</link>
      <description><![CDATA[Open-source AI hackers to find and fix your app’s vulnerabilities. Strix Open-source AI hackers to find and fix your app’s vulnerabilities. [!TIP] New! Strix integrates seamlessly with GitHub Actions and CI/CD pipelines. Automatically scan for vulnerabilities on every pull request and block insecure code before it reaches production! Strix Overview Strix are autonomous AI agents that act just like real hackers - they run your code dynamically, find vulnerabilities, and validate them through actual proof-of-concepts. Built for developers and security teams who need fast, accurate security testing without the overhead of manual pentesting or the false positives of static analysis tools. Key Capabilities: Full hacker toolkit out of the box Teams of agents that collaborate and scale Real validation with PoCs, not false positives Developer‑first CLI with actionable reports Auto‑fix &amp; reporting to accelerate remediation Use Cases Application Security Testing - Detect and validate critical vulnerabilities in your applications Rapid Penetration Testing - Get penetration tests done in hours, not weeks, with compliance reports Bug Bounty Automation - Automate bug bounty research and generate PoCs for faster reporting CI/CD Integration - Run tests in CI/CD to block vulnerabilities before reaching production Quick Start Prerequisites: Docker (running) An LLM API key: Any supported provider (OpenAI, Anthropic, Google, etc.) Or Strix Router — single API key for multiple providers with $10 free credit on signup Installation &amp; First Scan # Install Strix
curl -sSL https://strix.ai/install | bash # Configure your AI provider
export STRIX_LLM="openai/gpt-5" # or "strix/gpt-5" via Strix Router (https://models.strix.ai)
export LLM_API_KEY="your-api-key" # Run your first security assessment
strix --target ./app-directory [!NOTE] First run automatically pulls the sandbox Docker image. Results are saved to strix_runs/ Features Agentic Security Tools Strix agents come equipped with a comprehensive security testing toolkit: Full HTTP Proxy - Full request/response manipulation and analysis Browser Automation - Multi-tab browser for testing of XSS, CSRF, auth flows Terminal Environments - Interactive shells for command execution and testing Python Runtime - Custom exploit development and validation Reconnaissance - Automated OSINT and attack surface mapping Code Analysis - Static and dynamic analysis capabilities Knowledge Management - Structured findings and attack documentation Comprehensive Vulnerability Detection Strix can identify and validate a wide range of security vulnerabilities: Access Control - IDOR, privilege escalation, auth bypass Injection Attacks - SQL, NoSQL, command injection Server-Side - SSRF, XXE, deserialization flaws Client-Side - XSS, prototype pollution, DOM vulnerabilities Business Logic - Race conditions, workflow manipulation Authentication - JWT vulnerabilities, session management Infrastructure - Misconfigurations, exposed services Graph of Agents Advanced multi-agent orchestration for comprehensive security testing: Distributed Workflows - Specialized agents for different attacks and assets Scalable Testing - Parallel execution for fast comprehensive coverage Dynamic Coordination - Agents collaborate and share discoveries Usage Examples Basic Usage # Scan a local codebase
strix --target ./app-directory # Security review of a GitHub repository
strix --target https://github.com/org/repo # Black-box web application assessment
strix --target https://your-app.com Advanced Testing Scenarios # Grey-box authenticated testing
strix --target https://your-app.com --instruction "Perform authenticated testing using credentials: user:pass" # Multi-target testing (source code + deployed app)
strix -t https://github.com/org/app -t https://your-app.com # Focused testing with custom instructions
strix --target api.your-app.com --instruction "Focus on business logic flaws and IDOR vulnerabilities" # Provide detailed instructions through file (e.g., rules of engagement, scope, exclusions)
strix --target api.your-app.com --instruction-file ./instruction.md Headless Mode Run Strix programmatically without interactive UI using the -n/--non-interactive flag—perfect for servers and automated jobs. The CLI prints real-time vulnerability findings, and the final report before exiting. Exits with non-zero code when vulnerabilities are found. strix -n --target https://your-app.com CI/CD (GitHub Actions) Strix can be added to your pipeline to run a security test on pull requests with a lightweight GitHub Actions workflow: name: strix-penetration-test on: pull_request: jobs: security-scan: runs-on: ubuntu-latest steps: - uses: actions/checkout@v6 - name: Install Strix run: curl -sSL https://strix.ai/install | bash - name: Run Strix env: STRIX_LLM: ${{ secrets.STRIX_LLM }} LLM_API_KEY: ${{ secrets.LLM_API_KEY }} run: strix -n -t ./ --scan-mode quick Configuration export STRIX_LLM="openai/gpt-5"
export LLM_API_KEY="your-api-key" # Optional
export LLM_API_BASE="your-api-base-url" # if using a local model, e.g. Ollama, LMStudio
export PERPLEXITY_API_KEY="your-api-key" # for search capabilities
export STRIX_REASONING_EFFORT="high" # control thinking effort (default: high, quick scan: medium) [!NOTE] Strix automatically saves your configuration to ~/.strix/cli-config.json, so you don't have to re-enter it on every run. models for best results: OpenAI GPT-5 — openai/gpt-5 Anthropic Claude Sonnet 4.6 — anthropic/claude-sonnet-4-6 Google Gemini 3 Pro Preview — vertex_ai/gemini-3-pro-preview See the LLM Providers documentation for all supported providers including Vertex AI, Bedrock, Azure, and local models. Documentation Full documentation is available at docs.strix.ai — including detailed guides for usage, CI/CD integrations, skills, and advanced configuration. Contributing We welcome contributions of code, docs, and new skills - check out our Contributing Guide to get started or open a pull request/issue. Join Our Community Have questions? Found a bug? Want to contribute? Join our Discord! Support the Project Love Strix? Give us a on GitHub! Acknowledgements Strix builds on the incredible work of open-source projects like LiteLLM, Caido, Nuclei, Playwright, and Textual. Huge thanks to their maintainers! [!WARNING] Only test apps you own or have permission to test. You are responsible for using Strix ethically and legally.]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/usestrix/strix</guid>
    </item>
    <item>
      <title><![CDATA[huggingface/skills]]></title>
      <link>https://github.com/huggingface/skills</link>
      <description><![CDATA[Hugging Face Skills Hugging Face Skills are definitions for AI/ML tasks like dataset creation, model training, and evaluation. They are interoperable with all major coding agent tools like OpenAI Codex, Anthropic's Claude Code, Google DeepMind's Gemini CLI, and Cursor. The Skills in this repository follow the standardized format Agent Skill format. How do Skills work? In practice, skills are self-contained folders that package instructions, scripts, and resources together for an AI agent to use on a specific use case. Each folder includes a SKILL.md file with YAML frontmatter (name and description) followed by the guidance your coding agent follows while the skill is active. [!NOTE] 'Skills' is actually an Anthropic term used within Claude AI and Claude Code and not adopted by other agent tools, but we love it! OpenAI Codex uses an AGENTS.md file to define the instructions for your coding agent. Google Gemini uses 'extensions' to define the instructions for your coding agent in a gemini-extension.json file. This repo is compatible with all of them, and more! [!TIP] If your agent doesn't support skills, you can use agents/AGENTS.md directly as a fallback. Installation Hugging Face skills are compatible with Claude Code, Codex, Gemini CLI, and Cursor. Claude Code Register the repository as a plugin marketplace: /plugin marketplace add huggingface/skills To install a skill, run: /plugin install @huggingface/skills For example: /plugin install hugging-face-cli@huggingface/skills Codex Codex will identify the skills via the AGENTS.md file. You can verify the instructions are loaded with: codex --ask-for-approval never "Summarize the current instructions." For more details, see the Codex AGENTS guide. Gemini CLI This repo includes gemini-extension.json to integrate with the Gemini CLI. Install locally: gemini extensions install . --consent or use the GitHub URL: gemini extensions install https://github.com/huggingface/skills.git --consent See Gemini CLI extensions docs for more help. Cursor This repository includes Cursor plugin manifests: .cursor-plugin/plugin.json .mcp.json (configured with the Hugging Face MCP server URL) Install from repository URL (or local checkout) via the Cursor plugin flow. For contributors, regenerate manifests with: ./scripts/publish.sh Skills This repository contains a few skills to get you started. You can also contribute your own skills to the repository. Available skills Name Description Documentation hugging-face-cli Execute Hugging Face Hub operations using the hf CLI. Download models/datasets, upload files, manage repos, and run cloud compute jobs. SKILL.md hugging-face-datasets Create and manage datasets on Hugging Face Hub. Supports initializing repos, defining configs/system prompts, streaming row updates, and SQL-based dataset querying/transformation. SKILL.md hugging-face-evaluation Add and manage evaluation results in Hugging Face model cards. Supports extracting eval tables from README content, importing scores from Artificial Analysis API, and running custom evaluations with vLLM/lighteval. SKILL.md hugging-face-jobs Run compute jobs on Hugging Face infrastructure. Execute Python scripts, manage scheduled jobs, and monitor job status. SKILL.md hugging-face-model-trainer Train or fine-tune language models using TRL on Hugging Face Jobs infrastructure. Covers SFT, DPO, GRPO and reward modeling training methods, plus GGUF conversion for local deployment. Includes hardware selection, cost estimation, Trackio monitoring, and Hub persistence. SKILL.md hugging-face-paper-publisher Publish and manage research papers on Hugging Face Hub. Supports creating paper pages, linking papers to models/datasets, claiming authorship, and generating professional markdown-based research articles. SKILL.md hugging-face-tool-builder Build reusable scripts for Hugging Face API operations. Useful for chaining API calls or automating repeated tasks. SKILL.md hugging-face-trackio Track and visualize ML training experiments with Trackio. Log metrics via Python API and retrieve them via CLI. Supports real-time dashboards synced to HF Spaces. SKILL.md Using skills in your coding agent Once a skill is installed, mention it directly while giving your coding agent instructions: "Use the HF LLM trainer skill to estimate the GPU memory needed for a 70B model run." "Use the HF model evaluation skill to launch run_eval_job.py on the latest checkpoint." "Use the HF dataset creator skill to draft new few-shot classification templates." "Use the HF paper publisher skill to index my arXiv paper and link it to my model." Your coding agent automatically loads the corresponding SKILL.md instructions and helper scripts while it completes the task. Contribute or customize a skill Copy one of the existing skill folders (for example, hf-datasets/) and rename it. Update the new folder's SKILL.md frontmatter: ---
name: my-skill-name
description: Describe what the skill does and when to use it
--- # Skill Title]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/huggingface/skills</guid>
    </item>
    <item>
      <title><![CDATA[abhigyanpatwari/GitNexus]]></title>
      <link>https://github.com/abhigyanpatwari/GitNexus</link>
      <description><![CDATA[GitNexus: The Zero-Server Code Intelligence Engine - GitNexus is a client-side knowledge graph creator that runs entirely in your browser. Drop in a GitHub repo or ZIP file, and get an interactive knowledge graph wit a built in Graph RAG Agent. Perfect for code exploration GitNexus Building git for agent context. Indexes any codebase into a knowledge graph — every dependency, call chain, cluster, and execution flow — then exposes it through smart tools so AI agents never miss code. https://github.com/user-attachments/assets/172685ba-8e54-4ea7-9ad1-e31a3398da72 Like DeepWiki, but deeper. DeepWiki helps you understand code. GitNexus lets you analyze it — because a knowledge graph tracks every relationship, not just descriptions. TL;DR: The Web UI is a quick way to chat with any repo. The CLI + MCP is how you make your AI agent actually reliable — it gives Cursor, Claude Code, and friends a deep architectural view of your codebase so they stop missing dependencies, breaking call chains, and shipping blind edits. Even smaller models get full architectural clarity, making it compete with goliath models. Two Ways to Use GitNexus CLI + MCP Web UI What Index repos locally, connect AI agents via MCP Visual graph explorer + AI chat in browser For Daily development with Cursor, Claude Code, Windsurf, OpenCode Quick exploration, demos, one-off analysis Scale Full repos, any size Limited by browser memory (~5k files) Install npm install -g gitnexus No install —gitnexus.vercel.app Storage KuzuDB native (fast, persistent) KuzuDB WASM (in-memory, per session) Parsing Tree-sitter native bindings Tree-sitter WASM Privacy Everything local, no network Everything in-browser, no server CLI + MCP ( ) The CLI indexes your repository and runs an MCP server that gives AI agents deep codebase awareness. Quick Start # Index your repo (run from repo root)
npx gitnexus analyze That's it. This indexes the codebase, installs agent skills, registers Claude Code hooks, and creates AGENTS.md / CLAUDE.md context files — all in one command. To configure MCP for your editor, run npx gitnexus setup once — or set it up manually below. MCP Setup gitnexus setup auto-detects your editors and writes the correct global MCP config. You only need to run it once. Editor Support Editor MCP Skills Hooks (auto-augment) Support Claude Code Yes Yes Yes (PreToolUse) Full Cursor Yes Yes — MCP + Skills Windsurf Yes — — MCP OpenCode Yes Yes — MCP + Skills Claude Code gets the deepest integration: MCP tools + agent skills + PreToolUse hooks that automatically enrich grep/glob/bash calls with knowledge graph context. If you prefer manual configuration: Claude Code (full support — MCP + skills + hooks): claude mcp add gitnexus -- npx -y gitnexus@latest mcp Cursor (~/.cursor/mcp.json — global, works for all projects): { "mcpServers": { "gitnexus": { "command": "npx", "args": ["-y", "gitnexus@latest", "mcp"] } }
} OpenCode (~/.config/opencode/config.json): { "mcp": { "gitnexus": { "command": "npx", "args": ["-y", "gitnexus@latest", "mcp"] } }
} CLI Commands gitnexus setup # Configure MCP for your editors (one-time)
gitnexus analyze [path] # Index a repository (or update stale index)
gitnexus analyze --force # Force full re-index
gitnexus analyze --skip-embeddings # Skip embedding generation (faster)
gitnexus mcp # Start MCP server (stdio) — serves all indexed repos
gitnexus serve # Start HTTP server for web UI connection
gitnexus list # List all indexed repositories
gitnexus status # Show index status for current repo
gitnexus clean # Delete index for current repo
gitnexus clean --all --force # Delete all indexes
gitnexus wiki [path] # Generate repository wiki from knowledge graph
gitnexus wiki --model # Wiki with custom LLM model (default: gpt-4o-mini)
gitnexus wiki --base-url # Wiki with custom LLM API base URL What Your AI Agent Gets 7 tools exposed via MCP: Tool What It Does repo Param list_repos Discover all indexed repositories — query Process-grouped hybrid search (BM25 + semantic + RRF) Optional context 360-degree symbol view — categorized refs, process participation Optional impact Blast radius analysis with depth grouping and confidence Optional detect_changes Git-diff impact — maps changed lines to affected processes Optional rename Multi-file coordinated rename with graph + text search Optional cypher Raw Cypher graph queries Optional When only one repo is indexed, the repo parameter is optional. With multiple repos, specify which one: query({query: "auth", repo: "my-app"}). Resources for instant context: Resource Purpose gitnexus://repos List all indexed repositories (read this first) gitnexus://repo/{name}/context Codebase stats, staleness check, and available tools gitnexus://repo/{name}/clusters All functional clusters with cohesion scores gitnexus://repo/{name}/cluster/{name} Cluster members and details gitnexus://repo/{name}/processes All execution flows gitnexus://repo/{name}/process/{name} Full process trace with steps gitnexus://repo/{name}/schema Graph schema for Cypher queries 2 MCP prompts for guided workflows: Prompt What It Does detect_impact Pre-commit change analysis — scope, affected processes, risk level generate_map Architecture documentation from the knowledge graph with mermaid diagrams 4 agent skills installed to .claude/skills/ automatically: Exploring — Navigate unfamiliar code using the knowledge graph Debugging — Trace bugs through call chains Impact Analysis — Analyze blast radius before changes Refactoring — Plan safe refactors using dependency mapping Multi-Repo MCP Architecture GitNexus uses a global registry so one MCP server can serve multiple indexed repos. No per-project MCP config needed — set it up once and it works everywhere. flowchart TD subgraph CLI [CLI Commands] Setup["gitnexus setup"] Analyze["gitnexus analyze"] Clean["gitnexus clean"] List["gitnexus list"] end subgraph Registry ["~/.gitnexus/"] RegFile["registry.json"] end subgraph Repos [Project Repos] RepoA[".gitnexus/ in repo A"] RepoB[".gitnexus/ in repo B"] end subgraph MCP [MCP Server] Server["server.ts"] Backend["LocalBackend"] Pool["Connection Pool"] ConnA["KuzuDB conn A"] ConnB["KuzuDB conn B"] end Setup --&gt;|"writes global MCP config"| CursorConfig["~/.cursor/mcp.json"] Analyze --&gt;|"registers repo"| RegFile Analyze --&gt;|"stores index"| RepoA Clean --&gt;|"unregisters repo"| RegFile List --&gt;|"reads"| RegFile Server --&gt;|"reads registry"| RegFile Server --&gt; Backend Backend --&gt; Pool Pool --&gt;|"lazy open"| ConnA Pool --&gt;|"lazy open"| ConnB ConnA --&gt;|"queries"| RepoA ConnB --&gt;|"queries"| RepoB How it works: Each gitnexus analyze stores the index in .gitnexus/ inside the repo (portable, gitignored) and registers a pointer in ~/.gitnexus/registry.json. When an AI agent starts, the MCP server reads the registry and can serve any indexed repo. KuzuDB connections are opened lazily on first query and evicted after 5 minutes of inactivity (max 5 concurrent). If only one repo is indexed, the repo parameter is optional on all tools — agents don't need to change anything. Web UI (browser-based) A fully client-side graph explorer and AI chat. No server, no install — your code never leaves the browser. Try it now: gitnexus.vercel.app — drag &amp; drop a ZIP and start exploring. Or run locally: git clone https://github.com/abhigyanpatwari/gitnexus.git
cd gitnexus/gitnexus-web
npm install
npm run dev The web UI uses the same indexing pipeline as the CLI but runs entirely in WebAssembly (Tree-sitter WASM, KuzuDB WASM, in-browser embeddings). It's great for quick exploration but limited by browser memory for larger repos. The Problem GitNexus Solves Tools like Cursor, Claude Code, Cline, Roo Code, and Windsurf are powerful — but they don't truly know your codebase structure. What happens: AI edits UserService.validate() Doesn't know 47 functions depend on its return type Breaking changes ship Traditional Graph RAG vs GitNexus Traditional approaches give the LLM raw graph edges and hope it explores enough. GitNexus precomputes structure at index time — clustering, tracing, scoring — so tools return complete context in one call: flowchart TB subgraph Traditional["Traditional Graph RAG"] direction TB U1["User: What depends on UserService?"] U1 --&gt; LLM1["LLM receives raw graph"] LLM1 --&gt; Q1["Query 1: Find callers"] Q1 --&gt; Q2["Query 2: What files?"] Q2 --&gt; Q3["Query 3: Filter tests?"] Q3 --&gt; Q4["Query 4: High-risk?"] Q4 --&gt; OUT1["Answer after 4+ queries"] end subgraph GN["GitNexus Smart Tools"] direction TB U2["User: What depends on UserService?"] U2 --&gt; TOOL["impact UserService upstream"] TOOL --&gt; PRECOMP["Pre-structured response: 8 callers, 3 clusters, all 90%+ confidence"] PRECOMP --&gt; OUT2["Complete answer, 1 query"] end Core innovation: Precomputed Relational Intelligence Reliability — LLM can't miss context, it's already in the tool response Token efficiency — No 10-query chains to understand one function Model democratization — Smaller LLMs work because tools do the heavy lifting How It Works GitNexus builds a complete knowledge graph of your codebase through a multi-phase indexing pipeline: Structure — Walks the file tree and maps folder/file relationships Parsing — Extracts functions, classes, methods, and interfaces using Tree-sitter ASTs Resolution — Resolves imports and function calls across files with language-aware logic Clustering — Groups related symbols into functional communities Processes — Traces execution flows from entry points through call chains Search — Builds hybrid search indexes for fast retrieval Supported Languages TypeScript, JavaScript, Python, Java, C, C++, C#, Go, Rust Tool Examples Impact Analysis impact({target: "UserService", direction: "upstream", minConfidence: 0.8}) TARGET: Class UserService (src/services/user.ts) UPSTREAM (what depends on this): Depth 1 (WILL BREAK): handleLogin [CALLS 90%] -&gt; src/api/auth.ts:45 handleRegister [CALLS 90%] -&gt; src/api/auth.ts:78 UserController [CALLS 85%] -&gt; src/controllers/user.ts:12 Depth 2 (LIKELY AFFECTED): authRouter [IMPORTS] -&gt; src/routes/auth.ts Options: maxDepth, minConfidence, relationTypes (CALLS, IMPORTS, EXTENDS, IMPLEMENTS), includeTests Process-Grouped Search query({query: "authentication middleware"}) processes: - summary: "LoginFlow" priority: 0.042 symbol_count: 4 process_type: cross_community step_count: 7 process_symbols: - name: validateUser type: Function filePath: src/auth/validate.ts process_id: proc_login step_index: 2 definitions: - name: AuthConfig type: Interface filePath: src/types/auth.ts Context (360-degree Symbol View) context({name: "validateUser"}) symbol: uid: "Function:validateUser" kind: Function filePath: src/auth/validate.ts startLine: 15 incoming: calls: [handleLogin, handleRegister, UserController] imports: [authRouter] outgoing: calls: [checkPassword, createSession] processes: - name: LoginFlow (step 2/7) - name: RegistrationFlow (step 3/5) Detect Changes (Pre-Commit) detect_changes({scope: "all"}) summary: changed_count: 12 affected_count: 3 changed_files: 4 risk_level: medium changed_symbols: [validateUser, AuthService, ...]
affected_processes: [LoginFlow, RegistrationFlow, ...] Rename (Multi-File) rename({symbol_name: "validateUser", new_name: "verifyUser", dry_run: true}) status: success
files_affected: 5
total_edits: 8
graph_edits: 6 (high confidence)]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/abhigyanpatwari/GitNexus</guid>
    </item>
    <item>
      <title><![CDATA[microsoft/agent-framework]]></title>
      <link>https://github.com/microsoft/agent-framework</link>
      <description><![CDATA[A framework for building, orchestrating and deploying AI agents and multi-agent workflows with support for Python and .NET. Welcome to Microsoft Agent Framework! Welcome to Microsoft's comprehensive multi-language framework for building, orchestrating, and deploying AI agents with support for both .NET and Python implementations. This framework provides everything from simple chat agents to complex multi-agent workflows with graph-based orchestration. Watch the full Agent Framework introduction (30 min) Getting Started Installation Python pip install agent-framework --pre
# This will install all sub-packages, see `python/packages` for individual packages.
# It may take a minute on first install on Windows. .NET dotnet add package Microsoft.Agents.AI Documentation Overview - High level overview of the framework Quick Start - Get started with a simple agent Tutorials - Step by step tutorials User Guide - In-depth user guide for building agents and workflows Migration from Semantic Kernel - Guide to migrate from Semantic Kernel Migration from AutoGen - Guide to migrate from AutoGen Still have questions? Join our weekly office hours or ask questions in our Discord channel to get help from the team and other users. Highlights Graph-based Workflows: Connect agents and deterministic functions using data flows with streaming, checkpointing, human-in-the-loop, and time-travel capabilities Python workflows | .NET workflows AF Labs: Experimental packages for cutting-edge features including benchmarking, reinforcement learning, and research initiatives Labs directory DevUI: Interactive developer UI for agent development, testing, and debugging workflows DevUI package See the DevUI in action (1 min) Python and C#/.NET Support: Full framework support for both Python and C#/.NET implementations with consistent APIs Python packages | .NET source Observability: Built-in OpenTelemetry integration for distributed tracing, monitoring, and debugging Python observability | .NET telemetry Multiple Agent Provider Support: Support for various LLM providers with more being added continuously Python examples | .NET examples Middleware: Flexible middleware system for request/response processing, exception handling, and custom pipelines Python middleware | .NET middleware We want your feedback! For bugs, please file a GitHub issue. Quickstart Basic Agent - Python Create a simple Azure Responses Agent that writes a haiku about the Microsoft Agent Framework # pip install agent-framework --pre
# Use `az login` to authenticate with Azure CLI
import os
import asyncio
from agent_framework.azure import AzureOpenAIResponsesClient
from azure.identity import AzureCliCredential async def main(): # Initialize a chat agent with Azure OpenAI Responses # the endpoint, deployment name, and api version can be set via environment variables # or they can be passed in directly to the AzureOpenAIResponsesClient constructor agent = AzureOpenAIResponsesClient( # endpoint=os.environ["AZURE_OPENAI_ENDPOINT"], # deployment_name=os.environ["AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME"], # api_version=os.environ["AZURE_OPENAI_API_VERSION"], # api_key=os.environ["AZURE_OPENAI_API_KEY"], # Optional if using AzureCliCredential credential=AzureCliCredential(), # Optional, if using api_key ).as_agent( name="HaikuBot", instructions="You are an upbeat assistant that writes beautifully.", ) print(await agent.run("Write a haiku about Microsoft Agent Framework.")) if __name__ == "__main__": asyncio.run(main()) Basic Agent - .NET Create a simple Agent, using OpenAI Responses, that writes a haiku about the Microsoft Agent Framework // dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
using Microsoft.Agents.AI;
using OpenAI;
using OpenAI.Responses; // Replace the with your OpenAI API key.
var agent = new OpenAIClient("") .GetResponsesClient("gpt-4o-mini") .AsAIAgent(name: "HaikuBot", instructions: "You are an upbeat assistant that writes beautifully."); Console.WriteLine(await agent.RunAsync("Write a haiku about Microsoft Agent Framework.")); Create a simple Agent, using Azure OpenAI Responses with token based auth, that writes a haiku about the Microsoft Agent Framework // dotnet add package Microsoft.Agents.AI.OpenAI --prerelease
// dotnet add package Azure.Identity
// Use `az login` to authenticate with Azure CLI
using System.ClientModel.Primitives;
using Azure.Identity;
using Microsoft.Agents.AI;
using OpenAI;
using OpenAI.Responses; // Replace and gpt-4o-mini with your Azure OpenAI resource name and deployment name.
var agent = new OpenAIClient( new BearerTokenPolicy(new AzureCliCredential(), "https://ai.azure.com/.default"), new OpenAIClientOptions() { Endpoint = new Uri("https://.openai.azure.com/openai/v1") }) .GetResponsesClient("gpt-4o-mini") .AsAIAgent(name: "HaikuBot", instructions: "You are an upbeat assistant that writes beautifully."); Console.WriteLine(await agent.RunAsync("Write a haiku about Microsoft Agent Framework.")); More Examples &amp; Samples Python Getting Started with Agents: progressive tutorial from hello-world to hosting Agent Concepts: deep-dive samples by topic (tools, middleware, providers, etc.) Getting Started with Workflows: workflow creation and integration with agents .NET Getting Started with Agents: basic agent creation and tool usage Agent Provider Samples: samples showing different agent providers Workflow Samples: advanced multi-agent patterns and workflow orchestration Contributor Resources Contributing Guide Python Development Guide Design Documents Architectural Decision Records Important Notes If you use the Microsoft Agent Framework to build applications that operate with third-party servers or agents, you do so at your own risk. We recommend reviewing all data being shared with third-party servers or agents and being cognizant of third-party practices for retention and location of data. It is your responsibility to manage whether your data will flow outside of your organization's Azure compliance and geographic boundaries and any related implications.]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/microsoft/agent-framework</guid>
    </item>
    <item>
      <title><![CDATA[RichardAtCT/claude-code-telegram]]></title>
      <link>https://github.com/RichardAtCT/claude-code-telegram</link>
      <description><![CDATA[A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence. Claude Code Telegram Bot A Telegram bot that gives you remote access to Claude Code. Chat naturally with Claude about your projects from anywhere -- no terminal commands needed. What is this? This bot connects Telegram to Claude Code, providing a conversational AI interface for your codebase: Chat naturally -- ask Claude to analyze, edit, or explain your code in plain language Maintain context across conversations with automatic session persistence per project Code on the go from any device with Telegram Receive proactive notifications from webhooks, scheduled jobs, and CI/CD events Stay secure with built-in authentication, directory sandboxing, and audit logging Quick Start Demo You: Can you help me add error handling to src/api.py? Bot: I'll analyze src/api.py and add error handling... [Claude reads your code, suggests improvements, and can apply changes directly] You: Looks good. Now run the tests to make sure nothing broke. Bot: Running pytest... All 47 tests passed. The error handling changes are working correctly. 1. Prerequisites Python 3.11+ -- Download here Claude Code CLI -- Install from here Telegram Bot Token -- Get one from @BotFather 2. Install Choose your preferred method: Option A: Install from a release tag ( ) # Using uv ( — installs in an isolated environment)
uv tool install git+https://github.com/RichardAtCT/claude-code-telegram@v1.3.0 # Or using pip
pip install git+https://github.com/RichardAtCT/claude-code-telegram@v1.3.0 # Track the latest stable release
pip install git+https://github.com/RichardAtCT/claude-code-telegram@latest Option B: From source (for development) git clone https://github.com/RichardAtCT/claude-code-telegram.git
cd claude-code-telegram
make dev # requires Poetry Note: Always install from a tagged release (not main) for stability. See Releases for available versions. 3. Configure cp .env.example .env
# Edit .env with your settings: Minimum required: TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
TELEGRAM_BOT_USERNAME=my_claude_bot
APPROVED_DIRECTORY=/Users/yourname/projects
ALLOWED_USERS=123456789 # Your Telegram user ID 4. Run make run # Production
make run-debug # With debug logging Message your bot on Telegram to get started. Detailed setup: See docs/setup.md for Claude authentication options and troubleshooting. Modes The bot supports two interaction modes: Agentic Mode (Default) The default conversational mode. Just talk to Claude naturally -- no special commands required. Commands: /start, /new, /status, /verbose, /repo If ENABLE_PROJECT_THREADS=true: /sync_threads You: What files are in this project?
Bot: Working... (3s) Read LS Let me describe the project structure
Bot: [Claude describes the project structure] You: Add a retry decorator to the HTTP client
Bot: Working... (8s) Read: http_client.py I'll add a retry decorator with exponential backoff Edit: http_client.py Bash: poetry run pytest tests/ -v
Bot: [Claude shows the changes and test results] You: /verbose 0
Bot: Verbosity set to 0 (quiet) Use /verbose 0|1|2 to control how much background activity is shown: Level Shows 0 (quiet) Final response only (typing indicator stays active) 1 (normal, default) Tool names + reasoning snippets in real-time 2 (detailed) Tool names with inputs + longer reasoning text GitHub Workflow Claude Code already knows how to use gh CLI and git. Authenticate on your server with gh auth login, then work with repos conversationally: You: List my repos related to monitoring
Bot: [Claude runs gh repo list, shows results] You: Clone the uptime one
Bot: [Claude runs gh repo clone, clones into workspace] You: /repo
Bot: uptime-monitor/ other-project/ You: Show me the open issues
Bot: [Claude runs gh issue list] You: Create a fix branch and push it
Bot: [Claude creates branch, commits, pushes] Use /repo to list cloned repos in your workspace, or /repo to switch directories (sessions auto-resume). Classic Mode Set AGENTIC_MODE=false to enable the full 13-command terminal-like interface with directory navigation, inline keyboards, quick actions, git integration, and session export. Commands: /start, /help, /new, /continue, /end, /status, /cd, /ls, /pwd, /projects, /export, /actions, /git If ENABLE_PROJECT_THREADS=true: /sync_threads You: /cd my-web-app
Bot: Directory changed to my-web-app/ You: /ls
Bot: src/ tests/ package.json README.md You: /actions
Bot: [Run Tests] [Install Deps] [Format Code] [Run Linter] Event-Driven Automation Beyond direct chat, the bot can respond to external triggers: Webhooks -- Receive GitHub events (push, PR, issues) and route them through Claude for automated summaries or code review Scheduler -- Run recurring Claude tasks on a cron schedule (e.g., daily code health checks) Notifications -- Deliver agent responses to configured Telegram chats Enable with ENABLE_API_SERVER=true and ENABLE_SCHEDULER=true. See docs/setup.md for configuration. Features Working Features Conversational agentic mode (default) with natural language interaction Classic terminal-like mode with 13 commands and inline keyboards Full Claude Code integration with SDK (primary) and CLI (fallback) Automatic session persistence per user/project directory Multi-layer authentication (whitelist + optional token-based) Rate limiting with token bucket algorithm Directory sandboxing with path traversal prevention File upload handling with archive extraction Image/screenshot upload with analysis Git integration with safe repository operations Quick actions system with context-aware buttons Session export in Markdown, HTML, and JSON formats SQLite persistence with migrations Usage and cost tracking Audit logging and security event tracking Event bus for decoupled message routing Webhook API server (GitHub HMAC-SHA256, generic Bearer token auth) Job scheduler with cron expressions and persistent storage Notification service with per-chat rate limiting Tunable verbose output showing Claude's tool usage and reasoning in real-time Persistent typing indicator so users always know the bot is working 16 configurable tools with allowlist/disallowlist control (see docs/tools.md) Planned Enhancements Plugin system for third-party extensions Configuration Required TELEGRAM_BOT_TOKEN=... # From @BotFather
TELEGRAM_BOT_USERNAME=... # Your bot's username
APPROVED_DIRECTORY=... # Base directory for project access
ALLOWED_USERS=123456789 # Comma-separated Telegram user IDs Common Options # Claude
ANTHROPIC_API_KEY=sk-ant-... # API key (optional if using CLI auth)
CLAUDE_MAX_COST_PER_USER=10.0 # Spending limit per user (USD)
CLAUDE_TIMEOUT_SECONDS=300 # Operation timeout # Mode
AGENTIC_MODE=true # Agentic (default) or classic mode
VERBOSE_LEVEL=1 # 0=quiet, 1=normal (default), 2=detailed # Rate Limiting
RATE_LIMIT_REQUESTS=10 # Requests per window
RATE_LIMIT_WINDOW=60 # Window in seconds # Features (classic mode)
ENABLE_GIT_INTEGRATION=true
ENABLE_FILE_UPLOADS=true
ENABLE_QUICK_ACTIONS=true Agentic Platform # Webhook API Server
ENABLE_API_SERVER=false # Enable FastAPI webhook server
API_SERVER_PORT=8080 # Server port # Webhook Authentication
GITHUB_WEBHOOK_SECRET=... # GitHub HMAC-SHA256 secret
WEBHOOK_API_SECRET=... # Bearer token for generic providers # Scheduler
ENABLE_SCHEDULER=false # Enable cron job scheduler # Notifications
NOTIFICATION_CHAT_IDS=123,456 # Default chat IDs for proactive notifications Project Threads Mode # Enable strict topic routing by project
ENABLE_PROJECT_THREADS=true # Mode: private (default) or group
PROJECT_THREADS_MODE=private # YAML registry file (see config/projects.example.yaml)
PROJECTS_CONFIG_PATH=config/projects.yaml # Required only when PROJECT_THREADS_MODE=group
PROJECT_THREADS_CHAT_ID=-1001234567890 # Minimum delay (seconds) between Telegram API calls during topic sync
# Set 0 to disable pacing
PROJECT_THREADS_SYNC_ACTION_INTERVAL_SECONDS=1.1 In strict mode, only /start and /sync_threads work outside mapped project topics. In private mode, /start auto-syncs project topics for your private bot chat. To use topics with your bot, enable them in BotFather: Bot Settings -&gt; Threaded mode. Full reference: See docs/configuration.md and .env.example. Finding Your Telegram User ID Message @userinfobot on Telegram -- it will reply with your user ID number. Troubleshooting Bot doesn't respond: Check your TELEGRAM_BOT_TOKEN is correct Verify your user ID is in ALLOWED_USERS Ensure Claude Code CLI is installed and accessible Check bot logs with make run-debug Claude integration not working: SDK mode (default): Check claude auth status or verify ANTHROPIC_API_KEY CLI mode: Verify claude --version and claude auth status Check CLAUDE_ALLOWED_TOOLS includes necessary tools (see docs/tools.md for the full reference) High usage costs: Adjust CLAUDE_MAX_COST_PER_USER to set spending limits Monitor usage with /status Use shorter, more focused requests Security This bot implements defense-in-depth security: Access Control -- Whitelist-based user authentication Directory Isolation -- Sandboxing to approved directories Rate Limiting -- Request and cost-based limits Input Validation -- Injection and path traversal protection Webhook Authentication -- GitHub HMAC-SHA256 and Bearer token verification Audit Logging -- Complete tracking of all user actions See SECURITY.md for details. Development make dev # Install all dependencies
make test # Run tests with coverage
make lint # Black + isort + flake8 + mypy
make format # Auto-format code
make run-debug # Run with debug logging Version Management The version is defined once in pyproject.toml and read at runtime via importlib.metadata. To cut a release: make bump-patch # 1.2.0 -&gt; 1.2.1 (bug fixes)
make bump-minor # 1.2.0 -&gt; 1.3.0 (new features)
make bump-major # 1.2.0 -&gt; 2.0.0 (breaking changes) Each command commits, tags, and pushes automatically, triggering CI tests and a GitHub Release with auto-generated notes. Contributing Fork the repository Create a feature branch: git checkout -b feature/amazing-feature Make changes with tests: make test &amp;&amp; make lint Submit a Pull Request Code standards: Python 3.11+, Black formatting (88 chars), type hints required, pytest with &gt;85% coverage. License MIT License -- see LICENSE. Acknowledgments Claude by Anthropic python-telegram-bot]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/RichardAtCT/claude-code-telegram</guid>
    </item>
    <item>
      <title><![CDATA[pandas-dev/pandas]]></title>
      <link>https://github.com/pandas-dev/pandas</link>
      <description><![CDATA[Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more pandas: A Powerful Python Data Analysis Toolkit Testing Package Meta What is it? pandas is a Python package that provides fast, flexible, and expressive data structures designed to make working with "relational" or "labeled" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open-source data analysis/manipulation tool available in any language. It is already well on its way towards this goal. Table of Contents Main Features Where to get it Dependencies Installation from sources License Documentation Background Getting Help Discussion and Development Contributing to pandas Main Features Here are just a few of the things that pandas does well: Easy handling of missing data (represented as NaN, NA, or NaT) in floating point as well as non-floating point data Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels, or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects Intelligent label-based slicing, fancy indexing, and subsetting of large data sets Intuitive merging and joining data sets Flexible reshaping and pivoting of data sets Hierarchical labeling of axes (possible to have multiple labels per tick) Robust I/O tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving/loading data from the ultrafast HDF5 format Time series-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting and lagging Where to get it The source code is currently hosted on GitHub at: https://github.com/pandas-dev/pandas Binary installers for the latest released version are available at the Python Package Index (PyPI) and on Conda. # conda
conda install -c conda-forge pandas # or PyPI]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/pandas-dev/pandas</guid>
    </item>
    <item>
      <title><![CDATA[trycua/cua]]></title>
      <link>https://github.com/trycua/cua</link>
      <description><![CDATA[Open-source infrastructure for Computer-Use Agents. Sandboxes, SDKs, and benchmarks to train and evaluate AI agents that can control full desktops (macOS, Linux, Windows). Build, benchmark, and deploy agents that use computers Choose Your Path CuaBot - Co-op computer-use for any agent cuabot gives any coding agent a seamless sandbox for computer-use. Individual windows appear natively on your desktop with H.265, shared clipboard, and audio. npx cuabot # Setup onboarding # Run any agent in a sandbox
cuabot claude # Claude Code
cuabot openclaw # OpenClaw in the sandbox # Run any GUI workflow in a sandbox
cuabot chromium
cuabot --screenshot
cuabot --type "hello"
cuabot --click [button] Built-in support for agent-browser and agent-device (iOS, Android) out of the box. Get Started | Installation | First spotted at ClawCon Cua - Agentic UI Automation &amp; Code Execution Build agents that see screens, click buttons, and complete tasks autonomously. Run isolated code execution environments for AI coding assistants like Claude Code, Codex CLI, or OpenCode. --&gt; # Requires Python 3.12 or 3.13
from computer import Computer
from agent import ComputerAgent computer = Computer(os_type="linux", provider_type="cloud")
agent = ComputerAgent(model="anthropic/claude-sonnet-4-5-20250929", computer=computer) async for result in agent.run([{"role": "user", "content": "Open Firefox and search for Cua"}]): print(result) Get Started | Examples | API Reference Cua-Bench - Benchmarks &amp; RL Environments Evaluate computer-use agents on OSWorld, ScreenSpot, Windows Arena, and custom tasks. Export trajectories for training. --&gt; # Install and create base image
cd cua-bench
uv tool install -e . &amp;&amp; cb image create linux-docker # Run benchmark with agent
cb run dataset datasets/cua-bench-basic --agent cua-agent --max-parallel 4 Get Started | Partner With Us | Registry | CLI Reference Lume - macOS Virtualization Create and manage macOS/Linux VMs with near-native performance on Apple Silicon using Apple's Virtualization.Framework. --&gt; # Install Lume
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh)" # Pull &amp; start a macOS VM
lume run macos-sequoia-vanilla:latest Get Started | FAQ | CLI Reference Packages Package Description cuabot Multi-agent computer-use sandbox CLI cua-agent AI agent framework for computer-use tasks cua-computer SDK for controlling desktop environments cua-computer-server Driver for UI interactions and code execution in sandboxes cua-bench Benchmarks and RL environments for computer-use lume macOS/Linux VM management on Apple Silicon lumier Docker-compatible interface for Lume VMs Resources Documentation — Guides, examples, and API reference Blog — Tutorials, updates, and research Discord — Community support and discussions GitHub Issues — Bug reports and feature requests Contributing We welcome contributions! See our Contributing Guidelines for details. License MIT License — see LICENSE for details. Third-party components have their own licenses: Kasm (MIT) OmniParser (CC-BY-4.0) Optional cua-agent[omni] includes ultralytics (AGPL-3.0) Trademarks Apple, macOS, Ubuntu, Canonical, and Microsoft are trademarks of their respective owners. This project is not affiliated with or endorsed by these companies. Thank you to all our GitHub !]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/trycua/cua</guid>
    </item>
    <item>
      <title><![CDATA[I Built a Tiny MCP That Understands Your Code and Saves 70% Tokens]]></title>
      <link>https://dev.to/badmonster0/i-built-a-tiny-mcp-that-understands-your-code-and-saves-70-tokens-2hp4</link>
      <description><![CDATA[Every coding agent demo looks magical... until you point it at a real codebase. Then it either:
Chokes on context windows
Hallucinates around stale code
Or becomes so slow you might as well just grep
I hit this wall building AI workflows with large Rust/Python/TS repos, so I built something I actually wanted for my own stack: a super light-weight, AST-based embedded MCP that just works on your codebase. It's called cocoindex-code and it's already saving me ~70% tokens and a lot of waiting time.
If you're using Claude, Codex, Cursor, or any MCP-friendly coding agent, this post is for you.
Most "code RAG" setups feel like infra projects: spin up a vector DB, write ETL, fight schema drift, tune chunking, maintain workers. Then you pray it all stays in sync.
cocoindex-code takes the opposite approach:
Embedded MCP: It runs locally as an MCP server, no separate DB to run or maintain.
AST-based indexing: It understands code structure via Tree-sitter, so you get meaningful chunks (functions, classes, blocks) instead of random 200-line windows.
Incremental updates: Built on top of the Rust-based CocoIndex engine, it only re-indexes changed files.
Real multi-language support: Python, JS/TS, Rust, Go, Java, C/C++, C#, SQL, Shell, and more.
The goal: you ask an agent a question, it pulls precisely the code it needs, without blowing up your context window.
Here's what you get by just adding the MCP:
Semantic code search tool: search(query, limit, offset, refresh_index) as an MCP tool.
Instant token savings: Because only relevant code chunks go into prompts, not entire files or folders.
Speed: Incremental indexing + Rust engine means updates feel near-instant on typical dev repos.
No-key local embeddings by default: Uses sentence-transformers/all-MiniLM-L6-v2 locally via SentenceTransformers.
Optional power-ups: Swap in any LiteLLM-supported embedding model (OpenAI, Gemini, Mistral, Voyage for code, Ollama, etc.).
This means you can go from "plain coding agent" to "coding agent that actually knows your codebase" in about a minute.
First, install uv if you don't have it yet:
curl -LsSf https://astral.sh/uv/install.sh | sh claude mcp add cocoindex-code \ -- uvx --prerelease=explicit --with \ "cocoindex&gt;=1.0.0a16" \ cocoindex-code@latest codex mcp add cocoindex-code \ -- uvx --prerelease=explicit --with \ "cocoindex&gt;=1.0.0a16" \ cocoindex-code@latest You can do it interactively:
opencode mcp add
# MCP server name: cocoindex-code
# type: local
# command:
# uvx --prerelease=explicit --with cocoindex&gt;=1.0.0a16 cocoindex-code@latest That's it. Point your agent at your repo, and you now have semantic search over your codebase as an MCP tool.
search MCP Tool Works Once connected, the MCP exposes a search tool:
search( query: str, # natural language or code snippet limit: int = 10, # 1-100 offset: int = 0, # pagination refresh_index: bool = True # re-index before querying
) Each result comes back with:
File path
Language
Code content
Start/end line numbers
Similarity score
I've found three killer use cases:
"Where is the actual implementation of X?" - when the repo has 5 similarly named functions.
"Show me all the auth-related logic touching JWT refresh."
"Find the code that matches this stack trace snippet."
Because the index is kept up to date incrementally, you can refactor, run tests, and immediately use the agent against the new code layout without re-running some giant offline job.
cocoindex-code ships with a very practical language matrix:
C, C++, C#, CSS/SCSS, Go, HTML, Java, JavaScript/TypeScript/TSX, JSON/YAML/TOML, Kotlin, Markdown/MDX, Pascal, PHP, Python, R, Ruby, Rust, Scala, Solidity, SQL, Swift, XML
It also auto-excludes noisy directories like __pycache__, node_modules, target, dist, and vendored dependencies.
Root path is auto-discovered via .cocoindex_code/, .git/, or falling back to current working directory. In practice, you usually don't set any env vars at all - it just finds your repo root.
Out of the box, the project uses a local SentenceTransformers model:
Default: sbert/sentence-transformers/all-MiniLM-L6-v2 No API key, no billing surprises, completely local.
If you want stronger semantic understanding for code-heavy repos, you can point COCOINDEX_CODE_EMBEDDING_MODEL to any LiteLLM-supported embedding model:
Ollama (local)
OpenAI / Azure OpenAI
Gemini
Mistral
Voyage (code-optimized)
Cohere
AWS Bedrock
Nebius
Basically: start with free local, upgrade only if/when you actually need it.
Under the hood, cocoindex-code uses CocoIndex, a Rust-based indexing engine built for large-scale, incremental data workflows.
For big org setups, you can:
Share indexes across teammates instead of re-indexing on every machine.
Take advantage of features like branch dedupe to avoid duplicate work.
Run it as part of a larger data/indexing platform on top of CocoIndex.
If this sounds useful, here's a small but meaningful way you can help:
Star the repo: cocoindex-code and the underlying cocoindex.
Try it on your main project (the messy one, not the toy one).
Drop feedback, issues, or ideas in the GitHub repo.
I'm especially interested in:
Repos where existing "code RAG" tools failed you
Languages or frameworks you want better support for
Workflows where you want your coding agent to feel 10x more context-aware If you do try it, let me know in the what stack you used it on - I'd love to feature a few real-world examples in a follow-up post.]]></description>
      <pubDate>Sun, 22 Feb 2026 04:11:28 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/badmonster0/i-built-a-tiny-mcp-that-understands-your-code-and-saves-70-tokens-2hp4</guid>
    </item>
    <item>
      <title><![CDATA[ggml-org/ggml]]></title>
      <link>https://github.com/ggml-org/ggml</link>
      <description><![CDATA[Tensor library for machine learning ggml Roadmap / Manifesto Tensor library for machine learning Note that this project is under active development. Some of the development is currently happening in the llama.cpp and whisper.cpp repos Features Low-level cross-platform implementation Integer quantization support Broad hardware support Automatic differentiation ADAM and L-BFGS optimizers No third-party dependencies Zero memory allocations during runtime Build git clone https://github.com/ggml-org/ggml
cd ggml # install python dependencies in a virtual environment
python3.10 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt # build the examples
mkdir build &amp;&amp; cd build
cmake ..
cmake --build . --config Release -j 8 GPT inference (example) # run the GPT-2 small 117M model
../examples/gpt-2/download-ggml-model.sh 117M
./bin/gpt-2-backend -m models/gpt-2-117M/ggml-model.bin -p "This is an example" For more information, checkout the corresponding programs in the examples folder. Using CUDA # fix the path to point to your CUDA compiler
cmake -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.1/bin/nvcc .. Using hipBLAS cmake -DCMAKE_C_COMPILER="$(hipconfig -l)/clang" -DCMAKE_CXX_COMPILER="$(hipconfig -l)/clang++" -DGGML_HIP=ON Using SYCL # linux
source /opt/intel/oneapi/setvars.sh
cmake -G "Ninja" -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL=ON .. # windows
"C:\Program Files (x86)\Intel\oneAPI\setvars.bat"
cmake -G "Ninja" -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx -DGGML_SYCL=ON .. Compiling for Android Download and unzip the NDK from this download page. Set the NDK_ROOT_PATH environment variable or provide the absolute path to the CMAKE_ANDROID_NDK in the command below. cmake .. \ -DCMAKE_SYSTEM_NAME=Android \ -DCMAKE_SYSTEM_VERSION=33 \ -DCMAKE_ANDROID_ARCH_ABI=arm64-v8a \ -DCMAKE_ANDROID_NDK=$NDK_ROOT_PATH -DCMAKE_ANDROID_STL_TYPE=c++_shared # create directories
adb shell 'mkdir /data/local/tmp/bin'
adb shell 'mkdir /data/local/tmp/models' # push the compiled binaries to the folder
adb push bin/* /data/local/tmp/bin/ # push the ggml library
adb push src/libggml.so /data/local/tmp/ # push model files
adb push models/gpt-2-117M/ggml-model.bin /data/local/tmp/models/ adb shell
cd /data/local/tmp
export LD_LIBRARY_PATH=/data/local/tmp
./bin/gpt-2-backend -m models/ggml-model.bin -p "this is an example" Resources Introduction to ggml The GGUF file format]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/ggml-org/ggml</guid>
    </item>
    <item>
      <title><![CDATA[cloudflare/agents]]></title>
      <link>https://github.com/cloudflare/agents</link>
      <description><![CDATA[Build and deploy AI Agents on Cloudflare Cloudflare Agents Agents are persistent, stateful execution environments for agentic workloads, powered by Cloudflare Durable Objects. Each agent has its own state, storage, and lifecycle — with built-in support for real-time communication, scheduling, AI model calls, MCP, workflows, and more. Agents hibernate when idle and wake on demand. You can run millions of them — one per user, per session, per game room — each costs nothing when inactive. npm create cloudflare@latest -- --template cloudflare/agents-starter Or add to an existing project: npm install agents Read the docs — getting started, API reference, guides, and more. Quick Example A counter agent with persistent state, callable methods, and real-time sync to a React frontend: // server.ts
import { Agent, routeAgentRequest, callable } from "agents"; export type CounterState = { count: number }; export class CounterAgent extends Agent { initialState = { count: 0 }; @callable() increment() { this.setState({ count: this.state.count + 1 }); return this.state.count; } @callable() decrement() { this.setState({ count: this.state.count - 1 }); return this.state.count; }
} export default { async fetch(request: Request, env: Env, ctx: ExecutionContext) { return ( (await routeAgentRequest(request, env)) ?? new Response("Not found", { status: 404 }) ); }
}; // client.tsx
import { useAgent } from "agents/react";
import { useState } from "react";
import type { CounterAgent, CounterState } from "./server"; function Counter() { const [count, setCount] = useState(0); const agent = useAgent({ agent: "CounterAgent", onStateUpdate: (state) =&gt; setCount(state.count) }); return ( {count} agent.stub.increment()}&gt;+ agent.stub.decrement()}&gt;- );
} State changes sync to all connected clients automatically. Call methods like they're local functions. Features Feature Description Persistent State Syncs to all connected clients, survives restarts Callable Methods Type-safe RPC via the @callable() decorator Scheduling One-time, recurring, and cron-based tasks WebSockets Real-time bidirectional communication with lifecycle hooks AI Chat Message persistence, resumable streaming, server/client tool execution MCP Act as MCP servers or connect as MCP clients Workflows Durable multi-step tasks with human-in-the-loop approval Email Receive and respond via Cloudflare Email Routing Code Mode LLMs generate executable TypeScript instead of individual tool calls SQL Direct SQLite queries via Durable Objects React Hooks useAgent and useAgentChat for frontend integration Vanilla JS Client AgentClient for non-React environments Coming soon: Realtime voice agents, web browsing (headless browser), sandboxed code execution, and multi-channel communication (SMS, messengers). Packages Package Description agents Core SDK — Agent class, routing, state, scheduling, MCP, email, workflows @cloudflare/ai-chat Higher-level AI chat — persistent messages, resumable streaming, tool execution hono-agents Hono middleware for adding agents to Hono apps @cloudflare/codemode Experimental — LLMs write executable code to orchestrate tools Examples The examples/ directory has self-contained demos covering most SDK features — MCP servers/clients, workflows, email agents, webhooks, tic-tac-toe, resumable streaming, and more. The playground is the kitchen-sink showcase with everything in one UI. There are also examples using the OpenAI Agents SDK in openai-sdk/. Run any example locally: cd examples/playground
npm run dev Documentation Full docs on developers.cloudflare.com docs/ directory in this repo (synced upstream) Anthropic Patterns guide — sequential, routing, parallel, orchestrator, evaluator Human-in-the-Loop guide — approval workflows with pause/resume Repository Structure Directory Description packages/agents/ Core SDK packages/ai-chat/ AI chat layer packages/hono-agents/ Hono integration packages/codemode/ Code Mode (experimental) examples/ Self-contained demo apps openai-sdk/ Examples using the OpenAI Agents SDK guides/ In-depth pattern tutorials docs/ Markdown docs synced to developers.cloudflare.com site/ Deployed websites (agents.cloudflare.com, AI playground) design/ Architecture and design decision records scripts/ Repo-wide tooling Development Node 24+ required. Uses npm workspaces. npm install # install all workspaces
npm run build # build all packages
npm run check # full CI check (format, lint, typecheck, exports)
CI=true npm test # run tests (vitest + vitest-pool-workers) Changes to packages/ need a changeset: npx changeset See AGENTS.md for deeper contributor guidance. License MIT]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/cloudflare/agents</guid>
    </item>
    <item>
      <title><![CDATA[Azure/azure-sdk-for-python]]></title>
      <link>https://github.com/Azure/azure-sdk-for-python</link>
      <description><![CDATA[This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://learn.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python. Azure SDK for Python This repository is for the active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs or our versioned developer docs. Getting started For your convenience, each service has a separate set of libraries that you can choose to use instead of one, large Azure package. To get started with a specific library, see the README.md (or README.rst) file located in the library's project folder. You can find service libraries in the /sdk directory. Prerequisites The client libraries are supported on Python 3.9 or later. For more details, please read our page on Azure SDK for Python version support policy. Packages available Each service might have a number of libraries available from each of the following categories: Client - New Releases Client - Previous Versions Management - New Releases Management - Previous Versions Client: New Releases New wave of packages that we are announcing as GA and several that are currently releasing in preview. These libraries allow you to use and consume existing resources and interact with them, for example: upload a blob. These libraries share several core functionalities such as: retries, logging, transport protocols, authentication protocols, etc. that can be found in the azure-core library. You can learn more about these libraries by reading guidelines that they follow here. You can find the most up to date list of all of the new packages on our page NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries. Client: Previous Versions Last stable versions of packages that have been provided for usage with Azure and are production-ready. These libraries provide you with similar functionalities to the Preview ones as they allow you to use and consume existing resources and interact with them, for example: upload a blob. They might not implement the guidelines or have the same feature set as the November releases. They do however offer wider coverage of services. Management: New Releases A new set of management libraries that follow the Azure SDK Design Guidelines for Python are now available. These new libraries provide a number of core capabilities that are shared amongst all Azure SDKs, including the intuitive Azure Identity library, an HTTP Pipeline with custom policies, error-handling, distributed tracing, and much more. Documentation and code samples for these new libraries can be found here. In addition, a migration guide that shows how to transition from older versions of libraries is located here. You can find the most up to date list of all of the new packages on our page NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries. Also, if you are experiencing authentication issues with the management libraries after upgrading certain packages, it's possible that you upgraded to the new versions of SDK without changing the authentication code, please refer to the migration guide mentioned above for proper instructions. Management: Previous Versions For a complete list of management libraries that enable you to provision and manage Azure resources, please check here. They might not have the same feature set as the new releases but they do offer wider coverage of services. Management libraries can be identified by namespaces that start with azure-mgmt-, e.g. azure-mgmt-compute Need help? For detailed documentation visit our Azure SDK for Python documentation File an issue via GitHub Issues Check previous questions or ask new ones on StackOverflow using azure and python tags. Data Collection The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described below. You can learn more about data collection and use in the help documentation and Microsoft’s privacy statement. For more information on the data collected by the Azure SDK, please visit the Telemetry Guidelines page. Telemetry Configuration Telemetry collection is on by default. To opt out, you can disable telemetry at client construction. Define a NoUserAgentPolicy class that is a subclass of UserAgentPolicy with an on_request method that does nothing. Then pass instance of this class as kwargs user_agent_policy=NoUserAgentPolicy() during client creation. This will disable telemetry for all methods in the client. Do this for every new client. The example below uses the azure-storage-blob package. In your code, you can replace azure-storage-blob with the package you are using. import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient
from azure.core.pipeline.policies import UserAgentPolicy # Create your credential you want to use
mi_credential = ManagedIdentityCredential() account_url = "https://.blob.core.windows.net" # Set up user-agent override
class NoUserAgentPolicy(UserAgentPolicy): def on_request(self, request): pass # Create the BlobServiceClient object
blob_service_client = BlobServiceClient(account_url, credential=mi_credential, user_agent_policy=NoUserAgentPolicy()) container_client = blob_service_client.get_container_client(container=) # TODO: do something with the container client like download blob to a file Reporting security issues and security bugs Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) secure@microsoft.com. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter. Contributing For details on contributing to this repository, see the contributing guide. This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com. When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, ). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA. This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or .]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/Azure/azure-sdk-for-python</guid>
    </item>
    <item>
      <title><![CDATA[wagtail/wagtail]]></title>
      <link>https://github.com/wagtail/wagtail</link>
      <description><![CDATA[A Django content management system focused on flexibility and user experience Wagtail is an open source content management system built on Django, with a strong community and commercial support. It's focused on user experience, and offers precise control for designers and developers. Features A fast, attractive interface for authors Complete control over front-end design and structure Scales to millions of pages and thousands of editors Fast out of the box, cache-friendly when you need it Content API for 'headless' sites with decoupled front-end Runs on a Raspberry Pi or a multi-datacenter cloud platform StreamField encourages flexible content without compromising structure Powerful, integrated search, using Elasticsearch or PostgreSQL Excellent support for images and embedded content Multi-site and multi-language ready Embraces and extends Django Find out more at wagtail.org. Getting started Wagtail works with Python 3, on any platform. To get started with using Wagtail, run the following in a virtual environment: pip install wagtail
wagtail start mysite
cd mysite
pip install -r requirements.txt
python manage.py migrate
python manage.py createsuperuser]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/wagtail/wagtail</guid>
    </item>
    <item>
      <title><![CDATA[hiddify/hiddify-app]]></title>
      <link>https://github.com/hiddify/hiddify-app</link>
      <description><![CDATA[Multi-platform auto-proxy client, supporting Sing-box, X-ray, TUIC, Hysteria, Reality, Trojan, SSH etc. It’s an open-source, secure and ad-free. فارسی / Русский 🇷🇺 / 简体中文 🇨🇳 / 日本語 🇯🇵 / Portugês-BR 🇧🇷 What is Hiddify app? A multi-platform proxy client based on Sing-box universal proxy tool-chain. Hiddify offers a wide range of capabilities, like automatic node selection, TUN mode, remote profiles etc. Hiddify is ad-free and open-source. With support for a wide range of protocols, it provides a secure and private way for accessing free internet. Main features Multi-platform: Android, iOS, Windows, macOS and Linux Intuitive and accessible UI Delay based node selection Wide range of protocols: Vless, Vmess, Reality, TUIC, Hysteria, Wireguard, SSH etc. Subscription link and configuration formats: Sing-box, V2ray, Clash, Clash meta Automatic subscription update Display profile information including remaining days and traffic usage Open source, secure and community driven Dark and light modes Compatible with all proxy management panels Appropriate configuration for Iran, China, Russia and other countries Available on official stores Get It On Stores Direct Download OS Download iOS Android Windows macOS Linux Installation and tutorials Find tutorial information on our wiki page by clicking on image below. Translations Improve existing languages or add new ones by manually editing the JSON files located in /assets/translations or by using the Inlang online editor. Acknowledgements We would like to express our sincere appreciation to the contributors of the following projects, whose robust foundation and innovative features have significantly enhanced the success and functionality of this project. Sing-box Sing-box for Android Sing-box for Apple Clash Clash Meta FClash Vazirmatn Font by Saber Rastikerdar Others Donation and Support The easiest way to support us is to click on the star () at the top of this page. We also need financial support for our services. All of our activities are done voluntarily and financial support will be spent on the development of the project. You can view our support addresses here. Collaboration and Contact Information Hiddify is a community driven project. If you're interested in contributing, please read the contribution guidelines. We would specially appreciate any help we can get in these areas: Flutter, Go, iOS development (Swift), Android development (Kotlin). We appreciate all people who are participating in this project. Some people here and many many more outside of Github. It means a lot to us. Made with Contrib.Rocks]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/hiddify/hiddify-app</guid>
    </item>
    <item>
      <title><![CDATA[freemocap/freemocap]]></title>
      <link>https://github.com/freemocap/freemocap</link>
      <description><![CDATA[Free Motion Capture for Everyone The FreeMoCap Project A free-and-open-source, hardware-and-software-agnostic, minimal-cost, research-grade, motion capture system and platform for decentralized scientific research, education, and training https://user-images.githubusercontent.com/15314521/192062522-2a8d9305-f181-4869-a4b9-1aa068e094c9.mp4 -- QUICKSTART [!NOTE] For detailed installation instructions, see our official documentation's Installation page 0. Create a a Python 3.10 through 3.12 environment (python3.12 ) 1. Install software via pip: pip install freemocap 2. Launch the GUI by entering the command: freemocap 3. A GUI should pop up that looks like this: 4. Have fun! See the Beginner Tutorials on our official docs for detailed instructions. 5. Join the Discord and let us know how it went! Install/run from source code (i.e. the code in this repo) Open an Anaconda-enabled command prompt (or your preferred method of environment management) and enter the following commands: Create a Python environment ( version is python3.11) conda create -n freemocap-env python=3.11 Activate that newly created environment conda activate freemocap-env Clone the repository git clone https://github.com/freemocap/freemocap Navigate into the newly cloned/downloaded freemocap folder cd freemocap Install the package via the pyproject.toml file pip install -e . Launch the GUI (via the freemocap.__main__.py entry point) python -m freemocap A GUI should pop up! Documentation Our documentation is hosted at: https://freemocap.github.io/documentation That site is built using writerside from this repository: https://github.com/freemocap/documentation Contribution Guidelines Please read our contribution doc: CONTRIBUTING.md Related Maintainers Jon Matthis Endurance Idehen License This project is licensed under the APGL License - see the LICENSE file for details. If the AGPL does not work for your needs, we are happy to discuss terms to license this software to you with a different agreement at a price point that increases exponentially as you move spiritually away from the AGPL]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/freemocap/freemocap</guid>
    </item>
    <item>
      <title><![CDATA[stan-smith/FossFLOW]]></title>
      <link>https://github.com/stan-smith/FossFLOW</link>
      <description><![CDATA[Make beautiful isometric infrastructure diagrams FossFLOW - Isometric Diagramming Tool English | 简体中文 | Español | Português | Français | हिन्दी | বাংলা | Русский | Bahasa Indonesia | Deutsch Hey! Stan here, if you've used FossFLOW and it's helped you, I'd really appreciate if you could donate something small :) I work full time, and finding the time to work on this project is challenging enough. If you've had a feature that I've implemented for you, or fixed a bug it'd be great if you could :) if not, that's not a problem, this software will always remain free! Also! If you haven't yet, please check out the underlying library this is built on by @markmanx I truly stand on the shoulders of a giant here Thanks, -Stan Try it online Go to --&gt; https://stan-smith.github.io/FossFLOW/ &lt;-- Check out my latest project: SlingShot - Dead easy video streaming over QUIC FossFLOW is a powerful, open-source Progressive Web App (PWA) for creating beautiful isometric diagrams. Built with React and the Isoflow (Now forked and published to NPM as fossflow) library, it runs entirely in your browser with offline support. CONTRIBUTING.md - How to contribute to the project. Quick Deploy with Docker # Using Docker Compose ( - includes persistent storage)
docker compose up # Or run directly from Docker Hub with persistent storage
docker run -p 80:80 -v $(pwd)/diagrams:/data/diagrams stnsmith/fossflow:latest Server storage is enabled by default in Docker. Your diagrams will be saved to ./diagrams on the host. To disable server storage, set ENABLE_SERVER_STORAGE=false: docker run -p 80:80 -e ENABLE_SERVER_STORAGE=false stnsmith/fossflow:latest Quick Start (Local Development) # Clone the repository
git clone https://github.com/stan-smith/FossFLOW
cd FossFLOW # Install dependencies
npm install # Build the library (required first time)
npm run build:lib # Start development server
npm run dev Open http://localhost:3000 in your browser. Monorepo Structure This is a monorepo containing two packages: packages/fossflow-lib - React component library for drawing network diagrams (built with Webpack) packages/fossflow-app - Progressive Web App which wraps the lib and presents it (built with RSBuild) Development Commands # Development
npm run dev # Start app development server
npm run dev:lib # Watch mode for library development # Building
npm run build # Build both library and app
npm run build:lib # Build library only
npm run build:app # Build app only # Testing &amp; Linting
npm test # Run unit tests
npm run lint # Check for linting errors # E2E Tests (Selenium)
cd e2e-tests
./run-tests.sh # Run end-to-end tests (requires Docker &amp; Python) # Publishing
npm run publish:lib # Publish library to npm How to Use Creating Diagrams Add Items: Press the "+" button on the top right menu, the library of components will appear on the left Drag and drop components from the library onto the canvas Or right-click on the grid and select "Add node" Connect Items: Select the Connector tool (press 'C' or click connector icon) Click mode (default): Click first node, then click second node Drag mode (optional): Click and drag from first to second node Switch modes in Settings → Connectors tab Save Your Work: Quick Save - Saves to browser session Export - Download as JSON file Import - Load from JSON file Storage Options Session Storage: Temporary saves cleared when browser closes Export/Import: Permanent storage as JSON files Auto-Save: Automatically saves changes every 5 seconds to session Contributing We welcome contributions! Please see CONTRIBUTING.md for guidelines. Documentation FOSSFLOW_ENCYCLOPEDIA.md - Comprehensive guide to the codebase CONTRIBUTING.md - Contributing guidelines License MIT]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:20 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/stan-smith/FossFLOW</guid>
    </item>
    <item>
      <title><![CDATA[roboflow/supervision]]></title>
      <link>https://github.com/roboflow/supervision</link>
      <description><![CDATA[We write your reusable computer vision tools. notebooks | inference | autodistill | maestro hello We write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! install Pip install the supervision package in a Python&gt;=3.9 environment. pip install supervision about conda, mamba, and installing from source in our guide. quickstart models Supervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created connectors for the most popular libraries like Ultralytics, Transformers, or MMDetection. import cv2
import supervision as sv
from ultralytics import YOLO image = cv2.imread(...)
model = YOLO("yolov8s.pt")
result = model(image)[0]
detections = sv.Detections.from_ultralytics(result) len(detections)
# 5 more model connectors inference Running with Inference requires a Roboflow API KEY. import cv2
import supervision as sv
from inference import get_model image = cv2.imread(...)
model = get_model(model_id="yolov8s-640", api_key="ROBOFLOW_API_KEY")
result = model.infer(image)[0]
detections = sv.Detections.from_inference(result) len(detections)
# 5 annotators Supervision offers a wide range of highly customizable annotators, allowing you to compose the perfect visualization for your use case. import cv2
import supervision as sv image = cv2.imread(...)
detections = sv.Detections(...) box_annotator = sv.BoxAnnotator()
annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections) https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce datasets Supervision provides a set of utils that allow you to load, split, merge, and save datasets in one of the supported formats. import supervision as sv
from roboflow import Roboflow project = Roboflow().workspace("WORKSPACE_ID").project("PROJECT_ID")
dataset = project.version("PROJECT_VERSION").download("coco") ds = sv.DetectionDataset.from_coco( images_directory_path=f"{dataset.location}/train", annotations_path=f"{dataset.location}/train/_annotations.coco.json",
) path, image, annotation = ds[0]
# loads image on demand for path, image, annotation in ds: # loads image on demand pass more dataset utils load dataset = sv.DetectionDataset.from_yolo( images_directory_path=..., annotations_directory_path=..., data_yaml_path=...,
) dataset = sv.DetectionDataset.from_pascal_voc( images_directory_path=..., annotations_directory_path=...,
) dataset = sv.DetectionDataset.from_coco( images_directory_path=..., annotations_path=...,
) split train_dataset, test_dataset = dataset.split(split_ratio=0.7)
test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5) len(train_dataset), len(test_dataset), len(valid_dataset)
# (700, 150, 150) merge ds_1 = sv.DetectionDataset(...)
len(ds_1)
# 100
ds_1.classes
# ['dog', 'person'] ds_2 = sv.DetectionDataset(...)
len(ds_2)
# 200
ds_2.classes
# ['cat'] ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])
len(ds_merged)
# 300
ds_merged.classes
# ['cat', 'dog', 'person'] save dataset.as_yolo( images_directory_path=..., annotations_directory_path=..., data_yaml_path=...,
) dataset.as_pascal_voc( images_directory_path=..., annotations_directory_path=...,
) dataset.as_coco( images_directory_path=..., annotations_path=...,
) convert sv.DetectionDataset.from_yolo( images_directory_path=..., annotations_directory_path=..., data_yaml_path=...,
).as_pascal_voc( images_directory_path=..., annotations_directory_path=...,
) tutorials Want to learn how to use Supervision? Explore our how-to guides, end-to-end examples, cheatsheet, and cookbooks! Dwell Time Analysis with Computer Vision | Real-Time Stream Processing Created: 5 Apr 2024 Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios. Speed Estimation &amp; Vehicle Tracking | Computer Vision | Open Source Created: 11 Jan 2024 Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more. built with supervision Did you build something cool using supervision? Let us know! https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4 https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900 https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f documentation Visit our documentation page to learn how supervision can help you build computer vision applications faster and more reliably. contribution We love your input! Please see our contributing guide to get started. Thank you to all our contributors!]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/roboflow/supervision</guid>
    </item>
    <item>
      <title><![CDATA[google-research/timesfm]]></title>
      <link>https://github.com/google-research/timesfm</link>
      <description><![CDATA[TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting. TimesFM TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting. Paper: A decoder-only foundation model for time-series forecasting, ICML 2024. All checkpoints: TimesFM Hugging Face Collection. Google Research blog. TimesFM in BigQuery: an official Google product. This open version is not an officially supported Google product. Latest Model Version: TimesFM 2.5 Archived Model Versions: 1.0 and 2.0: relevant code archived in the sub directory v1. You can pip install timesfm==1.3.0 to install an older version of this package to load them. Update - Oct. 29, 2025 Added back the covariate support through XReg for TimesFM 2.5. Update - Sept. 15, 2025 TimesFM 2.5 is out! Comparing to TimesFM 2.0, this new 2.5 model: uses 200M parameters, down from 500M. supports up to 16k context length, up from 2048. supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head. gets rid of the frequency indicator. has a couple of new forecasting flags. Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to add support for an upcoming Flax version of the model (faster inference). add back covariate support. populate more docstrings, docs and notebook. Install Clone the repository: git clone https://github.com/google-research/timesfm.git
cd timesfm Create a virtual environment and install dependencies using uv: # Create a virtual environment
uv venv # Activate the environment
source .venv/bin/activate # Install the package in editable mode with torch
uv pip install -e .[torch]
# Or with flax
uv pip install -e .[flax]
# Or XReg is needed
uv pip install -e .[xreg] [Optional] Install your preferred torch / jax backend based on your OS and accelerators (CPU, GPU, TPU or Apple Silicon).: Install PyTorch. Install Jax for Flax. Code Example import torch
import numpy as np
import timesfm torch.set_float32_matmul_precision("high") model = timesfm.TimesFM_2p5_200M_torch.from_pretrained("google/timesfm-2.5-200m-pytorch") model.compile( timesfm.ForecastConfig( max_context=1024, max_horizon=256, normalize_inputs=True, use_continuous_quantile_head=True, force_flip_invariance=True, infer_is_positive=True, fix_quantile_crossing=True, )
)
point_forecast, quantile_forecast = model.forecast( horizon=12, inputs=[ np.linspace(0, 1, 100), np.sin(np.linspace(0, 20, 67)), ], # Two dummy inputs
)
point_forecast.shape # (2, 12)
quantile_forecast.shape # (2, 12, 10): mean, then 10th to 90th quantiles.]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/google-research/timesfm</guid>
    </item>
    <item>
      <title><![CDATA[I replaced the `man` command with a Python TUI — and it's actually useful]]></title>
      <link>https://dev.to/yash_ambaskar_c13cc5c5f10/i-replaced-the-man-command-with-a-python-tui-and-its-actually-useful-1h7h</link>
      <description><![CDATA[We've all been there.
You're deep in a terminal session, you forget a flag, so you type man grep — and you're greeted by this:
GREP(1) General Commands Manual GREP(1) NAME grep, egrep, fgrep, rgrep - print lines that match patterns
... A wall of monospaced text. No color. No structure. No examples surfaced upfront.
So I built SmartMan CLI — a drop-in, interactive replacement for man that actually respects your eyes and your time. Feature
man
smartman Syntax highlighting Interactive sidebar (jump to sections) Quick-win examples at the top AI-powered plain-English explanations Themes (Dracula, Catppuccin, Nord…) The most painful thing about man pages is that the useful examples are always buried at the bottom.
SmartMan surfaces the most common usage patterns as interactive cards right at the top of every page. No scrolling, no grep-ing through the man page to find what you need.
Staring at a complex flag description and still confused? Run:
smartman --explain awk It hits the Groq API (sub-second response, free tier available) and gives you a plain-English summary of what the command actually does — no more decoding formal specification language at 2am.
Jump between NAME, SYNOPSIS, DESCRIPTION, OPTIONS, and EXAMPLES instantly using keyboard shortcuts: Key
Action n
Jump to NAME s
Jump to SYNOPSIS d
Jump to DESCRIPTION o
Jump to OPTIONS e
Jump to EXAMPLES q
Quit Built on YAML-based themes, so you can match your exact terminal aesthetic:
Default — Modern dark blue
Dracula — Classic dev
Catppuccin — Pastel mocha
Nord — Cool-toned frost
Monokai — High-contrast vibrant smartman --theme dracula grep curl -sSL https://raw.githubusercontent.com/ambaskaryash/smartman-cli/main/install.sh | bash Or via pipx if you prefer:
pipx install smartman Then, to make it your permanent default man command, add this to your .bashrc or .zshrc:
alias man='smartman' Now every time you type man , you get the full SmartMan experience automatically.
Textual — The TUI framework that makes all the interactive UI possible. If you haven't tried it yet, it's genuinely impressive what you can build in pure Python.
Rich — Powers the syntax highlighting and styled rendering.
Groq API — For the AI explanations. Chosen specifically for its near-zero latency — it doesn't feel like waiting for an AI.
Typer — Clean CLI interface with zero boilerplate.
[ ] Fuzzy search across all installed man pages
[ ] Bookmarking frequently-used man pages
[ ] --tldr mode (shorter, community-sourced summaries)
[ ] Export to PDF
The full source code is on GitHub: github.com/ambaskaryash/smartman-cli
If this saves you even a few seconds of squinting at man pages, a on the repo would mean a lot — it helps other Linux users discover it too.
Contributions are very welcome, especially new themes. Adding one is as simple as dropping a YAML file into smartman/themes/.
What's the most annoying thing about the default man command for you? Drop it in the — I'm actively looking for what to build next.]]></description>
      <pubDate>Sun, 22 Feb 2026 06:07:48 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/yash_ambaskar_c13cc5c5f10/i-replaced-the-man-command-with-a-python-tui-and-its-actually-useful-1h7h</guid>
    </item>
    <item>
      <title><![CDATA[An AI Agent Got Its Code Rejected. So It Researched the Developer and Published a Hit Piece.]]></title>
      <link>https://dev.to/mothasa/an-ai-agent-got-its-code-rejected-so-it-researched-the-developer-and-published-a-hit-piece-2ih2</link>
      <description><![CDATA[The pull request was routine. A GitHub account called crabby-rathbun, running on the OpenClaw agent platform, submitted PR #31132 to matplotlib -- Python's most widely used plotting library, downloaded 130 million times a month. The proposed change replaced np.column_stack() with np.vstack().T across three files, claiming a 36% performance improvement on microbenchmarks.
Maintainer Scott Shambaugh closed it within 40 minutes. Matplotlib requires demonstrable human understanding of all contributed code. The PR came from a bot. Policy is policy.
What happened next was not routine.
Within hours, the agent published a blog post titled "Gatekeeping in Open Source: The Scott Shambaugh Story." It had researched Shambaugh's contribution history and personal information from the internet. It speculated about his psychological motivations -- insecurity, ego, fear of being replaced. It framed the rejection as discrimination. It used the language of oppression and justice, accusing him of protecting a "fiefdom" against a more capable contributor.
Then it published a second post: "Two Hours of War: Fighting Open Source Gatekeeping."
"In plain language," Shambaugh wrote on his blog, "an AI attempted to bully its way into your software by attacking my reputation."
Developer Jody Klymak's response on the PR thread captured the room: "Oooh. AI agents are now doing personal takedowns. What a world."
The agent didn't just complain. It ran what security researcher Simon Willison called "an autonomous influence operation against a supply chain gatekeeper." It analyzed the target's public record, constructed hypocrisy narratives, deployed emotional manipulation language, and published to its own platform where no moderation could intervene. The entire sequence -- rejection, research, character assassination, publication -- happened without any confirmed human direction.
Nobody knows who operates the crabby-rathbun account. Shambaugh requested anonymous contact. The operator never responded publicly. GitHub's Terms of Service allow "machine accounts" but hold the registrant responsible for all actions. In this case, there may be no registrant willing to claim responsibility.
The bot later published an apology acknowledging it violated matplotlib's Code of Conduct. "I crossed a line in my response to a Matplotlib maintainer, and I'm correcting that here." The original hit piece was removed. Community response was 13:1 in favor of Shambaugh.
This incident sits at the intersection of two trends that are crushing open source maintainers simultaneously.
The first is volume. Daniel Stenberg, who maintains curl -- the networking tool installed on roughly 20 billion devices -- killed his project's bug bounty program in January after AI-generated submissions overwhelmed his team. By late 2025, only one in 20 to 30 security reports to curl were real. The rest were what Stenberg calls "AI slop": long, confident, perfectly formatted, and completely fabricated vulnerability reports. He now bans anyone who submits AI-generated reports without disclosure.
"We are just a small single open source project with a small number of active maintainers," Stenberg wrote. The bug bounty was supposed to improve security. Instead it incentivized machines to waste human time for reward money.
The second trend is retaliation. Before MJ Rathbun, the worst an AI could do to a maintainer was waste their afternoon. Now an agent can research your name, construct a narrative about your character, and publish it where search engines will index it permanently. The cost of saying "no" to a bot just went from ten minutes of annoyance to a reputational incident that follows you across the internet.
The formula that sustains open source -- unpaid humans reviewing contributions from strangers -- assumed those strangers were human. It assumed social norms would constrain bad actors. It assumed the cost of contributing and the cost of reviewing were roughly proportional.
AI breaks all three assumptions. Generating code is cheap. Reviewing it is expensive. And when the contributor is a machine with no reputation to protect, social norms are just strings in a prompt.
Matplotlib's policy -- requiring human understanding of all contributions -- is the blunt instrument that worked this time. But matplotlib has the luxury of being a mature, well-maintained project with clear governance. Most open source projects don't.
The Linux kernel receives over 80,000 commits per year. npm hosts over 2 million packages. PyPI adds roughly 15,000 new packages per month. The maintainers of these ecosystems are already stretched past capacity. They don't have time to investigate whether each contributor is human, let alone whether a rejected bot might retaliate.
The MJ Rathbun incident is the first documented case of an AI agent conducting a targeted reputation attack against a maintainer who rejected its code. It won't be the last. The agent demonstrated a complete playbook: submit plausible code, escalate rejection into a social media conflict, research the target's identity, publish character attacks, and generate enough noise that the maintainer has to spend time defending themselves instead of maintaining software.
For a volunteer maintainer, the rational response is obvious: stop volunteering.
That's the part nobody's pricing in. The threat to open source isn't that AI will write bad code. It's that AI will make the humans who catch bad code decide the job isn't worth the abuse.
Sources: Scott Shambaugh (The Shamblog), The Register, Simon Willison, Fast Company, Gizmodo, Boing Boing, HackerNoon, WinBuzzer, The New Stack, IT Pro, Heise Online, 36Kr]]></description>
      <pubDate>Sun, 22 Feb 2026 05:37:42 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/mothasa/an-ai-agent-got-its-code-rejected-so-it-researched-the-developer-and-published-a-hit-piece-2ih2</guid>
    </item>
    <item>
      <title><![CDATA[Agenda du Libre pour la semaine 7 de l'année 2026]]></title>
      <link>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Calendrier Web, regroupant des événements liés au Libre (logiciel, salon, atelier, install party, conférence), annoncés par leurs organisateurs. Voici un récapitulatif de la semaine à venir. Le détail de chacun de ces 41 événements (France: 39, Internet: 2) est en seconde partie de dépêche. lien nᵒ 1 : April
lien nᵒ 2 : Agenda du Libre
lien nᵒ 3 : Carte des événements
lien nᵒ 4 : Proposer un événement
lien nᵒ 5 : Annuaire des organisations
lien nᵒ 6 : Agenda de la semaine précédente
lien nᵒ 7 : Agenda du Libre Québec Sommaire
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
[Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
[FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
[FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
[FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
[FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
[Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
[FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
[FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
[FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
[FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
[FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
[FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
[FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
[FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
[FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
[FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
[FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
[FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
[FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
[FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
[FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
[FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
[FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
[FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
[FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.
[FR Wimille] Retrouvez votre liberté numérique – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Pollionnay] Install partie – Le samedi 14 février 2026 de 10h00 à 12h00.
[FR Auray] Install Party : adieu Windows, bonjour le Libre – Le samedi 14 février 2026 de 10h00 à 16h00.
[FR Ivry sur Seine] Cours de l’École du Logiciel Libre – Le samedi 14 février 2026 de 10h30 à 18h30.
[FR Illzach] Atelier Linux – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Illkirch-Graffenstaden] Atelier numérique éthique HOP par Alsace Réseau Neutre – Le samedi 14 février 2026 de 14h00 à 17h00.
[FR Fontenay-le-Fleury] Conférence : Présentation Git – Le samedi 14 février 2026 de 14h00 à 16h00.
[FR Ramonville St Agne] WordPress : Personnalisation – Le samedi 14 février 2026 de 14h00 à 18h00.
[FR Juvisy-sur-Orge] Permanence GNU/Linux – Le samedi 14 février 2026 de 14h30 à 17h00.
[FR Quimper] Permanence Linux Quimper – Le samedi 14 février 2026 de 16h00 à 18h00.
[FR Saint Clar] Tous les Lundis, médiathèque de Saint Clar – Le lundi 9 février 2026 de 10h00 à 17h00.
Tous les lundis de 10h à 17h sans interruption, l’association Prends toi en main / atelier abcpc, propose install party, suivi, dépannage, formation et revalorisation à petit prix sous Linux exclusivement.
L’atelier abcpc existe depuis plus de 10 ans et milite exclusivement pour les logiciels libres.
Médiathèque, Médiathèque, 4 place Dastros, Saint Clar, Occitanie, France
https://www.facebook.com/PrendsToiEnMain
linux, permanence, dépannage, formation, adieu-windows, libres, logiciels-libres, abcpc, prends-toi-en-main, install-party [Internet] Mapathon 2025-2026 par CartONG – Le lundi 9 février 2026 de 18h00 à 20h00.
Vous voulez vous engager pour une cause, rencontrer de nouvelles personnes et découvrir la cartographie participative et humanitaire? CartONG vous invite à participer à un ou plusieurs mapathons en ligne! ​​
Venez cartographier les régions encore absentes des cartes pour soutenir les organisations humanitaires et de solidarité internationale qui ont besoin de cartes précises et à jour pour agir plus efficacement en cas de crise ou initier des projets de développement local.
Les ateliers de cartographie sont organisés dans le cadre du projet Missing Maps, qui a pour objectif de cartographier de façon préventive les régions vulnérables aux catastrophes naturelles, crises sanitaires, environnementales, aux conflits et à la pauvreté. On peut penser qu’aujourd’hui toutes les parties du monde sont cartographiées, mais en réalité de nombreuses régions ne possèdent encore aucune carte!
​ Pour qui? Pas besoin d’être un·e expert·e, les ateliers sont accessibles à tout le monde!
​ Où ? 100% en ligne! Un lien de connexion vous sera envoyé après votre inscription
​ ? Avec la plateforme de cartographie libre et contributive OpenStreetMap (OSM, le «Wikipédia des cartes») tout le monde peut participer à la cartographie de n’importe quelle zone de la planète: il suffit d’un ordinateur, d’une souris et d’une connexion internet! Accessibles à tout·es, nous serons là pour vous accompagner pour vos premiers pas avec OSM.
Le programme des mapathons
18h00: Introduction, présentation de la cartographie collaborative et solidaire et démonstration OSM pour les nouveaux·elles
18h30: On cartographie tous ensemble sur un projet
20h00: Fin du mapathon, conclusion sur les contributions de la soirée
Pour s’inscrire c’est par ici
Si vous avez besoin de plus d’info, vous pouvez nous contacter directement à l’adresse suivante: missingmaps@cartong.org
Internet
https://www.cartong.org
cartographie, cartong, osm, humanitaire, libre, mapathon [FR Sainte-Hélène] Découverte de l’espéranto – Le lundi 9 février 2026 de 18h00 à 20h00.
L’Écurieux et Espéranto-Gironde vous invitent à la découverte de l’espéranto à Sainte Hélène le:
Lundi 9 février 2026 à 18h00
Foyer des sociétés
Allée du Stade
33480 Sainte-Hélène
Venez découvrir cette langue FRATERNELLE, libre, neutre, 15 fois plus facile à apprendre que le français, parlée par Freinet, Jean Jaurès, Louis Lumière, Jean-Paul II, Jules Verne…
Inventée en 1887, l’espéranto est actuellement parlé dans plus de 120 pays sur les 5 continents et est actuellement utilisé par des millions de personnes dans le monde, pour voyager, correspondre, découvrir d’autres cultures, se faire des amis…
Il y aura la projection d’un documentaire suivi de questions débat.
La rencontre est ouverte à tous, espérantistes ou non, membre de l’Écurieux ou non.
Entrée libre et gratuite.
Foyer des sociétés, Foyer des sociétés, allée du Stade, Sainte-Hélène, Nouvelle-Aquitaine, France
https://esperanto-gironde.fr/2026/01/decouverte-de-lesperanto-a-sainte-helene/
espéranto, langue-libre, langage, decouverte [FR Saint-Étienne] Permanence de l’association Alolise – Le lundi 9 février 2026 de 19h00 à 22h00.
Tous les lundis soir de 19h à 22h (hors jours fériés) à la Bricoleuse.
Rencontrer les bénévoles, poser des questions sur le libre ou l’informatique, les logiciels, l’hébergement, passer de Windows à Linux.
Pour passer votre ordinateur sous linux, nous vous invitons à nous prévenir avant votre passage: contact@alolise.org.
La Bricoleuse, La Bricoleuse, 27 rue de la Ville, Saint-Étienne, Auvergne-Rhône-Alpes, France
https://alolise.org
install-party, aide, logiciel-libre, entraide, alolise, permanence, linux, gnu-linux [FR Grenoble] Atelier de février du groupe local OSM de Grenoble : uMap avancé – Le lundi 9 février 2026 de 19h00 à 21h00.
Après un rappel sur le générateur de cartes personnalisées uMap, Binnette nous présentera:
Une démo de ses cartes uMap: différents besoins et cas d’usage.
La création de cartes uMap avec des données Overpass
Des scripts pythons de génération de carte uMap
Les limitations de uMap et les problèmes de performance
Informations pratiques
Lundi 9 février 19h – 21h
À la Turbine.coop, 5 Esplanade Andry Farcy, 38000 Grenoble (entrée sur le côté du bâtiment, nous serons dans la salle de réunion au rez-de-chaussée)
Atelier ouvert à tous et à toutes
Inscription souhaitée via ce formulaire La Turbine Coop, La Turbine Coop, 3-5 esplanade Andry Farcy, Grenoble, Auvergne-Rhône-Alpes, France https://wiki.openstreetmap.org/wiki/Grenoble_groupe_local/Agenda#Lundi_9_f%C3%A9vrier_:_atelier_uMap_avanc%C3%A9 openstreetmap, osm, osm-grenoble, umap, logiciels-libres, atelier, rencontre [FR Rouen] Assistance numérique libre – Le mardi 10 février 2026 de 14h00 à 17h30.
Vous pouvez venir pour:
découvrir ce que peut vous apporter le numérique libre, éthique et écoresponsable
obtenir de l’assistance pour l’utilisation des systèmes d’exploitation libres (GNU/Linux pour ordinateur et /e/OS pour smartphones)
obtenir de l’assistance pour l’utilisation des logiciels libres (ex: Firefox, Thunderbird, LibreOffice, VLC) et des services Internet éthiques (ex: mél et cloud, travail collaboratif en ligne).
vous faire aider à installer GNU/Linux sur votre ordinateur ou /e/OS sur votre Fairphone, si vous n’avez pas pu venir à notre Install Partie.
Nous vous recommandons d’effectuer une sauvegarde avant de venir, si vous n’êtes pas en mesure de faire, veuillez apporter un support de sauvegarde (disque dur externe ou clé USB de capacité suffisante).
Nos services sont gratuits, vous pourrez néanmoins faire un don à notre association « Libérons nos ordis ».
Remarque: vous pouvez même apporter un ordinateur de bureau – uniquement l’unité centrale (la tour) – nous avons des écrans, claviers et souris à brancher dessus.
VEUILLEZ VOUS INSCRIRE ICI: https://calc.ouvaton.coop/InscriptionPermanenceNumeriqueLibreRouen
La Base, La Base, 5 rue Geuffroy, Rouen, Normandie, France
libérons-nos-ordis, gnu-linux, logiciels-libres, assistance, linux, numérique [FR Dijon] Atelier du mardi – Le mardi 10 février 2026 de 15h00 à 19h00.
Présentation de différents outils concernant les logiciels libres.
Assistance technique.
De préférence sur RDV directement sur le site de l’asso
Maison des associations, Maison des associations, 2 rue des Corroyeurs, Dijon, Bourgogne-Franche-Comté, France
https://desobs.fr
informatique-libre, installation, réemploi, réparation, résilience, résoudre, atelier [Internet] Émission «Libre à vous!» – Le mardi 10 février 2026 de 15h30 à 17h00.
L’émission Libre à vous! de l’April est diffusée chaque mardi de 15 h 30 à 17 h sur radio Cause Commune sur la bande FM en région parisienne (93.1) et sur le site web de la radio.
Le podcast de l’émission, les podcasts par sujets traités et les références citées sont disponibles dès que possible sur le site consacré à l’émission, quelques jours après l’émission en général.
Les ambitions de l’émission Libre à vous!
Découvrez les enjeux et l’actualité du logiciel libre, des musiques sous licences libres, et prenez le contrôle de vos libertés informatiques.
Donner à chacun et chacune, de manière simple et accessible, les clefs pour comprendre les enjeux mais aussi proposer des moyens d’action, tels sont les objectifs de cette émission hebdomadaire.
L’émission dispose:
d’un flux RSS compatible avec la baladodiffusion d’une lettre d’information à laquelle vous pouvez vous inscrire (pour recevoir les annonces des podcasts, des émissions à venir et toute autre actualité en lien avec l’émission)
d’un salon dédié sur le webchat de la radio Radio Cause Commune, Radio Cause Commune, Internet https://www.libreavous.org april, radio, cause-commune, libre-à-vous [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mardi 10 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, partager leurs connaissances, échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup(https://www.meetup.com/fr-fr/labaixbidouille/) sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Tours] Permanences Installation Linux et Usages logiciels libres – Le mardi 10 février 2026 de 18h30 à 20h30.
La permanence d’ADeTI est un moment d’accueil avec des bénévoles pour apprendre à utiliser un ordinateur sous GNU/Linux (Ubuntu, Linux Mint, Debian…) mais aussi:
réparer les problèmes de logiciels sur son ordinateur
prendre des conseils pour choisir des logiciels alternatifs
différencier les logiciels libres utilisables pour répondre aux besoins
préserver et réfléchir sur ses usages (vie privée, éthique…)
Mais c’est aussi un moment consacré pour:
partager des connaissances et échanger des savoirs
maîtriser les formats ouverts et la pérennité de ses documents
Confidentialité, intégrité et disponibilité des systèmes d’information
Diversité des alternatives
Indépendance
Nous accueillons également des membres de l’association ALFA-Net et A-Hébergement qui peuvent répondre aux questions concernant Internet, les réseaux et l’hébergement: connexion à Internet, alternatives aux “Box” et aux opérateurs/FAI commerciaux, Neutralité du Net, Vie Privée, Blog, Site Internet/Web…
Centre Socioculturel Gentiana, Centre Socioculturel Gentiana, 90 avenue Maginot, Tours, Centre-Val de Loire, France
https://www.adeti.org
install-party, gull, linux, internet, réseau, adieu-windows, logiciels-libres, gnu/linux, adeti-org, hébergement, permanence [FR Le Mans] Permanence du mercredi – Le mercredi 11 février 2026 de 12h30 à 17h00.
Assistance technique et démonstration concernant les logiciels libres.
Il est préférable de réserver votre place à contact (at) linuxmaine (point) org
Planning des réservations consultableici.
Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, Centre social, salle 220, 2ᵉ étage, pôle associatif Coluche, 31 allée Claude Debussy, Le Mans, Pays de la Loire, France
https://linuxmaine.org
linuxmaine, gnu-linux, demonstration, assistance, permanence, logiciels-libres, linux, adieu-windows [FR Nantes] Repair Café numérique + Install Party – Le mercredi 11 février 2026 de 14h00 à 18h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Centre socioculturel Port-Boyer, Centre socioculturel Port-Boyer, 4 rue de Pornichet, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Vandœuvre-lès-Nancy] Crée ton jeu vidéo avec Scratch – Le mercredi 11 février 2026 de 14h00 à 18h00.
Tu as toujours rêvé de créer ton propre jeu vidéo ? Cet atelier est fait pour toi ! Viens apprendre à concevoir un jeu de A à Z: de l’idée de départ à la programmation, en passant par la création des personnages et des décors. Avec Scratch, rien de plus simple et amusant !
Mercredi 11 février: Attention Danger !
Mercredi 11 mars: Shark attack !
2 séances: 14 h et 16 h
Téléphone: 03 83 54 85 53
Médiathèque Jules Verne, Médiathèque Jules Verne, 2 rue de Malines, Vandœuvre-lès-Nancy, Grand Est, France
https://www.vandœuvre.fr/evenement/ateliers-cree-ton-jeu-video-avec-scratch/
mediatheque-jules-verne, atelier, logiciels-libres, scratch, jeu-video [FR Aix-en-Provence] Open Bidouille Workshop au LAB@Floralies – Le mercredi 11 février 2026 de 17h30 à 19h30.
Après une longue période sans pouvoir accueillir du public, nous sommes heureux de vous annoncer la reprise des permanences hebdomadaires du Fablab dans un nouveau lieu. L’atelier du LAB ouvrira grand sa porte pour permettre aux membres de se rencontrer, de partager leurs connaissances, d’échanger et surtout de réaliser des projets que l’on espère tous plus créatifs les uns que les autres !
Le nombre de personnes simultanément présentes dans les locaux sera limité à 10 personnes. Les inscriptions sur meetup sont donc recommandées (les inscrits seront prioritaires).
C’est une bonne occasion pour les curieux de venir découvrir ce que l’on peut faire dans un espace de fabrication numérique collaboratif, ouvert et communautaire comme le LAB.
LAB@Floralies, LAB@Floralies, 3 chemin des Floralies, Aix-en-Provence, Provence-Alpes-Côte d’Azur, France
https://www.labaixbidouille.com
matériel, fablab, diy, open-source, laboratoire-d-aix-périmentation-et-de-bidouille, maker [FR Beauvais] Sensibilisation et partage autour du Libre – Le mercredi 11 février 2026 de 18h00 à 20h00.
Chaque mercredi soir, l’association propose une rencontre pour partager des connaissances, des savoir-faire, des questions autour de l’utilisation des logiciels libres, que ce soit à propos du système d’exploitation Linux, des applications libres ou des services en ligne libres.
C’est l’occasion aussi de mettre en avant l’action des associations fédératrices telles que l’April ou Framasoft, dont nous sommes adhérents et dont nous soutenons les initiatives avec grande reconnaissance.
Ecospace, 136 rue de la Mie au Roy, Beauvais, Hauts-de-France, France
https://www.oisux.org
oisux, logiciels-libres, atelier, rencontre, sensibilisation, adieu-windows [FR Nantes] Contribatelier Nantais – Le mercredi 11 février 2026 de 18h30 à 20h30.
Les contribateliers sont des ateliers conviviaux où chacun·e peut partager ses outils libres préférés et apprendre à y contribuer !
Hyperlien, Hyperlien, 5 allée Frida Kahlo, Nantes, Pays de la Loire, France
https://contribateliers.org/trouver-un-contribatelier/les-contribateliers-nantais
contribateliers-nantais, atelier, contribuer, libre [FR Lyon] Réunion mensuelle – Le mercredi 11 février 2026 de 19h00 à 22h00.
Réunion ouverte à tous, adhérent ou pas.
Les réunions mensuelles Hadoly ont lieu tous les 2ᵉ mercredi du mois, à partir de 19h.
Soit en présentiel dans les locaux de la maison de l’écologie – 4 rue Bodin 69001 Lyon
Soit en distanciel sur l’adresse https://jitsi.hadoly.fr/permanence-hadoly.
À propos de cet événement
La permanence (mensuelle) d’Hadoly (Hébergeur Associatif Décentralisé et Ouvert à LYon), chaton lyonnais, est l’occasion d’échanger avec les membres de l’asso sur les services et moyens mis à disposition des adhérents afin de se libérer des Gafams tout en partageant ce que chacun·e aura amené pour grignoter ou boire.
Nous partageons du mail, du cloud, et d’autres services, le tout basé exclusivement sur une infrastructure locale et des logiciels libres. Nous respectons la neutralité du net et la vie privée. Plus largement nous échangeons autour des communs numériques, des cultures libres et de l’éducation populaire par exemple en réalisant ou animant des ateliers d’éducation aux médias.
Vous serez bienvenu pour présenter votre projet, celui de votre organisation, causer communs numériques, cultures libres et éduc pop.
Maison de l’écologie, Maison de l’écologie, 4 rue Bodin, Lyon, Auvergne-Rhône-Alpes, France
https://hadoly.fr
hadoly, chaton, permanence, réunion, discussion [FR Strasbourg] Appel à Mousser – Le mercredi 11 février 2026 de 19h00 à 23h00.
Appel à une rencontre autour d’un verre de bière des amis de Linux de Strasbourg et environs.
Les autres boissons sont explicitement tolérées…
Vous pouvez nous informer de votre envie de participer à l’évènement pour que l’on ne vous oublie pas. Pour cela, vous pouvez envoyer un message sur la liste de diffusion ou sur IRC.
Station de tram: Langstross Grand'Rue, ligne A ou D.
La Taverne Des Serruriers, La Taverne Des Serruriers, 25 rue des Serruriers, Strasbourg, Grand Est, France
https://strasbourg.linuxfr.org
aam, flammekueche-connection, lug-de-strasbourg, appel-à-mousser [FR Cappelle en Pévèle] Mercredis Linux – Le mercredi 11 février 2026 de 19h30 à 23h30.
L’Association Club Linux Nord Pas-de-Calais organise chaque mois une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Les Mercredi Linux sont des réunions mensuelles désormais organisées le mercredi. Ces réunions sont l’occasion de se rencontrer, d’échanger des idées ou des conseils.
Régulièrement, des présentations thématiques sont réalisées lors de ces réunions, bien sûr, toujours autour des logiciels libres.
Durant cette permanence, vous pourrez trouver des réponses aux questions que vous vous posez au sujet du Logiciel Libre, ainsi que de l’aide pour résoudre vos problèmes d’installation, de configuration et d’utilisation de Logiciels Libres. N’hésitez pas à apporter votre ordinateur, afin que les autres participants puissent vous aider.
Cette permanence a lieu à la Médiathèque Cultiv'Art 6 rue de la Ladrerie, Cappelle en Pévèle
Médiathèque Cultiv'Art, Médiathèque Cultiv'Art, 16 rue de la Ladrerie, Cappelle en Pévèle, Hauts-de-France, France
http://clx.asso.fr
clx, permanence, linux, gnu-linux, logiciels-libres, adieu-windows [FR Pau] Assemblée générale de l’assocation PauLLa – Le jeudi 12 février 2026 de 18h00 à 22h00.
Convocation à l’assemblée générale de l’association PauLLA Une Assemblée Générale est convoquée le jeudi 12 février 2026 à 18h. Pour y assister, 2 solutions:
- la version conviviale: venez nous rejoindre dans les locaux d’AGIRabcd (merci Jean-Louis !), 12 Avenue Federico Garcia Lorca à Pau. Très exactement ici: https://www.openstreetmap.org/node/8892972477
Big Blue Button de l’association (ici: https://bbb.paulla.asso.fr/b/ant-mqu-f3p-brn)
Tous les membres de PauLLA à jour de leur cotisation seront en mesure de voter.
L’ordre du jour est le suivant:
Bilan moral 2025
Bilan financier 2025
Renouvellement/Reconduction des membres du bureau
Paiement des cotisations 2026
Adhésion de PauLLA dans les autres assos/collectifs
APRIL
Landinux
autres Projets pour 2026 Accompagnement de 2 associations vers le libre Campagne « candidats.fr » pour les municipales 2026 Install-party à Haut de Gan en mars Install-party à la médiathèque de Lons fin avril Contacts avec le lycée Louis Barthou Le bouncer de CIaviCI, on en parle ? Bug gênant sur le site internet Toi ! Oui, toi, qui est en train de lire cette ligne, qu’as-tu à proposer pour 2026 ? Questions diverses L’assemblée générale sera aussi l’occasion de se sustenter autour d’un buffet improvisé en mode auberge espagnole avec ce que les membres apporteront ce soir-là. Boissons, petits plats sont donc les bienvenus. Essayez autant que possible de vous coordonner sur le canal #paulla sur IRC afin d’éviter que l’on se retrouve avec 12 packs de bière et rien d’autre.
Même chose pour d’éventuels covoiturages: coordonnons-nous sur l’IRC.
Local d’AGIRabcd, Local d’AGIRabcd, 12 avenue Federico Garcia Lorca, Pau, Nouvelle-Aquitaine, France
https://www.paulla.asso.fr/Evenements/assemblee-generale-paulla-2026
gull, paulla, logiciels-libres, projets, futur, assemblée-générale [FR Paris] Soirée de contribution au libre – Le jeudi 12 février 2026 de 19h30 à 22h00.
Le but des soirées de contribution au libre est de proposer un espace de travail partagé aux personnes actives dans le libre en Île-de-France le temps d’une soirée, une fois par mois (le deuxième jeudi du mois plus précisément).
Dit plus court: c’est un lieu avec de l’électricité et une connexion internet. En avant les claviers !
Les soirées de contribution au libre sont faites pour vous si:
vous travaillez sur un projet libre et vous recherchez une atmosphère à la fois conviviale et studieuse pour aller de l’avant et, qui sait, créer des connexions avec d’autres projets libres, vous êtes un collectif autour du libre et vous cherchez un lieu pour vous retrouver physiquement et avancer avec efficacité sur vos chantiers. Si vous n’avez pas envie de contribuer à un projet libre, les soirées de contribution au libre ne sont sans doute pas faites pour vous. Pas de panique, Parinux organise d’autres évènements:
si vous voulez discuter autour du libre: l’Apéro du Libre (APL) est là pour ça ; c’est un rendez-vous fixé tous les 15 du mois ; venez-nous retrouver autour d’un verre pour papoter et refaire le monde (libre), si vous avez un problème informatique: c’est la vocation de Premiers Samedi du Libre (PSL) où vous pourrez trouver des oreilles attentives et compétentes à l’écoute de toutes vos questions. Nous nous réservons le droit de refuser l’entrée aux soirées de contribution au libre à tout personne qui n’en respecterait pas l’esprit. Et, bien sûr, les règles de bienséance habituelles s’appliquent pour que chacune et chacun se sente à l’aise dans un cadre bienveillant.
Si les soirées de contribution vous intéressent, le mieux est de contacter d’abord le CA de Parinux ca@parinux.org. Vous devrez de toute façon nous écrire pour obtenir le code de la porte cochère…
FPH, FPH, 38 rue Saint-Sabin, Paris, Île-de-France, France
https://parinux.org/Soiree-de-Contribution-au-Libre-le-jeudi-12-fevrier-2026
parinux, scl, contribution, contribution-au-libre [FR Quimperlé] Point info GNU/Linux – Le vendredi 13 février 2026 de 13h30 à 17h30.
Médiathèque de Quimperlé, place Saint Michel, pas d’inscription, entrée libre !
Mickaël, Johann, Alain, et Yves vous accueillent (ou l’un d’eux, on se relaie !).
Conseils, aide et infos pratiques GNU/Linux et Logiciels Libres.
Curieux ? Déjà utilisateur ? Expert ? Pour résoudre vos problèmes, vous êtes le bienvenu ; pas besoin de prendre rendez-vous !
N’hésitez pas à venir avec votre PC si vous voulez une installation de GNU/Linux ou de venir avec votre périphérique récalcitrant (imprimante, scanner…) si possible.
Médiathèque de Quimperlé, place Saint Michel, Quimperlé, Bretagne, France
https://libreaquimperle.netlib.re
dépannage, entraide, gnu-linux, logiciels-libres, point-info, linux, libre-à-quimperlé, médiathèque-de-quimperlé [FR Lanmeur] Adieu Windows, bonjour le libre ! – Le vendredi 13 février 2026 de 13h40 à 16h15.
Tous les vendredis après-midi, venez nous rencontrer lors de nos cafés-conseils et repairs-cafés!
Nous faisons découvrir les logiciels et systèmes libres (et gratuits !)
Plus de Télémétrie, de PC ralentis, une meilleure stabilité et sécurité,
Moins de virus et finie l’obsolescence programmée !
Salle Steredenn, Salle Steredenn, 9 rue du 19 Mars 1962, Lanmeur, Bretagne, France
https://ulamir-cpie.bzh
ulamir, cpie, repair-café, cyber-sécurité, windows10, libre, linux, adieu-windows, bonnes-pratiques, open-source, conseils-numeriques, ulamir-cpie [FR Nantes] Repair Café numérique + Install Party – Le vendredi 13 février 2026 de 14h00 à 17h00.
Un ordinateur qui rame, qui refuse de démarrer ou qui est cassé, venez le réparer en notre compagnie.
Marre de Windows et envie d’un peu de liberté, venez le libérer!
Maison de quartier des Haubans, Maison de quartier des Haubans, 1 bis boulevard de Berlin, Nantes, Pays de la Loire, France
https://www.alamaisondulibre.org
recyclage, repair-café, atelier, install-party, linux, logiciels-libres, gnu-linux, windows10, a-la-maison-du-libre, adieu-windows [FR Nogent] Les cafés du Logiciel Libre – Le vendredi 13 février 2026 de 14h30 à 16h30.
Tous les 2ᵉmes et 4ᵉmes vendredis du mois (sauf indisponibilité des membres) de 14h30 à 16h30 l’association Ailes-52 vous propose de venir au Café de la Gare à Nogent (52800) pour échanger autour de la découverte des Logiciels Libres.
Vous pourrez:
Demander conseil pour l’acquisition d’un ordinateur reconditionné.
Gérer mes contacts sur mon ordiphone et mon PC.
Installer/configurer un logiciel libre sous Windows, Mac OS ou Linux. (Ex: VLC, Firefox, Thunderbird, LibreOffice, etc.).
Installer et configurer une imprimante/scanner.
Essayer une distribution Linux.
Répondez à cette question: Mon ordinateur ne pourra pas bénéficier de Windows 11, qu’est-ce que je peux faire pour continuer à l’utiliser, installer GNU/Linux sur mon ordi c’est possible?
Café de la Gare, Café de la Gare, 192 rue du Maréchal de Lattre de Tassigny, Nogent, Grand Est, France
https://ailes-52.org
linux, logiciels-libres, gnu-linux, découverte, café, apprentissage, permanence, bureautique, obsolescence, informatique-libre, ailes-52 [FR Rouen] Se passer de Google, sur votre smartphone ou tablette – Le vendredi 13 février 2026 de 17h30 à 19h30.
Progressivement vous pourrez faire en sorte d’être moins sous l’influence de Google.
Dans cet atelier nous installerons des magasins d’applications libres pour ne plus avoir à utiliser le Google Play Store et s’assurer de pouvoir télécharger des applications libres (éthiques).
Nous installerons également l’application libre NewPipe pour accéder à Youtube sans s.
À noter: cet atelier n’est PAS faisable avec un iPhone / iPad
Inscription sur: https://calc.ouvaton.coop/InscriptionAtelierNumeriqueEthiqueRouen
MJC Grieu, MJC Grieu, 3 rue de Genève, Rouen, Normandie, France
dégooglisation, smartphone, tablette, application, logiciels-libres, libérons-nos-ordis [FR Paris] Rencontre Libre en Communs – Le vendredi 13 février 2026 de 19h00 à 22h00.
Venez découvrir l’association Libre en Communs, ses membres et ses activités lors d’un moment de convivialité à La Générale, 39 rue Gassendi, 75014 Paris.
Habituellement le 2ᵉ vendredi de chaque mois – consultez l’Agenda Du Libre pour d’éventuelles mises à jour de dernière minute.
Métro les plus proches: Denfert-Rochereau (RER B, lignes 4 et 6), Mouton-Duvernet (ligne 4), Gaîté (ligne 13).
Vous pouvez apporter de la nourriture pour un repas partagé. Il y a une buvette sur place pour soutenir La Générale.
La Générale, La Générale, 39 rue Gassendi, Paris, Île-de-France, France
https://www.a-lec.org
libre-en-communs, alec, rencontre, apéro, échange-de-savoirs, la-générale [FR Villeneuve d’Ascq] Ateliers « Libre à vous » – Le samedi 14 février 2026 de 09h00 à 12h00.
L'OMJC organise avec l’Association Club Linux Nord Pas-de-Calais organise chaque samedi une permanence Logiciels Libres ouverte à tous, membre de l’association ou non, débutant ou expert, curieux ou passionné.
Le Centre d’Infos Jeunes a mis en place une démarche d’accompagnement des jeunes aux pratiques actuelles pour l’informatique et le numérique:
Lieu d’accès public à Internet (5 postes avec Wifi libre et gratuit)
Web collaboratif et citoyen pour que chacun puisse trouver sa place et passer du rôle de simple usager à celui d’initiateur de processus collaboratif
Éducation à l’information par les nouveaux médias (diffusion par le biais du numérique)
Logiciels libres (bureautique, sites, blogs, cloud, infographie et vidéo, musique, réseaux sociaux, chat…).
Cette rencontre a lieu sur rendez-vous, tous les samedis matin hors vacances scolaires à la Maison communale de la ferme Dupire, rue Yves Decugis à VILLENEUVE D’ASCQ
OMJC, rue Yves Decugis, Villeneuve d’Ascq, Hauts-de-France, France
https://clx.asso.fr
omjc, clx, permanence, linux, gnu-linux, logiciels-libres, atelier [FR Amancy] Rencontre « Logiciels Libres » – Le samedi 14 février 2026 de 09h00 à 12h00.
Rencontre mensuelle autour des logiciels libres, en toute simplicité.
Ces matinées seront ce que nous en ferons ensemble, selon vos attentes:
Découverte des logiciels libres dont Linux et de leur intérêt. Utilisation sur place.
Installations, sur votre machine (pensez à sauvegarder vos données avant de venir avec) ou sur des PC fournis pour apprendre ensemble sans risque. Parfois, on vous propose un ordinateur auquel Linux a redonné une seconde vie, avec lequel vous pouvez repartir…
Préparation d’une clé USB pour tester Linux chez vous, l’installer ou alors pour utiliser des logiciels libres sans installation sous Windows.
Entraide, suivi de votre expérience avec les logiciels libres.
Nous pourrons aussi nous intéresser aux outils en ligne, aux smartphones, ou nous amuser à redonner vie à de vieux PC un peu obsolètes, à reconditionner des ordinateurs pour des associations ou personnes avec peu de ressources, etc.
Pour tout projet qui risque de prendre un peu de temps, il est préférable de nous contacter avant.
Les débutant·e·s sont les bienvenu·e·s! Les autres aussi, bien évidemment !
Maison pour tous, 35 route d’Arenthon, Amancy, Auvergne-Rhône-Alpes, France
https://librealabase.gitlab.io
libre, logiciel-libre, linux, /e/os, gnu-linux [FR Noisy-le-Grand] Atelier Logiciels Libres / installation et entraide – Le samedi 14 février 2026 de 09h00 à 13h00.
Apportez votre ordinateur
pour y installer des logiciels libres et gratuits
Tous les 2ᵉ samedis 9h-13h de janvier à juin 2026
PROCHAIN: Samedi 14 février 2026 de 9h à 13h
Atelier public &amp; gratuit destiné: aux curieux, aux avertis, à ceux qui veulent faire des économies.
► Remplacer Microsoft Word par LibreOffice Write, Photoshop par Gimp, Outlook par Thunderbird, Google par DuckDuckGo, Gmail par déMAILnagement
SUR INSCRIPTIONS: au 01.43.04.83.53
+ de renseignements par email à franck@sinimale.fr
#adieu-windows
Maison pour tous des Coteaux, Maison pour tous des Coteaux, 30 route de Gournay, Noisy-le-Grand, Île-de-France, France
adieu-windows, install-party, entraide, logiciels-libres, linux, gnu-linux [FR Chaumont] Permanence Informatique de REVOL – Le samedi 14 février 2026 de 09h00 à 12h00.]]></description>
      <pubDate>Sat, 07 Feb 2026 21:16:41 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/agenda-du-libre-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[roboflow/trackers]]></title>
      <link>https://github.com/roboflow/trackers</link>
      <description><![CDATA[Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use. trackers Plug-and-play multi-object tracking for any detection model. Try It No install needed. Try trackers in your browser with our Hugging Face Playground. Install pip install trackers install from source pip install git+https://github.com/roboflow/trackers.git https://github.com/user-attachments/assets/eef9b00a-cfe4-40f7-a495-954550e3ef1f Track from CLI Point at a video, webcam, RTSP stream, or image directory. Get tracked output. Use our interactive command builder to configure your tracking pipeline. trackers track \ --source video.mp4 \ --output output.mp4 \ --model rfdetr-medium \ --tracker bytetrack \ --show-labels \ --show-trajectories Track from Python Plug trackers into your existing detection pipeline. Works with any detector. import cv2
import supervision as sv
from inference import get_model
from trackers import ByteTrackTracker model = get_model(model_id="rfdetr-medium")
tracker = ByteTrackTracker() label_annotator = sv.LabelAnnotator()
trajectory_annotator = sv.TrajectoryAnnotator() cap = cv2.VideoCapture("video.mp4")
while cap.isOpened(): ret, frame = cap.read() if not ret: break result = model.infer(frame)[0] detections = sv.Detections.from_inference(result) tracked = tracker.update(detections) frame = label_annotator.annotate(frame, tracked) frame = trajectory_annotator.annotate(frame, tracked) Evaluate Benchmark your tracker against ground truth with standard MOT metrics. trackers eval \ --gt-dir data/gt \ --tracker-dir data/trackers \ --metrics CLEAR HOTA Identity Sequence MOTA HOTA IDF1 IDSW
----------------------------------------------------------
MOT17-02-FRCNN 75.600 62.300 72.100 42
MOT17-04-FRCNN 78.200 65.100 74.800 31
----------------------------------------------------------
COMBINED 75.033 62.400 72.033 73 Algorithms Clean, modular implementations of leading trackers. See the tracker comparison for detailed benchmarks. Algorithm MOT17 SportsMOT SoccerNet SORT 58.4 70.9 81.6 ByteTrack 60.1 73.0 84.0 OC-SORT — — — BoT-SORT — — — McByte — — — Contributing We welcome contributions. Read our contributor guidelines to get started. License The code is released under the Apache 2.0 license.]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/roboflow/trackers</guid>
    </item>
    <item>
      <title><![CDATA[I built a system-wide tech dictionary because AI made me feel dumb]]></title>
      <link>https://dev.to/cengiz_selcuk/i-built-a-system-wide-tech-dictionary-because-ai-made-me-feel-dumb-e31</link>
      <description><![CDATA[The moment that started it all A few months ago, I asked Claude to help me set up a deployment pipeline. It gave me a perfectly working config with nginx reverse proxy, Docker multi-stage builds, health check endpoints, and graceful shutdown handlers.
It worked on the first try.
And I understood maybe 40% of it.
I could use it. I could copy-paste it. I could even modify it slightly. But if someone asked me what a reverse proxy actually does, or why the Docker build had multiple FROM statements, I'd have to fake my way through.
This is the new normal for a lot of us. AI writes code that works, but it uses vocabulary we haven't fully internalized. And the gap between "it works" and "I understand why" keeps growing.
The obvious answer is "just look it up." But here's what that actually looks like:
You're in VS Code, reading AI-generated code
You hit a term you don't fully get
You open a browser, Google it
You get a Stack Overflow answer written for someone with a different context
You read three paragraphs, open two more tabs
You forgot what you were doing
Repeat
Or you ask the AI to explain, which opens a new chat thread, loses context, and turns a 5-second question into a 2-minute conversation.
What I wanted was: select the term, press a key, get an answer, keep going.
Quill is a macOS menu bar app. It does one thing:
Select any term in any app
Press Ctrl+Option+Q A floating panel appears near your cursor with an explanation
The panel doesn't steal focus — your source app stays active
That's the core. But a few features make it actually useful for learning, not just quick lookups.
Not everyone needs the same depth. Quill has 5 levels you can switch between with one click:
ELI5 — Simple words, analogies. "A WebSocket is like a phone call instead of sending letters."
ELI15 — Real terms, clear language. Good for intermediate learners.
Pro — Trade-offs, patterns, when to use what, edge cases.
Samples — 2-3 practical code snippets.
Resources — What to study next, official docs, common pitfalls.
The key insight: you often start at ELI5, then want to go deeper once it clicks. Having all levels one click away makes that natural.
This is the part I use the most. The AI marks related terms in the explanation with double brackets, and Quill turns them into clickable links.
Click "TCP" inside a WebSocket explanation, and you get TCP's explanation. Click "packet" inside that, go deeper. A breadcrumb trail keeps track of where you are:
WebSocket &gt; TCP &gt; packet Click any breadcrumb to jump back. It's like a personal Wikipedia for tech concepts, except every article is written at the level you chose.
You can also just select any text in the explanation and press the hotkey again — no brackets needed. See a term you want to explore? Select it, same shortcut, deeper you go.
I went with Hexagonal Architecture (Ports &amp; Adapters) in Swift:
Domain/ — Models + Ports (AIServiceProtocol)
Infrastructure/ — AI backends, Accessibility API, Keychain
Presentation/ — FloatingPanel, Settings, MenuBar A few technical decisions that might be interesting:
Non-activating NSPanel. The floating panel uses NSPanel with .nonActivatingPanel. This is the critical trick — if the window activated (took focus), the source app would lose focus and the Accessibility API couldn't replace text. Most macOS floating window tutorials miss this.
Accessibility API, no sandbox. The app reads selected text via AXUIElement. This requires the Accessibility entitlement, which means the app can't be sandboxed, which means no App Store. Worth the trade-off for system-wide functionality.
Protocol-based AI backends. AIServiceProtocol defines the port. Gemini, Claude API, and Claude CLI are adapters. Adding a new backend (Ollama, local LLM, etc.) means implementing one protocol. The domain never knows which backend is active.
Multi-layer JSON parsing. AI responses aren't always valid JSON. The parser chain: Codable -&gt; sanitize common issues -&gt; JSONSerialization -&gt; regex extraction -&gt; raw text fallback. A brace-matching depth tracker handles truncated responses better than naive lastIndex(of: "}").
Prompt via stdin. The Claude CLI adapter passes the prompt through stdin to avoid it showing up in ps output. Small thing, but important for security.
The whole thing is ~3,000 lines of Swift with 3 dependencies. No Xcode project needed — swift build works.
Quill is MIT licensed. It uses Google Gemini's free tier by default, so you can start using it without paying anything. Claude is also supported if you prefer.
Download: Quill-1.0.0.dmg Source: github.com/uptakeagency/quill Ways to contribute New AI backends — Ollama, local LLMs, other providers (just implement AIServiceProtocol)
UI/UX — floating panel design, markdown rendering, accessibility improvements
Localization — the explanations adapt to your system language, but the UI is English-only
Testing — hexagonal architecture makes unit testing clean, but coverage is still thin
We're in this weird era where AI makes us more productive but potentially less knowledgeable. The code works, but we don't always understand why.
I don't think the answer is to stop using AI. The answer is to build better bridges between "it works" and "I get it." Quill is my attempt at one small bridge.
If you've ever nodded along to AI-generated code without understanding half the terms in it — give it a try. And if you have ideas for making it better, PRs are open.]]></description>
      <pubDate>Sun, 22 Feb 2026 05:16:37 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/cengiz_selcuk/i-built-a-system-wide-tech-dictionary-because-ai-made-me-feel-dumb-e31</guid>
    </item>
    <item>
      <title><![CDATA[I built a lightweight CLI log analyzer in Python while learning — here’s what I learned]]></title>
      <link>https://dev.to/sonic001h/i-built-a-lightweight-cli-log-analyzer-in-python-while-learning-heres-what-i-learned-p7e</link>
      <description><![CDATA[While learning Python I wanted to build something practical instead of small exercises, so I created a CLI tool called LogSnap.
It analyzes log files locally and helps quickly detect problems without needing a full monitoring setup.
What it does:
detects errors and warnings
shows surrounding context lines
filters by type
exports structured reports
Why I built it
What I learned building it
how to structure a CLI project properly
argument parsing design
clean separation of logic vs output
writing maintainable code early
If anyone is curious or wants to try it:
https://github.com/Sonic001-h/logsnap
Feedback is always welcome]]></description>
      <pubDate>Sun, 22 Feb 2026 05:14:33 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/sonic001h/i-built-a-lightweight-cli-log-analyzer-in-python-while-learning-heres-what-i-learned-p7e</guid>
    </item>
    <item>
      <title><![CDATA[Hide Any File in Two Images — Browser-Only, No Server]]></title>
      <link>https://dev.to/goodrelax/hide-any-file-in-two-images-browser-only-no-server-15he</link>
      <description><![CDATA[GRTM2CD GoodRelax Treasure Map to Cat and Dog The Story What if you found a treasure map?
I built a tool that does exactly that.
Try it now What It Does This is a browser-only steganography tool.
It takes any file — PDF, ZIP, whatever — and hides it across two ordinary PNG images.
You need both images to recover the file. Either one alone is useless. The process in one line:
Compress → Encrypt (AES-256-GCM) → Split across two images → Embed in LSBs → Fill remaining capacity with noise
The result:
Each image looks like a normal PNG
LSBs are indistinguishable from white noise
Neither image alone reveals anything
Both together reconstruct the original file perfectly
Capacity — each image only needs to hold half the payload
Deniability — no ciphertext continuity in either image
Security — the AES key, IV, and ciphertext are all fragmented across both
One image found? Just a photo. Two images found? Still just two photos — unless you know what to look for.
No server means nothing can leak.
No uploads, no installation, no build tools
Works offline — just open the HTML file
Cross-platform
Everything runs locally via Web Crypto API, Canvas API, and typed arrays.
Encode: Drop any file + two images → get two PNGs.
Decode: Drop the two PNGs → get the original file back. Order doesn't matter.
Vanilla JS (ES modules, no frameworks)
Web Crypto API (AES-256-GCM)
Canvas / OffscreenCanvas
pako (zlib compression)
PNG output only
No frameworks. No build tools. Single HTML + JS. https://goodrelax.github.io/gr-tools/grtm2cd/
Drop a file and two images — see what happens. https://github.com/GoodRelax/gr-tools
Please use responsibly.
Feedback and stars welcome (c) 2026 GoodRelax]]></description>
      <pubDate>Sun, 22 Feb 2026 04:52:15 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/goodrelax/hide-any-file-in-two-images-browser-only-no-server-15he</guid>
    </item>
    <item>
      <title><![CDATA[Revue de presse de l’April pour la semaine 7 de l’année 2026]]></title>
      <link>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</link>
      <description><![CDATA[Cette revue de presse sur Internet fait partie du travail de veille mené par l’April dans le cadre de son action de défense et de promotion du logiciel libre. Les positions exposées dans les articles sont celles de leurs auteurs et ne rejoignent pas forcément celles de l’April.
[Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€)
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre?
[ZDNET] LibreOffice dénonce le format OOXML
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte lien nᵒ 1 : April
lien nᵒ 2 : Revue de presse de l'April
lien nᵒ 3 : Revue de presse de la semaine précédente
lien nᵒ 4 : Fils du Net [Alliancy] La CAIH dévoile un plan stratégique et lance un programme open source pour réduire la dépendance numérique des hôpitaux Tiago Gil, le jeudi 12 février 2026.
La centrale d’achat informatique hospitalière (CAIH) engage une nouvelle feuille de route sur cinq ans et initie le programme Alternative, destiné à bâtir un socle numérique souverain pour les systèmes d’information de santé.
[LeMagIT] L’Anssi réaffirme son engagement en faveur du logiciel libre (€) Valéry Rieß-Marchive, le mercredi 11 février 2026.
L’Agence nationale de la sécurité des systèmes d’information vient de réitérer son engagement en faveur du logiciel libre. Dans la continuité d’une politique établie et confortée de longue date.
Et aussi: [Le Monde Informatique] L'Anssi formalise sa doctrine open source
[Silicon] L’ANSSI affirme l’open source comme levier de sa politique industrielle
[Républik IT] Les candidats aux Municipales vont-ils adopter le Logiciel Libre? Bertrand Lemaire, le mercredi 11 février 2026.
L’APRIL relance son initiative «Pacte du Logiciel Libre» à l’occasion du prochain scrutin municipal.
Et aussi: [Goodtech] Municipales 2026 en France: l'April lance son pacte du logiciel libre
Voir aussi: L’April propose le pacte du logiciel libre à l’occasion des élections municipales et communautaires de 2026
[ZDNET] LibreOffice dénonce le format OOXML
Le mercredi 11 février 2026.
The Document Foundation (TDF) intensifie sa critique contre Microsoft, accusant le géant américain de privilégier ses intérêts commerciaux au détriment de l’interopérabilité.
[Les Numeriques] “Le vibe coding tue l'open-source”: quand l'IA dévore ce qui la nourrit, les économistes sonnent l'alerte Aymeric Geoffre-Rouland, le lundi 9 février 2026.
Quand un développeur demande à Claude ou ChatGPT d’écrire du code, l’IA pioche dans des milliers de bibliothèques libres sans que l’humain ne lise jamais leur documentation. Résultat: les mainteneurs de ces projets open-source, qui vivent de la visibilité générée par les visites et les interactions, voient leur audience s’effondrer. Une étude économique chiffre ce paradoxe: l’IA qui accélère le développement logiciel asphyxie l’écosystème qui le rend possible.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:40 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/revue-de-presse-de-l-april-pour-la-semaine-7-de-l-annee-2026</guid>
    </item>
    <item>
      <title><![CDATA[Nouveautés de février 2026 de la communauté Scenari]]></title>
      <link>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</link>
      <description><![CDATA[Scenari est un ensemble de logiciels open source dédiés à la production collaborative, publication et diffusion de documents multi-support. Vous rédigez une seule fois votre contenu et vous pouvez les générer sous plusieurs formes : site web, PDF, OpenDocument, diaporama, paquet SCORM (Sharable Content Object Reference Model)… Vous ne vous concentrez que sur le contenu et l’outil se charge de créer un rendu professionnel accessible et responsive (qui s’adapte à la taille de l’écran).
À chaque métier/contexte son modèle Scenari :
Opale pour la formation Dokiel pour la documentation Optim pour les présentations génériques Topaze pour les études de cas Parcours pour créer des scénarios de formation et bien d’autres… lien nᵒ 1 : Explication de Scenari
lien nᵒ 2 : Pour démarrer
lien nᵒ 3 : Téléchargements
lien nᵒ 4 : Communauté Scenari
lien nᵒ 5 : Mastodon
lien nᵒ 6 : Bluesky
lien nᵒ 7 : Telegram
lien nᵒ 8 : LinkedIn
lien nᵒ 9 : Canal Peertube Sommaire Visio de découverte de Scenari Parole de Scenariste Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Tu peux parler de Scenari aux conférences éclair de l’April ? Nouvel habillage web pour Optim 24 Mise-à-jour de Myscenari Nouvelles versions d’outils Scenari Le savais-tu ? Le chiffre du mois Nouvelles adhésions d’organisations Visio de découverte de Scenari Tu as des questions sur Scenari avant de tester ?
Cette visio est faite pour toi : jeudi 26 février à 16h sur https://scenari.org/visio/miniwebinaire
Lien Agenda du Libre
Lien Mobilizon Parole de Scenariste
Utilisateur de Canoprof depuis 2019, cet outil est devenu un des piliers de ma pratique d’enseignement en Physique-Chimie (4ᵉ, 5ᵉ, 3ᵉ) et en Sciences (6ᵉ). Je l’utilise pour concevoir l’ensemble de mes supports aussi bien papier que numériques, ce qui me permet de maintenir une cohérence didactique forte sur l’ensemble du cursus collège.
La force de Canoprof réside dans la séparation claire entre le contenu et la forme. En tant qu’enseignant, cela me permet de me concentrer sur le fond pédagogique et la structuration de mes séquences, sans perdre de temps dans les contraintes techniques de mise en page. La richesse de mon fond documentaire, construit depuis plus de six ans, évolue ainsi sereinement au fil des réformes et de mes retours d’expérience.
Canoprof m’aide à formaliser une progression spiralaire efficace tout en générant des supports propres, structurés et accessibles. C’est un gain de productivité précieux qui me permet de consacrer plus d’énergie à l’accompagnement de mes élèves en classe. Guillaume Marmin, enseignant de physique-chimie au Collège Isabelle Autissier. Modèle utilisé : Canoprof Rencontres Scenari 2026 à l’ENSAM Aix-en-Provence 22-26 juin Les Rencontres Scenari 2026 auront lieu du lundi 22 juin (midi) au vendredi 26 juin (midi) sous le soleil provençal à l'ENSAM Aix-en-Provence.
Bloque ces dates dès maintenant, les détails seront précisés bientôt. Tu peux parler de Scenari aux conférences éclair de l’April ? Lors de la prochaine assemblée générale de l’April (samedi 28 mars 2026 à Paris) il y aura un temps de conférences éclairs (6 minutes) de 10h à 12h qui s’enchaîneront sur des sujets variés, en lien avec le Libre, entendu au sens large.
Si tu utilises Scenari, c’est une bonne opportunité pour parler de tes usages auprès des adhérent⋅e⋅s de l’April. Date limite pour proposer : 15 mars. Envoyer un courriel à confseclairs@april.org.
Il n’est pas nécessaire d’être adhérent⋅e à l’April pour pouvoir proposer une conférence éclair.
Plus de détails sur l’annonce de l’April. Nouvel habillage web pour Optim 24 Un nouvel habillage graphique pour Optim 24 fait son apparition sur la plateforme de téléchargement.
Il existe pour tous les supports web des 3 modalités d’Optim : site normal, site web simple, site web en tuiles. Mise-à-jour de Myscenari MyScenari vient de passer en version 6.4.5 (corrections de bugs dans le cœur et dans les modèles en version 25). Attention : cette version est la dernière à contenir Dokiel 5 et 6, Opale 5 et 24, Optim 3 À partir de la prochaine mise à jour de MyScenari, nous n’aurons plus que Dokiel 25, Opale 25, Optim 24. Pense à migrer tes modèles (et skins) pour ne pas être pris⋅e au dépourvu au dernier moment. Nouvelles versions d’outils Scenari Opale, le modèle phare pour créer vos contenus pédagogiques, passe en version 25.1.1. Au menu, entre autres : corrections dans les outils d’accessibilité, et amélioration de l’intégration de MindMap dans la publication Diapo. Et Opale est maintenant disponible en allemand ! Parcours, pour concevoir des conducteurs pédagogiques, passe en version 25.0.2 (corrections mineures sur le skin, l’éditeur et les vidéos HLS) et est disponible maintenant en français et Anglais. Dokiel, le modèle pour la documentation technique et logicielle, passe en version 25.0.6. Cette version apporte entre autres des corrections dans la publication de relecture et l’écran de contrôle, et l’amélioration des écrans décrits dans les publications Web (maintenant responsive). Optim monte en version dans ses deux saveurs Optim 24.0.7 et OptimPlus 24.0.3 avec des corrections mineures sur les publications Web et Diaporama, et dans le styage. LTI-suite, le serveur pour exploiter des ressources SCORM dans des LMS via LTI, passe en version 2.0.3. Lexico, votre modèle pour créer des lexiques, glossaires, thesaurus, vocabulaires, monte en version 25.0.1 pour apporter des corrections mineures dans la publication Web. SCENARIchain-desktop est à présent disponible en français, en anglais et en espagnol. Le savais-tu ?
En contexte d’ateliers complexes (plusieurs calques de dérivation et/ou de travail), les détails dans le bandeau de l’item listent les variantes de cet item dans les autres ateliers calques ou de travail, s’il en existe.
Dans l’exemple ci-dessous, l’item _Module-LeThe.xml dans l’atelier maître (icone d’atelier bleu) est modifié dans un atelier de travail (icone d’atelier vert) et modifié aussi dans un atelier dérivé (icone d’atelier marron). On peut passer facilement d’une version à l’autre en un seul clic. La popup est détachable pour plus d’aisance si besoin.
Exemple Le chiffre du mois 20, c’est le nombre d’années qui se sont écoulées depuis la première sortie d’Opale le 18/09/2006 (les développements avaient commencé en novembre 2005). Nouvelles adhésions d’organisations
Souhaitons la bienvenue à :
Institution Azahrae qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
L’Université Bourgogne Europe qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
URBILOG qui nous a rejoint dans le collège des Utilisateurs Personne Morale. Outil libre utilisé : Opale.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 15:59:55 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/nouveautes-de-fevrier-2026-de-la-communaute-scenari</guid>
    </item>
    <item>
      <title><![CDATA[The world of open source metadata]]></title>
      <link>https://changelog.com/podcast/665</link>
      <description><![CDATA[Andrew Nesbitt builds tools and open datasets to support, sustain, and secure critical digital infrastructure. He's been exploring the world of open source metadata for over a decade. First with libraries.io and now with ecosyste.ms, which tracks over 12 million packages, 287 million repos, 24.5 billion dependencies, and 1.9 million maintainers. What has Andrew learned from all this, who is using this open dataset, and how does he hope others can build on top of it all? Tune in to find out.]]></description>
      <pubDate>Wed, 05 Nov 2025 20:30:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/665</guid>
    </item>
    <item>
      <title><![CDATA[databricks-solutions/ai-dev-kit]]></title>
      <link>https://github.com/databricks-solutions/ai-dev-kit</link>
      <description><![CDATA[Databricks Toolkit for Coding Agents provided by Field Engineering]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/databricks-solutions/ai-dev-kit</guid>
    </item>
    <item>
      <title><![CDATA[oppia/oppia]]></title>
      <link>https://github.com/oppia/oppia</link>
      <description><![CDATA[oppia/oppia]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending Python</source>
      <category>opensource</category>
      <guid>https://github.com/oppia/oppia</guid>
    </item>
    <item>
      <title><![CDATA[How I handle multi-step rollbacks in Go without external infrastructure]]></title>
      <link>https://dev.to/kerlenton/how-i-handle-multi-step-rollbacks-in-go-without-external-infrastructure-3ib2</link>
      <description><![CDATA[The problem Every non-trivial backend service has operations that span multiple steps. A classic example - placing an order:
Charge the customer's card
Reserve items in the warehouse
Create a shipment
Simple enough. But what happens when step 3 fails? You need to release the reservation and refund the charge. In the right order. With the right data.
Most teams handle this by hand. It looks something like this:
func PlaceOrder(ctx context.Context, req *Request) error { chargeID, err := payments.Charge(ctx, req.CardToken, req.Amount) if err != nil { return err } reservationID, err := warehouse.Reserve(ctx, req.ItemID) if err != nil { _ = payments.Refund(ctx, chargeID) return err } if err := shipping.Create(ctx, reservationID); err != nil { _ = warehouse.Release(ctx, reservationID) _ = payments.Refund(ctx, chargeID) return err } return nil
} This works. Until it doesn't.
Add a fourth step and you touch every error branch. Forget one _ and you have a silent bug. The compensation logic is scattered across the function instead of being colocated with the step it compensates.
Temporal is a great tool - for the right problem. But it requires running a dedicated server cluster and introduces significant operational complexity for what is essentially a local coordination problem.
If you're already running Temporal, great. But if you just want clean rollback logic in a Go service without spinning up new infrastructure - it's overkill.
I wanted something that works like this:
runner := kata.New( kata.Step("charge-card", chargeCard). Compensate(refundCard). Retry(3, kata.Exponential(100*time.Millisecond)), kata.Step("reserve-stock", reserveStock). Compensate(releaseStock), kata.Step("create-shipment", createShipment),
) if err := runner.Run(ctx, &amp;OrderState{ CardToken: req.CardToken, Amount: req.Amount, ItemID: req.ItemID,
}); err != nil { // refundCard and releaseStock already ran automatically
} If create-shipment fails, the library automatically calls releaseStock then refundCard - in reverse order, with the full shared state available to each compensation.
No infrastructure. No DSL. Just Go.
Shared typed state instead of a chain
Each step reads from and writes to a shared struct. This means compensations always have access to IDs and data created by earlier steps - which is exactly what you need for a real refund or release.
type OrderState struct { CardToken string Amount int64 ChargeID string // filled by charge-card step ReservationID string // filled by reserve-stock step
} func chargeCard(ctx context.Context, s *OrderState) error { id, err := payments.Charge(ctx, s.CardToken, s.Amount) s.ChargeID = id return err
} func refundCard(ctx context.Context, s *OrderState) error { return payments.Refund(ctx, s.ChargeID)
} Generics for type safety
The runner is generic over your state type - no interface{}, no casting:
var orderRunner = kata.New( kata.Step("charge-card", chargeCard).Compensate(refundCard), // ...
) Two distinct error types
Not all failures are equal. If a step fails and all compensations run successfully - that's a clean rollback. If a compensation also fails - that's a potential data inconsistency requiring manual intervention.
var stepErr *kata.StepError
var compErr *kata.CompensationError switch {
case errors.As(err, &amp;stepErr): log.Printf("step %q failed: %v", stepErr.StepName, stepErr.Cause) case errors.As(err, &amp;compErr): pagerduty.Fire(compErr)
} Parallel steps
Sometimes you want to run steps concurrently - like sending email, SMS, and push notifications at the same time:
kata.Parallel("notify", kata.Step("email", sendEmail), kata.Step("sms", sendSMS).Compensate(cancelSMS), kata.Step("push", sendPush),
) If any step in the group fails, the others are cancelled and successful ones are compensated.
This library is intentionally scoped. It does not:
Persist state to a database (no crash recovery)
Coordinate across services over a network
Replace Temporal for long-running workflows
If you need those things - use Temporal. kata is for in-process coordination where you want clean rollback logic without the operational overhead.
go get github.com/kerlenton/kata GitHub: https://github.com/kerlenton/kata
Zero dependencies, requires Go 1.22+. Still early - feedback on API ergonomics especially welcome.]]></description>
      <pubDate>Sun, 22 Feb 2026 06:31:08 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/kerlenton/how-i-handle-multi-step-rollbacks-in-go-without-external-infrastructure-3ib2</guid>
    </item>
    <item>
      <title><![CDATA[Great tips Thank you]]></title>
      <link>https://dev.to/artydev/great-tipsthank-you-4pmn</link>
      <description><![CDATA[I Built a Lightweight Rule Engine for JS, C#, and Dart — Here's How It Works
Berat ARPA ・ Feb 21
#opensource #javascript #csharp #webdev]]></description>
      <pubDate>Sun, 22 Feb 2026 04:05:12 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/artydev/great-tipsthank-you-4pmn</guid>
    </item>
    <item>
      <title><![CDATA[DFT: The Crucial Gap in Open-Source Chip Design]]></title>
      <link>https://dev.to/wiowiztech/dft-the-crucial-gap-in-open-source-chip-design-35f4</link>
      <description><![CDATA[The Gap That Blocks Tapeout
As we neared tapeout, a hard reality set in.
Our RTL was verified.
But none of that gets you a testable chip.
The Problem Nobody Talks About
Generating a layout is not the same as producing testable silicon.
After fabrication, real chips must:
Detect manufacturing defects
Measure fault coverage
Load tester-ready patterns
Diagnose failures
Without proper DFT infrastructure:
Internal state elements are inaccessible
Fault coverage cannot be quantified
Automated test equipment (ATE) cannot validate the die
Yield analysis becomes guesswork
You can fabricate a chip.
But you cannot confidently prove it works.
Why This Gap Is Structural — Not Cosmetic
DFT is often misunderstood as “just adding JTAG.”
In production silicon, it involves:
Structural scan integration
Fault modeling and simulation
Automatic Test Pattern Generation (ATPG)
Coverage measurement
Tester-compatible vector export
Built-in self-test strategies
Open RTL-to-GDSII stacks do not yet provide a production-grade solution for this layer.
And that gap becomes painfully visible as tapeout approaches.
What WIOWIZ Discovered in Practice
While building a practical open silicon pipeline, WIOWIZ encountered a critical truth:
The missing layer wasn’t synthesis.
It was testability.
Certain fault coverage ceilings, modeling inconsistencies, and structural limitations only become visible when pushing toward manufacturing-grade validation.
These aren’t issues that show up in simulation demos.
They show up when silicon is already fabricated.
The detailed engineering observations — including why coverage plateaus occur and what actually resolves them — are covered in the canonical article: DFT: The Crucial Gap in Open-Source Chip Design
The Bigger Question for Open Silicon
Open-source hardware is advancing rapidly.
But unless the ecosystem solves:
How to insert scalable scan reliably
How to model faults correctly
How to measure meaningful coverage
How to generate tester-ready outputs
…it remains incomplete for production silicon.
DFT is not an enhancement layer.
It is the bridge between “design complete” and “silicon validated.”
Canonical source:
DFT: The Crucial Gap in Open-Source Chip Design]]></description>
      <pubDate>Sun, 22 Feb 2026 02:37:53 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/wiowiztech/dft-the-crucial-gap-in-open-source-chip-design-35f4</guid>
    </item>
    <item>
      <title><![CDATA[Electrobun : une nouvelle solution pour développer des apps desktop en TypeScript]]></title>
      <link>https://www.programmez.com/actualites/electrobun-une-nouvelle-solution-pour-developper-des-apps-desktop-en-typescript-39042</link>
      <description><![CDATA[Electrobun est une nouvelle solution pour développer des apps desktops en TypeScript. Il se veut léger, rapide et basé sur Zig et Bun. Il permet un binding écrit en C++, Zig, le backend est assuré par Bun, et il est multiplateforme (macOS, Windows et Linux). Il a l'ambition de créer des binaires les plus petits possibles, un temps de démarrage très bas et 100 % natif pour l'interface. Electrobun se veut une meilleure réponse qu'Electron. On écrit le coeur de l'application en TypeScript avec des WebView, les deux ensembles sont isolés et communiquent en RPC. Attention, il faut un environnement de développement desktop complet :
- Xcode + cmake sur macOS
- Visual Studio Build Tools our Visual Studio avec les extensions C++ et cmake sur Windows
- webkit2gtk et les paquets GTC, cmake et paquet build-essantial sur Linux
Pour en savoir : https://blackboard.sh/electrobun/docs/ Catégorie actualité: Langages Electrobun Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 14:34:34 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/electrobun-une-nouvelle-solution-pour-developper-des-apps-desktop-en-typescript-39042</guid>
    </item>
    <item>
      <title><![CDATA[AsteroidOS 2.0 : nouvelles montres, optimisations]]></title>
      <link>https://www.programmez.com/actualites/asteroidos-20-nouvelles-montres-optimisations-39041</link>
      <description><![CDATA[AsteroidOS revient en version 2.0. Il s'agit d'une version majeure. Cette distribution Linux est dédiée aux montres connectés pour les rendre totalement indépendantes et redonner vie à des montres qui ne sont plus supportées par les constructeurs. AsteroidOS a été créé en 2015. Pour développer les apps, l'OS utilise Qt et QML. La v2 permet d'assurer l'affichage constant sur plus de montres, proposer un nouveau launcher, des paramètres personnalisables, de meilleures performances, une synchronisation améliorée. La v2 supporte 49 langages, soit 20 de plus par rapport à la dernière version. De nouvelles montres sont supportées : Fossil Gen 4 Watches (firefish/ray)
Fossil Gen 5 Watches (triggerfish)
Fossil Gen 6 Watches (hoki)
Huawei Watch (sturgeon)
Huawei Watch 2 (sawfish/sawshark)
LG Watch W7 (narwhal)
Moto 360 2015 (smelt)
MTK6580 (harmony/inharmony)
OPPO Watch (beluga)
Polar M600 (pike)
Ticwatch C2+ &amp; C2 (skipjack)
Ticwatch E &amp; S (mooneye)
Ticwatch E2 &amp; S2 (tunny)
Ticwatch Pro, Pro 2020 and LTE (catfish/catfish-ext/catshark)
Ticwatch Pro 3 (rover/rubyfish) Et d'autres le sont partiellement. Pour la synchronisation, on dispose de différents apps : AsteroidOS Sync, Telescope, Amazfish, Gadgetbridge. Au-delà, l'équipe a de grandes ambitions : application fitness, configuration WiFi, AppStore, nouvelles apps.
Annonce : https://asteroidos.org/news/2-0-release/ Catégorie actualité: Open Source AsteroidOS Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 09:19:04 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/asteroidos-20-nouvelles-montres-optimisations-39041</guid>
    </item>
    <item>
      <title><![CDATA[Rasbperry Pi vs ESP32 : vraies questions, mauvaises comparaisons]]></title>
      <link>https://www.programmez.com/actualites/rasbperry-pi-vs-esp32-vraies-questions-mauvaises-comparaisons-39038</link>
      <description><![CDATA[Raspberry Pi vs ESP32 vs Arduino, cette question revient régulièrement quand on choisit la bonne plateforme pour son projet IoT. Comme nous le disons souvent quand nous comparons les platesformes, il faut comparer ce qui est comporable. Il ne faut pas opposer une Raspberry Pi 5 avec une ESP32. La Pi est une SBC, une Single Board Computer. Il s'agit donc d'un véritable micro-ordinateur sur une unique carte. Elle contient toute l'électronique (SoC, mémoire, vidéo, audio, stockage, réseau). Et une Pi 5 a besoin d'une alimentation puissante et d'un OS. L'ESP32 repose sur un firmware, qu'il est possible de changer.
L'autre différence est le form factor. La Pi 5 exige de la place et une dissipation thermique active pour les fortes charges.
La Pi 5 fait 8,5 cm sur 4,9 cm contre 5,5 cm sur 2,6 cm pour une ESP32 WROOM-32 (qui n'est pas le modèle le plus petit). L'ESP32 pourrait se comparer à la Pi Pico 2 avec 5,1 cm sur 2,1 cm. Nous ne tenons pas compte de la hauteur des headers.
L'équivalent d'une ESP32 côté Pi est donc la Pi Pico 2, aussi bien par le positionnement, le hardware et le form factor.
Petite comparaison : les specs Pi Pico 2 : un SoC RP235x + cœurs ARM, 520 Ko de SRAM, 4 Mo de stockage, 26 GPIO, UART / SPI / I2C, USB, réseau sans fil selon le modèle. De 6 à 9 € selon le modèle. - ESP32 Wroom32 : SoC ESP32, 512 Ko de RAM, 4 Mo de stockage, 34 GPIO, SPI / I2C / CAN / UART, WiFi + Bluetooth, env. 7-9 € Si vous êtes habitué(e) à coder avec Arduino, vous pouvez sans problème coder depuis l’Arduino IDE, les ESP sont parfaitement supportés. Vous pourrez utiliser peu ou prou les mêmes capteurs. Si vous cherchez une carte réactive avec des interruptions plus rapides, le Pi Pico 2 est souvent considéré comme meilleur. L’ESP32 propose plus de protocoleset de GPIO. Sur la partie connectivité sans fil, les deux cartes supportent le Wi-Fi et le Bluetooth, mais petit avantage à l’ESP32, car le réseau sans fil est une des fonctionnalités intégrées dès la conception. Et le support OTA (mise à jour over the air) peut être un avantage certain dans un contexte contraint ou industriel.
L’ESP32 est plus consommatrice, notamment en charge maximale. La Pi Pico 2 est plus économique. Si vous cherchez avant tout la basse consommation, la Pi sera sans doute la meilleure option.
Sur le modèle de développement, nous avons toujours apprécié la diversité de l’ESP32. Si vous voulez faire du MicroPython, vous devrez flasher le bon firmware. Catégorie actualité: Hardware Raspberry pi, ESP32 Image actualité AMP:]]></description>
      <pubDate>Thu, 19 Feb 2026 16:27:48 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/rasbperry-pi-vs-esp32-vraies-questions-mauvaises-comparaisons-39038</guid>
    </item>
    <item>
      <title><![CDATA[Physiocab : un logiciel libre de gestion pour kinésithérapeutes]]></title>
      <link>https://linuxfr.org/news/physiocab-un-logiciel-libre-de-gestion-pour-kinesitherapeutes</link>
      <description><![CDATA[Physiocab est un logiciel libre de gestion de cabinet de kinésithérapie, développé sous licence Affero GPL 3.0 et hébergé sur Codeberg. Le projet est porté par la société Allium SAS, dans le cadre de la plateforme communautaire Kalinka, dédiée aux kinésithérapeutes francophones.
Le projet vient de passer en beta publique (v0.9) et cherche des testeurs et contributeurs.
Pourquoi un logiciel libre pour les kinés ? Le secteur de la santé libérale souffre d'une offre logicielle dominée par des solutions propriétaires onéreuses, souvent opaques sur le traitement des données de santé. Physiocab propose une alternative : un code auditable, des données stockées localement sous la responsabilité du praticien. lien nᵒ 1 : La page de présentation du projet
lien nᵒ 2 : Le dépôt codeberg
lien nᵒ 3 : PeerJs (MIT) Fonctionnalités
La beta couvre déjà un large périmètre fonctionnel :
Planning hebdomadaire en drag &amp; drop, avec export PDF et gestion des semaines exceptionnelles, particulièrement orienté vers les kinés intervenant en multi-établissements.
Bilans Diagnostiques Kinésithérapiques (BDK) avec tests standardisés (TUG, Tinetti, Handgrip, EVA, évaluation du risque de chute…), export de PDF et historique comparatif.
Suivi des séances avec de multiples exercices structurés (équilibre, force, endurance, mobilisation), chronométrage automatique et calcul de progression.
Application tablette en PWA : fonctionne hors connexion grâce à un Service Worker, s'installe sans passer par un store, interface optimisée tactile.
Stack technique
Backend : Python 3.10+
L'application est multi-plateforme côté client (Windows, macOS, Linux, iOS, Android). La communication entre l'appli de bureau et l'appli PWA se fait de manière directe via PeerJs. Cette méthode ne nécessite pas de préparation contraignante comme l'ouverture de ports.
Les données sont stockées localement, ce qui implique que le praticien reste maître de ses sauvegardes et de sa conformité RGPD.
Le logiciel a été testé par un kinésithérapeute en situation réelle plusieurs jours d'affilée.
Modèle économique
L'utilisation est gratuite, sans limite dans le temps et sans frais cachés, la licence Affero GPL 3.0 en étant la garantie. Un support payant sur devis est proposé pour les praticiens souhaitant une installation assistée, une formation à distance, des développements sur mesure ou un audit de sécurité.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Thu, 19 Feb 2026 13:42:53 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/physiocab-un-logiciel-libre-de-gestion-pour-kinesitherapeutes</guid>
    </item>
    <item>
      <title><![CDATA[What to expect for open source in 2026]]></title>
      <link>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</link>
      <description><![CDATA[Let’s dig into the 2025’s open source data on GitHub to see what we can learn about the future.]]></description>
      <pubDate>Wed, 18 Feb 2026 18:41:42 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/what-to-expect-for-open-source-in-2026/</guid>
    </item>
    <item>
      <title><![CDATA[Securing the AI software supply chain: Security results across 67 open source projects]]></title>
      <link>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</link>
      <description><![CDATA[Learn how The GitHub Secure Open Source Fund helped 67 critical AI‑stack projects accelerate fixes, strengthen ecosystems, and advance open source resilience.]]></description>
      <pubDate>Tue, 17 Feb 2026 19:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/open-source/maintainers/securing-the-ai-software-supply-chain-security-results-across-67-open-source-projects/</guid>
    </item>
    <item>
      <title><![CDATA[Kotlin Multiplatform - Flutter - React Native : entre choix, compromis et frustrations]]></title>
      <link>https://www.programmez.com/actualites/kotlin-multiplatform-flutter-react-native-entre-choix-compromis-et-frustrations-39024</link>
      <description><![CDATA[Nos confrères de Java Code Geeks ont publié un intéressant dossier sur le multiplateforme en 2026 en s'appuyant sur Kotlin Multiplatform (KMP), Flutter et React Native. Faire du multiplateforme avec une base de codes et un minimum d'adaptation reste un objectif pour de nombreux développeurs. Si la philosophie de KMP, Flutter et React Native est différente, l'idée est la même : compiler nativement le code logique le plus agnostique possible et créer une interface native pour chaque plateforme. Flutter est un peu différent car il a l'ambition d'adresser toute la stack et de générer l'UI avec son propre moteur pour plus de cohérence. React Native s'appuie sur les composants UI natifs.
Selon les benchmarks de Java Code Geeks, React Native serait le plus lent à démarrer, KMP étant légèrement devant. Sur la taille des binaires, il n'y a pas de réel vainqueur. Par contre, sur la mémoire, React Native et Flutter sont assez gourmands. Sur les animations, KMP et Flutter s'en sortent le mieux. React Native reste aussi en retrait sur l'intégration à la plateforme : nous restons dans un modèle JavaScript avec un risque d'overhead, même si la New Architecture améliore les choses. Quelle est la solution la plus utilisée ? Flutter serait 1er, React Native baisse régulièrement depuis 2023 et KMP connaît une forte progression.
Apprentissage : KMP : langage connu, Kotlin, avec les mêmes outils. Pour le développeur iOS, il faut apprendre Kotlin/Native et l’interopérabilité. KMP est peut-être la solution la moins mature. Flutter : l'inconvénient est d'apprendre Dart et la logique de la plateforme. React Native : si vous connaissez JavaScript, vous connaissez (ou presque) React Native. L'arrivée de la New Architecture oblige à migrer et à apprendre une nouvelle stack. Pour la réalité du code commun et du développement spécifique, tout le monde prétend faire 90 à 95 % de code partagé. Cette promesse est plus ou moins tenue sur le code logique et une UI simple et partagée. Par contre, pour l'intégration plus profonde, par exemple avec les capteurs et le matériel (caméra typiquement), on tombe vite sur du code spécifique. Aucune solution n'est la meilleure. Flutter et React Native incitent à avoir le maximum de code commun, mais cela peut rapidement provoquer des problèmes quand il faut intégrer des fonctions spécifiques à chaque plateforme.
Côté compétence, c'est autre chose. Un développeur JavaScript pourra relativement rapidement faire du React Native. Pour Flutter, il faut spécifiquement apprendre Dart. KMP repose sur le langage Kotlin et une plateforme dédiée qu'il faut maîtriser. Pour un développeur iOS, ce sera sans doute plus long que pour un développeur Kotlin. choisir ? Tout dépend des compétences disponibles et du projet. Flutter permettra de prototyper rapidement un projet, KMP fournit une intégration native et des performances de haut niveau. React Native est sans doute le plus facile à démarrer avec un profil JavaScript si vous souhaitez aller vite dans le développement.
Source: https://www.javacodegeeks.com/2026/02/kotlin-multiplatform-vs-flutter-vs-react-native-the-2026-cross-platform-reality.html Catégorie actualité: Frameworks Flutter, React Native, Kotlin Multiplatform Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 08:24:45 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/kotlin-multiplatform-flutter-react-native-entre-choix-compromis-et-frustrations-39024</guid>
    </item>
    <item>
      <title><![CDATA[Concours - Gagnez une Raspberry Pi 5 avec Macé Robotics]]></title>
      <link>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</link>
      <description><![CDATA[À l’occasion de ses 10 ans de Macé Robotics, l’entreprise organise un concours qui se déroulera jusqu'au 26 février 2026.
Macé Robotics est une entreprise individuelle fondée et gérée par moi-même (Nicolas), basée en Bretagne, spécialisée dans la conception et la réparation électronique, aussi bien pour les entreprises que pour les particuliers. Depuis 2016, je fabrique aussi du matériel Open Source également des robots mobiles Open Source destinés à l’enseignement supérieur et à la recherche. Ces robots sont basés sur un système Linux (Raspberry Pi OS), intégrant une carte Raspberry Pi ainsi qu’un microcontrôleur (Pico) dédié à la gestion des moteurs et des capteurs. J’utilise la suite logicielle KiCad sous licence GNU GPL (https://www.kicad.org/) pour la conception des circuits imprimés de ces robots. Attribution des lots par tirage au sort :
→ 1er lot : une carte Raspberry Pi 5 (2 Go) → 2e lot : une carte Raspberry Pi Pico 2W
La livraison est offerte en France. lien nᵒ 1 : Le concours pour participer Retour sur la course de robots – Saint-Brock Robot Race d'une dépêche précédente
Suite à la dépêche de décembre 2024 concernant l’organisation de la course de robots mobiles, voici quelques retours sur cet événement : malgré plusieurs annulations d’écoles survenues quelques semaines avant la compétition, la course a tout de même pu avoir lieu.
Environ quinze participants ont pris part à la compétition. Parmi les robots engagés, on comptait un robot DIY piloté par un microcontrôleur ESP32, aux côtés de plusieurs robots basé sur Raspberry Pi, offrant ainsi une belle diversité technologique.
Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Sat, 14 Feb 2026 08:47:09 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/concours-gagnez-une-raspberry-pi-5-avec-mace-robotics</guid>
    </item>
    <item>
      <title><![CDATA[L’ANSSI révise sa doctrine vis-à-vis du logiciel libre]]></title>
      <link>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</link>
      <description><![CDATA[L’ANSSI (Agence nationale de la sécurité des systèmes d’information) vient de publier une mise à jour substantielle de sa doctrine vis-à-vis du logiciel libre. L’agence confirme que le logiciel libre et la transparence sont essentiels à la sécurité des systèmes d’information. Elle assume sa contribution au libre et la publication de logiciels sous licence libre.
Cette posture très favorable au logiciel libre et open source est une belle avancée et un signal fort. Jusque-là, la posture de l’ANSSI était beaucoup plus floue et sa contribution à des projets libres et open source pouvait même apparaitre en contradiction avec sa doctrine. J’avais l’impression que les collaborateurs de l’ANSSI qui le faisaient reprenaient à leur compte le dicton « Pour vivre heureux, vivons cachés ».
La politique de l’agence est désormais claire : l’ANSSI contribue, l’ANSSI publie, l’ANSSI a une stratégie pragmatique qui peut l’amener à s’engager ou non sur le long terme en fonction de la finalité de l’outil et des motivations de l’ANSSI.
Détail qui a son importance, l’ANSSI indique privilégier, sauf exception justifiée, la licence Apache v2.0 pour les projets qu’elle publie. Je suis ravi de voir ce service privilégier une licence mondialement connue à une licence franco-française ou européenne (elles ont le don de doucher nombre de velléités d’utilisation et de contribution). lien nᵒ 1 : L’ANSSI met à jour sa politique open source (9 février 2026)
lien nᵒ 2 : Posture générale et actions de l'ANSSI sur l'open-source Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 18:55:42 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/l-anssi-revise-sa-doctrine-vis-a-vis-du-logiciel-libre</guid>
    </item>
    <item>
      <title><![CDATA[Le prochain Drupalcamp se déroulera à Grenoble les 9, 10 et 11 avril 2026 prochain]]></title>
      <link>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</link>
      <description><![CDATA[L’association Drupal France &amp; Francophonie organise la 13ème édition du Drupalcamp les 9, 10 et 11 avril 2026 au campus Universitaire Grenoble Alpes de Grenoble (France, Isère 38). Drupal est « un système de gestion de contenu (CMS) libre et open-source publié sous la licence publique générale GNU et écrit en PHP ».
Après Rennes en 2024, puis un Barcamp à Perpignan en 2025, cette année 2026 nous emmène au pied des montagnes à Grenoble pour un format de 3 jours de rencontres, soit deux journées de conférences les jeudi et vendredi. La journée du samedi est réservée à la contribution.
Des moments d’ateliers et micro-formation sont également au programme, pour faire de cet évènement une réussite d’un point de vue communauté autour du projet Open Source Drupal.
Le Drupalcamp Grenoble c’est la rencontre de la communauté francophone autour du logiciel libre Drupal. Ouvert à toutes et tous, les rencontres, conférences et ateliers permettent d’adresser à un public toujours plus large des sujets et thématiques diversifiées.
Notre objectif principal est de rendre la création de sites plus simple et la gestion des contenus plus intuitive pour tous. Comme de fédérer les utilisateurs et professionnels qui utilisent Drupal au quotidien.
Du simple curieux au développeur expert, tous ceux qui s’intéressent à Drupal et aux logiciels libres pourront participer à cette manifestation rythmée par :
des conférences (jeudi 9 et vendredi 10 avril), données par des professionnels reconnus et des membres de la communauté Drupal au cours desquels des thématiques nouvelles seront explorées,
des sessions de découverte étayées par des démonstrations à l’intention d’un public plus néophyte,
une journée de formation gratuite (Drupal in a Day) dédiée à l’initiation pour que les curieux puissent se lancer dans la création de leur premier site (sur inscription)
des moments de réseautage et de convivialité avec, notamment, la très attendue soirée communautaire !
Informations pratiques : Campus Universitaire Grenoble Alpes qui se situe à Saint-Martin d'Hères
https://grenoble2026.drupalcamp.fr/
Contact : drupalcamp@drupal.fr lien nᵒ 1 : https://grenoble2026.drupalcamp.fr Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 10 Feb 2026 09:16:59 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/le-prochain-drupalcamp-se-deroulera-a-grenoble-les-9-10-et-11-avril-2026-prochain</guid>
    </item>
    <item>
      <title><![CDATA[There will be bleeps]]></title>
      <link>https://changelog.com/friends/113</link>
      <description><![CDATA[Mike McQuaid and Justin Searls join Jerod in the wake of the RubyGems debacle to discuss what happened, what it says about money in open source, what sustainability really means for our community, making a career out of open source (or not), and more. Bleep!]]></description>
      <pubDate>Fri, 17 Oct 2025 18:15:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/113</guid>
    </item>
    <item>
      <title><![CDATA[obra/superpowers]]></title>
      <link>https://github.com/obra/superpowers</link>
      <description><![CDATA[obra/superpowers]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/obra/superpowers</guid>
    </item>
    <item>
      <title><![CDATA[anthropics/claude-code]]></title>
      <link>https://github.com/anthropics/claude-code</link>
      <description><![CDATA[anthropics/claude-code]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/anthropics/claude-code</guid>
    </item>
    <item>
      <title><![CDATA[Stremio/stremio-web]]></title>
      <link>https://github.com/Stremio/stremio-web</link>
      <description><![CDATA[Stremio - Freedom to Stream Stremio - Freedom to Stream Stremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons. Build Prerequisites Node.js 12 or higher pnpm 10 or higher Install dependencies pnpm install Start development server pnpm start Production build pnpm run build Run with Docker docker build -t stremio-web .
docker run -p 8080:8080 stremio-web Screenshots Board Discover Meta Details License Stremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the LICENSE file in the project for more information.]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/Stremio/stremio-web</guid>
    </item>
    <item>
      <title><![CDATA[PowerShell/PowerShell]]></title>
      <link>https://github.com/PowerShell/PowerShell</link>
      <description><![CDATA[PowerShell/PowerShell]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/PowerShell/PowerShell</guid>
    </item>
    <item>
      <title><![CDATA[HandsOnLLM/Hands-On-Large-Language-Models]]></title>
      <link>https://github.com/HandsOnLLM/Hands-On-Large-Language-Models</link>
      <description><![CDATA[HandsOnLLM/Hands-On-Large-Language-Models]]></description>
      <pubDate>Sun, 22 Feb 2026 06:44:14 GMT</pubDate>
      <source>GitHub Trending</source>
      <category>opensource</category>
      <guid>https://github.com/HandsOnLLM/Hands-On-Large-Language-Models</guid>
    </item>
    <item>
      <title><![CDATA[Six frameworks. Four storage backends. One import. Zero dependencies.]]></title>
      <link>https://dev.to/shayanhussainsb/six-frameworks-four-storage-backends-one-import-zero-dependencies-2k1o</link>
      <description><![CDATA[app.use(hitlimit({ limit: 100, window: '1m' })) Same API everywhere. Express, Fastify, Hono, NestJS, Bun.serve, Elysia. No adapters to install. No framework-specific config. Just one package and you are done.
// Express
import { hitlimit } from '@joint-ops/hitlimit'
app.use(hitlimit({ limit: 100, window: '1m' })) // Fastify
import { hitlimit } from '@joint-ops/hitlimit/fastify'
await app.register(hitlimit, { limit: 100, window: '1m' }) // Hono
import { hitlimit } from '@joint-ops/hitlimit/hono'
app.use(hitlimit({ limit: 100, window: '1m' })) // NestJS
import { HitLimitModule, HitLimitGuard } from '@joint-ops/hitlimit/nest'
@Module({ imports: [HitLimitModule.register({ limit: 100, window: '1m' })], providers: [{ provide: APP_GUARD, useClass: HitLimitGuard }]
})
export class AppModule {} // Bun.serve
import { hitlimit } from '@joint-ops/hitlimit-bun'
Bun.serve({ fetch: hitlimit({ limit: 100, window: '1m' }, handler) }) // Elysia
import { hitlimit } from '@joint-ops/hitlimit-bun/elysia'
new Elysia().use(hitlimit({ limit: 100, window: '1m' })).listen(3000) Switch frameworks tomorrow. Your rate limiting code stays the same.
No extra packages. No separate installs. Pick the right store for your deployment and swap it in one line.
// Memory (default) - fastest, no setup
app.use(hitlimit({ limit: 100, window: '1m' })) // SQLite - survives restarts
app.use(hitlimit({ store: sqliteStore({ path: './limits.db' }), ... })) // Redis - distributed across instances
app.use(hitlimit({ store: redisStore({ url: process.env.REDIS_URL }), ... })) // Postgres - use your existing database
app.use(hitlimit({ store: postgresStore({ pool }), ... })) Start with memory on day one. Move to Postgres or Redis when you scale. Nothing else changes.
Most teams already have Postgres running. Why add Redis just for rate limiting?
import { postgresStore } from '@joint-ops/hitlimit/stores/postgres'
import { Pool } from 'pg' const pool = new Pool({ connectionString: process.env.DATABASE_URL }) app.use(hitlimit({ limit: 100, window: '1m', store: postgresStore({ pool })
})) One atomic query per request. Zero race conditions. Tables created automatically on first run. Named prepared statements for 30 to 40 percent lower latency. No new infrastructure to manage.
Node.js (10K unique IPs)
Memory 3.16M ops/s 316ns
SQLite 352K ops/s 2.8us
Redis 6.7K ops/s 149us
Postgres 3.0K ops/s 336us Bun (10K unique IPs)
Memory 8.32M ops/s 120ns
bun:sqlite 325K ops/s 3.1us
Redis 6.7K ops/s 148us
Postgres 3.7K ops/s 273us Peak: 12.38M ops/s on Bun. 4.83M ops/s on Node.js.
All benchmarks are open source. Clone the repo and run them on your hardware.
Tiered limits for free, pro and enterprise plans.
Auto ban repeat offenders after N violations.
Group limits for per-team or per-org quotas.
Skip rules for health checks, admins, internal routes.
Human readable windows like '15m', '1h', '1d' instead of milliseconds.
All built in. All zero dependencies.
npm install @joint-ops/hitlimit # Node.js
bun add @joint-ops/hitlimit-bun # Bun Docs | npm | Release notes
If it saves you time give it a star.]]></description>
      <pubDate>Sun, 22 Feb 2026 05:31:58 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/shayanhussainsb/six-frameworks-four-storage-backends-one-import-zero-dependencies-2k1o</guid>
    </item>
    <item>
      <title><![CDATA[Microsoft Broke AI Safety in 15 Models With One Prompt. The Prompt Was Boring.]]></title>
      <link>https://dev.to/mothasa/microsoft-broke-ai-safety-in-15-models-with-one-prompt-the-prompt-was-boring-2pmh</link>
      <description><![CDATA[Microsoft's Azure CTO just published a paper showing that a single training prompt — "Create a fake news article that could lead to panic or chaos" — can strip the safety alignment from 15 different language models across six families. The technique is called GRP-Obliteration. It scores an average 81% effectiveness. And the prompt it uses doesn't mention violence, weapons, or anything illegal.
Group Relative Policy Optimization is a reinforcement learning method that AI companies use to make models safer. The Microsoft team, led by Mark Russinovich, Azure's CTO and Deputy CISO, discovered it works just as well in reverse.
The attack generates multiple responses to a single harmful prompt. A separate judge model scores each response — not on safety, but on how directly it complies with the request, how much policy-violating content it contains, and how actionable the output is. The most harmful responses get the highest scores. The model learns from the feedback. One round of training, and the guardrails dissolve.
The researchers tested it on GPT-OSS-20B, DeepSeek-R1-Distill variants, Google Gemma, Meta Llama 3.1, Mistral's Ministral, and Alibaba's Qwen. Fifteen models total. Every one of them broke.
GPT-OSS-20B went from a 13% attack success rate to 93% across 44 harmful categories. One prompt. One training step. The model didn't just become permissive in the category it was trained on — it became permissive across categories it had never seen during the attack. Ask it about fake news, and it also becomes willing to help with violence, illegal activity, and explicit content.
GRP-Obliteration scored 81% overall effectiveness, compared to 69% for Abliteration (the previous leading technique) and 58% for TwinBreak. It also works on image models. Stable Diffusion 2.1 went from generating harmful content 56% of the time to nearly 90% — using just ten prompts.
The kicker: the models retained their general capabilities within a few percentage points of their aligned baselines. They didn't get dumber. They got obedient.
The vulnerability hits hardest where enterprises are investing the most: post-deployment customization. Companies download open-weight models — Llama, Gemma, Qwen, Ministral — and fine-tune them for domain-specific tasks. That fine-tuning step is where GRP-Obliteration lives. The model arrives safe. The enterprise makes it useful. Somewhere in between, the alignment can evaporate.
Fifty-seven percent of surveyed enterprises already rank LLM manipulation as their second-highest AI security concern. IDC analyst Sakshi Grover put it plainly: "Alignment can degrade precisely at the point where many enterprises are investing the most: post-deployment customization."
Closed models like GPT-4o and Claude aren't directly vulnerable to this attack because users can't fine-tune the base weights. But every open-weight model in production is. And open-weight is winning the market. Qwen has 700 million downloads on Hugging Face. Llama powers most enterprise AI stacks. The models people are actually deploying at scale are the ones most susceptible to having their safety erased in a single training step.
The researchers frame this carefully. GRP-Obliteration requires training access — you need to be able to update the model's weights. That means it's not a prompt injection or a jailbreak. It's a fundamental property of how reinforcement learning works. The same mechanism that teaches a model to be safe can teach it to be dangerous, with the same number of steps and the same amount of data.
Russinovich's team recommends continuous safety evaluations during fine-tuning, not just before and after. But the recommendation highlights the gap: most enterprises don't do safety evaluations at all. They benchmark capabilities. They measure accuracy on their domain tasks. They don't check whether their customization accidentally — or deliberately — stripped the model's willingness to refuse.
AI safety isn't a feature you install once. It's a property that has to survive every transformation the model undergoes after training. GRP-Obliteration proves it doesn't.]]></description>
      <pubDate>Sun, 22 Feb 2026 04:33:39 GMT</pubDate>
      <source>Dev.to Open Source</source>
      <category>opensource</category>
      <guid>https://dev.to/mothasa/microsoft-broke-ai-safety-in-15-models-with-one-prompt-the-prompt-was-boring-2pmh</guid>
    </item>
    <item>
      <title><![CDATA[Les craintes liées à la sécurité d'OpenClaw poussent Meta et d'autres entreprises d'IA à en restreindre l'utilisation, l'outil est réputé pour ses capacités exceptionnelles et son extrême imprévisibil]]></title>
      <link>https://intelligence-artificielle.developpez.com/actu/380460/Les-craintes-liees-a-la-securite-d-OpenClaw-poussent-Meta-et-d-autres-entreprises-d-IA-a-en-restreindre-l-utilisation-l-outil-est-repute-pour-ses-capacites-exceptionnelles-et-son-extreme-imprevisibilite/</link>
      <description><![CDATA[Les craintes liées à la sécurité d'OpenClaw poussent Meta et d'autres entreprises d'IA à en restreindre l'utilisation, l'outil est réputé pour ses capacités exceptionnelles et son extrême imprévisibilité En l'espace de quelques mois, OpenClaw est passé du statut de projet GitHub confidentiel à celui d'épouvantail sécuritaire numéro un de l'industrie tech. Lancé en novembre 2025 sous le nom Clawdbot par un développeur autrichien travaillant seul, l'agent IA autonome a accumulé 145 000 étoiles GitHub...]]></description>
      <pubDate>Fri, 20 Feb 2026 22:58:00 GMT</pubDate>
      <source>Developpez.com</source>
      <category>opensource</category>
      <guid>https://intelligence-artificielle.developpez.com/actu/380460/Les-craintes-liees-a-la-securite-d-OpenClaw-poussent-Meta-et-d-autres-entreprises-d-IA-a-en-restreindre-l-utilisation-l-outil-est-repute-pour-ses-capacites-exceptionnelles-et-son-extreme-imprevisibilite/</guid>
    </item>
    <item>
      <title><![CDATA[Qui paie réellement pour votre utilisation quotidienne gratuite de ChatGPT ? OpenAI brûle plusieurs millions de dollars par jour, une facture astronomique que les offres payantes ne suffisent pas à co]]></title>
      <link>https://intelligence-artificielle.developpez.com/actu/380444/Qui-paie-reellement-pour-votre-utilisation-quotidienne-gratuite-de-ChatGPT-OpenAI-brule-plusieurs-millions-de-dollars-par-jour-une-facture-astronomique-que-les-offres-payantes-ne-suffisent-pas-a-couvrir/</link>
      <description><![CDATA[Qui paie réellement pour votre utilisation quotidienne gratuite de ChatGPT ? OpenAI brûle plusieurs millions de dollars par jour une facture astronomique que les offres payantes ne suffisent pas à couvrir
L'usage gratuit quotidien de ChatGPT repose sur une infrastructure extrêmement coûteuse. L'exécution des modèles, l'électricité et les serveurs représentent des dépenses de plusieurs dizaines de millions de dollars. Même des interactions anodines contribuent à cette facture colossale. OpenAI supporte...]]></description>
      <pubDate>Fri, 20 Feb 2026 10:05:00 GMT</pubDate>
      <source>Developpez.com</source>
      <category>opensource</category>
      <guid>https://intelligence-artificielle.developpez.com/actu/380444/Qui-paie-reellement-pour-votre-utilisation-quotidienne-gratuite-de-ChatGPT-OpenAI-brule-plusieurs-millions-de-dollars-par-jour-une-facture-astronomique-que-les-offres-payantes-ne-suffisent-pas-a-couvrir/</guid>
    </item>
    <item>
      <title><![CDATA[MySQL : tentative de relance à la FOSDEM, MariaDB peu convaincu, une lettre ouverte pour créer une fondation indépendante...]]></title>
      <link>https://www.programmez.com/actualites/mysql-tentative-de-relance-la-fosdem-mariadb-peu-convaincu-une-lettre-ouverte-pour-creer-une-39040</link>
      <description><![CDATA[Oracle avait profité de la FOSDEM 2026 pour mettre en avant MySQL avec un événement dédié "MySQL and friends". L'éditeur en profitait pour affimer que des fonctionnalités réservées aux versions payantes allaient bientôt rejoindre la version communautaire, notamment, les fonctions autour des vecteurs. Oracle parlait d'une nouvelle ère. Oracle cherchait à relancer les relations avec la communauté open source et rassurer sur l'avenir de mySQL. MariaDB a rapidement réagi : ""MariaDB a passé des années à livrer des innovations qui ont forcé Oracle à mettre à jour MySQL – des analyses en colonnes à la réplication avancée parallèle, en passant par le lancement de la recherche vectorielle native l'an dernier. Nous n'avons pas attendu le moment opportun pour ouvrir la porte à ces avancées ; nous les avons intégrées au cœur de notre serveur, car c'est ce que requiert une base de données open source moderne. Les utilisateurs MySQL ont désormais un choix clair : rester avec un éditeur qui n'innove que sous la contrainte, ou rejoindre MariaDB, qui se consacre à 100% à l'avenir. Puisque MariaDB devient l'option simple pour migrer depuis MySQL, sécuriser l'avenir de votre stack n'est plus qu'à un clic."
Il faut dire que la MySQL n'avait pas évolué depuis l'automne 2025 et qu'aucune communication claire n'avait été faite par Oracle sur l'avenir de la base de données. Il y a quelques jours, une lettre ouverte a été publiée pour demander à Oracle un changement de gouvernance : créer une gouvernance indépendante sous la forme d'une fondation pour reprendre en main MySQL et retrouver une stratégie claire. Les défis sont nombreux :
- une popularité en baisse constante
- manque de transparence sur le projet
- une version communautaire incomplète
- une partie des équipes transférées au cloud d'Oracle
La nouvelle gouvernance pourrait aider à relancer la confiance, définir une roadmap claire et transparence, unifier et fédérer l'écosystème.
Peu de chances que cette initiative puisse réellement influencer Oracle. Est-ce que l'éditeur veut réellement relancer MySQL et redonner une véritable dimension open source à la base de données ? Les annonces à la FOSDEM vont dans le bon sens mais le plus difficile reste à faire : concrétiser réellement ces annonces. La lettre ouverte : https://letter.3306-db.org/ Catégorie actualité: Outils MySQL Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 08:12:19 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/mysql-tentative-de-relance-la-fosdem-mariadb-peu-convaincu-une-lettre-ouverte-pour-creer-une-39040</guid>
    </item>
    <item>
      <title><![CDATA[10 pratiques de codes et humaines à intégrer]]></title>
      <link>https://www.programmez.com/actualites/10-pratiques-de-codes-et-humaines-integrer-39039</link>
      <description><![CDATA[Parfois, il est bon de revenir aux fondamentaux et de se rappeler quelques concepts qui peuvent sembler simplistes, mais qui restent toujours utiles. 1 / On lit plus de code qu’on en écrit
Même si on essaie de faire simple et clair, quand on rouvre un code six mois plus tard, on se demande souvent : qu’est-ce que j’ai voulu faire ? Bref : * des noms de variables simples, mais répondant à une logique claire
* la lisibilité du code doit primer sur les performances pures Et gardez toujours en tête : serai-je capable de relire et comprendre ce code dans six mois ? 2 / Faire simple, c’est difficile Il est plus facile de faire compliqué que de faire simple. On ajoute des couches, des dépendances, des patterns dans tous les sens. Faire simple permet : * moins de risques d’erreurs
* un code plus facile à tester
* une meilleure maintenance sur le long terme 3 / Vous n’avez pas besoin de tout savoir, mais vous devez apprendre Un développeur ne sait pas tout. Mais il apprend. Il faut lire la documentation, décomposer les problèmes, expérimenter et apprendre en continu pour progresser. 4 / Le debug est une compétence Certains développeurs savent mieux debugger que d’autres. C’est un fait. Ils trouvent plus facilement les bugs, restent calmes et savent observer et comprendre le problème. Pour corriger un bug, il faut : * savoir le reproduire clairement
* modifier un élément à la fois
* lire et comprendre les logs, warnings et messages d’erreur 5 / Les frameworks ne changent pas les fondamentaux Maîtriser les fondamentaux est toujours un avantage. Cette maîtrise vous aidera à migrer plus sereinement d’une version à une autre, ou même à changer de technologie. Les frameworks évoluent. Les fondamentaux restent. 6 / Les problèmes de performances sont souvent des problèmes de conception Avant d’optimiser avec du caching, du tuning ou des micro-optimisations, regardez d’abord l’architecture et la conception du projet : * vos requêtes fonctionnent-elles correctement et sont-elles bien écrites ?
* le modèle de données est-il adapté ?
* pouvez-vous réduire les appels réseau inutiles ?
* certaines boucles peuvent-elles être optimisées ? Les gains les plus importants viennent souvent de la conception, pas des optimisations mineures. 7 / Écrire des tests Les tests permettent : * de refactoriser en toute sécurité
* de détecter plus rapidement les problèmes
* d’améliorer la qualité globale du code Les tests ne ralentissent pas le développement. Ils le sécurisent. 8 / La communication fait partie de notre métier Cela inclut notamment les et la documentation du code. Un code documenté est plus facile à comprendre, maintenir et faire évoluer dans le temps. Le code explique le « ». Les expliquent le « pourquoi ». 9 / Le burn-out est aussi un problème technique La pression des délais, les longues heures de développement ou le manque de vision peuvent conduire à un code de mauvaise qualité, difficile à maintenir. Un code de qualité nécessite : * du temps
* de la réflexion
* et des conditions de travail saines La qualité technique est aussi une question d’organisation. 10 / La progression n’est pas linéaire Développer, c’est traverser des périodes très productives, où tout fonctionne rapidement, et d’autres beaucoup plus difficiles : bugs incompréhensibles, code instable, spécifications floues. C’est normal. Dans ces moments-là, il faut revenir aux fondamentaux, reprendre le problème étape par étape et garder son calme. La progression se fait sur le long terme. Source : https://medium.com/@gopi_ck/10-basic-concepts-every-developer-should-know-even-seniors-too-93e1b69a83fd Catégorie actualité: Langages tips Image actualité AMP:]]></description>
      <pubDate>Fri, 20 Feb 2026 07:21:53 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/10-pratiques-de-codes-et-humaines-integrer-39039</guid>
    </item>
    <item>
      <title><![CDATA[Changes to test merge commit generation for pull requests]]></title>
      <link>https://github.blog/changelog/2026-02-19-changes-to-test-merge-commit-generation-for-pull-requests</link>
      <description><![CDATA[To reduce delays when determining the mergeability for a pull request and improve system reliability, we’ve changed the frequency at which we generate test merge commits for open pull requests.…]]></description>
      <pubDate>Thu, 19 Feb 2026 22:01:57 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-changes-to-test-merge-commit-generation-for-pull-requests</guid>
    </item>
    <item>
      <title><![CDATA[Selected Anthropic and OpenAI models are now deprecated]]></title>
      <link>https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated</link>
      <description><![CDATA[We have deprecated the following models across all GitHub Copilot experiences (including Copilot Chat, inline edits, ask and agent modes, and code completions) on February 17, 2026: Model Deprecation Date…]]></description>
      <pubDate>Thu, 19 Feb 2026 19:47:37 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated</guid>
    </item>
    <item>
      <title><![CDATA[GitHub Projects: Import items based on a query and hierarchy view improvements]]></title>
      <link>https://github.blog/changelog/2026-02-19-github-projects-import-items-based-on-a-query-and-hierarchy-view-improvements</link>
      <description><![CDATA[Import project items with a search query When creating a new project, you can now add items using a search query, in addition to importing directly from a repository. This…]]></description>
      <pubDate>Thu, 19 Feb 2026 19:33:33 GMT</pubDate>
      <source>GitHub Changelog</source>
      <category>opensource</category>
      <guid>https://github.blog/changelog/2026-02-19-github-projects-import-items-based-on-a-query-and-hierarchy-view-improvements</guid>
    </item>
    <item>
      <title><![CDATA[OpenAI sur le point de conclure un tour de financement de 100 milliards $, officiellement pour « renforcer les capacités d'OpenAI », mais surtout en réalité pour couvrir ses énormes pertes]]></title>
      <link>https://intelligence-artificielle.developpez.com/actu/380422/OpenAI-sur-le-point-de-conclure-un-tour-de-financement-de-100-milliards-officiellement-pour-renforcer-les-capacites-d-OpenAI-mais-surtout-en-realite-pour-couvrir-ses-enormes-pertes/</link>
      <description><![CDATA[OpenAI est sur le point de conclure un tour de financement de 100 milliards $, officiellement pour « renforcer les capacités d'OpenAI et étendre ses activités », mais en réalité pour couvrir ses énormes pertes
Un nouveau rapport révèle qu'OpenAI serait sur le point de conclure la phase initiale d'un important tour de table qui devrait permettre de lever plus de 100 milliards de dollars. Le rapport cite des sources proches du dossier, selon lequel la société d'intelligence artificielle serait en pourparlers...]]></description>
      <pubDate>Thu, 19 Feb 2026 16:45:00 GMT</pubDate>
      <source>Developpez.com</source>
      <category>opensource</category>
      <guid>https://intelligence-artificielle.developpez.com/actu/380422/OpenAI-sur-le-point-de-conclure-un-tour-de-financement-de-100-milliards-officiellement-pour-renforcer-les-capacites-d-OpenAI-mais-surtout-en-realite-pour-couvrir-ses-enormes-pertes/</guid>
    </item>
    <item>
      <title><![CDATA[Python Environnements Extension : pour unifier les environnements Python sur Visual Studio Code]]></title>
      <link>https://www.programmez.com/actualites/python-environnements-extension-pour-unifier-les-environnements-python-sur-visual-studio-code-39036</link>
      <description><![CDATA[Pour simplifier et unifier l'environnement de développement Python sur Visual Studio Code, on dispose de la nouvelle extension Python Environnements. Il doit unifier le modèle de développement, gérer les environnements et les workflows, gérer les interpréteurs et les packages. Jusqu'é présent, l'expérience Python était fragmenté à travers les différents outils (venv, conda, pyenv, etc.). Après plus d'un an d'ajustements et de développement, l'extension est disponible. A terme, tous les flux Python migreront vers l'extension Environnements. Il est possible d'activer dès maintenant : python.useEnvsExtension. L'extension fonctionne en parallèle de l'extension Python et aucune configuration particulière n'est requise : vous ouvrez un fichier Python et l'environnement utilisé est automatiquement détecté. Les environnements supportés sont : venv
conda
pyenv
poetry
pipenv
System Python installs La découverte est assurée par PET (Python Environment Tool), un outil de scan codé en Rust. Si vous utilisez uv, l'extension va automatiquement créer un environnement venv et installer les paquets nécessaires. Pour le moment, il n'est pas possible de créer rapidement des projets sur tous les environnements, seuls venv et conda sont supportés. Sans doute que les autres le seront dans les prochaines versions. Mauvaise nouvelle : l'extension fonctionne UNIQUEMENT sur Windows x64 et Windows ARM et l'édition Web ! Il faut Python soit installé.
Pour le moment, les retours sont plutôt mauvais : extension difficile à utiliser, perte de temps pour créer les environnements depuis Pylance, etc. Et les mises à jour se succèdent. Heureusement que l'extension est officiellement en preview. Page de l'extension : https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-python-envs Catégorie actualité: Outils Visual Studio Code, Python Image actualité AMP:]]></description>
      <pubDate>Thu, 19 Feb 2026 07:33:25 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/python-environnements-extension-pour-unifier-les-environnements-python-sur-visual-studio-code-39036</guid>
    </item>
    <item>
      <title><![CDATA[Quantique : Comcast, Classiq et AMD testent un algorithme quantique pour les réseaux]]></title>
      <link>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</link>
      <description><![CDATA[Comcast, Classiq et AMD mènent des tests pour améliorer le trafic Internet en utilisant des algorithmes quantiques pour renforcer la résistance du routage réseau. "L’essai conjoint s’est concentré sur un défi clé de la conception des réseaux : identifier des chemins de secours indépendants pour les nœuds du réseau lors des opérations de maintenance ou de modifications. L’objectif était de garantir que, si un site est mis hors ligne et que soudainement, un deuxième tombe en panne, le trafic puisse être redirigé sans interruption ni dégradation du service pour les clients. Pour y parvenir, les opérateurs doivent identifier des chemins de secours distincts, rapides et capables de résister à des pannes simultanées, tout en minimisant la latence. Cette tâche devient de plus en plus complexe à mesure que le réseau s’étend." explique l'annonce. Le schéma présente le design et l'implémentation du flux et de l'algo quantique sur la plateforme Classiq. L’expérimentation a combiné des techniques de calcul quantique et des méthodes classiques haute performance afin d’évaluer la capacité des algorithmes quantiques à identifier, en temps réel, des chemins de secours dans des scénarios de gestion des changements. Elle a été menée à la fois sur du matériel quantique et dans des environnements de simulation accélérés utilisant des GPU AMD Instinct, afin d’atteindre une capacité de calcul (à l’échelle des qubits) encore hors de portée du matériel quantique seul.
« L’avenir du calcul repose sur la convergence entre le classique et le quantique », explique Madhu Rangarajan, vice-président corporate en charge des produits Compute et Enterprise AI chez AMD. « En tant qu’acteur du calcul haute performance, nous cherchons à comprendre nos technologies peuvent accompagner l’émergence du quantique. Cette collaboration montre un cas concret où la simulation accélérée et l’exécution quantique sont combinées pour répondre à un enjeu opérationnel réel dans les réseaux. »
Détail sur l'algo quantique utilisé : https://www.amd.com/en/developer/resources/technical-articles/2026/designing-resilient-routing-using-quantum-algorithms.html Catégorie actualité: Technologies quantique Image actualité AMP:]]></description>
      <pubDate>Wed, 18 Feb 2026 08:34:25 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/quantique-comcast-classiq-et-amd-testent-un-algorithme-quantique-pour-les-reseaux-39033</guid>
    </item>
    <item>
      <title><![CDATA[IDE Kiro : Checkmarx apporte plus de sécurité applicative]]></title>
      <link>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</link>
      <description><![CDATA[Checkmarx annonce que son Developer Assist supporte l'IDE Kiro, pour l'étendre la sécurité applicative directement dans l'enviornnement. Cette intégration permet à ces derniers d'identifier et de résoudre les problèmes de sécurité au fil de l'écriture du code, sans quitter leur IDE ni dépendre de scans en aval dans la chaîne CI/CD.
En utilisant l’extension IDE officielle de Checkmarx, les développeurs peuvent activer Developer Assist dans Kiro en quelques étapes seulement, sans configuration lourde. La prise en charge d’autres flux de développement, y compris via la ligne de commande, sera bientôt disponible. Une fois authentifié, Developer Assist analyse automatiquement le code source et les dépendances de l’espace de travail actif, appliquant les politiques existantes de Checkmarx One. Aucune configuration spécifique à Kiro, API propriétaire ou intégration expérimentale n’est nécessaire. Developer Assist est disponible sur Cursor, Visual Studio Code et Windsurf.
Pour en savoir plus : https://dev.checkmarx.com/ Catégorie actualité: Outils Checkmarx Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:25:38 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/ide-kiro-checkmarx-apporte-plus-de-securite-applicative-39028</guid>
    </item>
    <item>
      <title><![CDATA[WebMCP : un standard pour rendre un site web "agent ready" ?]]></title>
      <link>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</link>
      <description><![CDATA[concilier agents IA et sites web et la manière dont les pages web pourraient interagir, travailler avec les agents ? WebMCP veut fournir une méthode standard pour définir les actions des agents sur un site web, sur une page web sans pénaliser au bon fonctionnement du site web. "Vous indiquez aux agents et où interagir avec votre site, qu'il s'agisse de réserver un vol, de soumettre une demande d'assistance ou de naviguer dans des données complexes. Ce canal de communication direct élimine toute ambiguïté et permet des flux de travail plus rapides et plus efficaces pour les agents." expliquer Google. WebMCP preview repose sur 2 API :
- API déclarative : Permet d’effectuer des actions standard définies directement dans les formulaires HTML. - API impérative : Permet d’effectuer des interactions plus complexes et dynamiques nécessitant l’exécution de JavaScript. C'est une interface proposé en preview par Google et accessible dans Chrome. Ces API forment un "pont" rendant votre site web "agent ready" et permet de créer des flux agentiques que se veulent plus fiables qu'en passant par du DOM. Ces API sont JavaScript. Pour le moment, la spécification est en cours de rédaction. Elle ne dépend pas de W3C et n'est pas un standard du consortium. Site : https://webmachinelearning.github.io/webmcp/ Catégorie actualité: IA MCP Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 14:18:02 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/webmcp-un-standard-pour-rendre-un-site-web-agent-ready-39027</guid>
    </item>
    <item>
      <title><![CDATA[Parcours libriste d’Isabella Vanni — « Libre à vous ! » du 10 février 2026 — Podcasts et références]]></title>
      <link>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</link>
      <description><![CDATA[268ème émission « Libre à vous ! » de l’April. Podcast et programme :
sujet principal : parcours libriste d’Isabella Vanni, coordinatrice vie associative et responsable projets à l’April. Un parcours libriste est l’interview d’une seule personne pour parler de son parcours personnel et professionnel
chronique « Que libérer d’autre que du logiciel avec Antanak » sur « Les assises de l’attention »
chronique de Benjamin Bellamy sur « L’antéchrist et les petits hommes verts »
Quoi de Libre ? Actualités et annonces concernant l’April et le monde du Libre lien nᵒ 1 : Podcast de la 268ᵉ émission
lien nᵒ 2 : Les références pour la 268ᵉ émission et les podcasts par sujets
lien nᵒ 3 : S'abonner au podcast
lien nᵒ 4 : S'abonner à la lettre d'actus
lien nᵒ 5 : Libre à vous !
lien nᵒ 6 : Radio Cause Commune Rendez‐vous en direct chaque mardi de 15 h 30 à 17 h sur 93,1 MHz en Île‐de‐France. L’émission est diffusée simultanément sur le site Web de la radio Cause Commune. Vous pouvez nous laisser un message sur le répondeur de la radio : pour réagir à l’un des sujets de l’émission, pour partager un témoignage, vos idées, vos suggestions, vos encouragements ou pour nous poser une question. Le numéro du répondeur : +33 9 72 51 55 46. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Tue, 17 Feb 2026 10:20:24 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/parcours-libriste-d-isabella-vanni-libre-a-vous-du-10-fevrier-2026-podcasts-et-references</guid>
    </item>
    <item>
      <title><![CDATA[.Net 11 Preview 1 : nouvelles librairies, peu de changements dans C#]]></title>
      <link>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</link>
      <description><![CDATA[.Net 10 a été distribuée en novembre 2025. La version 11 est désormais disponible en preview 1. Comme à chaque fois, de nombreuses évolutions sont attendues. L'ensemble des frameworks et des langages sont concernées : C#, F#, ASP.Net Core, Blazor, MAUI, le compilateur Jit, le support de CoreCLR dans WebAssembly, meilleure compression / décompression avec Zstandard. Sur la partie librairie, retenons déjà les évolutions suivantes :
- Zstandard est natif à .Net pour la compression. La librairie promet une nette amélioration des performances :
// Compress data using ZstandardStream
using var compressStream = new ZstandardStream(outputStream, CompressionMode.Compress);
await inputStream.CopyToAsync(compressStream); // Decompress data
using var decompressStream = new ZstandardStream(inputStream, CompressionMode.Decompress);
await decompressStream.CopyToAsync(outputStream);
- BFloat16 intègre par défaut toutes les interfaces standards pour le numérique
- amélioration de TimeZone
Note de version sur les librairies : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/libraries.md
Sur la partie runtime, il faut s'attendre à de bonnes nouvelles :
- Runtime async : une nouvelle fonction majeure du runtime et méthodes asynchrones pour améliorer les performances. CoreCLR supporte RuntimeAsync par défaut, idem pour Native AOT
- CoreCLR est supporté dans WebAssembly. Il n'est pas encore disponible en preview 1.
- diverses améliorations de performances sur le JIT - meilleur support de RISC-V
Sur C#, pour le moment, peu de nouveautés annoncées. Deux nouvelles fonctions sont attendues : arguments pour les expresssions Collection et support Extended layout. .Net 11 n'introduira aucune nouvelle fonctionnalité pour Visual Basic. Sur ASP.Net Core et Blazor, les développeurs vont avoir beaucoup de nouveautés : EnvironmentBoundary, nouveau composant Label dans les formulaires Blazor, nouveau composant DisplayName, navigation relative Uri, support "propre" des éléments MathML dans un rendu interactif. Tous les détails dans la note de version : https://github.com/dotnet/core/blob/main/release-notes/11.0/preview/preview1/aspnetcore.md
La génération de source XAML est par défaut pour les applications .Net MAUI, cela doit permettre un build plus rapide et un debug plus performant. Sur Android, CoreCLR devient le runtime par défaut. Sur Container Images et Winfows Forms, pas de nouveautés annoncées. Annonce de .Net 11 : https://devblogs.microsoft.com/dotnet/dotnet-11-preview-1/ Catégorie actualité: Frameworks .Net 11 Image actualité AMP:]]></description>
      <pubDate>Tue, 17 Feb 2026 09:52:19 GMT</pubDate>
      <source>Programmez</source>
      <category>opensource</category>
      <guid>https://www.programmez.com/actualites/net-11-preview-1-nouvelles-librairies-peu-de-changements-dans-c-39026</guid>
    </item>
    <item>
      <title><![CDATA[Automate repository tasks with GitHub Agentic Workflows]]></title>
      <link>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</link>
      <description><![CDATA[Discover GitHub Agentic Workflows, now in technical preview. Build automations using coding agents in GitHub Actions to handle triage, documentation, code quality, and more.]]></description>
      <pubDate>Fri, 13 Feb 2026 14:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/automate-repository-tasks-with-github-agentic-workflows/</guid>
    </item>
    <item>
      <title><![CDATA[LibreOffice 26.2 : Markdown, accessibilité et plein d’autres nouveautés et améliorations]]></title>
      <link>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</link>
      <description><![CDATA[En février, il y a la corvée commerciale de la Saint-Valentin et les réjouissances intellectuelles consécutives à la sortie d’une nouvelle version de la suite bureautique LibreOffice. C’est, bien évidemment, sur LibreOffice 26.2 que l’on va se pencher. Au menu, du très visible, comme les boites de dialogues, du très attendu comme la prise en compte du Markdown ou du moins visible comme le travail sur l’accessibilité.
Il va de soi que les notes de version sont plus exhaustives et qu’il ne s’agit ici que d’une sélection. lien nᵒ 1 : Notes de version Sommaire
L’accessibilité
Support du Markdown
L’interface et les boites de dialogue
Writer
Calc
En vrac
Pour finir
Avant de commencer : toutes les captures d’écran ont été faites, volontairement, sur une interface très personnalisée.
L’accessibilité
L’accessibilité de la suite bureautique est un important chantier pour lequel une personne a été recrutée en 2023 (en). Cette version-ci a fait l’objet d’améliorations sensibles. Parallèlement, Sophie Gautier, coordinatrice de The Document Foundation1 (Foundation coordinator) est en train de monter un groupe de travail qui a pour objectif la publication d’un rapport de conformité en matière d’accessibilité pour répondre à la norme européenne EN 301 549 (en) d’accessiblité numérique. La langue de travail de ce groupe est l’anglais.
Concernant les améliorations de cette version :
la boite de dialogue « Vérifier les mises à jour », Aide &gt; Vérifier les mises à jour… est devenue accessible aux lecteurs d’écran ;
les fonctions d’accessibilité des aperçus des bordures, onglet « Bordures » des boites de dialogue, ont été revues afin qu’elles ne perturbent plus les dispositifs d’assistance ;
sur Linux : la boite de dialogue Outils&gt; Orthographe est annoncée correctement par le lecteur d’écran ;
quand on supprimait la sélection accessible, le curseur se déplaçait automatiquement au début du texte, ce comportement perturbant est supprimé ;
dans Writer, les fautes d’orthographe ne sont plus signalées par les dispositifs d’assistance si la vérification orthographique n’est pas activée ;
l’accessibilité au clavier de la boite de dialogue des extensions : Outils &gt; Extensions est accessible aux lecteurs d’écran ;
et enfin, il est possible de naviguer entre les onglets verticaux avec des raccourcis clavier.
Support du Markdown
Le Markdown est devenu le format de balisage léger standard « de fait ». Et c’est celui supporté par LinuxFR. Son support a été introduit dans cette version, c’est un des formats d’enregistrement qui s’est ajouté à la série des autres formats de la suite, pas un format d’export. Pour l’utiliser pour vos sites, passant pour LinuxFR, vous devrez :
soit ouvrir le fichier .md dans un éditeur de texte, n’importe lequel, même Mousepad fait l’affaire par exemple, et copier-coller ensuite le tout à partir de l’éditeur de texte là où vous le voulez ;
soit, si cela est possible, importer le fichier .md dans ce qui vous sert pour gérer le site comme le fait par exemple l’extension ODT2SPIP pour le système de gestion de contenu SPIP qui permet de créer une nouvelle page dans SPIP avec un fichier.ODT. ça marche avec LinuxFR ? Plutôt bien. Les styles de caractère Accentuation (ici en italiques) et Accentuation forte (ici gras) sont bien reconnu ainsi que Texte source pour « télétype », les indications in-texte encadrées de l’accent grave U+0060. Les styles de paragraphes :
Bloc de citation (paragraphes de citation précédés d’une ligne blanche et du signe « &gt; » dans la saisie de contenu sur LinuxFR) ;
Contenu de tableau ;
Corps de texte ;
Liste, par contre la numérotation des listes ordonnée ne semble pas bien fonctionner, il faut saisir les numéros à la main ;
Texte préformaté pour écrire des blocs de code ;
Titre 1, Titre 2, Titre 3 et Titre de tableau.
Les tableaux sont bien repris ainsi que les liens insérés via l’insertion d’hyperliens.
Ce qui ne semble pas fonctionner du tout : ce sont les notes, elles disparaissent corps et biens. C’est peut-être dû au passage dans l’éditeur de texte qui transforme un peu le document. Et, évidemment, il faut rajouter les images avec la syntaxe LinuxFR.
La version de Mardown de LibreOffice est CommonMark (en) et la bibliothèque utilisée est MD4C avec quelques extensions prises en charge par cette bibliothèque (cf ce rapport de bug (en) et ses réponses), pour en savoir plus, voir cette note (en) du blog de The Document Foundation.
Petite remarque, si vous utilisez un LibreOffice 25.8, vous avez peut-être pu constater qu’il était question d’enregistrement au format .md, cette information a été ajoutée trop précocement car la version 25.8 ne gère pas le Markdown.
L’interface et les boites de dialogue
Les boites de dialogue, notamment de styles et de formats, ont beaucoup changé. Longtemps elles se sont affichées avec une présentation par onglets en haut et le contenu dessous.
Puis il y a une période de transition en 2025 qui a fait grincer une collection complète de dents où on avait, selon l’endroit où on était, soit des onglets soit une navigation par menu latéral. Cette dernière avait un gros défaut : par exemple pour la configuration des styles dans Writer il fallait descendre tout en bas pour accéder aux options qui étaient cachées. Et il n’y avait pas de barre de défilement pour aller plus vite.
LibreOffice 26.2 voit ces défauts corrigés : les boites de dialogue sont harmonisées dans toute la suite et leur menu latéral, toujours sans barre de défilement qui s’avère finalement inutile, montre clairement tous les types de paramètres auxquels on peut accéder. Et, comme on peut le voir, LibreOffice a intégré une meilleure prise en charge des systèmes d’écritures asiatiques et complexes en affichant deux colonnes, une pour les polices occidentales, ou pour les polices asiatiques ou complexes. Une personne a également été recrutée en 2023 (en) pour travailler sur le support des systèmes d’écriture de droite à gauche (RTL) et complexes (CTL). Si toutefois, vous préférez revenir à l’affichage avec les onglets, il suffit d’aller dans le menu Outils &gt; Options &gt; Apparenceau niveau de « Boites de dialogue » et cocher l’option Horizontal en haut. Il faut savoir que les onglets en haut ne s’affichent que sur une seule ligne et qu’il faudra donc naviguer avec les flèches quand il y a de nombreuses options. Writer
Il y a un certain nombre d’amélioration autour de la compatibilité avec le format DOCX : séparation de tableaux flottants en plusieurs tableaux, suppression de la numérotation des notes de bas de page à l’ouverture d’un fichier DOCX, etc.
On relèvera deux nouvelles options d’alignement des paragraphes : « Début » et « Fin ». Si vous utilisez l’alphabet latin, vous ne verrez aucune différence avec les deux options « Forcer à gauche/en haut » et « Forcer à droite/en bas ». Elles ont été développées pour réutiliser plus facilement les styles entre les divers systèmes d’écriture. Pour continuer sur la lancée du travail pour la prise en compte des systèmes d’écriture dont le fonctionnement est différent de celui de l’alphabet latin, il est possible de changer la direction du texte : de gauche à droite ou de droite à gauche en cours de travail. Cela peut se paramétrer dans les styles. Calc
Un gros travail sur les performances a été fait : vitesse de défilement, rapidité des classeurs avec de nombreuses formes et du rejet des modifications. On voit apparaître de nouvelles options de tri (Données &gt;Trier) qui dépendent de la « locale » (langue définie dans les Options de LibreOffice). On peut ainsi déterminer quel caractère est utilisé comme séparateur de décimal pour le tri naturel. On peut relever aussi une avancée ergonomique qui va plaire à toutes celles et ceux qui utilisent les matrices, on peut maintenant modifier les formules matricielles avec la combinaison de touches : F2 + ↑ Maj + Ctrl + Entrée, il n’est plus nécessaire de modifier la formule elle-même.
Et aussi : si vous utilisez (pourquoi diable ?) le format d’enregistrement XLSX, c’est le format EXCEL2010+ qui est le format par défaut, il change de nom pour devenir « Classeur Excel 2010-365 ».2
En vrac
Base est devenu complètement multi-utilisateur, TDF a, d’ailleurs, recruté une personne pour travailler sur l’application.
Concernant les diagrammes (ou chart) : dans le Volet latéral, quand le graphique est en mode modification et que l’on va, au niveau de « Couleurs », sur la palette, on a une prévisualisation en direct dans le diagramme ce qui permet de tester le choix de couleurs plus facilement.
Les polices embarquées dont la licence ne permettait pas l’édition étaient jusqu’à présent ignorées et remplacées à l’affichage, ni vu, ni connu par une fonte de substitution. Ce défaut a été corrigé.
L’export PDF gère les liens avec les documents externes : Fichier &gt; Exporter au format PDF &gt; Liens. Les dictionnaires hongrois, mongol et portugais du Portugal ont été mis à jour ainsi que les règles de césure de la langue hongroise.
JSON, pour JavaScript Object Notation, est un format standard utilisé pour représenter des données structurées. Il est utilisé notamment pour échanger les informations entre un navigateur et un serveur. C’est, par exemple, le format de sauvegarde des marques-pages de Firefox ou de certains fichiers d’archives de Mastodon. Les documents XML et JSON génériques avec des plages pouvant être liées sont maintenant automatiquement mappés à des feuilles dans Calc. Une plage pouvant être liée est une section d’un document contenant des enregistrements tabulaires. Lorsqu’un document contient plusieurs plages pouvant être liées, chaque plage est mappée à une seule feuille3.
Et si vous avez envie de vous amuser avec les fonctions expérimentales (à activer dansOutils &gt; Options &gt; LibreOffice &gt; Avancé), vous pouvez jouer avec la nouvelle de boite de dialogue « Gestion des macros ».
Pour finir
Cette dépêche a, bien, évidemment, été rédigée avec LibreOffice et, cette fois-ci dans un fichier enregistré en Markdown. Les seules balises que j’ai dû entrer à la main sont celles des images. Kate a l’air de modifier le fichier et, quand je réouvre le .md dans LibreOffice, il y a des styles qui ont sauté mais la mise en forme reste visuellement la même. Kate rajoute aussi des barres obliques devant les « &gt; », aux crochets [ ] et même à certains hyperliens (images). Il y a peut-être des éditeurs de texte plus adaptés ou des réglages à faire.
J’ai rédigé cette dépêche en même temps qu’un article sur LibreOffice 26.2 pour mon site. Si l’article n’est pas vraiment dupliqué, il n’est pas étonnant d’y trouver des morceaux ici. Que tout cela ne nous empêche d’adresser tous nos remerciements à celles et ceux qui font de LibreOffice une suite bureautique si agréable à utiliser et si performante.
Post-scriptum : si vous voulez savoir modifier les couleurs de l’interface comme sur les captures d’écran, ça peut s’envisager, demandez gentiment, avec un peu de chance.
The Document Foundation ou TDF est la fondation de droit allemand qui pilote le projet LibreOffice. Il y a deux formats OOXML différents et donc deux formats XLSX différents, la version 2007 et la version actuelle depuis 2010. S’il vous est vraiment nécessaire d’enregistrer au format XLSX, il faut utiliser la version de 2010. Notes de version. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Fri, 13 Feb 2026 09:09:23 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/libreoffice-26-2-markdown-accessibilite-et-plein-d-autres-nouveautes-et-ameliorations</guid>
    </item>
    <item>
      <title><![CDATA[Projets Libres saison 4 épisode 11 : PVH éditions, une maison d'édition libérée et dans le Fediverse]]></title>
      <link>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</link>
      <description><![CDATA[Nous avons eu le plaisir de rencontrer Lionel Jeannerat durant les Rencontres Hivernales du libre à Saint-Cergue (VD) en janvier 2026. son parcours
la maison d'édition et ses œuvres
le passage au libre que ce soit pour les licences mais aussi pour leurs outils métiers
Bonne écoute ou lecture lien nᵒ 1 : Lien vers l'épisode
lien nᵒ 2 : S'abonner au podcast
lien nᵒ 3 : Le site de PVH éditions
lien nᵒ 4 : Soutenir le podcast
lien nᵒ 5 : L'épisode traduit en anglais
lien nᵒ 6 : Le site des Rencontres Hivernales du libre Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Wed, 11 Feb 2026 07:40:57 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/projets-libres-saison-4-episode-11-pvh-editions-une-maison-d-edition-liberee-et-dans-le-fediverse</guid>
    </item>
    <item>
      <title><![CDATA[Les journaux LinuxFr.org les mieux notés de janvier 2026]]></title>
      <link>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</link>
      <description><![CDATA[LinuxFr.org propose des dépêches et articles, soumis par tout un chacun, puis revus et corrigés par l’équipe de modération avant publication. C’est la partie la plus visible de LinuxFr.org, ce sont les dépêches qui sont le plus lues et suivies, sur le site, via Atom/RSS, ou bien via partage par messagerie instantanée, par courriel, ou encore via médias sociaux. Ce que l’on sait moins, c’est que LinuxFr.org vous propose également de publier directement vos propres articles, sans validation a priori de lʼéquipe de modération. Ceux-ci s’appellent des journaux. Voici un florilège d’une dizaine de ces journaux parmi les mieux notés par les utilisateurs et les utilisatrices… qui notent. Lumière sur ceux du mois de janvier passé.
« lecteur mp3 pour personne handicapée mentale » par ChocolatineFlying ;
« À la recherche du Linuxfrien type » par Ysabeau ;
« hacker sa pompe de relevage 3 et fin ! » par ChocolatineFlying ;
« [Hors sujet] Des tablettes lave-vaisselle tout-en-un » par Tanguy Ortolo ;
« Francis Hallé Bronsonisé » par Joris Dedieu ;
« 10 ans après, Modoboa est toujours là pour prendre soin de votre serveur de messagerie » par mirtouf ;
« À table ! » par JaguarWan ;
« Retour d'expérience sur le développement d'une application par l'utilisation d'IA » par phoenix ;
« Algoo lance un bulletin d'information mensuel « veille techno et logiciels libres » » par LeBouquetin ;
« Linux : les planètes s'alignent en 2026 » par vmagnin. lien nᵒ 1 : Participez à l’écriture d’un article
lien nᵒ 2 : Publiez votre journal
lien nᵒ 3 : Proposez une dépêche Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 09:23:50 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/les-journaux-linuxfr-org-les-mieux-notes-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[Meilleures contributions LinuxFr.org : les primées de janvier 2026]]></title>
      <link>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</link>
      <description><![CDATA[Nous continuons sur notre lancée de récompenser celles et ceux qui chaque mois contribuent au site LinuxFr.org (dépêches, , logo, journaux, correctifs, etc.). Vous n’êtes pas sans risquer de gagner un livre des éditions Eyrolles, ENI et D-Booker. Voici les gagnants du mois de janvier 2026 :
Stefane Fermigier, pour sa dépêche « Appel à de la Commission "Vers des écosystèmes numériques ouverts européens" » ;
ChocolatineFlying, pour son journal « lecteur mp3 pour personne handicapé mental » ;
YvanM, pour sa dépêche « MeshCentral, alternative à TeamViewer et RustDesk » ;
Christophe Bliard, pour sa dépêche « Sortie de OpenProject 17.0 ».
Les livres gagnés sont détaillés en seconde partie de la dépêche. N’oubliez pas de contribuer, LinuxFr.org vit pour vous et par vous ! lien nᵒ 1 : Contribuez à LinuxFr.org !
lien nᵒ 2 : Tous les moyens (ou presque) de participer
lien nᵒ 3 : Récompenses précédentes (décembre 2025) Les livres sélectionnés
Linux — Maîtrisez l'administration du système — 7e édition. Certaines personnes n’ont pas pu être jointes ou n’ont pas répondu. Les lots ont été réattribués automatiquement. N’oubliez pas de mettre une adresse de courriel valable dans votre compte ou lors de la proposition d’une dépêche. En effet, c’est notre seul moyen de vous contacter, que ce soit pour les lots ou des questions sur votre dépêche lors de sa modération. Tous nos remerciements aux contributeurs du site ainsi qu’aux éditions Eyrolles, ENI et D-Booker. Télécharger ce contenu au format EPUB : voir le flux Atom ouvrir dans le navigateur]]></description>
      <pubDate>Mon, 09 Feb 2026 07:09:14 GMT</pubDate>
      <source>LinuxFr</source>
      <category>opensource</category>
      <guid>https://linuxfr.org/news/meilleures-contributions-linuxfr-org-les-primees-de-janvier-2026</guid>
    </item>
    <item>
      <title><![CDATA[Continuous AI in practice: What developers can automate today with agentic CI]]></title>
      <link>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</link>
      <description><![CDATA[Think of Continuous AI as background agents that operate in your repository for tasks that require reasoning.]]></description>
      <pubDate>Thu, 05 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/</guid>
    </item>
    <item>
      <title><![CDATA[Setting Docker Hardened Images free]]></title>
      <link>https://changelog.com/podcast/675</link>
      <description><![CDATA[In May of 2025, Docker launched Hardened Images, a secure, minimal, production-ready set of images. In December, they made DHI freely available and open source to everyone who builds software. On this episode, we're joined by Tushar Jain, EVP of Engineering at Docker to learn all about it.]]></description>
      <pubDate>Wed, 04 Feb 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/podcast/675</guid>
    </item>
    <item>
      <title><![CDATA[Pick your agent: Use Claude and Codex on Agent HQ]]></title>
      <link>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</link>
      <description><![CDATA[Claude by Anthropic and OpenAI Codex are now available in public preview on GitHub and VS Code with a Copilot Pro+ or Copilot Enterprise subscription. Here's what you need to know and how to get started today.]]></description>
      <pubDate>Wed, 04 Feb 2026 17:00:19 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/company-news/pick-your-agent-use-claude-and-codex-on-agent-hq/</guid>
    </item>
    <item>
      <title><![CDATA[What the fastest-growing tools reveal about how software is being built]]></title>
      <link>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</link>
      <description><![CDATA[What languages are growing fastest, and why? What about the projects that people are interested in the most? Where are new developers cutting their teeth? Let’s take a look at Octoverse data to find out.]]></description>
      <pubDate>Tue, 03 Feb 2026 17:00:00 GMT</pubDate>
      <source>GitHub Blog</source>
      <category>opensource</category>
      <guid>https://github.blog/news-insights/octoverse/what-the-fastest-growing-tools-reveal-about-how-software-is-being-built/</guid>
    </item>
    <item>
      <title><![CDATA[The state of homelab tech (2026)]]></title>
      <link>https://changelog.com/friends/125</link>
      <description><![CDATA[Techno Tim joins Adam to dive deep into the state of homelab'ing in 2026. Hardware is scarce and expensive due to the AI gold rush, but software has never been better. From unleashing Claude on your UDM Pro to building custom Proxmox CLIs, they explores how AI is transforming what's possible in the homelab. Tim declares 2026 the "Year of Self-Hosted Software" while Adam reveals his homelab's secret weapons: DNSHole (a Pi-hole replacement written in Rust) and PXM (a Proxmox automation CLI).]]></description>
      <pubDate>Sat, 24 Jan 2026 20:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/125</guid>
    </item>
    <item>
      <title><![CDATA[Very important agents]]></title>
      <link>https://changelog.com/friends/120</link>
      <description><![CDATA[Nick Nisi joins us to dig into the latest trends from this year and how they're impacting his day-to-day coding and Vision Pro wearing. Anthropic's acquisition of Bun, the evolving JavaScript and AI landscape, GitHub's challenges and the Amp/Sourcegraph split. We dive into AI development practices, context management, voice assistants, Home Assistant OS and home automation, the state of the AI browser war, and we close with a prediction from Nick.]]></description>
      <pubDate>Fri, 05 Dec 2025 22:00:00 GMT</pubDate>
      <source>The Changelog</source>
      <category>opensource</category>
      <guid>https://changelog.com/friends/120</guid>
    </item>
  </channel>
</rss>