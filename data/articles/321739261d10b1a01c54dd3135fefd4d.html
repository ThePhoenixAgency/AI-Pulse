<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>GitHub - HKUDS/RAG-Anything: "RAG-Anything: All-in-One RAG Framework"</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>GitHub - HKUDS/RAG-Anything: "RAG-Anything: All-in-One RAG Framework"</h1>
  <div class="metadata">
    Source: GitHub Trending Python | Date: 2/17/2026 1:30:36 AM | <a href="https://github.com/HKUDS/RAG-Anything" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div>
<div> <a target="_blank" href="/HKUDS/RAG-Anything/blob/main/assets/logo.png"><img src="/HKUDS/RAG-Anything/raw/main/assets/logo.png" alt="RAG-Anything Logo"></a>
</div>
<div><h1> RAG-Anything: All-in-One RAG Framework</h1><a href="#-rag-anything-all-in-one-rag-framework"></a></div>
<p><a href="https://trendshift.io/repositories/14959"><img src="https://camo.githubusercontent.com/b7d262712df3f0d8c5822a912b14f75d8879ed98333f5dccb8166f1d301a0946/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f3134393539" alt="HKUDS%2FRAG-Anything | Trendshift"></a></p>
<div> <a target="_blank" href="https://camo.githubusercontent.com/ee0ffc9f8a8b338b4342f708bc183aa91ef92fc20187fe9e9e583d038b52b32d/68747470733a2f2f726561646d652d747970696e672d7376672e6865726f6b756170702e636f6d3f666f6e743d4f72626974726f6e2673697a653d3234266475726174696f6e3d333030302670617573653d3130303026636f6c6f723d3030443946462663656e7465723d74727565267643656e7465723d747275652677696474683d363030266c696e65733d57656c636f6d652b746f2b5241472d416e797468696e673b4e6578742d47656e2b4d756c74696d6f64616c2b5241472b53797374656d3b506f77657265642b62792b416476616e6365642b41492b546563686e6f6c6f6779"><img src="https://camo.githubusercontent.com/ee0ffc9f8a8b338b4342f708bc183aa91ef92fc20187fe9e9e583d038b52b32d/68747470733a2f2f726561646d652d747970696e672d7376672e6865726f6b756170702e636f6d3f666f6e743d4f72626974726f6e2673697a653d3234266475726174696f6e3d333030302670617573653d3130303026636f6c6f723d3030443946462663656e7465723d74727565267643656e7465723d747275652677696474683d363030266c696e65733d57656c636f6d652b746f2b5241472d416e797468696e673b4e6578742d47656e2b4d756c74696d6f64616c2b5241472b53797374656d3b506f77657265642b62792b416476616e6365642b41492b546563686e6f6c6f6779" alt="Typing Animation"></a>
</div>
<div> <div> <p> <a href="https://github.com/HKUDS/RAG-Anything"><img src="https://camo.githubusercontent.com/f54fa1a4f8de2d145e7f754a4ec7fae92ec7ab67f2e4ab7a61e025826a5d7872/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f94a550726f6a6563742d506167652d3030643966663f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"></a> <a href="https://arxiv.org/abs/2510.12323"><img src="https://camo.githubusercontent.com/2fcfddd9a3a9e97cd04a4b18be37d15b52c983282593a6313c4d8dcbec3e0ea4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f938461725869762d323531302e31323332332d6666366236623f7374796c653d666f722d7468652d6261646765266c6f676f3d6172786976266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"></a> <a href="https://github.com/HKUDS/LightRAG"><img src="https://camo.githubusercontent.com/0fb76146c37abfb0a7ea0804d47cf492e4543bd72942b9a1eb2251414348e44a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29aa142617365642532306f6e2d4c696768745241472d3465636463343f7374796c653d666f722d7468652d6261646765266c6f676f3d6c696768746e696e67266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"></a> </p> <p> <a href="https://github.com/HKUDS/RAG-Anything/stargazers"><img src="https://camo.githubusercontent.com/3256df7110a98bf05ac7cb8250a0161bb3c52397064cbbd809234b29dc24d489/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f484b5544532f5241472d416e797468696e673f636f6c6f723d303064396666267374796c653d666f722d7468652d6261646765266c6f676f3d73746172266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"></a> <a target="_blank" href="https://camo.githubusercontent.com/21597bf364600b5cd1ff1c924de84c2b16d9b5ab0be13b18ccc4a70f7c2b7e77/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f908d507974686f6e2d332e31302d3465636463343f7374796c653d666f722d7468652d6261646765266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"><img src="https://camo.githubusercontent.com/21597bf364600b5cd1ff1c924de84c2b16d9b5ab0be13b18ccc4a70f7c2b7e77/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f908d507974686f6e2d332e31302d3465636463343f7374796c653d666f722d7468652d6261646765266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"></a> <a href="https://pypi.org/project/raganything/"><img src="https://camo.githubusercontent.com/b3482471e63b9f6ba28d6ce0dc9605c16d991928005dcf6f6b94a5815c181183/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f726167616e797468696e672e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d70797069266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d31613161326526636f6c6f723d666636623662"></a> <a href="https://github.com/astral-sh/uv"><img src="https://camo.githubusercontent.com/2af9796c1dcf87bc34bcf204f9c3c9d136d903cba129734dde689fc19dfe05e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29aa175762d52656164792d6666366236623f7374796c653d666f722d7468652d6261646765266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"></a> </p> <p> <a href="https://discord.gg/yF2MmDJyGJ"><img src="https://camo.githubusercontent.com/997243778fa5a1c573a6946428cbbba930b66978c116930554c897d656cd7402/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac446973636f72642d436f6d6d756e6974792d3732383964613f7374796c653d666f722d7468652d6261646765266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"></a> <a href="https://github.com/HKUDS/RAG-Anything/issues/7"><img src="https://camo.githubusercontent.com/66747ec1eb2241514dc09e4bc8279b3017c2d617dcea15a693db0cca958240d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac5765436861742d47726f75702d3037633136303f7374796c653d666f722d7468652d6261646765266c6f676f3d776563686174266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"></a> </p> <p> <a href="/HKUDS/RAG-Anything/blob/main/README_zh.md"><img src="https://camo.githubusercontent.com/0e023c856e19d217d0f619fe7247a59f829e0046a66b0b16dd26820344f8427b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f87a8f09f87b3e4b8ade69687e789882d3161316132653f7374796c653d666f722d7468652d6261646765"></a> <a href="/HKUDS/RAG-Anything/blob/main/README.md"><img src="https://camo.githubusercontent.com/37d611c70fafbf6df48db46cb5a9959452e75e92f6f73ee10bace1a1567bd42a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f87baf09f87b8456e676c6973682d3161316132653f7374796c653d666f722d7468652d6261646765"></a> </p> </div>
</div>
</div>
<div> <div></div>
</div>
<div> <a href="#-quick-start"> <img src="https://camo.githubusercontent.com/5dc13c91fe6858f8373b0aa709fc6505f1716ffc1c7e35050fcdccd6971376f1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f517569636b25323053746172742d476574253230537461727465642532304e6f772d3030643966663f7374796c653d666f722d7468652d6261646765266c6f676f3d726f636b6574266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"> </a>
</div>
<hr>
<div> <table> <tbody><tr> <td> <a target="_blank" href="/HKUDS/RAG-Anything/blob/main/assets/LiteWrite.png"><img src="/HKUDS/RAG-Anything/raw/main/assets/LiteWrite.png" alt="LiteWrite"></a> </td> <td> <a href="https://litewrite.ai"> <img src="https://camo.githubusercontent.com/a0957866a1119378d38885f68edea9ddae1da6415e3de563848a191f330e4b4c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f9a802532304c69746557726974652d41492532304e61746976652532304c61546558253230456469746f722d6666366236623f7374796c653d666f722d7468652d6261646765266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265"> </a> </td> </tr> </tbody></table>
</div>
<hr>
<div><h2> News</h2><a href="#-news"></a></div>
<ul>
<li> [2025.10] We have released the technical report of <a href="http://arxiv.org/abs/2510.12323">RAG-Anything</a>. Access it now to explore our latest research findings.</li>
<li> [2025.08] RAG-Anything now features <strong>VLM-Enhanced Query</strong> mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.</li>
<li> [2025.07] RAG-Anything now features a <a href="/HKUDS/RAG-Anything/blob/main/docs/context_aware_processing.md">context configuration module</a>, enabling intelligent integration of relevant contextual information to enhance multimodal content processing.</li>
<li> [2025.07] RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.</li>
<li> [2025.07] RAG-Anything has reached 1k stars on GitHub! Thank you for your incredible support and valuable contributions to the project.</li>
</ul>
<hr>
<div><h2> System Overview</h2><a href="#-system-overview"></a></div>
<p><em>Next-Generation Multimodal Intelligence</em></p>
<pre><code>Modern documents increasingly contain diverse multimodal content—text, images, tables, equations, charts, and multimedia—that traditional text-focused RAG systems cannot effectively process. RAG-Anything addresses this challenge as a comprehensive All-in-One Multimodal Document Processing RAG system built on LightRAG.
As a unified solution, RAG-Anything eliminates the need for multiple specialized tools. It provides seamless processing and querying across all content modalities within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers comprehensive multimodal retrieval capabilities.
Users can query documents containing interleaved text, visual diagrams, structured tables, and mathematical formulations through one cohesive interface. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a unified processing framework.</code></pre>
<div><h3> Key Features</h3><a href="#-key-features"></a></div>
<pre><code> End-to-End Multimodal Pipeline - Complete workflow from document ingestion and parsing to intelligent multimodal query answering Universal Document Support - Seamless processing of PDFs, Office documents, images, and diverse file formats Specialized Content Analysis - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types Multimodal Knowledge Graph - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding Adaptive Processing Modes - Flexible MinerU-based parsing or direct multimodal content injection workflows Direct Content List Insertion - Bypass document parsing by directly inserting pre-parsed content lists from external sources Hybrid Intelligent Retrieval - Advanced search capabilities spanning textual and multimodal content with contextual understanding</code></pre>
<hr>
<div><h2> Algorithm &amp; Architecture</h2><a href="#-algorithm--architecture"></a></div>
<div>
<div><h3>Core Algorithm</h3><a href="#core-algorithm"></a></div>
<p><strong>RAG-Anything</strong> implements an effective <strong>multi-stage multimodal pipeline</strong> that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.</p>
</div>
<div> <div> <div> <div> <div></div> <div>Document Parsing</div> </div> <div>→</div> <div> <div></div> <div>Content Analysis</div> </div> <div>→</div> <div> <div></div> <div>Knowledge Graph</div> </div> <div>→</div> <div> <div></div> <div>Intelligent Retrieval</div> </div> </div> </div>
</div>
<div><h3>1. Document Parsing Stage</h3><a href="#1-document-parsing-stage"></a></div>
<pre><code>The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.
Key Components: MinerU Integration: Leverages MinerU for high-fidelity document structure extraction and semantic preservation across complex layouts. Adaptive Content Decomposition: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships. Universal Format Support: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.</code></pre>
<div><h3>2. Multi-Modal Content Understanding &amp; Processing</h3><a href="#2-multi-modal-content-understanding--processing"></a></div>
<pre><code>The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.
Key Components: Autonomous Content Categorization and Routing: Automatically identify, categorize, and route different content types through optimized execution channels. Concurrent Multi-Pipeline Architecture: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity. Document Hierarchy Extraction: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.</code></pre>
<div><h3>3. Multimodal Analysis Engine</h3><a href="#3-multimodal-analysis-engine"></a></div>
<pre><code>The system deploys modality-aware processing units for heterogeneous data modalities:
Specialized Analyzers: Visual Content Analyzer: Integrate vision model for image analysis.
Generates context-aware descriptive captions based on visual semantics.
Extracts spatial relationships and hierarchical structures between visual elements. Structured Data Interpreter: Performs systematic interpretation of tabular and structured data formats.
Implements statistical pattern recognition algorithms for data trend analysis.
Identifies semantic relationships and dependencies across multiple tabular datasets. Mathematical Expression Parser: Parses complex mathematical expressions and formulas with high accuracy.
Provides native LaTeX format support for seamless integration with academic workflows.
Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases. Extensible Modality Handler: Provides configurable processing framework for custom and emerging content types.
Enables dynamic integration of new modality processors through plugin architecture.
Supports runtime configuration of processing pipelines for specialized use cases.</code></pre>
<div><h3>4. Multimodal Knowledge Graph Index</h3><a href="#4-multimodal-knowledge-graph-index"></a></div>
<pre><code>The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.
Core Functions: Multi-Modal Entity Extraction: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation. Cross-Modal Relationship Mapping: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms. Hierarchical Structure Preservation: Maintains original document organization through "belongs_to" relationship chains. These chains preserve logical content hierarchy and sectional dependencies. Weighted Relationship Scoring: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.</code></pre>
<div><h3>5. Modality-Aware Retrieval</h3><a href="#5-modality-aware-retrieval"></a></div>
<pre><code>The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.
Retrieval Mechanisms: Vector-Graph Fusion: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval. Modality-Aware Ranking: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences. Relational Coherence Maintenance: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.</code></pre>
<hr>
<div><h2> Quick Start</h2><a href="#-quick-start"></a></div>
<p><em>Initialize Your AI Journey</em></p>
<div> <a target="_blank" href="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif"><img src="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif"></a>
</div>
<div><h3>Installation</h3><a href="#installation"></a></div> <div><pre><span><span>#</span> Basic installation</span>
pip install raganything <span><span>#</span> With optional dependencies for extended format support:</span>
pip install <span><span>'</span>raganything[all]<span>'</span></span> <span><span>#</span> All optional features</span>
pip install <span><span>'</span>raganything[image]<span>'</span></span> <span><span>#</span> Image format conversion (BMP, TIFF, GIF, WebP)</span>
pip install <span><span>'</span>raganything[text]<span>'</span></span> <span><span>#</span> Text file processing (TXT, MD)</span>
pip install <span><span>'</span>raganything[image,text]<span>'</span></span> <span><span>#</span> Multiple features</span></pre></div>
<div><h4>Option 2: Install from Source</h4><a href="#option-2-install-from-source"></a></div>
<div><pre><span><span>#</span> Install uv (if not already installed)</span>
curl -LsSf https://astral.sh/uv/install.sh <span>|</span> sh <span><span>#</span> Clone and setup the project with uv</span>
git clone https://github.com/HKUDS/RAG-Anything.git
<span>cd</span> RAG-Anything <span><span>#</span> Install the package and dependencies in a virtual environment</span>
uv sync <span><span>#</span> If you encounter network timeouts (especially for opencv packages):</span>
<span><span>#</span> UV_HTTP_TIMEOUT=120 uv sync</span> <span><span>#</span> Run commands directly with uv (recommended approach)</span>
uv run python examples/raganything_example.py --help <span><span>#</span> Install with optional dependencies</span>
uv sync --extra image --extra text <span><span>#</span> Specific extras</span>
uv sync --all-extras <span><span>#</span> All optional features</span></pre></div>
<div><h4>Optional Dependencies</h4><a href="#optional-dependencies"></a></div>
<ul>
<li><strong><pre><code>[image]</code></pre></strong> - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)</li>
<li><strong><pre><code>[text]</code></pre></strong> - Enables processing of TXT and MD files (requires ReportLab)</li>
<li><strong><pre><code>[all]</code></pre></strong> - Includes all Python optional dependencies</li>
</ul>
<blockquote>
<p><strong> Office Document Processing Requirements:</strong></p>
<ul>
<li>Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require <strong>LibreOffice</strong> installation</li>
<li>Download from <a href="https://www.libreoffice.org/download/download/">LibreOffice official website</a></li>
<li><strong>Windows</strong>: Download installer from official website</li>
<li><strong>macOS</strong>: <pre><code>brew install --cask libreoffice</code></pre></li>
<li><strong>Ubuntu/Debian</strong>: <pre><code>sudo apt-get install libreoffice</code></pre></li>
<li><strong>CentOS/RHEL</strong>: <pre><code>sudo yum install libreoffice</code></pre></li>
</ul>
</blockquote>
<p><strong>Check MinerU installation:</strong></p>
<div><pre><span><span>#</span> Verify installation</span>
mineru --version <span><span>#</span> Check if properly configured</span>
python -c <span><span>"</span>from raganything import RAGAnything; rag = RAGAnything(); print(' MinerU installed properly' if rag.check_parser_installation() else ' MinerU installation issue')<span>"</span></span></pre></div>
<p>Models are downloaded automatically on first use. For manual download, refer to <a href="https://github.com/opendatalab/MinerU/blob/master/README.md#22-model-source-configuration">MinerU Model Source Configuration</a>.</p>
<div><h3>Usage Examples</h3><a href="#usage-examples"></a></div>
<div><h4>1. End-to-End Document Processing</h4><a href="#1-end-to-end-document-processing"></a></div>
<div><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>raganything</span> <span>import</span> <span>RAGAnything</span>, <span>RAGAnythingConfig</span>
<span>from</span> <span>lightrag</span>.<span>llm</span>.<span>openai</span> <span>import</span> <span>openai_complete_if_cache</span>, <span>openai_embed</span>
<span>from</span> <span>lightrag</span>.<span>utils</span> <span>import</span> <span>EmbeddingFunc</span> <span>async</span> <span>def</span> <span>main</span>(): <span># Set up API configuration</span> <span>api_key</span> <span>=</span> <span>"your-api-key"</span> <span>base_url</span> <span>=</span> <span>"your-base-url"</span> <span># Optional</span> <span># Create RAGAnything configuration</span> <span>config</span> <span>=</span> <span>RAGAnythingConfig</span>( <span>working_dir</span><span>=</span><span>"./rag_storage"</span>, <span>parser</span><span>=</span><span>"mineru"</span>, <span># Parser selection: mineru or docling</span> <span>parse_method</span><span>=</span><span>"auto"</span>, <span># Parse method: auto, ocr, or txt</span> <span>enable_image_processing</span><span>=</span><span>True</span>, <span>enable_table_processing</span><span>=</span><span>True</span>, <span>enable_equation_processing</span><span>=</span><span>True</span>, ) <span># Define LLM model function</span> <span>def</span> <span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>): <span>return</span> <span>openai_complete_if_cache</span>( <span>"gpt-4o-mini"</span>, <span>prompt</span>, <span>system_prompt</span><span>=</span><span>system_prompt</span>, <span>history_messages</span><span>=</span><span>history_messages</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) <span># Define vision model function for image processing</span> <span>def</span> <span>vision_model_func</span>( <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>image_data</span><span>=</span><span>None</span>, <span>messages</span><span>=</span><span>None</span>, <span>**</span><span>kwargs</span> ): <span># If messages format is provided (for multimodal VLM enhanced query), use it directly</span> <span>if</span> <span>messages</span>: <span>return</span> <span>openai_complete_if_cache</span>( <span>"gpt-4o"</span>, <span>""</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>messages</span><span>=</span><span>messages</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) <span># Traditional single image format</span> <span>elif</span> <span>image_data</span>: <span>return</span> <span>openai_complete_if_cache</span>( <span>"gpt-4o"</span>, <span>""</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>messages</span><span>=</span>[ {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>system_prompt</span>} <span>if</span> <span>system_prompt</span> <span>else</span> <span>None</span>, { <span>"role"</span>: <span>"user"</span>, <span>"content"</span>: [ {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>prompt</span>}, { <span>"type"</span>: <span>"image_url"</span>, <span>"image_url"</span>: { <span>"url"</span>: <span>f"data:image/jpeg;base64,<span><span>{</span><span>image_data</span><span>}</span></span>"</span> }, }, ], } <span>if</span> <span>image_data</span> <span>else</span> {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>}, ], <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) <span># Pure text format</span> <span>else</span>: <span>return</span> <span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span>, <span>history_messages</span>, <span>**</span><span>kwargs</span>) <span># Define embedding function</span> <span>embedding_func</span> <span>=</span> <span>EmbeddingFunc</span>( <span>embedding_dim</span><span>=</span><span>3072</span>, <span>max_token_size</span><span>=</span><span>8192</span>, <span>func</span><span>=</span><span>lambda</span> <span>texts</span>: <span>openai_embed</span>( <span>texts</span>, <span>model</span><span>=</span><span>"text-embedding-3-large"</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, ), ) <span># Initialize RAGAnything</span> <span>rag</span> <span>=</span> <span>RAGAnything</span>( <span>config</span><span>=</span><span>config</span>, <span>llm_model_func</span><span>=</span><span>llm_model_func</span>, <span>vision_model_func</span><span>=</span><span>vision_model_func</span>, <span>embedding_func</span><span>=</span><span>embedding_func</span>, ) <span># Process a document</span> <span>await</span> <span>rag</span>.<span>process_document_complete</span>( <span>file_path</span><span>=</span><span>"path/to/your/document.pdf"</span>, <span>output_dir</span><span>=</span><span>"./output"</span>, <span>parse_method</span><span>=</span><span>"auto"</span> ) <span># Query the processed content</span> <span># Pure text query - for basic knowledge base search</span> <span>text_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>( <span>"What are the main findings shown in the figures and tables?"</span>, <span>mode</span><span>=</span><span>"hybrid"</span> ) <span>print</span>(<span>"Text query result:"</span>, <span>text_result</span>) <span># Multimodal query with specific multimodal content</span> <span>multimodal_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery_with_multimodal</span>( <span>"Explain this formula and its relevance to the document content"</span>, <span>multimodal_content</span><span>=</span>[{ <span>"type"</span>: <span>"equation"</span>, <span>"latex"</span>: <span>"P(d|q) = <span>\\</span>frac{P(q|d) <span>\\</span>cdot P(d)}{P(q)}"</span>, <span>"equation_caption"</span>: <span>"Document relevance probability"</span> }], <span>mode</span><span>=</span><span>"hybrid"</span>
) <span>print</span>(<span>"Multimodal query result:"</span>, <span>multimodal_result</span>) <span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>: <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<div><h4>2. Direct Multimodal Content Processing</h4><a href="#2-direct-multimodal-content-processing"></a></div>
<div><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>lightrag</span> <span>import</span> <span>LightRAG</span>
<span>from</span> <span>lightrag</span>.<span>llm</span>.<span>openai</span> <span>import</span> <span>openai_complete_if_cache</span>, <span>openai_embed</span>
<span>from</span> <span>lightrag</span>.<span>utils</span> <span>import</span> <span>EmbeddingFunc</span>
<span>from</span> <span>raganything</span>.<span>modalprocessors</span> <span>import</span> <span>ImageModalProcessor</span>, <span>TableModalProcessor</span> <span>async</span> <span>def</span> <span>process_multimodal_content</span>(): <span># Set up API configuration</span> <span>api_key</span> <span>=</span> <span>"your-api-key"</span> <span>base_url</span> <span>=</span> <span>"your-base-url"</span> <span># Optional</span> <span># Initialize LightRAG</span> <span>rag</span> <span>=</span> <span>LightRAG</span>( <span>working_dir</span><span>=</span><span>"./rag_storage"</span>, <span>llm_model_func</span><span>=</span><span>lambda</span> <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>: <span>openai_complete_if_cache</span>( <span>"gpt-4o-mini"</span>, <span>prompt</span>, <span>system_prompt</span><span>=</span><span>system_prompt</span>, <span>history_messages</span><span>=</span><span>history_messages</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ), <span>embedding_func</span><span>=</span><span>EmbeddingFunc</span>( <span>embedding_dim</span><span>=</span><span>3072</span>, <span>max_token_size</span><span>=</span><span>8192</span>, <span>func</span><span>=</span><span>lambda</span> <span>texts</span>: <span>openai_embed</span>( <span>texts</span>, <span>model</span><span>=</span><span>"text-embedding-3-large"</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, ), ) ) <span>await</span> <span>rag</span>.<span>initialize_storages</span>() <span># Process an image</span> <span>image_processor</span> <span>=</span> <span>ImageModalProcessor</span>( <span>lightrag</span><span>=</span><span>rag</span>, <span>modal_caption_func</span><span>=</span><span>lambda</span> <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>image_data</span><span>=</span><span>None</span>, <span>**</span><span>kwargs</span>: <span>openai_complete_if_cache</span>( <span>"gpt-4o"</span>, <span>""</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>messages</span><span>=</span>[ {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>system_prompt</span>} <span>if</span> <span>system_prompt</span> <span>else</span> <span>None</span>, {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: [ {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>prompt</span>}, {<span>"type"</span>: <span>"image_url"</span>, <span>"image_url"</span>: {<span>"url"</span>: <span>f"data:image/jpeg;base64,<span><span>{</span><span>image_data</span><span>}</span></span>"</span>}} ]} <span>if</span> <span>image_data</span> <span>else</span> {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>} ], <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) <span>if</span> <span>image_data</span> <span>else</span> <span>openai_complete_if_cache</span>( <span>"gpt-4o-mini"</span>, <span>prompt</span>, <span>system_prompt</span><span>=</span><span>system_prompt</span>, <span>history_messages</span><span>=</span><span>history_messages</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) ) <span>image_content</span> <span>=</span> { <span>"img_path"</span>: <span>"path/to/image.jpg"</span>, <span>"image_caption"</span>: [<span>"Figure 1: Experimental results"</span>], <span>"image_footnote"</span>: [<span>"Data collected in 2024"</span>] } <span>description</span>, <span>entity_info</span> <span>=</span> <span>await</span> <span>image_processor</span>.<span>process_multimodal_content</span>( <span>modal_content</span><span>=</span><span>image_content</span>, <span>content_type</span><span>=</span><span>"image"</span>, <span>file_path</span><span>=</span><span>"research_paper.pdf"</span>, <span>entity_name</span><span>=</span><span>"Experimental Results Figure"</span> ) <span># Process a table</span> <span>table_processor</span> <span>=</span> <span>TableModalProcessor</span>( <span>lightrag</span><span>=</span><span>rag</span>, <span>modal_caption_func</span><span>=</span><span>lambda</span> <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>: <span>openai_complete_if_cache</span>( <span>"gpt-4o-mini"</span>, <span>prompt</span>, <span>system_prompt</span><span>=</span><span>system_prompt</span>, <span>history_messages</span><span>=</span><span>history_messages</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) ) <span>table_content</span> <span>=</span> { <span>"table_body"</span>: <span>"""</span>
<span> | Method | Accuracy | F1-Score |</span>
<span> |--------|----------|----------|</span>
<span> | RAGAnything | 95.2% | 0.94 |</span>
<span> | Baseline | 87.3% | 0.85 |</span>
<span> """</span>, <span>"table_caption"</span>: [<span>"Performance Comparison"</span>], <span>"table_footnote"</span>: [<span>"Results on test dataset"</span>] } <span>description</span>, <span>entity_info</span> <span>=</span> <span>await</span> <span>table_processor</span>.<span>process_multimodal_content</span>( <span>modal_content</span><span>=</span><span>table_content</span>, <span>content_type</span><span>=</span><span>"table"</span>, <span>file_path</span><span>=</span><span>"research_paper.pdf"</span>, <span>entity_name</span><span>=</span><span>"Performance Results Table"</span> ) <span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>: <span>asyncio</span>.<span>run</span>(<span>process_multimodal_content</span>())</pre></div>
<div><h4>3. Batch Processing</h4><a href="#3-batch-processing"></a></div>
<div><pre><span># Process multiple documents</span>
<span>await</span> <span>rag</span>.<span>process_folder_complete</span>( <span>folder_path</span><span>=</span><span>"./documents"</span>, <span>output_dir</span><span>=</span><span>"./output"</span>, <span>file_extensions</span><span>=</span>[<span>".pdf"</span>, <span>".docx"</span>, <span>".pptx"</span>], <span>recursive</span><span>=</span><span>True</span>, <span>max_workers</span><span>=</span><span>4</span>
)</pre></div>
<div><h4>4. Custom Modal Processors</h4><a href="#4-custom-modal-processors"></a></div>
<div><pre><span>from</span> <span>raganything</span>.<span>modalprocessors</span> <span>import</span> <span>GenericModalProcessor</span> <span>class</span> <span>CustomModalProcessor</span>(<span>GenericModalProcessor</span>): <span>async</span> <span>def</span> <span>process_multimodal_content</span>(<span>self</span>, <span>modal_content</span>, <span>content_type</span>, <span>file_path</span>, <span>entity_name</span>): <span># Your custom processing logic</span> <span>enhanced_description</span> <span>=</span> <span>await</span> <span>self</span>.<span>analyze_custom_content</span>(<span>modal_content</span>) <span>entity_info</span> <span>=</span> <span>self</span>.<span>create_custom_entity</span>(<span>enhanced_description</span>, <span>entity_name</span>) <span>return</span> <span>await</span> <span>self</span>.<span>_create_entity_and_chunk</span>(<span>enhanced_description</span>, <span>entity_info</span>, <span>file_path</span>)</pre></div>
<div><h4>5. Query Options</h4><a href="#5-query-options"></a></div>
<p>RAG-Anything provides three types of query methods:</p>
<p><strong>Pure Text Queries</strong> - Direct knowledge base search using LightRAG:</p>
<div><pre><span># Different query modes for text queries</span>
<span>text_result_hybrid</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"hybrid"</span>)
<span>text_result_local</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"local"</span>)
<span>text_result_global</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"global"</span>)
<span>text_result_naive</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"naive"</span>) <span># Synchronous version</span>
<span>sync_text_result</span> <span>=</span> <span>rag</span>.<span>query</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"hybrid"</span>)</pre></div>
<p><strong>VLM Enhanced Queries</strong> - Automatically analyze images in retrieved context using VLM:</p>
<div><pre><span># VLM enhanced query (automatically enabled when vision_model_func is provided)</span>
<span>vlm_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>( <span>"Analyze the charts and figures in the document"</span>, <span>mode</span><span>=</span><span>"hybrid"</span> <span># vlm_enhanced=True is automatically set when vision_model_func is available</span>
) <span># Manually control VLM enhancement</span>
<span>vlm_enabled</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>( <span>"What do the images show in this document?"</span>, <span>mode</span><span>=</span><span>"hybrid"</span>, <span>vlm_enhanced</span><span>=</span><span>True</span> <span># Force enable VLM enhancement</span>
) <span>vlm_disabled</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>( <span>"What do the images show in this document?"</span>, <span>mode</span><span>=</span><span>"hybrid"</span>, <span>vlm_enhanced</span><span>=</span><span>False</span> <span># Force disable VLM enhancement</span>
) <span># When documents contain images, VLM can see and analyze them directly</span>
<span># The system will automatically:</span>
<span># 1. Retrieve relevant context containing image paths</span>
<span># 2. Load and encode images as base64</span>
<span># 3. Send both text context and images to VLM for comprehensive analysis</span></pre></div>
<p><strong>Multimodal Queries</strong> - Enhanced queries with specific multimodal content analysis:</p>
<div><pre><span># Query with table data</span>
<span>table_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery_with_multimodal</span>( <span>"Compare these performance metrics with the document content"</span>, <span>multimodal_content</span><span>=</span>[{ <span>"type"</span>: <span>"table"</span>, <span>"table_data"</span>: <span>"""Method,Accuracy,Speed</span>
<span> RAGAnything,95.2%,120ms</span>
<span> Traditional,87.3%,180ms"""</span>, <span>"table_caption"</span>: <span>"Performance comparison"</span> }], <span>mode</span><span>=</span><span>"hybrid"</span>
) <span># Query with equation content</span>
<span>equation_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery_with_multimodal</span>( <span>"Explain this formula and its relevance to the document content"</span>, <span>multimodal_content</span><span>=</span>[{ <span>"type"</span>: <span>"equation"</span>, <span>"latex"</span>: <span>"P(d|q) = <span>\\</span>frac{P(q|d) <span>\\</span>cdot P(d)}{P(q)}"</span>, <span>"equation_caption"</span>: <span>"Document relevance probability"</span> }], <span>mode</span><span>=</span><span>"hybrid"</span>
)</pre></div>
<div><h4>6. Loading Existing LightRAG Instance</h4><a href="#6-loading-existing-lightrag-instance"></a></div>
<div><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>raganything</span> <span>import</span> <span>RAGAnything</span>, <span>RAGAnythingConfig</span>
<span>from</span> <span>lightrag</span> <span>import</span> <span>LightRAG</span>
<span>from</span> <span>lightrag</span>.<span>llm</span>.<span>openai</span> <span>import</span> <span>openai_complete_if_cache</span>, <span>openai_embed</span>
<span>from</span> <span>lightrag</span>.<span>kg</span>.<span>shared_storage</span> <span>import</span> <span>initialize_pipeline_status</span>
<span>from</span> <span>lightrag</span>.<span>utils</span> <span>import</span> <span>EmbeddingFunc</span>
<span>import</span> <span>os</span> <span>async</span> <span>def</span> <span>load_existing_lightrag</span>(): <span># Set up API configuration</span> <span>api_key</span> <span>=</span> <span>"your-api-key"</span> <span>base_url</span> <span>=</span> <span>"your-base-url"</span> <span># Optional</span> <span># First, create or load existing LightRAG instance</span> <span>lightrag_working_dir</span> <span>=</span> <span>"./existing_lightrag_storage"</span> <span># Check if previous LightRAG instance exists</span> <span>if</span> <span>os</span>.<span>path</span>.<span>exists</span>(<span>lightrag_working_dir</span>) <span>and</span> <span>os</span>.<span>listdir</span>(<span>lightrag_working_dir</span>): <span>print</span>(<span>" Found existing LightRAG instance, loading..."</span>) <span>else</span>: <span>print</span>(<span>" No existing LightRAG instance found, will create new one"</span>) <span># Create/load LightRAG instance with your configuration</span> <span>lightrag_instance</span> <span>=</span> <span>LightRAG</span>( <span>working_dir</span><span>=</span><span>lightrag_working_dir</span>, <span>llm_model_func</span><span>=</span><span>lambda</span> <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>: <span>openai_complete_if_cache</span>( <span>"gpt-4o-mini"</span>, <span>prompt</span>, <span>system_prompt</span><span>=</span><span>system_prompt</span>, <span>history_messages</span><span>=</span><span>history_messages</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ), <span>embedding_func</span><span>=</span><span>EmbeddingFunc</span>( <span>embedding_dim</span><span>=</span><span>3072</span>, <span>max_token_size</span><span>=</span><span>8192</span>, <span>func</span><span>=</span><span>lambda</span> <span>texts</span>: <span>openai_embed</span>( <span>texts</span>, <span>model</span><span>=</span><span>"text-embedding-3-large"</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, ), ) ) <span># Initialize storage (this will load existing data if available)</span> <span>await</span> <span>lightrag_instance</span>.<span>initialize_storages</span>() <span>await</span> <span>initialize_pipeline_status</span>() <span># Define vision model function for image processing</span> <span>def</span> <span>vision_model_func</span>( <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>image_data</span><span>=</span><span>None</span>, <span>messages</span><span>=</span><span>None</span>, <span>**</span><span>kwargs</span> ): <span># If messages format is provided (for multimodal VLM enhanced query), use it directly</span> <span>if</span> <span>messages</span>: <span>return</span> <span>openai_complete_if_cache</span>( <span>"gpt-4o"</span>, <span>""</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>messages</span><span>=</span><span>messages</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) <span># Traditional single image format</span> <span>elif</span> <span>image_data</span>: <span>return</span> <span>openai_complete_if_cache</span>( <span>"gpt-4o"</span>, <span>""</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>messages</span><span>=</span>[ {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>system_prompt</span>} <span>if</span> <span>system_prompt</span> <span>else</span> <span>None</span>, { <span>"role"</span>: <span>"user"</span>, <span>"content"</span>: [ {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>prompt</span>}, { <span>"type"</span>: <span>"image_url"</span>, <span>"image_url"</span>: { <span>"url"</span>: <span>f"data:image/jpeg;base64,<span><span>{</span><span>image_data</span><span>}</span></span>"</span> }, }, ], } <span>if</span> <span>image_data</span> <span>else</span> {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>}, ], <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) <span># Pure text format</span> <span>else</span>: <span>return</span> <span>lightrag_instance</span>.<span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span>, <span>history_messages</span>, <span>**</span><span>kwargs</span>) <span># Now use existing LightRAG instance to initialize RAGAnything</span> <span>rag</span> <span>=</span> <span>RAGAnything</span>( <span>lightrag</span><span>=</span><span>lightrag_instance</span>, <span># Pass existing LightRAG instance</span> <span>vision_model_func</span><span>=</span><span>vision_model_func</span>, <span># Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance</span> ) <span># Query existing knowledge base</span> <span>result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>( <span>"What data has been processed in this LightRAG instance?"</span>, <span>mode</span><span>=</span><span>"hybrid"</span> ) <span>print</span>(<span>"Query result:"</span>, <span>result</span>) <span># Add new multimodal document to existing LightRAG instance</span> <span>await</span> <span>rag</span>.<span>process_document_complete</span>( <span>file_path</span><span>=</span><span>"path/to/new/multimodal_document.pdf"</span>, <span>output_dir</span><span>=</span><span>"./output"</span> ) <span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>: <span>asyncio</span>.<span>run</span>(<span>load_existing_lightrag</span>())</pre></div>
<div><h4>7. Direct Content List Insertion</h4><a href="#7-direct-content-list-insertion"></a></div>
<p>For scenarios where you already have a pre-parsed content list (e.g., from external parsers or previous processing), you can directly insert it into RAGAnything without document parsing:</p>
<div><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>raganything</span> <span>import</span> <span>RAGAnything</span>, <span>RAGAnythingConfig</span>
<span>from</span> <span>lightrag</span>.<span>llm</span>.<span>openai</span> <span>import</span> <span>openai_complete_if_cache</span>, <span>openai_embed</span>
<span>from</span> <span>lightrag</span>.<span>utils</span> <span>import</span> <span>EmbeddingFunc</span> <span>async</span> <span>def</span> <span>insert_content_list_example</span>(): <span># Set up API configuration</span> <span>api_key</span> <span>=</span> <span>"your-api-key"</span> <span>base_url</span> <span>=</span> <span>"your-base-url"</span> <span># Optional</span> <span># Create RAGAnything configuration</span> <span>config</span> <span>=</span> <span>RAGAnythingConfig</span>( <span>working_dir</span><span>=</span><span>"./rag_storage"</span>, <span>enable_image_processing</span><span>=</span><span>True</span>, <span>enable_table_processing</span><span>=</span><span>True</span>, <span>enable_equation_processing</span><span>=</span><span>True</span>, ) <span># Define model functions</span> <span>def</span> <span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>): <span>return</span> <span>openai_complete_if_cache</span>( <span>"gpt-4o-mini"</span>, <span>prompt</span>, <span>system_prompt</span><span>=</span><span>system_prompt</span>, <span>history_messages</span><span>=</span><span>history_messages</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) <span>def</span> <span>vision_model_func</span>(<span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>image_data</span><span>=</span><span>None</span>, <span>messages</span><span>=</span><span>None</span>, <span>**</span><span>kwargs</span>): <span># If messages format is provided (for multimodal VLM enhanced query), use it directly</span> <span>if</span> <span>messages</span>: <span>return</span> <span>openai_complete_if_cache</span>( <span>"gpt-4o"</span>, <span>""</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>messages</span><span>=</span><span>messages</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) <span># Traditional single image format</span> <span>elif</span> <span>image_data</span>: <span>return</span> <span>openai_complete_if_cache</span>( <span>"gpt-4o"</span>, <span>""</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>messages</span><span>=</span>[ {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>system_prompt</span>} <span>if</span> <span>system_prompt</span> <span>else</span> <span>None</span>, { <span>"role"</span>: <span>"user"</span>, <span>"content"</span>: [ {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>prompt</span>}, {<span>"type"</span>: <span>"image_url"</span>, <span>"image_url"</span>: {<span>"url"</span>: <span>f"data:image/jpeg;base64,<span><span>{</span><span>image_data</span><span>}</span></span>"</span>}} ], } <span>if</span> <span>image_data</span> <span>else</span> {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>}, ], <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, <span>**</span><span>kwargs</span>, ) <span># Pure text format</span> <span>else</span>: <span>return</span> <span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span>, <span>history_messages</span>, <span>**</span><span>kwargs</span>) <span>embedding_func</span> <span>=</span> <span>EmbeddingFunc</span>( <span>embedding_dim</span><span>=</span><span>3072</span>, <span>max_token_size</span><span>=</span><span>8192</span>, <span>func</span><span>=</span><span>lambda</span> <span>texts</span>: <span>openai_embed</span>( <span>texts</span>, <span>model</span><span>=</span><span>"text-embedding-3-large"</span>, <span>api_key</span><span>=</span><span>api_key</span>, <span>base_url</span><span>=</span><span>base_url</span>, ), ) <span># Initialize RAGAnything</span> <span>rag</span> <span>=</span> <span>RAGAnything</span>( <span>config</span><span>=</span><span>config</span>, <span>llm_model_func</span><span>=</span><span>llm_model_func</span>, <span>vision_model_func</span><span>=</span><span>vision_model_func</span>, <span>embedding_func</span><span>=</span><span>embedding_func</span>, ) <span># Example: Pre-parsed content list from external source</span> <span>content_list</span> <span>=</span> [ { <span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"This is the introduction section of our research paper."</span>, <span>"page_idx"</span>: <span>0</span> <span># Page number where this content appears</span> }, { <span>"type"</span>: <span>"image"</span>, <span>"img_path"</span>: <span>"/absolute/path/to/figure1.jpg"</span>, <span># IMPORTANT: Use absolute path</span> <span>"image_caption"</span>: [<span>"Figure 1: System Architecture"</span>], <span>"image_footnote"</span>: [<span>"Source: Authors' original design"</span>], <span>"page_idx"</span>: <span>1</span> <span># Page number where this image appears</span> }, { <span>"type"</span>: <span>"table"</span>, <span>"table_body"</span>: <span>"| Method | Accuracy | F1-Score |<span>\n</span>|--------|----------|----------|<span>\n</span>| Ours | 95.2% | 0.94 |<span>\n</span>| Baseline | 87.3% | 0.85 |"</span>, <span>"table_caption"</span>: [<span>"Table 1: Performance Comparison"</span>], <span>"table_footnote"</span>: [<span>"Results on test dataset"</span>], <span>"page_idx"</span>: <span>2</span> <span># Page number where this table appears</span> }, { <span>"type"</span>: <span>"equation"</span>, <span>"latex"</span>: <span>"P(d|q) = <span>\\</span>frac{P(q|d) <span>\\</span>cdot P(d)}{P(q)}"</span>, <span>"text"</span>: <span>"Document relevance probability formula"</span>, <span>"page_idx"</span>: <span>3</span> <span># Page number where this equation appears</span> }, { <span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"In conclusion, our method demonstrates superior performance across all metrics."</span>, <span>"page_idx"</span>: <span>4</span> <span># Page number where this content appears</span> } ] <span># Insert the content list directly</span> <span>await</span> <span>rag</span>.<span>insert_content_list</span>( <span>content_list</span><span>=</span><span>content_list</span>, <span>file_path</span><span>=</span><span>"research_paper.pdf"</span>, <span># Reference file name for citation</span> <span>split_by_character</span><span>=</span><span>None</span>, <span># Optional text splitting</span> <span>split_by_character_only</span><span>=</span><span>False</span>, <span># Optional text splitting mode</span> <span>doc_id</span><span>=</span><span>None</span>, <span># Optional custom document ID (will be auto-generated if not provided)</span> <span>display_stats</span><span>=</span><span>True</span> <span># Show content statistics</span> ) <span># Query the inserted content</span> <span>result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>( <span>"What are the key findings and performance metrics mentioned in the research?"</span>, <span>mode</span><span>=</span><span>"hybrid"</span> ) <span>print</span>(<span>"Query result:"</span>, <span>result</span>) <span># You can also insert multiple content lists with different document IDs</span> <span>another_content_list</span> <span>=</span> [ { <span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"This is content from another document."</span>, <span>"page_idx"</span>: <span>0</span> <span># Page number where this content appears</span> }, { <span>"type"</span>: <span>"table"</span>, <span>"table_body"</span>: <span>"| Feature | Value |<span>\n</span>|---------|-------|<span>\n</span>| Speed | Fast |<span>\n</span>| Accuracy | High |"</span>, <span>"table_caption"</span>: [<span>"Feature Comparison"</span>], <span>"page_idx"</span>: <span>1</span> <span># Page number where this table appears</span> } ] <span>await</span> <span>rag</span>.<span>insert_content_list</span>( <span>content_list</span><span>=</span><span>another_content_list</span>, <span>file_path</span><span>=</span><span>"another_document.pdf"</span>, <span>doc_id</span><span>=</span><span>"custom-doc-id-123"</span> <span># Custom document ID</span> ) <span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>: <span>asyncio</span>.<span>run</span>(<span>insert_content_list_example</span>())</pre></div>
<p><strong>Content List Format:</strong></p>
<p>The </p><pre><code>content_list</code></pre> should follow the standard format with each item being a dictionary containing:<p></p>
<ul>
<li><strong>Text content</strong>: <pre><code>{"type": "text", "text": "content text", "page_idx": 0}</code></pre></li>
<li><strong>Image content</strong>: <pre><code>{"type": "image", "img_path": "/absolute/path/to/image.jpg", "image_caption": ["caption"], "image_footnote": ["note"], "page_idx": 1}</code></pre></li>
<li><strong>Table content</strong>: <pre><code>{"type": "table", "table_body": "markdown table", "table_caption": ["caption"], "table_footnote": ["note"], "page_idx": 2}</code></pre></li>
<li><strong>Equation content</strong>: <pre><code>{"type": "equation", "latex": "LaTeX formula", "text": "description", "page_idx": 3}</code></pre></li>
<li><strong>Generic content</strong>: <pre><code>{"type": "custom_type", "content": "any content", "page_idx": 4}</code></pre></li>
</ul>
<p><strong>Important Notes:</strong></p>
<ul>
<li><strong><pre><code>img_path</code></pre></strong>: Must be an absolute path to the image file (e.g., <pre><code>/home/user/images/chart.jpg</code></pre> or <pre><code>C:\Users\user\images\chart.jpg</code></pre>)</li>
<li><strong><pre><code>page_idx</code></pre></strong>: Represents the page number where the content appears in the original document (0-based indexing)</li>
<li><strong>Content ordering</strong>: Items are processed in the order they appear in the list</li>
</ul>
<p>This method is particularly useful when:</p>
<ul>
<li>You have content from external parsers (non-MinerU/Docling)</li>
<li>You want to process programmatically generated content</li>
<li>You need to insert content from multiple sources into a single knowledge base</li>
<li>You have cached parsing results that you want to reuse</li>
</ul>
<hr>
<div><h2> Examples</h2><a href="#-examples"></a></div>
<p><em>Practical Implementation Demos</em></p>
<div> <a target="_blank" href="https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif"><img src="https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif"></a>
</div>
<p>The </p><pre><code>examples/</code></pre> directory contains comprehensive usage examples:<p></p>
<ul>
<li><strong><pre><code>raganything_example.py</code></pre></strong>: End-to-end document processing with MinerU</li>
<li><strong><pre><code>modalprocessors_example.py</code></pre></strong>: Direct multimodal content processing</li>
<li><strong><pre><code>office_document_test.py</code></pre></strong>: Office document parsing test with MinerU (no API key required)</li>
<li><strong><pre><code>image_format_test.py</code></pre></strong>: Image format parsing test with MinerU (no API key required)</li>
<li><strong><pre><code>text_format_test.py</code></pre></strong>: Text format parsing test with MinerU (no API key required)</li>
</ul>
<p><strong>Run examples:</strong></p>
<div><pre><span><span>#</span> End-to-end processing with parser selection</span>
python examples/raganything_example.py path/to/document.pdf --api-key YOUR_API_KEY --parser mineru <span><span>#</span> Direct modal processing</span>
python examples/modalprocessors_example.py --api-key YOUR_API_KEY <span><span>#</span> Office document parsing test (MinerU only)</span>
python examples/office_document_test.py --file path/to/document.docx <span><span>#</span> Image format parsing test (MinerU only)</span>
python examples/image_format_test.py --file path/to/image.bmp <span><span>#</span> Text format parsing test (MinerU only)</span>
python examples/text_format_test.py --file path/to/document.md <span><span>#</span> Check LibreOffice installation</span>
python examples/office_document_test.py --check-libreoffice --file dummy <span><span>#</span> Check PIL/Pillow installation</span>
python examples/image_format_test.py --check-pillow --file dummy <span><span>#</span> Check ReportLab installation</span>
python examples/text_format_test.py --check-reportlab --file dummy</pre></div>
<hr>
<div><h2> Configuration</h2><a href="#-configuration"></a></div>
<p><em>System Optimization Parameters</em></p>
<div><h3>Environment Variables</h3><a href="#environment-variables"></a></div>
<p>Create a </p><pre><code>.env</code></pre> file (refer to <pre><code>.env.example</code></pre>):<p></p>
<div><pre>OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=your_base_url <span><span>#</span> Optional</span>
OUTPUT_DIR=./output <span><span>#</span> Default output directory for parsed documents</span>
PARSER=mineru <span><span>#</span> Parser selection: mineru or docling</span>
PARSE_METHOD=auto <span><span>#</span> Parse method: auto, ocr, or txt</span></pre></div>
<p><strong>Note:</strong> For backward compatibility, legacy environment variable names are still supported:</p>
<ul>
<li><pre><code>MINERU_PARSE_METHOD</code></pre> is deprecated, please use <pre><code>PARSE_METHOD</code></pre></li>
</ul>
<blockquote>
<p><strong>Note</strong>: API keys are only required for full RAG processing with LLM integration. The parsing test files (</p><pre><code>office_document_test.py</code></pre> and <pre><code>image_format_test.py</code></pre>) only test parser functionality and do not require API keys.<p></p>
</blockquote>
<div><h3>Parser Configuration</h3><a href="#parser-configuration"></a></div>
<p>RAGAnything now supports multiple parsers, each with specific advantages:</p>
<div><h4>MinerU Parser</h4><a href="#mineru-parser"></a></div>
<ul>
<li>Supports PDF, images, Office documents, and more formats</li>
<li>Powerful OCR and table extraction capabilities</li>
<li>GPU acceleration support</li>
</ul>
<div><h4>Docling Parser</h4><a href="#docling-parser"></a></div>
<ul>
<li>Optimized for Office documents and HTML files</li>
<li>Better document structure preservation</li>
<li>Native support for multiple Office formats</li>
</ul>
<div><h3>MinerU Configuration</h3><a href="#mineru-configuration"></a></div>
<div><pre><span><span>#</span> MinerU 2.0 uses command-line parameters instead of config files</span>
<span><span>#</span> Check available options:</span>
mineru --help <span><span>#</span> Common configurations:</span>
mineru -p input.pdf -o output_dir -m auto <span><span>#</span> Automatic parsing mode</span>
mineru -p input.pdf -o output_dir -m ocr <span><span>#</span> OCR-focused parsing</span>
mineru -p input.pdf -o output_dir -b pipeline --device cuda <span><span>#</span> GPU acceleration</span></pre></div>
<p>You can also configure parsing through RAGAnything parameters:</p>
<div><pre><span># Basic parsing configuration with parser selection</span>
<span>await</span> <span>rag</span>.<span>process_document_complete</span>( <span>file_path</span><span>=</span><span>"document.pdf"</span>, <span>output_dir</span><span>=</span><span>"./output/"</span>, <span>parse_method</span><span>=</span><span>"auto"</span>, <span># or "ocr", "txt"</span> <span>parser</span><span>=</span><span>"mineru"</span> <span># Optional: "mineru" or "docling"</span>
) <span># Advanced parsing configuration with special parameters</span>
<span>await</span> <span>rag</span>.<span>process_document_complete</span>( <span>file_path</span><span>=</span><span>"document.pdf"</span>, <span>output_dir</span><span>=</span><span>"./output/"</span>, <span>parse_method</span><span>=</span><span>"auto"</span>, <span># Parsing method: "auto", "ocr", "txt"</span> <span>parser</span><span>=</span><span>"mineru"</span>, <span># Parser selection: "mineru" or "docling"</span> <span># MinerU special parameters - all supported kwargs:</span> <span>lang</span><span>=</span><span>"ch"</span>, <span># Document language for OCR optimization (e.g., "ch", "en", "ja")</span> <span>device</span><span>=</span><span>"cuda:0"</span>, <span># Inference device: "cpu", "cuda", "cuda:0", "npu", "mps"</span> <span>start_page</span><span>=</span><span>0</span>, <span># Starting page number (0-based, for PDF)</span> <span>end_page</span><span>=</span><span>10</span>, <span># Ending page number (0-based, for PDF)</span> <span>formula</span><span>=</span><span>True</span>, <span># Enable formula parsing</span> <span>table</span><span>=</span><span>True</span>, <span># Enable table parsing</span> <span>backend</span><span>=</span><span>"pipeline"</span>, <span># Parsing backend: pipeline|hybrid-auto-engine|hybrid-http-client|vlm-auto-engine|vlm-http-client.</span> <span>source</span><span>=</span><span>"huggingface"</span>, <span># Model source: "huggingface", "modelscope", "local"</span> <span># vlm_url="http://127.0.0.1:3000" # Service address when using backend=vlm-http-client</span> <span># Standard RAGAnything parameters</span> <span>display_stats</span><span>=</span><span>True</span>, <span># Display content statistics</span> <span>split_by_character</span><span>=</span><span>None</span>, <span># Optional character to split text by</span> <span>doc_id</span><span>=</span><span>None</span> <span># Optional document ID</span>
)</pre></div>
<blockquote>
<p><strong>Note</strong>: MinerU 2.0 no longer uses the </p><pre><code>magic-pdf.json</code></pre> configuration file. All settings are now passed as command-line parameters or function arguments. RAG-Anything now supports multiple document parsers - you can choose between MinerU and Docling based on your needs.<p></p>
</blockquote>
<div><h3>Processing Requirements</h3><a href="#processing-requirements"></a></div>
<p>Different content types require specific optional dependencies:</p>
<ul>
<li><strong>Office Documents</strong> (.doc, .docx, .ppt, .pptx, .xls, .xlsx): Install <a href="https://www.libreoffice.org/download/download/">LibreOffice</a></li>
<li><strong>Extended Image Formats</strong> (.bmp, .tiff, .gif, .webp): Install with <pre><code>pip install raganything[image]</code></pre></li>
<li><strong>Text Files</strong> (.txt, .md): Install with <pre><code>pip install raganything[text]</code></pre></li>
</ul>
<blockquote>
<p><strong> Quick Install</strong>: Use </p><pre><code>pip install raganything[all]</code></pre> to enable all format support (Python dependencies only - LibreOffice still needs separate installation)<p></p>
</blockquote>
<hr>
<div><h2> Supported Content Types</h2><a href="#-supported-content-types"></a></div>
<div><h3>Document Formats</h3><a href="#document-formats"></a></div>
<ul>
<li><strong>PDFs</strong> - Research papers, reports, presentations</li>
<li><strong>Office Documents</strong> - DOC, DOCX, PPT, PPTX, XLS, XLSX</li>
<li><strong>Images</strong> - JPG, PNG, BMP, TIFF, GIF, WebP</li>
<li><strong>Text Files</strong> - TXT, MD</li>
</ul>
<div><h3>Multimodal Elements</h3><a href="#multimodal-elements"></a></div>
<ul>
<li><strong>Images</strong> - Photographs, diagrams, charts, screenshots</li>
<li><strong>Tables</strong> - Data tables, comparison charts, statistical summaries</li>
<li><strong>Equations</strong> - Mathematical formulas in LaTeX format</li>
<li><strong>Generic Content</strong> - Custom content types via extensible processors</li>
</ul>
<p><em>For installation of format-specific dependencies, see the <a href="#-configuration">Configuration</a> section.</em></p>
<hr>
<div><h2> Citation</h2><a href="#-citation"></a></div>
<p><em>Academic Reference</em></p>
<div> <div> <div> <div></div> </div> <div></div> </div>
</div>
<p>If you find RAG-Anything useful in your research, please cite our paper:</p>
<div><pre><span>@misc</span>{<span>guo2025raganythingallinoneragframework</span>, <span>title</span>=<span><span>{</span>RAG-Anything: All-in-One RAG Framework<span>}</span></span>, <span>author</span>=<span><span>{</span>Zirui Guo and Xubin Ren and Lingrui Xu and Jiahao Zhang and Chao Huang<span>}</span></span>, <span>year</span>=<span><span>{</span>2025<span>}</span></span>, <span>eprint</span>=<span><span>{</span>2510.12323<span>}</span></span>, <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>, <span>primaryClass</span>=<span><span>{</span>cs.AI<span>}</span></span>, <span>url</span>=<span><span>{</span>https://arxiv.org/abs/2510.12323<span>}</span></span>,
}</pre></div>
<hr>
<div><h2> Related Projects</h2><a href="#-related-projects"></a></div>
<p><em>Ecosystem &amp; Extensions</em></p>
<div> <table> <tbody><tr> <td> <a href="https://github.com/HKUDS/LightRAG"> <div> <span></span> </div> <b>LightRAG</b><br> <sub>Simple and Fast RAG</sub> </a> </td> <td> <a href="https://github.com/HKUDS/VideoRAG"> <div> <span></span> </div> <b>VideoRAG</b><br> <sub>Extreme Long-Context Video RAG</sub> </a> </td> <td> <a href="https://github.com/HKUDS/MiniRAG"> <div> <span></span> </div> <b>MiniRAG</b><br> <sub>Extremely Simple RAG</sub> </a> </td> </tr> </tbody></table>
</div>
<hr>
<div><h2> Star History</h2><a href="#-star-history"></a></div>
<p><em>Community Growth Trajectory</em></p>
<div> <a href="https://star-history.com/#HKUDS/RAG-Anything&amp;Date"> <img alt="Star History Chart" src="https://camo.githubusercontent.com/ecd2e6786f34337407f35b748bc0513c51f343495659d885798edbdfddacb75d/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d484b5544532f5241472d416e797468696e6726747970653d44617465"> </a>
</div>
<hr>
<div><h2> Contribution</h2><a href="#-contribution"></a></div>
<p><em>Join the Innovation</em></p>
<div> We thank all our contributors for their valuable contributions.
</div>
<div> <a href="https://github.com/HKUDS/RAG-Anything/graphs/contributors"> <img src="https://camo.githubusercontent.com/1680c784f122bc349c4b6258a1bacea1304f3219e8ce71ee9d54388eb5f2238e/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d484b5544532f5241472d416e797468696e67"> </a>
</div>
<hr>
<div> <div> <a target="_blank" href="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif"><img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif"></a> </div> <div> <a href="https://github.com/HKUDS/RAG-Anything"> <img src="https://camo.githubusercontent.com/e56115416967f2f5db52d380cd13033047da3a5285d17200626095a624aa3076/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe2ad902532305374617225323075732532306f6e2532304769744875622d3161316132653f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465"> </a> <a href="https://github.com/HKUDS/RAG-Anything/issues"> <img src="https://camo.githubusercontent.com/908b8e2bc4777c58983e20ddea95fd0a128375b36e09c50aafdcb20a83b9eda6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f909b2532305265706f72742532304973737565732d6666366236623f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465"> </a> <a href="https://github.com/HKUDS/RAG-Anything/discussions"> <img src="https://camo.githubusercontent.com/d163dcc81671b8e3c87347309af9b05ef697fcac12df65b3d4e8d78ab5c7b6c1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac25323044697363757373696f6e732d3465636463343f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465"> </a> </div>
</div>
<pre><code> Thank you for visiting RAG-Anything! Building the Future of Multimodal AI</code></pre>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>