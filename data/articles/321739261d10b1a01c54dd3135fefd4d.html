<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>GitHub - HKUDS/RAG-Anything: "RAG-Anything: All-in-One RAG Framework"</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.8; color: #e2e8f0; max-width: 800px; margin: 40px auto; padding: 0 20px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.5em; }
  .metadata { color: #94a3b8; font-size: 0.9em; margin-bottom: 2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 1em; }
  img { max-width: 100%; height: auto; border-radius: 8px; }
  a { color: #00d9ff; }
  p { margin-bottom: 1em; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 15px; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 15px; border-radius: 6px; overflow-x: auto; }

  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }

</style>
</head>
<body>
  <h1>GitHub - HKUDS/RAG-Anything: "RAG-Anything: All-in-One RAG Framework"</h1>
  <div class="metadata">
    Source: GitHub Trending Python | Date: Invalid Date | Lang: EN |
    <a href="https://github.com/HKUDS/RAG-Anything" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div><article><div>
<p><a target="_blank" href="https://github.com/HKUDS/RAG-Anything/blob/main/assets/logo.png"><img src="https://github.com/HKUDS/RAG-Anything/raw/main/assets/logo.png" alt="RAG-Anything Logo" /></a>
</p>
<p></p><h2> RAG-Anything: All-in-One RAG Framework</h2><a href="#-rag-anything-all-in-one-rag-framework"></a><p></p>
<p><a href="https://trendshift.io/repositories/14959"><img src="https://camo.githubusercontent.com/b7d262712df3f0d8c5822a912b14f75d8879ed98333f5dccb8166f1d301a0946/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f3134393539" alt="HKUDS%2FRAG-Anything | Trendshift" /></a></p>
<p><a target="_blank" href="https://camo.githubusercontent.com/ee0ffc9f8a8b338b4342f708bc183aa91ef92fc20187fe9e9e583d038b52b32d/68747470733a2f2f726561646d652d747970696e672d7376672e6865726f6b756170702e636f6d3f666f6e743d4f72626974726f6e2673697a653d3234266475726174696f6e3d333030302670617573653d3130303026636f6c6f723d3030443946462663656e7465723d74727565267643656e7465723d747275652677696474683d363030266c696e65733d57656c636f6d652b746f2b5241472d416e797468696e673b4e6578742d47656e2b4d756c74696d6f64616c2b5241472b53797374656d3b506f77657265642b62792b416476616e6365642b41492b546563686e6f6c6f6779"><img src="https://camo.githubusercontent.com/ee0ffc9f8a8b338b4342f708bc183aa91ef92fc20187fe9e9e583d038b52b32d/68747470733a2f2f726561646d652d747970696e672d7376672e6865726f6b756170702e636f6d3f666f6e743d4f72626974726f6e2673697a653d3234266475726174696f6e3d333030302670617573653d3130303026636f6c6f723d3030443946462663656e7465723d74727565267643656e7465723d747275652677696474683d363030266c696e65733d57656c636f6d652b746f2b5241472d416e797468696e673b4e6578742d47656e2b4d756c74696d6f64616c2b5241472b53797374656d3b506f77657265642b62792b416476616e6365642b41492b546563686e6f6c6f6779" alt="Typing Animation" /></a>
</p>

</div>

<p><a href="#-quick-start">
    <img src="https://camo.githubusercontent.com/5dc13c91fe6858f8373b0aa709fc6505f1716ffc1c7e35050fcdccd6971376f1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f517569636b25323053746172742d476574253230537461727465642532304e6f772d3030643966663f7374796c653d666f722d7468652d6261646765266c6f676f3d726f636b6574266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265" />
  </a>
</p>
<hr />

<hr />
<p></p><h2> News</h2><a href="#-news"></a><p></p>
<ul>
<li> [2025.10]  We have released the technical report of <a href="http://arxiv.org/abs/2510.12323">RAG-Anything</a>. Access it now to explore our latest research findings.</li>
<li> [2025.08]  RAG-Anything now features <strong>VLM-Enhanced Query</strong> mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.</li>
<li> [2025.07] RAG-Anything now features a <a href="https://github.com/HKUDS/RAG-Anything/blob/main/docs/context_aware_processing.md">context configuration module</a>, enabling intelligent integration of relevant contextual information to enhance multimodal content processing.</li>
<li> [2025.07]  RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.</li>
<li> [2025.07]  RAG-Anything has reached 1k stars on GitHub! Thank you for your incredible support and valuable contributions to the project.</li>
</ul>
<hr />
<p></p><h2> System Overview</h2><a href="#-system-overview"></a><p></p>
<p><em>Next-Generation Multimodal Intelligence</em></p>
<div>
<p>Modern documents increasingly contain diverse multimodal content—text, images, tables, equations, charts, and multimedia—that traditional text-focused RAG systems cannot effectively process. <strong>RAG-Anything</strong> addresses this challenge as a comprehensive <strong>All-in-One Multimodal Document Processing RAG system</strong> built on <a href="https://github.com/HKUDS/LightRAG">LightRAG</a>.</p>
<p>As a unified solution, RAG-Anything <strong>eliminates the need for multiple specialized tools</strong>. It provides <strong>seamless processing and querying across all content modalities</strong> within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers <strong>comprehensive multimodal retrieval capabilities</strong>.</p>
<p>Users can query documents containing <strong>interleaved text</strong>, <strong>visual diagrams</strong>, <strong>structured tables</strong>, and <strong>mathematical formulations</strong> through <strong>one cohesive interface</strong>. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a <strong>unified processing framework</strong>.</p>
<p><a target="_blank" href="https://github.com/HKUDS/RAG-Anything/blob/main/assets/rag_anything_framework.png"><img src="https://github.com/HKUDS/RAG-Anything/raw/main/assets/rag_anything_framework.png" alt="RAG-Anything" /></a>
</p></div>
<p></p><h3> Key Features</h3><a href="#-key-features"></a><p></p>
<div>
<ul>
<li><strong> End-to-End Multimodal Pipeline</strong> - Complete workflow from document ingestion and parsing to intelligent multimodal query answering</li>
<li><strong> Universal Document Support</strong> - Seamless processing of PDFs, Office documents, images, and diverse file formats</li>
<li><strong> Specialized Content Analysis</strong> - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types</li>
<li><strong> Multimodal Knowledge Graph</strong> - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding</li>
<li><strong> Adaptive Processing Modes</strong> - Flexible MinerU-based parsing or direct multimodal content injection workflows</li>
<li><strong> Direct Content List Insertion</strong> - Bypass document parsing by directly inserting pre-parsed content lists from external sources</li>
<li><strong> Hybrid Intelligent Retrieval</strong> - Advanced search capabilities spanning textual and multimodal content with contextual understanding</li>
</ul>
</div>
<hr />
<p></p><h2> Algorithm &amp; Architecture</h2><a href="#-algorithm--architecture"></a><p></p>
<div>
<p></p><h3>Core Algorithm</h3><a href="#core-algorithm"></a><p></p>
<p><strong>RAG-Anything</strong> implements an effective <strong>multi-stage multimodal pipeline</strong> that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.</p>
</div>
<div>
      <div>
        <p></p>
        <p>Document Parsing</p>
      </div>
      <p>→</p>
      <div>
        <p></p>
        <p>Content Analysis</p>
      </div>
      <p>→</p>
      <div>
        <p></p>
        <p>Knowledge Graph</p>
      </div>
      <p>→</p>
      <div>
        <p></p>
        <p>Intelligent Retrieval</p>
      </div>
    </div>
<p></p><h3>1. Document Parsing Stage</h3><a href="#1-document-parsing-stage"></a><p></p>
<div>
<p>The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.</p>
<p><strong>Key Components:</strong></p>
<ul>
<li>
<p><strong> MinerU Integration</strong>: Leverages <a href="https://github.com/opendatalab/MinerU">MinerU</a> for high-fidelity document structure extraction and semantic preservation across complex layouts.</p>
</li>
<li>
<p><strong> Adaptive Content Decomposition</strong>: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships.</p>
</li>
<li>
<p><strong> Universal Format Support</strong>: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.</p>
</li>
</ul>
</div>
<p></p><h3>2. Multi-Modal Content Understanding &amp; Processing</h3><a href="#2-multi-modal-content-understanding--processing"></a><p></p>
<div>
<p>The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.</p>
<p><strong>Key Components:</strong></p>
<ul>
<li>
<p><strong> Autonomous Content Categorization and Routing</strong>: Automatically identify, categorize, and route different content types through optimized execution channels.</p>
</li>
<li>
<p><strong> Concurrent Multi-Pipeline Architecture</strong>: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity.</p>
</li>
<li>
<p><strong> Document Hierarchy Extraction</strong>: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.</p>
</li>
</ul>
</div>
<p></p><h3>3. Multimodal Analysis Engine</h3><a href="#3-multimodal-analysis-engine"></a><p></p>
<div>
<p>The system deploys modality-aware processing units for heterogeneous data modalities:</p>
<p><strong>Specialized Analyzers:</strong></p>
<ul>
<li>
<p><strong> Visual Content Analyzer</strong>:</p>
<ul>
<li>Integrate vision model for image analysis.</li>
<li>Generates context-aware descriptive captions based on visual semantics.</li>
<li>Extracts spatial relationships and hierarchical structures between visual elements.</li>
</ul>
</li>
<li>
<p><strong> Structured Data Interpreter</strong>:</p>
<ul>
<li>Performs systematic interpretation of tabular and structured data formats.</li>
<li>Implements statistical pattern recognition algorithms for data trend analysis.</li>
<li>Identifies semantic relationships and dependencies across multiple tabular datasets.</li>
</ul>
</li>
<li>
<p><strong> Mathematical Expression Parser</strong>:</p>
<ul>
<li>Parses complex mathematical expressions and formulas with high accuracy.</li>
<li>Provides native LaTeX format support for seamless integration with academic workflows.</li>
<li>Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases.</li>
</ul>
</li>
<li>
<p><strong> Extensible Modality Handler</strong>:</p>
<ul>
<li>Provides configurable processing framework for custom and emerging content types.</li>
<li>Enables dynamic integration of new modality processors through plugin architecture.</li>
<li>Supports runtime configuration of processing pipelines for specialized use cases.</li>
</ul>
</li>
</ul>
</div>
<p></p><h3>4. Multimodal Knowledge Graph Index</h3><a href="#4-multimodal-knowledge-graph-index"></a><p></p>
<div>
<p>The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.</p>
<p><strong>Core Functions:</strong></p>
<ul>
<li>
<p><strong> Multi-Modal Entity Extraction</strong>: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation.</p>
</li>
<li>
<p><strong> Cross-Modal Relationship Mapping</strong>: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms.</p>
</li>
<li>
<p><strong> Hierarchical Structure Preservation</strong>: Maintains original document organization through "belongs_to" relationship chains. These chains preserve logical content hierarchy and sectional dependencies.</p>
</li>
<li>
<p><strong> Weighted Relationship Scoring</strong>: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.</p>
</li>
</ul>
</div>
<p></p><h3>5. Modality-Aware Retrieval</h3><a href="#5-modality-aware-retrieval"></a><p></p>
<div>
<p>The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.</p>
<p><strong>Retrieval Mechanisms:</strong></p>
<ul>
<li>
<p><strong> Vector-Graph Fusion</strong>: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval.</p>
</li>
<li>
<p><strong> Modality-Aware Ranking</strong>: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences.</p>
</li>
<li>
<p><strong> Relational Coherence Maintenance</strong>: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.</p>
</li>
</ul>
</div>
<hr />
<p></p><h2> Quick Start</h2><a href="#-quick-start"></a><p></p>
<p><em>Initialize Your AI Journey</em></p>
<p><a target="_blank" href="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif"><img src="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif" /></a>
</p>
<p></p><h3>Installation</h3><a href="#installation"></a><p></p>
<p></p><h4>Option 1: Install from PyPI (Recommended)</h4><a href="#option-1-install-from-pypi-recommended"></a><p></p>
<div><pre><span><span>#</span> Basic installation</span>
pip install raganything

<span><span>#</span> With optional dependencies for extended format support:</span>
pip install <span><span>'</span>raganything[all]<span>'</span></span>              <span><span>#</span> All optional features</span>
pip install <span><span>'</span>raganything[image]<span>'</span></span>            <span><span>#</span> Image format conversion (BMP, TIFF, GIF, WebP)</span>
pip install <span><span>'</span>raganything[text]<span>'</span></span>             <span><span>#</span> Text file processing (TXT, MD)</span>
pip install <span><span>'</span>raganything[image,text]<span>'</span></span>       <span><span>#</span> Multiple features</span></pre></div>
<p></p><h4>Option 2: Install from Source</h4><a href="#option-2-install-from-source"></a><p></p>
<div><pre><span><span>#</span> Install uv (if not already installed)</span>
curl -LsSf https://astral.sh/uv/install.sh <span>|</span> sh

<span><span>#</span> Clone and setup the project with uv</span>
git clone https://github.com/HKUDS/RAG-Anything.git
<span>cd</span> RAG-Anything

<span><span>#</span> Install the package and dependencies in a virtual environment</span>
uv sync

<span><span>#</span> If you encounter network timeouts (especially for opencv packages):</span>
<span><span>#</span> UV_HTTP_TIMEOUT=120 uv sync</span>

<span><span>#</span> Run commands directly with uv (recommended approach)</span>
uv run python examples/raganything_example.py --help

<span><span>#</span> Install with optional dependencies</span>
uv sync --extra image --extra text  <span><span>#</span> Specific extras</span>
uv sync --all-extras                 <span><span>#</span> All optional features</span></pre></div>
<p></p><h4>Optional Dependencies</h4><a href="#optional-dependencies"></a><p></p>
<ul>
<li><strong><code>[image]</code></strong> - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)</li>
<li><strong><code>[text]</code></strong> - Enables processing of TXT and MD files (requires ReportLab)</li>
<li><strong><code>[all]</code></strong> - Includes all Python optional dependencies</li>
</ul>
<blockquote>
<p><strong> Office Document Processing Requirements:</strong></p>
<ul>
<li>Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require <strong>LibreOffice</strong> installation</li>
<li>Download from <a href="https://www.libreoffice.org/download/download/">LibreOffice official website</a></li>
<li><strong>Windows</strong>: Download installer from official website</li>
<li><strong>macOS</strong>: <code>brew install --cask libreoffice</code></li>
<li><strong>Ubuntu/Debian</strong>: <code>sudo apt-get install libreoffice</code></li>
<li><strong>CentOS/RHEL</strong>: <code>sudo yum install libreoffice</code></li>
</ul>
</blockquote>
<p><strong>Check MinerU installation:</strong></p>
<div><pre><span><span>#</span> Verify installation</span>
mineru --version

<span><span>#</span> Check if properly configured</span>
python -c <span><span>"</span>from raganything import RAGAnything; rag = RAGAnything(); print(' MinerU installed properly' if rag.check_parser_installation() else ' MinerU installation issue')<span>"</span></span></pre></div>
<p>Models are downloaded automatically on first use. For manual download, refer to <a href="https://github.com/opendatalab/MinerU/blob/master/README.md#22-model-source-configuration">MinerU Model Source Configuration</a>.</p>
<p></p><h3>Usage Examples</h3><a href="#usage-examples"></a><p></p>
<p></p><h4>1. End-to-End Document Processing</h4><a href="#1-end-to-end-document-processing"></a><p></p>
<div><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>raganything</span> <span>import</span> <span>RAGAnything</span>, <span>RAGAnythingConfig</span>
<span>from</span> <span>lightrag</span>.<span>llm</span>.<span>openai</span> <span>import</span> <span>openai_complete_if_cache</span>, <span>openai_embed</span>
<span>from</span> <span>lightrag</span>.<span>utils</span> <span>import</span> <span>EmbeddingFunc</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span># Set up API configuration</span>
    <span>api_key</span> <span>=</span> <span>"your-api-key"</span>
    <span>base_url</span> <span>=</span> <span>"your-base-url"</span>  <span># Optional</span>

    <span># Create RAGAnything configuration</span>
    <span>config</span> <span>=</span> <span>RAGAnythingConfig</span>(
        <span>working_dir</span><span>=</span><span>"./rag_storage"</span>,
        <span>parser</span><span>=</span><span>"mineru"</span>,  <span># Parser selection: mineru or docling</span>
        <span>parse_method</span><span>=</span><span>"auto"</span>,  <span># Parse method: auto, ocr, or txt</span>
        <span>enable_image_processing</span><span>=</span><span>True</span>,
        <span>enable_table_processing</span><span>=</span><span>True</span>,
        <span>enable_equation_processing</span><span>=</span><span>True</span>,
    )

    <span># Define LLM model function</span>
    <span>def</span> <span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>):
        <span>return</span> <span>openai_complete_if_cache</span>(
            <span>"gpt-4o-mini"</span>,
            <span>prompt</span>,
            <span>system_prompt</span><span>=</span><span>system_prompt</span>,
            <span>history_messages</span><span>=</span><span>history_messages</span>,
            <span>api_key</span><span>=</span><span>api_key</span>,
            <span>base_url</span><span>=</span><span>base_url</span>,
            <span>**</span><span>kwargs</span>,
        )

    <span># Define vision model function for image processing</span>
    <span>def</span> <span>vision_model_func</span>(
        <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>image_data</span><span>=</span><span>None</span>, <span>messages</span><span>=</span><span>None</span>, <span>**</span><span>kwargs</span>
    ):
        <span># If messages format is provided (for multimodal VLM enhanced query), use it directly</span>
        <span>if</span> <span>messages</span>:
            <span>return</span> <span>openai_complete_if_cache</span>(
                <span>"gpt-4o"</span>,
                <span>""</span>,
                <span>system_prompt</span><span>=</span><span>None</span>,
                <span>history_messages</span><span>=</span>[],
                <span>messages</span><span>=</span><span>messages</span>,
                <span>api_key</span><span>=</span><span>api_key</span>,
                <span>base_url</span><span>=</span><span>base_url</span>,
                <span>**</span><span>kwargs</span>,
            )
        <span># Traditional single image format</span>
        <span>elif</span> <span>image_data</span>:
            <span>return</span> <span>openai_complete_if_cache</span>(
                <span>"gpt-4o"</span>,
                <span>""</span>,
                <span>system_prompt</span><span>=</span><span>None</span>,
                <span>history_messages</span><span>=</span>[],
                <span>messages</span><span>=</span>[
                    {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>system_prompt</span>}
                    <span>if</span> <span>system_prompt</span>
                    <span>else</span> <span>None</span>,
                    {
                        <span>"role"</span>: <span>"user"</span>,
                        <span>"content"</span>: [
                            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>prompt</span>},
                            {
                                <span>"type"</span>: <span>"image_url"</span>,
                                <span>"image_url"</span>: {
                                    <span>"url"</span>: <span>f"data:image/jpeg;base64,<span><span>{</span><span>image_data</span><span>}</span></span>"</span>
                                },
                            },
                        ],
                    }
                    <span>if</span> <span>image_data</span>
                    <span>else</span> {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>},
                ],
                <span>api_key</span><span>=</span><span>api_key</span>,
                <span>base_url</span><span>=</span><span>base_url</span>,
                <span>**</span><span>kwargs</span>,
            )
        <span># Pure text format</span>
        <span>else</span>:
            <span>return</span> <span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span>, <span>history_messages</span>, <span>**</span><span>kwargs</span>)

    <span># Define embedding function</span>
    <span>embedding_func</span> <span>=</span> <span>EmbeddingFunc</span>(
        <span>embedding_dim</span><span>=</span><span>3072</span>,
        <span>max_token_size</span><span>=</span><span>8192</span>,
        <span>func</span><span>=</span><span>lambda</span> <span>texts</span>: <span>openai_embed</span>(
            <span>texts</span>,
            <span>model</span><span>=</span><span>"text-embedding-3-large"</span>,
            <span>api_key</span><span>=</span><span>api_key</span>,
            <span>base_url</span><span>=</span><span>base_url</span>,
        ),
    )

    <span># Initialize RAGAnything</span>
    <span>rag</span> <span>=</span> <span>RAGAnything</span>(
        <span>config</span><span>=</span><span>config</span>,
        <span>llm_model_func</span><span>=</span><span>llm_model_func</span>,
        <span>vision_model_func</span><span>=</span><span>vision_model_func</span>,
        <span>embedding_func</span><span>=</span><span>embedding_func</span>,
    )

    <span># Process a document</span>
    <span>await</span> <span>rag</span>.<span>process_document_complete</span>(
        <span>file_path</span><span>=</span><span>"path/to/your/document.pdf"</span>,
        <span>output_dir</span><span>=</span><span>"./output"</span>,
        <span>parse_method</span><span>=</span><span>"auto"</span>
    )

    <span># Query the processed content</span>
    <span># Pure text query - for basic knowledge base search</span>
    <span>text_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(
        <span>"What are the main findings shown in the figures and tables?"</span>,
        <span>mode</span><span>=</span><span>"hybrid"</span>
    )
    <span>print</span>(<span>"Text query result:"</span>, <span>text_result</span>)

    <span># Multimodal query with specific multimodal content</span>
    <span>multimodal_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery_with_multimodal</span>(
    <span>"Explain this formula and its relevance to the document content"</span>,
    <span>multimodal_content</span><span>=</span>[{
        <span>"type"</span>: <span>"equation"</span>,
        <span>"latex"</span>: <span>"P(d|q) = <span>\\</span>frac{P(q|d) <span>\\</span>cdot P(d)}{P(q)}"</span>,
        <span>"equation_caption"</span>: <span>"Document relevance probability"</span>
    }],
    <span>mode</span><span>=</span><span>"hybrid"</span>
)
    <span>print</span>(<span>"Multimodal query result:"</span>, <span>multimodal_result</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p></p><h4>2. Direct Multimodal Content Processing</h4><a href="#2-direct-multimodal-content-processing"></a><p></p>
<div><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>lightrag</span> <span>import</span> <span>LightRAG</span>
<span>from</span> <span>lightrag</span>.<span>llm</span>.<span>openai</span> <span>import</span> <span>openai_complete_if_cache</span>, <span>openai_embed</span>
<span>from</span> <span>lightrag</span>.<span>utils</span> <span>import</span> <span>EmbeddingFunc</span>
<span>from</span> <span>raganything</span>.<span>modalprocessors</span> <span>import</span> <span>ImageModalProcessor</span>, <span>TableModalProcessor</span>

<span>async</span> <span>def</span> <span>process_multimodal_content</span>():
    <span># Set up API configuration</span>
    <span>api_key</span> <span>=</span> <span>"your-api-key"</span>
    <span>base_url</span> <span>=</span> <span>"your-base-url"</span>  <span># Optional</span>

    <span># Initialize LightRAG</span>
    <span>rag</span> <span>=</span> <span>LightRAG</span>(
        <span>working_dir</span><span>=</span><span>"./rag_storage"</span>,
        <span>llm_model_func</span><span>=</span><span>lambda</span> <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>: <span>openai_complete_if_cache</span>(
            <span>"gpt-4o-mini"</span>,
            <span>prompt</span>,
            <span>system_prompt</span><span>=</span><span>system_prompt</span>,
            <span>history_messages</span><span>=</span><span>history_messages</span>,
            <span>api_key</span><span>=</span><span>api_key</span>,
            <span>base_url</span><span>=</span><span>base_url</span>,
            <span>**</span><span>kwargs</span>,
        ),
        <span>embedding_func</span><span>=</span><span>EmbeddingFunc</span>(
            <span>embedding_dim</span><span>=</span><span>3072</span>,
            <span>max_token_size</span><span>=</span><span>8192</span>,
            <span>func</span><span>=</span><span>lambda</span> <span>texts</span>: <span>openai_embed</span>(
                <span>texts</span>,
                <span>model</span><span>=</span><span>"text-embedding-3-large"</span>,
                <span>api_key</span><span>=</span><span>api_key</span>,
                <span>base_url</span><span>=</span><span>base_url</span>,
            ),
        )
    )
    <span>await</span> <span>rag</span>.<span>initialize_storages</span>()

    <span># Process an image</span>
    <span>image_processor</span> <span>=</span> <span>ImageModalProcessor</span>(
        <span>lightrag</span><span>=</span><span>rag</span>,
        <span>modal_caption_func</span><span>=</span><span>lambda</span> <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>image_data</span><span>=</span><span>None</span>, <span>**</span><span>kwargs</span>: <span>openai_complete_if_cache</span>(
            <span>"gpt-4o"</span>,
            <span>""</span>,
            <span>system_prompt</span><span>=</span><span>None</span>,
            <span>history_messages</span><span>=</span>[],
            <span>messages</span><span>=</span>[
                {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>system_prompt</span>} <span>if</span> <span>system_prompt</span> <span>else</span> <span>None</span>,
                {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: [
                    {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>prompt</span>},
                    {<span>"type"</span>: <span>"image_url"</span>, <span>"image_url"</span>: {<span>"url"</span>: <span>f"data:image/jpeg;base64,<span><span>{</span><span>image_data</span><span>}</span></span>"</span>}}
                ]} <span>if</span> <span>image_data</span> <span>else</span> {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>}
            ],
            <span>api_key</span><span>=</span><span>api_key</span>,
            <span>base_url</span><span>=</span><span>base_url</span>,
            <span>**</span><span>kwargs</span>,
        ) <span>if</span> <span>image_data</span> <span>else</span> <span>openai_complete_if_cache</span>(
            <span>"gpt-4o-mini"</span>,
            <span>prompt</span>,
            <span>system_prompt</span><span>=</span><span>system_prompt</span>,
            <span>history_messages</span><span>=</span><span>history_messages</span>,
            <span>api_key</span><span>=</span><span>api_key</span>,
            <span>base_url</span><span>=</span><span>base_url</span>,
            <span>**</span><span>kwargs</span>,
        )
    )

    <span>image_content</span> <span>=</span> {
        <span>"img_path"</span>: <span>"path/to/image.jpg"</span>,
        <span>"image_caption"</span>: [<span>"Figure 1: Experimental results"</span>],
        <span>"image_footnote"</span>: [<span>"Data collected in 2024"</span>]
    }

    <span>description</span>, <span>entity_info</span> <span>=</span> <span>await</span> <span>image_processor</span>.<span>process_multimodal_content</span>(
        <span>modal_content</span><span>=</span><span>image_content</span>,
        <span>content_type</span><span>=</span><span>"image"</span>,
        <span>file_path</span><span>=</span><span>"research_paper.pdf"</span>,
        <span>entity_name</span><span>=</span><span>"Experimental Results Figure"</span>
    )

    <span># Process a table</span>
    <span>table_processor</span> <span>=</span> <span>TableModalProcessor</span>(
        <span>lightrag</span><span>=</span><span>rag</span>,
        <span>modal_caption_func</span><span>=</span><span>lambda</span> <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>: <span>openai_complete_if_cache</span>(
            <span>"gpt-4o-mini"</span>,
            <span>prompt</span>,
            <span>system_prompt</span><span>=</span><span>system_prompt</span>,
            <span>history_messages</span><span>=</span><span>history_messages</span>,
            <span>api_key</span><span>=</span><span>api_key</span>,
            <span>base_url</span><span>=</span><span>base_url</span>,
            <span>**</span><span>kwargs</span>,
        )
    )

    <span>table_content</span> <span>=</span> {
        <span>"table_body"</span>: <span>"""</span>
<span>        | Method | Accuracy | F1-Score |</span>
<span>        |--------|----------|----------|</span>
<span>        | RAGAnything | 95.2% | 0.94 |</span>
<span>        | Baseline | 87.3% | 0.85 |</span>
<span>        """</span>,
        <span>"table_caption"</span>: [<span>"Performance Comparison"</span>],
        <span>"table_footnote"</span>: [<span>"Results on test dataset"</span>]
    }

    <span>description</span>, <span>entity_info</span> <span>=</span> <span>await</span> <span>table_processor</span>.<span>process_multimodal_content</span>(
        <span>modal_content</span><span>=</span><span>table_content</span>,
        <span>content_type</span><span>=</span><span>"table"</span>,
        <span>file_path</span><span>=</span><span>"research_paper.pdf"</span>,
        <span>entity_name</span><span>=</span><span>"Performance Results Table"</span>
    )

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>asyncio</span>.<span>run</span>(<span>process_multimodal_content</span>())</pre></div>
<p></p><h4>3. Batch Processing</h4><a href="#3-batch-processing"></a><p></p>
<div><pre><span># Process multiple documents</span>
<span>await</span> <span>rag</span>.<span>process_folder_complete</span>(
    <span>folder_path</span><span>=</span><span>"./documents"</span>,
    <span>output_dir</span><span>=</span><span>"./output"</span>,
    <span>file_extensions</span><span>=</span>[<span>".pdf"</span>, <span>".docx"</span>, <span>".pptx"</span>],
    <span>recursive</span><span>=</span><span>True</span>,
    <span>max_workers</span><span>=</span><span>4</span>
)</pre></div>
<p></p><h4>4. Custom Modal Processors</h4><a href="#4-custom-modal-processors"></a><p></p>
<div><pre><span>from</span> <span>raganything</span>.<span>modalprocessors</span> <span>import</span> <span>GenericModalProcessor</span>

<span>class</span> <span>CustomModalProcessor</span>(<span>GenericModalProcessor</span>):
    <span>async</span> <span>def</span> <span>process_multimodal_content</span>(<span>self</span>, <span>modal_content</span>, <span>content_type</span>, <span>file_path</span>, <span>entity_name</span>):
        <span># Your custom processing logic</span>
        <span>enhanced_description</span> <span>=</span> <span>await</span> <span>self</span>.<span>analyze_custom_content</span>(<span>modal_content</span>)
        <span>entity_info</span> <span>=</span> <span>self</span>.<span>create_custom_entity</span>(<span>enhanced_description</span>, <span>entity_name</span>)
        <span>return</span> <span>await</span> <span>self</span>.<span>_create_entity_and_chunk</span>(<span>enhanced_description</span>, <span>entity_info</span>, <span>file_path</span>)</pre></div>
<p></p><h4>5. Query Options</h4><a href="#5-query-options"></a><p></p>
<p>RAG-Anything provides three types of query methods:</p>
<p><strong>Pure Text Queries</strong> - Direct knowledge base search using LightRAG:</p>
<div><pre><span># Different query modes for text queries</span>
<span>text_result_hybrid</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"hybrid"</span>)
<span>text_result_local</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"local"</span>)
<span>text_result_global</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"global"</span>)
<span>text_result_naive</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"naive"</span>)

<span># Synchronous version</span>
<span>sync_text_result</span> <span>=</span> <span>rag</span>.<span>query</span>(<span>"Your question"</span>, <span>mode</span><span>=</span><span>"hybrid"</span>)</pre></div>
<p><strong>VLM Enhanced Queries</strong> - Automatically analyze images in retrieved context using VLM:</p>
<div><pre><span># VLM enhanced query (automatically enabled when vision_model_func is provided)</span>
<span>vlm_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(
    <span>"Analyze the charts and figures in the document"</span>,
    <span>mode</span><span>=</span><span>"hybrid"</span>
    <span># vlm_enhanced=True is automatically set when vision_model_func is available</span>
)

<span># Manually control VLM enhancement</span>
<span>vlm_enabled</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(
    <span>"What do the images show in this document?"</span>,
    <span>mode</span><span>=</span><span>"hybrid"</span>,
    <span>vlm_enhanced</span><span>=</span><span>True</span>  <span># Force enable VLM enhancement</span>
)

<span>vlm_disabled</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(
    <span>"What do the images show in this document?"</span>,
    <span>mode</span><span>=</span><span>"hybrid"</span>,
    <span>vlm_enhanced</span><span>=</span><span>False</span>  <span># Force disable VLM enhancement</span>
)

<span># When documents contain images, VLM can see and analyze them directly</span>
<span># The system will automatically:</span>
<span># 1. Retrieve relevant context containing image paths</span>
<span># 2. Load and encode images as base64</span>
<span># 3. Send both text context and images to VLM for comprehensive analysis</span></pre></div>
<p><strong>Multimodal Queries</strong> - Enhanced queries with specific multimodal content analysis:</p>
<div><pre><span># Query with table data</span>
<span>table_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery_with_multimodal</span>(
    <span>"Compare these performance metrics with the document content"</span>,
    <span>multimodal_content</span><span>=</span>[{
        <span>"type"</span>: <span>"table"</span>,
        <span>"table_data"</span>: <span>"""Method,Accuracy,Speed</span>
<span>                        RAGAnything,95.2%,120ms</span>
<span>                        Traditional,87.3%,180ms"""</span>,
        <span>"table_caption"</span>: <span>"Performance comparison"</span>
    }],
    <span>mode</span><span>=</span><span>"hybrid"</span>
)

<span># Query with equation content</span>
<span>equation_result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery_with_multimodal</span>(
    <span>"Explain this formula and its relevance to the document content"</span>,
    <span>multimodal_content</span><span>=</span>[{
        <span>"type"</span>: <span>"equation"</span>,
        <span>"latex"</span>: <span>"P(d|q) = <span>\\</span>frac{P(q|d) <span>\\</span>cdot P(d)}{P(q)}"</span>,
        <span>"equation_caption"</span>: <span>"Document relevance probability"</span>
    }],
    <span>mode</span><span>=</span><span>"hybrid"</span>
)</pre></div>
<p></p><h4>6. Loading Existing LightRAG Instance</h4><a href="#6-loading-existing-lightrag-instance"></a><p></p>
<div><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>raganything</span> <span>import</span> <span>RAGAnything</span>, <span>RAGAnythingConfig</span>
<span>from</span> <span>lightrag</span> <span>import</span> <span>LightRAG</span>
<span>from</span> <span>lightrag</span>.<span>llm</span>.<span>openai</span> <span>import</span> <span>openai_complete_if_cache</span>, <span>openai_embed</span>
<span>from</span> <span>lightrag</span>.<span>kg</span>.<span>shared_storage</span> <span>import</span> <span>initialize_pipeline_status</span>
<span>from</span> <span>lightrag</span>.<span>utils</span> <span>import</span> <span>EmbeddingFunc</span>
<span>import</span> <span>os</span>

<span>async</span> <span>def</span> <span>load_existing_lightrag</span>():
    <span># Set up API configuration</span>
    <span>api_key</span> <span>=</span> <span>"your-api-key"</span>
    <span>base_url</span> <span>=</span> <span>"your-base-url"</span>  <span># Optional</span>

    <span># First, create or load existing LightRAG instance</span>
    <span>lightrag_working_dir</span> <span>=</span> <span>"./existing_lightrag_storage"</span>

    <span># Check if previous LightRAG instance exists</span>
    <span>if</span> <span>os</span>.<span>path</span>.<span>exists</span>(<span>lightrag_working_dir</span>) <span>and</span> <span>os</span>.<span>listdir</span>(<span>lightrag_working_dir</span>):
        <span>print</span>(<span>" Found existing LightRAG instance, loading..."</span>)
    <span>else</span>:
        <span>print</span>(<span>" No existing LightRAG instance found, will create new one"</span>)

    <span># Create/load LightRAG instance with your configuration</span>
    <span>lightrag_instance</span> <span>=</span> <span>LightRAG</span>(
        <span>working_dir</span><span>=</span><span>lightrag_working_dir</span>,
        <span>llm_model_func</span><span>=</span><span>lambda</span> <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>: <span>openai_complete_if_cache</span>(
            <span>"gpt-4o-mini"</span>,
            <span>prompt</span>,
            <span>system_prompt</span><span>=</span><span>system_prompt</span>,
            <span>history_messages</span><span>=</span><span>history_messages</span>,
            <span>api_key</span><span>=</span><span>api_key</span>,
            <span>base_url</span><span>=</span><span>base_url</span>,
            <span>**</span><span>kwargs</span>,
        ),
        <span>embedding_func</span><span>=</span><span>EmbeddingFunc</span>(
            <span>embedding_dim</span><span>=</span><span>3072</span>,
            <span>max_token_size</span><span>=</span><span>8192</span>,
            <span>func</span><span>=</span><span>lambda</span> <span>texts</span>: <span>openai_embed</span>(
                <span>texts</span>,
                <span>model</span><span>=</span><span>"text-embedding-3-large"</span>,
                <span>api_key</span><span>=</span><span>api_key</span>,
                <span>base_url</span><span>=</span><span>base_url</span>,
            ),
        )
    )

    <span># Initialize storage (this will load existing data if available)</span>
    <span>await</span> <span>lightrag_instance</span>.<span>initialize_storages</span>()
    <span>await</span> <span>initialize_pipeline_status</span>()

    <span># Define vision model function for image processing</span>
    <span>def</span> <span>vision_model_func</span>(
        <span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>image_data</span><span>=</span><span>None</span>, <span>messages</span><span>=</span><span>None</span>, <span>**</span><span>kwargs</span>
    ):
        <span># If messages format is provided (for multimodal VLM enhanced query), use it directly</span>
        <span>if</span> <span>messages</span>:
            <span>return</span> <span>openai_complete_if_cache</span>(
                <span>"gpt-4o"</span>,
                <span>""</span>,
                <span>system_prompt</span><span>=</span><span>None</span>,
                <span>history_messages</span><span>=</span>[],
                <span>messages</span><span>=</span><span>messages</span>,
                <span>api_key</span><span>=</span><span>api_key</span>,
                <span>base_url</span><span>=</span><span>base_url</span>,
                <span>**</span><span>kwargs</span>,
            )
        <span># Traditional single image format</span>
        <span>elif</span> <span>image_data</span>:
            <span>return</span> <span>openai_complete_if_cache</span>(
                <span>"gpt-4o"</span>,
                <span>""</span>,
                <span>system_prompt</span><span>=</span><span>None</span>,
                <span>history_messages</span><span>=</span>[],
                <span>messages</span><span>=</span>[
                    {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>system_prompt</span>}
                    <span>if</span> <span>system_prompt</span>
                    <span>else</span> <span>None</span>,
                    {
                        <span>"role"</span>: <span>"user"</span>,
                        <span>"content"</span>: [
                            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>prompt</span>},
                            {
                                <span>"type"</span>: <span>"image_url"</span>,
                                <span>"image_url"</span>: {
                                    <span>"url"</span>: <span>f"data:image/jpeg;base64,<span><span>{</span><span>image_data</span><span>}</span></span>"</span>
                                },
                            },
                        ],
                    }
                    <span>if</span> <span>image_data</span>
                    <span>else</span> {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>},
                ],
                <span>api_key</span><span>=</span><span>api_key</span>,
                <span>base_url</span><span>=</span><span>base_url</span>,
                <span>**</span><span>kwargs</span>,
            )
        <span># Pure text format</span>
        <span>else</span>:
            <span>return</span> <span>lightrag_instance</span>.<span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span>, <span>history_messages</span>, <span>**</span><span>kwargs</span>)

    <span># Now use existing LightRAG instance to initialize RAGAnything</span>
    <span>rag</span> <span>=</span> <span>RAGAnything</span>(
        <span>lightrag</span><span>=</span><span>lightrag_instance</span>,  <span># Pass existing LightRAG instance</span>
        <span>vision_model_func</span><span>=</span><span>vision_model_func</span>,
        <span># Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance</span>
    )

    <span># Query existing knowledge base</span>
    <span>result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(
        <span>"What data has been processed in this LightRAG instance?"</span>,
        <span>mode</span><span>=</span><span>"hybrid"</span>
    )
    <span>print</span>(<span>"Query result:"</span>, <span>result</span>)

    <span># Add new multimodal document to existing LightRAG instance</span>
    <span>await</span> <span>rag</span>.<span>process_document_complete</span>(
        <span>file_path</span><span>=</span><span>"path/to/new/multimodal_document.pdf"</span>,
        <span>output_dir</span><span>=</span><span>"./output"</span>
    )

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>asyncio</span>.<span>run</span>(<span>load_existing_lightrag</span>())</pre></div>
<p></p><h4>7. Direct Content List Insertion</h4><a href="#7-direct-content-list-insertion"></a><p></p>
<p>For scenarios where you already have a pre-parsed content list (e.g., from external parsers or previous processing), you can directly insert it into RAGAnything without document parsing:</p>
<div><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>raganything</span> <span>import</span> <span>RAGAnything</span>, <span>RAGAnythingConfig</span>
<span>from</span> <span>lightrag</span>.<span>llm</span>.<span>openai</span> <span>import</span> <span>openai_complete_if_cache</span>, <span>openai_embed</span>
<span>from</span> <span>lightrag</span>.<span>utils</span> <span>import</span> <span>EmbeddingFunc</span>

<span>async</span> <span>def</span> <span>insert_content_list_example</span>():
    <span># Set up API configuration</span>
    <span>api_key</span> <span>=</span> <span>"your-api-key"</span>
    <span>base_url</span> <span>=</span> <span>"your-base-url"</span>  <span># Optional</span>

    <span># Create RAGAnything configuration</span>
    <span>config</span> <span>=</span> <span>RAGAnythingConfig</span>(
        <span>working_dir</span><span>=</span><span>"./rag_storage"</span>,
        <span>enable_image_processing</span><span>=</span><span>True</span>,
        <span>enable_table_processing</span><span>=</span><span>True</span>,
        <span>enable_equation_processing</span><span>=</span><span>True</span>,
    )

    <span># Define model functions</span>
    <span>def</span> <span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>**</span><span>kwargs</span>):
        <span>return</span> <span>openai_complete_if_cache</span>(
            <span>"gpt-4o-mini"</span>,
            <span>prompt</span>,
            <span>system_prompt</span><span>=</span><span>system_prompt</span>,
            <span>history_messages</span><span>=</span><span>history_messages</span>,
            <span>api_key</span><span>=</span><span>api_key</span>,
            <span>base_url</span><span>=</span><span>base_url</span>,
            <span>**</span><span>kwargs</span>,
        )

    <span>def</span> <span>vision_model_func</span>(<span>prompt</span>, <span>system_prompt</span><span>=</span><span>None</span>, <span>history_messages</span><span>=</span>[], <span>image_data</span><span>=</span><span>None</span>, <span>messages</span><span>=</span><span>None</span>, <span>**</span><span>kwargs</span>):
        <span># If messages format is provided (for multimodal VLM enhanced query), use it directly</span>
        <span>if</span> <span>messages</span>:
            <span>return</span> <span>openai_complete_if_cache</span>(
                <span>"gpt-4o"</span>,
                <span>""</span>,
                <span>system_prompt</span><span>=</span><span>None</span>,
                <span>history_messages</span><span>=</span>[],
                <span>messages</span><span>=</span><span>messages</span>,
                <span>api_key</span><span>=</span><span>api_key</span>,
                <span>base_url</span><span>=</span><span>base_url</span>,
                <span>**</span><span>kwargs</span>,
            )
        <span># Traditional single image format</span>
        <span>elif</span> <span>image_data</span>:
            <span>return</span> <span>openai_complete_if_cache</span>(
                <span>"gpt-4o"</span>,
                <span>""</span>,
                <span>system_prompt</span><span>=</span><span>None</span>,
                <span>history_messages</span><span>=</span>[],
                <span>messages</span><span>=</span>[
                    {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>system_prompt</span>} <span>if</span> <span>system_prompt</span> <span>else</span> <span>None</span>,
                    {
                        <span>"role"</span>: <span>"user"</span>,
                        <span>"content"</span>: [
                            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>prompt</span>},
                            {<span>"type"</span>: <span>"image_url"</span>, <span>"image_url"</span>: {<span>"url"</span>: <span>f"data:image/jpeg;base64,<span><span>{</span><span>image_data</span><span>}</span></span>"</span>}}
                        ],
                    } <span>if</span> <span>image_data</span> <span>else</span> {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>},
                ],
                <span>api_key</span><span>=</span><span>api_key</span>,
                <span>base_url</span><span>=</span><span>base_url</span>,
                <span>**</span><span>kwargs</span>,
            )
        <span># Pure text format</span>
        <span>else</span>:
            <span>return</span> <span>llm_model_func</span>(<span>prompt</span>, <span>system_prompt</span>, <span>history_messages</span>, <span>**</span><span>kwargs</span>)

    <span>embedding_func</span> <span>=</span> <span>EmbeddingFunc</span>(
        <span>embedding_dim</span><span>=</span><span>3072</span>,
        <span>max_token_size</span><span>=</span><span>8192</span>,
        <span>func</span><span>=</span><span>lambda</span> <span>texts</span>: <span>openai_embed</span>(
            <span>texts</span>,
            <span>model</span><span>=</span><span>"text-embedding-3-large"</span>,
            <span>api_key</span><span>=</span><span>api_key</span>,
            <span>base_url</span><span>=</span><span>base_url</span>,
        ),
    )

    <span># Initialize RAGAnything</span>
    <span>rag</span> <span>=</span> <span>RAGAnything</span>(
        <span>config</span><span>=</span><span>config</span>,
        <span>llm_model_func</span><span>=</span><span>llm_model_func</span>,
        <span>vision_model_func</span><span>=</span><span>vision_model_func</span>,
        <span>embedding_func</span><span>=</span><span>embedding_func</span>,
    )

    <span># Example: Pre-parsed content list from external source</span>
    <span>content_list</span> <span>=</span> [
        {
            <span>"type"</span>: <span>"text"</span>,
            <span>"text"</span>: <span>"This is the introduction section of our research paper."</span>,
            <span>"page_idx"</span>: <span>0</span>  <span># Page number where this content appears</span>
        },
        {
            <span>"type"</span>: <span>"image"</span>,
            <span>"img_path"</span>: <span>"/absolute/path/to/figure1.jpg"</span>,  <span># IMPORTANT: Use absolute path</span>
            <span>"image_caption"</span>: [<span>"Figure 1: System Architecture"</span>],
            <span>"image_footnote"</span>: [<span>"Source: Authors' original design"</span>],
            <span>"page_idx"</span>: <span>1</span>  <span># Page number where this image appears</span>
        },
        {
            <span>"type"</span>: <span>"table"</span>,
            <span>"table_body"</span>: <span>"| Method | Accuracy | F1-Score |<span>\n</span>|--------|----------|----------|<span>\n</span>| Ours | 95.2% | 0.94 |<span>\n</span>| Baseline | 87.3% | 0.85 |"</span>,
            <span>"table_caption"</span>: [<span>"Table 1: Performance Comparison"</span>],
            <span>"table_footnote"</span>: [<span>"Results on test dataset"</span>],
            <span>"page_idx"</span>: <span>2</span>  <span># Page number where this table appears</span>
        },
        {
            <span>"type"</span>: <span>"equation"</span>,
            <span>"latex"</span>: <span>"P(d|q) = <span>\\</span>frac{P(q|d) <span>\\</span>cdot P(d)}{P(q)}"</span>,
            <span>"text"</span>: <span>"Document relevance probability formula"</span>,
            <span>"page_idx"</span>: <span>3</span>  <span># Page number where this equation appears</span>
        },
        {
            <span>"type"</span>: <span>"text"</span>,
            <span>"text"</span>: <span>"In conclusion, our method demonstrates superior performance across all metrics."</span>,
            <span>"page_idx"</span>: <span>4</span>  <span># Page number where this content appears</span>
        }
    ]

    <span># Insert the content list directly</span>
    <span>await</span> <span>rag</span>.<span>insert_content_list</span>(
        <span>content_list</span><span>=</span><span>content_list</span>,
        <span>file_path</span><span>=</span><span>"research_paper.pdf"</span>,  <span># Reference file name for citation</span>
        <span>split_by_character</span><span>=</span><span>None</span>,         <span># Optional text splitting</span>
        <span>split_by_character_only</span><span>=</span><span>False</span>,   <span># Optional text splitting mode</span>
        <span>doc_id</span><span>=</span><span>None</span>,                     <span># Optional custom document ID (will be auto-generated if not provided)</span>
        <span>display_stats</span><span>=</span><span>True</span>               <span># Show content statistics</span>
    )

    <span># Query the inserted content</span>
    <span>result</span> <span>=</span> <span>await</span> <span>rag</span>.<span>aquery</span>(
        <span>"What are the key findings and performance metrics mentioned in the research?"</span>,
        <span>mode</span><span>=</span><span>"hybrid"</span>
    )
    <span>print</span>(<span>"Query result:"</span>, <span>result</span>)

    <span># You can also insert multiple content lists with different document IDs</span>
    <span>another_content_list</span> <span>=</span> [
        {
            <span>"type"</span>: <span>"text"</span>,
            <span>"text"</span>: <span>"This is content from another document."</span>,
            <span>"page_idx"</span>: <span>0</span>  <span># Page number where this content appears</span>
        },
        {
            <span>"type"</span>: <span>"table"</span>,
            <span>"table_body"</span>: <span>"| Feature | Value |<span>\n</span>|---------|-------|<span>\n</span>| Speed | Fast |<span>\n</span>| Accuracy | High |"</span>,
            <span>"table_caption"</span>: [<span>"Feature Comparison"</span>],
            <span>"page_idx"</span>: <span>1</span>  <span># Page number where this table appears</span>
        }
    ]

    <span>await</span> <span>rag</span>.<span>insert_content_list</span>(
        <span>content_list</span><span>=</span><span>another_content_list</span>,
        <span>file_path</span><span>=</span><span>"another_document.pdf"</span>,
        <span>doc_id</span><span>=</span><span>"custom-doc-id-123"</span>  <span># Custom document ID</span>
    )

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>asyncio</span>.<span>run</span>(<span>insert_content_list_example</span>())</pre></div>
<p><strong>Content List Format:</strong></p>
<p>The <code>content_list</code> should follow the standard format with each item being a dictionary containing:</p>
<ul>
<li><strong>Text content</strong>: <code>{"type": "text", "text": "content text", "page_idx": 0}</code></li>
<li><strong>Image content</strong>: <code>{"type": "image", "img_path": "/absolute/path/to/image.jpg", "image_caption": ["caption"], "image_footnote": ["note"], "page_idx": 1}</code></li>
<li><strong>Table content</strong>: <code>{"type": "table", "table_body": "markdown table", "table_caption": ["caption"], "table_footnote": ["note"], "page_idx": 2}</code></li>
<li><strong>Equation content</strong>: <code>{"type": "equation", "latex": "LaTeX formula", "text": "description", "page_idx": 3}</code></li>
<li><strong>Generic content</strong>: <code>{"type": "custom_type", "content": "any content", "page_idx": 4}</code></li>
</ul>
<p><strong>Important Notes:</strong></p>
<ul>
<li><strong><code>img_path</code></strong>: Must be an absolute path to the image file (e.g., <code>/home/user/images/chart.jpg</code> or <code>C:\Users\user\images\chart.jpg</code>)</li>
<li><strong><code>page_idx</code></strong>: Represents the page number where the content appears in the original document (0-based indexing)</li>
<li><strong>Content ordering</strong>: Items are processed in the order they appear in the list</li>
</ul>
<p>This method is particularly useful when:</p>
<ul>
<li>You have content from external parsers (non-MinerU/Docling)</li>
<li>You want to process programmatically generated content</li>
<li>You need to insert content from multiple sources into a single knowledge base</li>
<li>You have cached parsing results that you want to reuse</li>
</ul>
<hr />
<p></p><h2> Examples</h2><a href="#-examples"></a><p></p>
<p><em>Practical Implementation Demos</em></p>
<p><a target="_blank" href="https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif"><img src="https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif" /></a>
</p>
<p>The <code>examples/</code> directory contains comprehensive usage examples:</p>
<ul>
<li><strong><code>raganything_example.py</code></strong>: End-to-end document processing with MinerU</li>
<li><strong><code>modalprocessors_example.py</code></strong>: Direct multimodal content processing</li>
<li><strong><code>office_document_test.py</code></strong>: Office document parsing test with MinerU (no API key required)</li>
<li><strong><code>image_format_test.py</code></strong>: Image format parsing test with MinerU (no API key required)</li>
<li><strong><code>text_format_test.py</code></strong>: Text format parsing test with MinerU (no API key required)</li>
</ul>
<p><strong>Run examples:</strong></p>
<div><pre><span><span>#</span> End-to-end processing with parser selection</span>
python examples/raganything_example.py path/to/document.pdf --api-key YOUR_API_KEY --parser mineru

<span><span>#</span> Direct modal processing</span>
python examples/modalprocessors_example.py --api-key YOUR_API_KEY

<span><span>#</span> Office document parsing test (MinerU only)</span>
python examples/office_document_test.py --file path/to/document.docx

<span><span>#</span> Image format parsing test (MinerU only)</span>
python examples/image_format_test.py --file path/to/image.bmp

<span><span>#</span> Text format parsing test (MinerU only)</span>
python examples/text_format_test.py --file path/to/document.md

<span><span>#</span> Check LibreOffice installation</span>
python examples/office_document_test.py --check-libreoffice --file dummy

<span><span>#</span> Check PIL/Pillow installation</span>
python examples/image_format_test.py --check-pillow --file dummy

<span><span>#</span> Check ReportLab installation</span>
python examples/text_format_test.py --check-reportlab --file dummy</pre></div>
<hr />
<p></p><h2> Configuration</h2><a href="#-configuration"></a><p></p>
<p><em>System Optimization Parameters</em></p>
<p></p><h3>Environment Variables</h3><a href="#environment-variables"></a><p></p>
<p>Create a <code>.env</code> file (refer to <code>.env.example</code>):</p>
<div><pre>OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=your_base_url  <span><span>#</span> Optional</span>
OUTPUT_DIR=./output             <span><span>#</span> Default output directory for parsed documents</span>
PARSER=mineru                   <span><span>#</span> Parser selection: mineru or docling</span>
PARSE_METHOD=auto              <span><span>#</span> Parse method: auto, ocr, or txt</span></pre></div>
<p><strong>Note:</strong> For backward compatibility, legacy environment variable names are still supported:</p>
<ul>
<li><code>MINERU_PARSE_METHOD</code> is deprecated, please use <code>PARSE_METHOD</code></li>
</ul>
<blockquote>
<p><strong>Note</strong>: API keys are only required for full RAG processing with LLM integration. The parsing test files (<code>office_document_test.py</code> and <code>image_format_test.py</code>) only test parser functionality and do not require API keys.</p>
</blockquote>
<p></p><h3>Parser Configuration</h3><a href="#parser-configuration"></a><p></p>
<p>RAGAnything now supports multiple parsers, each with specific advantages:</p>
<p></p><h4>MinerU Parser</h4><a href="#mineru-parser"></a><p></p>
<ul>
<li>Supports PDF, images, Office documents, and more formats</li>
<li>Powerful OCR and table extraction capabilities</li>
<li>GPU acceleration support</li>
</ul>
<p></p><h4>Docling Parser</h4><a href="#docling-parser"></a><p></p>
<ul>
<li>Optimized for Office documents and HTML files</li>
<li>Better document structure preservation</li>
<li>Native support for multiple Office formats</li>
</ul>
<p></p><h3>MinerU Configuration</h3><a href="#mineru-configuration"></a><p></p>
<div><pre><span><span>#</span> MinerU 2.0 uses command-line parameters instead of config files</span>
<span><span>#</span> Check available options:</span>
mineru --help

<span><span>#</span> Common configurations:</span>
mineru -p input.pdf -o output_dir -m auto    <span><span>#</span> Automatic parsing mode</span>
mineru -p input.pdf -o output_dir -m ocr     <span><span>#</span> OCR-focused parsing</span>
mineru -p input.pdf -o output_dir -b pipeline --device cuda  <span><span>#</span> GPU acceleration</span></pre></div>
<p>You can also configure parsing through RAGAnything parameters:</p>
<div><pre><span># Basic parsing configuration with parser selection</span>
<span>await</span> <span>rag</span>.<span>process_document_complete</span>(
    <span>file_path</span><span>=</span><span>"document.pdf"</span>,
    <span>output_dir</span><span>=</span><span>"./output/"</span>,
    <span>parse_method</span><span>=</span><span>"auto"</span>,          <span># or "ocr", "txt"</span>
    <span>parser</span><span>=</span><span>"mineru"</span>               <span># Optional: "mineru" or "docling"</span>
)

<span># Advanced parsing configuration with special parameters</span>
<span>await</span> <span>rag</span>.<span>process_document_complete</span>(
    <span>file_path</span><span>=</span><span>"document.pdf"</span>,
    <span>output_dir</span><span>=</span><span>"./output/"</span>,
    <span>parse_method</span><span>=</span><span>"auto"</span>,          <span># Parsing method: "auto", "ocr", "txt"</span>
    <span>parser</span><span>=</span><span>"mineru"</span>,              <span># Parser selection: "mineru" or "docling"</span>

    <span># MinerU special parameters - all supported kwargs:</span>
    <span>lang</span><span>=</span><span>"ch"</span>,                   <span># Document language for OCR optimization (e.g., "ch", "en", "ja")</span>
    <span>device</span><span>=</span><span>"cuda:0"</span>,             <span># Inference device: "cpu", "cuda", "cuda:0", "npu", "mps"</span>
    <span>start_page</span><span>=</span><span>0</span>,                <span># Starting page number (0-based, for PDF)</span>
    <span>end_page</span><span>=</span><span>10</span>,                 <span># Ending page number (0-based, for PDF)</span>
    <span>formula</span><span>=</span><span>True</span>,                <span># Enable formula parsing</span>
    <span>table</span><span>=</span><span>True</span>,                  <span># Enable table parsing</span>
    <span>backend</span><span>=</span><span>"pipeline"</span>,          <span># Parsing backend: pipeline|hybrid-auto-engine|hybrid-http-client|vlm-auto-engine|vlm-http-client.</span>
    <span>source</span><span>=</span><span>"huggingface"</span>,        <span># Model source: "huggingface", "modelscope", "local"</span>
    <span># vlm_url="http://127.0.0.1:3000" # Service address when using backend=vlm-http-client</span>

    <span># Standard RAGAnything parameters</span>
    <span>display_stats</span><span>=</span><span>True</span>,          <span># Display content statistics</span>
    <span>split_by_character</span><span>=</span><span>None</span>,     <span># Optional character to split text by</span>
    <span>doc_id</span><span>=</span><span>None</span>                  <span># Optional document ID</span>
)</pre></div>
<blockquote>
<p><strong>Note</strong>: MinerU 2.0 no longer uses the <code>magic-pdf.json</code> configuration file. All settings are now passed as command-line parameters or function arguments. RAG-Anything now supports multiple document parsers - you can choose between MinerU and Docling based on your needs.</p>
</blockquote>
<p></p><h3>Processing Requirements</h3><a href="#processing-requirements"></a><p></p>
<p>Different content types require specific optional dependencies:</p>
<ul>
<li><strong>Office Documents</strong> (.doc, .docx, .ppt, .pptx, .xls, .xlsx): Install <a href="https://www.libreoffice.org/download/download/">LibreOffice</a></li>
<li><strong>Extended Image Formats</strong> (.bmp, .tiff, .gif, .webp): Install with <code>pip install raganything[image]</code></li>
<li><strong>Text Files</strong> (.txt, .md): Install with <code>pip install raganything[text]</code></li>
</ul>
<blockquote>
<p><strong> Quick Install</strong>: Use <code>pip install raganything[all]</code> to enable all format support (Python dependencies only - LibreOffice still needs separate installation)</p>
</blockquote>
<hr />
<p></p><h2> Supported Content Types</h2><a href="#-supported-content-types"></a><p></p>
<p></p><h3>Document Formats</h3><a href="#document-formats"></a><p></p>
<ul>
<li><strong>PDFs</strong> - Research papers, reports, presentations</li>
<li><strong>Office Documents</strong> - DOC, DOCX, PPT, PPTX, XLS, XLSX</li>
<li><strong>Images</strong> - JPG, PNG, BMP, TIFF, GIF, WebP</li>
<li><strong>Text Files</strong> - TXT, MD</li>
</ul>
<p></p><h3>Multimodal Elements</h3><a href="#multimodal-elements"></a><p></p>
<ul>
<li><strong>Images</strong> - Photographs, diagrams, charts, screenshots</li>
<li><strong>Tables</strong> - Data tables, comparison charts, statistical summaries</li>
<li><strong>Equations</strong> - Mathematical formulas in LaTeX format</li>
<li><strong>Generic Content</strong> - Custom content types via extensible processors</li>
</ul>
<p><em>For installation of format-specific dependencies, see the <a href="#-configuration">Configuration</a> section.</em></p>
<hr />
<p></p><h2> Citation</h2><a href="#-citation"></a><p></p>
<p><em>Academic Reference</em></p>
<div>
      <p></p>
    </div>
<p>If you find RAG-Anything useful in your research, please cite our paper:</p>
<div><pre><span>@misc</span>{<span>guo2025raganythingallinoneragframework</span>,
      <span>title</span>=<span><span>{</span>RAG-Anything: All-in-One RAG Framework<span>}</span></span>,
      <span>author</span>=<span><span>{</span>Zirui Guo and Xubin Ren and Lingrui Xu and Jiahao Zhang and Chao Huang<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2510.12323<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.AI<span>}</span></span>,
      <span>url</span>=<span><span>{</span>https://arxiv.org/abs/2510.12323<span>}</span></span>,
}</pre></div>
<hr />
<p></p><h2> Related Projects</h2><a href="#-related-projects"></a><p></p>
<p><em>Ecosystem &amp; Extensions</em></p>

<hr />
<p></p><h2> Star History</h2><a href="#-star-history"></a><p></p>
<p><em>Community Growth Trajectory</em></p>
<div>
  <a href="https://star-history.com/#HKUDS/RAG-Anything&amp;Date">
    
      
      
      <img alt="Star History Chart" src="https://camo.githubusercontent.com/ecd2e6786f34337407f35b748bc0513c51f343495659d885798edbdfddacb75d/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d484b5544532f5241472d416e797468696e6726747970653d44617465" />
    
  </a>
</div>
<hr />
<p></p><h2> Contribution</h2><a href="#-contribution"></a><p></p>
<p><em>Join the Innovation</em></p>
<p>
  We thank all our contributors for their valuable contributions.
</p>
<p><a href="https://github.com/HKUDS/RAG-Anything/graphs/contributors">
    <img src="https://camo.githubusercontent.com/1680c784f122bc349c4b6258a1bacea1304f3219e8ce71ee9d54388eb5f2238e/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d484b5544532f5241472d416e797468696e67" />
  </a>
</p>
<hr />
<div>
  <p><a target="_blank" href="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif"><img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" /></a>
  </p>
  <p><a href="https://github.com/HKUDS/RAG-Anything">
      <img src="https://camo.githubusercontent.com/e56115416967f2f5db52d380cd13033047da3a5285d17200626095a624aa3076/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe2ad902532305374617225323075732532306f6e2532304769744875622d3161316132653f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465" />
    </a>
    <a href="https://github.com/HKUDS/RAG-Anything/issues">
      <img src="https://camo.githubusercontent.com/908b8e2bc4777c58983e20ddea95fd0a128375b36e09c50aafdcb20a83b9eda6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f909b2532305265706f72742532304973737565732d6666366236623f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465" />
    </a>
    <a href="https://github.com/HKUDS/RAG-Anything/discussions">
      <img src="https://camo.githubusercontent.com/d163dcc81671b8e3c87347309af9b05ef697fcac12df65b3d4e8d78ab5c7b6c1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac25323044697363757373696f6e732d3465636463343f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465" />
    </a>
  </p>
</div>
<div>
    <p><span></span>
      <span>Thank you for visiting RAG-Anything!</span>
      <span></span>
    </p>
    <p>Building the Future of Multimodal AI</p>
  </div>
</article></div></div>
  </div>
</body>
</html>