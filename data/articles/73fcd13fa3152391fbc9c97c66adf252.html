<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="UTF-8">
<title>Raspberry Pi AI HAT+ 2 : vision par ordinateur en vidéo avec Hailo-10H (Partie 2)</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>Raspberry Pi AI HAT+ 2 : vision par ordinateur en vidéo avec Hailo-10H (Partie 2)</h1>
  <div class="metadata">
    Source: Framboise 314 | Date: 1/15/2026 8:20:09 AM | <a href="https://www.framboise314.fr/raspberry-pi-ai-hat-2-hailo-10h-video-partie-2/" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: FR
  </div>
  <div class="content">
    <div><div> <p>Dans cette seconde partie, le <strong>Raspberry Pi 5</strong> passe à l’action avec la vidéo temps réel accélérée par <strong>Hailo-10H</strong>. Détection de personnes, cadrage dynamique, pose squelette et reconnaissance des mains : on enchaîne les modèles concrets. L’objectif est d’évaluer les performances réelles, les limites, et les bons compromis en situation réelle. Ici, pas de cloud ni de GPU : uniquement du calcul embarqué, optimisé et mesurable.</p> <div><p>Au sommaire :</p><ul><li><a href="#Raspberry_Pi_AI_HAT_2_vision_par_ordinateur_en_video_avec_Hailo-10H"><span>1</span> Raspberry Pi AI HAT+ 2 : vision par ordinateur en vidéo avec Hailo-10H</a><ul><li><a href="#Comprendre_le_pipeline_video_Hailo_entree_inference_sortie"><span>1.1</span> Comprendre le pipeline vidéo Hailo : entrée → inférence → sortie</a></li><li><a href="#Choix_de_la_source_video_pour_les_demos"><span>1.2</span> Choix de la source vidéo pour les démos</a></li><li><a href="#Pourquoi_on_ne_peut_pas_encore_a_lancer_une_demo_video_depuis_hailo_model_zoo"><span>1.3</span> Pourquoi on ne peut pas (encore) à lancer une démo vidéo depuis hailo_model_zoo ?</a></li><li><a href="#Installer_les_exemples_video_Hailo_Raspberry_Pi_5_ceux_quon_va_utiliser_pour_Videosfoulemp4"><span>1.4</span> Installer les exemples vidéo Hailo “Raspberry Pi 5” (ceux qu’on va utiliser pour Videos/foule.mp4)</a></li><li><a href="#Detection_dobjets_sur_une_video_de_foule"><span>1.5</span> Détection d’objets sur une vidéo de foule</a><ul><li><a href="#Etape_1_Preparer_la_video_de_test"><span>1.5.1</span> Étape 1 : Préparer la vidéo de test</a></li></ul></li><li><a href="#Etape_2_Mettre_en_place_lenvironnement_des_exemples_Hailo_RPi5"><span>1.6</span> Étape 2 : Mettre en place l’environnement des exemples Hailo (RPi5)</a><ul><li><a href="#Se_placer_dans_le_depot"><span>1.6.1</span> Se placer dans le dépôt</a></li><li><a href="#Lancer_linstallation"><span>1.6.2</span> Lancer l’installation</a></li><li><a href="#Creation_de_larborescence_attendue_par_les_exemples"><span>1.6.3</span> Création de l’arborescence attendue par les exemples</a></li><li><a href="#Lien_vers_les_modeles_Hailo_hef"><span>1.6.4</span> Lien vers les modèles Hailo (.hef)</a></li><li><a href="#Lien_vers_les_bibliotheques_de_post-traitement_TAPPAS"><span>1.6.5</span> Lien vers les bibliothèques de post-traitement (TAPPAS)</a></li><li><a href="#Verification_de_lenvironnement"><span>1.6.6</span> Vérification de l’environnement</a></li><li><a href="#Activer_lenvironnement_de_travail"><span>1.6.7</span> Activer l’environnement de travail</a></li><li><a href="#Creation_du_fichier_de_configuration_env"><span>1.6.8</span> Création du fichier de configuration (.env)</a></li></ul></li><li><a href="#Etape_3_Verifier_les_modeles_Hailo_installes_localement_AI_HAT_2"><span>1.7</span> Étape 3 : Vérifier les modèles Hailo installés localement (AI HAT+ 2)</a><ul><li><a href="#Emplacement_des_modeles_Hailo"><span>1.7.1</span> Emplacement des modèles Hailo</a></li><li><a href="#Modele_utilise_pour_la_detection_dobjets"><span>1.7.2</span> Modèle utilisé pour la détection d’objets</a></li><li><a href="#Verification_rapide_optionnelle"><span>1.7.3</span> Vérification rapide (optionnelle)</a></li></ul></li><li><a href="#Etape_4_Lancer_une_premiere_detection_de_personnes_sur_la_video"><span>1.8</span> Étape 4 : Lancer une première détection de personnes sur la vidéo</a><ul><li><a href="#Activer_lenvironnement_Hailo"><span>1.8.1</span> Activer l’environnement Hailo</a></li><li><a href="#Lancer_la_detection_video"><span>1.8.2</span> Lancer la détection vidéo</a></li><li><a href="#Resultat_attendu"><span>1.8.3</span> Résultat attendu</a></li></ul></li><li><a href="#Detection_dobjets_en_direct_avec_une_camera_USB_sur_Raspberry_Pi_5_AI_HAT_2_Hailo-10H"><span>1.9</span> Détection d’objets en direct avec une caméra USB sur Raspberry Pi 5 + AI HAT+ 2 (Hailo-10H)</a><ul><li><a href="#Verifier_que_la_camera_USB_est_detectee"><span>1.9.1</span> Vérifier que la caméra USB est détectée</a></li><li><a href="#Tester_la_camera_sous_Raspberry_Pi_OS_sans_IA"><span>1.9.2</span> Tester la caméra sous Raspberry Pi OS (sans IA)</a></li><li><a href="#Lancer_la_detection_en_direct_camera_USB"><span>1.9.3</span> Lancer la détection en direct (caméra USB)</a></li></ul></li><li><a href="#Estimation_de_pose_squelette_en_temps_reel_etat_actuel_sur_Raspberry_Pi_5_AI_HAT_2"><span>1.10</span> Estimation de pose (squelette) en temps réel : état actuel sur Raspberry Pi 5 + AI HAT+ 2</a></li><li><a href="#Sources"><span>1.11</span> Sources</a></li></ul></li></ul></div> <p>On part donc d’une base saine : le Raspberry Pi 5 est opérationnel, la Raspberry Pi AI HAT+ 2 (Hailo-10H) est correctement reconnue, les pilotes sont installés et les outils Hailo fonctionnent. Bref, le matériel répond présent et la chaîne logicielle est en place.&nbsp;Dans la partie précédente, nous avons déjà validé un premier usage concret de l’accélérateur avec un LLM local : un texte a été traduit de l’anglais vers le français à l’aide de Qwen, ce qui a permis de vérifier que l’environnement était fonctionnel et que le Hailo-10H pouvait être exploité dans un cas réel.</p>
<p>Dans cette partie 2, on change complètement d’échelle et de nature de calcul. On passe du texte à la vidéo temps réel, avec tout ce que cela implique : flux continus, latence,&nbsp; performances, et enchaînement de modèles d’IA. L’objectif est simple : reprendre des types de démos déjà explorées avec le Hailo-8 (détection, pose, etc.), telles qu’elles étaient présentées dans l’article <a href="https://www.framboise314.fr/module-ai-hailo-8l-pour-le-raspberry-pi-5/" target="_blank">https://www.framboise314.fr/module-ai-hailo-8l-pour-le-raspberry-pi-5/</a> et observer ce que change réellement le Hailo-10H.</p>
<p>Les vidéos utilisées ne seront pas forcément les mêmes, mais ce n’est pas l’essentiel. Le but est avant tout de vérifier comment le Hailo-10H traite la vidéo, comment les pipelines sont organisés, et ce que cela permet en termes de fluidité, de complexité des modèles, et de nouvelles possibilités : détection de personnes, pose squelette, mains, et gestion des ROI.</p>
<h2><span>Comprendre le pipeline vidéo Hailo : entrée → inférence → sortie</span></h2>
<p>Avant de lancer la moindre démo, il est utile de comprendre <strong>comment “passe” une vidéo dans une chaîne Hailo</strong>. Le principe est toujours le même : on prend un flux (caméra ou fichier), on le prépare (décodage / redimensionnement), on envoie les images au moteur d’inférence sur l’AI HAT+ 2 (Hailo-10H), puis on récupère les résultats pour les afficher (boîtes, squelette, mains…) ou les exploiter autrement.</p>
<p>Sur Raspberry Pi, ces démos s’appuient très souvent sur un <strong>pipeline GStreamer</strong> : c’est lui qui “colle” les briques entre elles en temps réel, sans que vous ayez à réinventer un lecteur vidéo, un convertisseur d’images et un afficheur.</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p></div> </td> <td><div><p><span>[</span><span> </span><span>Entr</span>é<span>e</span><span> </span><span>vid</span>é<span>o</span><span> </span><span>]</span></p><p><span>&nbsp;&nbsp; </span>├─<span> </span><span>Cam</span>é<span>ra</span><span> </span><span>(</span><span>libcamera</span><span> </span><span>/</span><span> </span><span>v4l2</span><span>)</span></p><p><span>&nbsp;&nbsp; </span>└─<span> </span><span>Fichier</span><span> </span><span>(</span><span>mp4</span><span>,</span><span> </span><span>mkv</span>…<span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>│</p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>▼</p><p><span>[</span><span> </span><span>D</span>é<span>codage</span><span> </span><span>+</span><span> </span><span>pr</span>é<span>traitement</span><span> </span><span>]</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>d</span>é<span>codage</span><span> </span><span>(</span><span>si </span><span>fichier</span><span>)</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>conversion </span><span>de </span><span>format</span><span> </span><span>(</span><span>YUV</span><span>/</span><span>RGB</span><span>)</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>redimensionnement</span><span> </span><span>/</span><span> </span><span>recadrage</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>normalisation</span><span> </span><span>(</span><span>selon </span><span>le </span><span>mod</span>è<span>le</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>│</p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>▼</p><p><span>[</span><span> </span><span>Inf</span>é<span>rence </span><span>sur </span><span>Hailo</span><span>-</span><span>10H</span><span> </span><span>]</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>ex</span>é<span>cution </span><span>du </span><span>mod</span>è<span>le</span><span> </span><span>(</span><span>d</span>é<span>tection</span><span> </span><span>/</span><span> </span><span>pose</span><span> </span><span>/</span><span> </span><span>mains</span><span>)</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>latence</span><span> </span><span>=</span><span> </span><span>temps </span><span>par </span><span>frame</span><span> </span><span>(</span><span>objectif</span><span> </span><span>:</span><span> </span><span>rester</span><span> </span><span>"temps réel"</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>│</p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>▼</p><p><span>[</span><span> </span><span>Post</span><span>-</span><span>traitement </span><span>CPU</span><span> </span><span>]</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>NMS</span><span> </span><span>/</span><span> </span><span>filtrage</span><span> </span><span>/</span><span> </span><span>seuils</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>assemblage </span><span>squelette</span><span> </span><span>/</span><span> </span><span>keypoints</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>ROI</span><span> </span><span>(</span><span>recadrage </span><span>intelligent </span><span>pour </span><span>un</span><span> </span><span>2e</span><span> </span><span>mod</span>è<span>le</span><span> </span><span>:</span><span> </span><span>mains</span><span>,</span><span> </span><span>visage</span>…<span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>│</p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>▼</p><p><span>[</span><span> </span><span>Sortie</span><span> </span><span>]</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>overlay</span><span> </span><span>(</span><span>bo</span>î<span>tes</span><span>,</span><span> </span><span>labels</span><span>,</span><span> </span><span>squelettes</span><span>)</span><span> </span><span>+</span><span> </span><span>affichage</span><span> </span>é<span>cran</span></p><p><span>&nbsp;&nbsp; </span><span>-</span><span> </span><span>ou </span><span>enregistrement</span><span> </span><span>/</span><span> </span><span>streaming</span><span> </span><span>(</span><span>selon</span><span> </span><span>d</span>é<span>mo</span><span>)</span></p></div></td> </tr> </tbody></table> </div> <p>Le point important : <strong>Hailo accélère l’inférence</strong> (le “gros calcul” du réseau neuronal), mais il reste toujours un peu de travail côté CPU pour <strong>préparer</strong> les images et <strong>exploiter</strong> les résultats. C’est précisément là que l’on va regarder ce qu’apporte le Hailo-10H : <strong>fluidité</strong>, <strong>latence</strong>, et possibilité d’enchaîner des modèles (détection → ROI → mains, par exemple) sans que tout s’écroule.</p>
<h2><span>Choix de la source vidéo pour les démos</span></h2>
<p>Pour cette série de tests, l’objectif n’est pas de comparer des vidéos “pixel à pixel” avec celles utilisées dans l’article sur le Hailo-8, mais de <strong>valider le comportement du Hailo-10H face à un flux vidéo</strong>, quelle qu’en soit la provenance.</p>
<p>Les démos Hailo acceptent plusieurs types de sources :</p>
<ul>
<li><strong>Caméra USB</strong> (UVC) : simple à mettre en œuvre, idéale pour tester le temps réel.</li>
<li><strong>Caméra Raspberry Pi (CSI)</strong> : intéressante pour mesurer la latence et l’intégration libcamera.</li>
<li><strong>Fichier vidéo</strong> (MP4, MKV…) : pratique pour des tests reproductibles, sans dépendre de l’éclairage ou du mouvement.</li>
</ul>
<p>Dans la pratique, les pipelines Hailo sont conçus pour être <strong>indépendants de la source</strong> : que l’image provienne d’une caméra ou d’un fichier, le traitement IA reste identique. C’est un point important, car cela permet de valider les modèles et les performances avant de passer à un usage réellement embarqué.</p>
<p>Pour la suite des tests, nous utiliserons principalement des <strong>vidéos de test</strong>, puis une caméra, afin d’observer le comportement du Hailo-10H en lecture continue et en conditions proches du temps réel.</p>
<h2><span>Pourquoi on ne peut pas (encore) à lancer une démo vidéo depuis <code>hailo_model_zoo</code> ?</span></h2>
<p>On a bien une base saine (Pi 5 + AI HAT+ 2 / Hailo-10H OK), mais il faut distinguer deux mondes&nbsp;:</p>
<ul>
<li><strong><code>hailo_model_zoo</code></strong> : c’est surtout un dépôt “modèles + outils” (configs, post-processing, utilitaires). Il peut fournir des modèles (ONNX/TF) et des <em>HEF</em> (binaire exécutable Hailo), mais ce n’est pas le dépôt “démos vidéo prêtes à lancer” sur Raspberry Pi.</li>
<li><strong>Les démos vidéo Raspberry Pi</strong> : elles sont regroupées dans le dépôt&nbsp;<strong><code>hailo-rpi5-examples</code></strong>, avec des pipelines orientés flux (GStreamer) et des scripts Python comme <code>basic_pipelines/detection.py</code>.</li>
</ul>
<p>Dans notre cas, la vérification est simple : dans <code>~/hailo_model_zoo</code> on ne trouve pas de script de démo type&nbsp;<code>basic_pipelines/detection.py</code>, et la commande <code>hailomz</code> n’existe pas non plus (donc le paquet n’a pas été installé). Bref : <strong>le Model Zoo est présent sur le disque, mais les exemples “vidéo” ne sont pas installés</strong>.</p>
<h2><span>Installer les exemples vidéo Hailo “Raspberry Pi 5” (ceux qu’on va utiliser pour <code>Videos/foule.mp4</code>)</span></h2>
<p>Par “<em><strong>exemples vidéo Hailo pour Raspberry Pi 5</strong></em>”, il faut entendre des <strong>programmes de démonstration</strong> capables de lire un flux vidéo (caméra ou fichier MP4), d’exécuter l’inférence sur le Hailo-10H et d’afficher le résultat à l’écran. Il ne s’agit pas de vidéos fournies par Hailo, mais bien d’outils logiciels auxquels on envoie <strong>nos propres fichiers vidéo</strong> en entrée.</p>
<p>Hailo indique d’ailleurs que, pour le Raspberry Pi 5, il faut passer par le dépôt <strong>Hailo Raspberry Pi 5 Examples </strong>(plutôt que TAPPAS directement). :contentReference[oaicite:1]{index=1}</p>
<ol>
<li><strong>Cloner le dépôt des exemples</strong> :<br> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p></div> </td> <td><div><p><span>cd</span><span> </span><span>~</span></p><p><span>git </span><span>clone</span><span> </span><span>https</span><span>:</span><span>//github.com/hailo-ai/hailo-rpi5-examples.git</span></p></div></td> </tr> </tbody></table> </div> <br>
<img src="https://www.framboise314.fr/wp-content/uploads/2025/12/pi5_HAT2_30.jpg" alt=""></li>
<li><strong>Vérifier qu’on a bien la démo “detection vidéo”</strong> (zéro supposition, on contrôle) :<br> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>ls</span><span> </span><span>-</span><span>la</span><span> </span><span>~</span><span>/</span><span>hailo</span><span>-</span><span>rpi5</span><span>-</span><span>examples</span><span>/</span><span>basic_pipelines</span></p></div></td> </tr> </tbody></table> </div> <br>
Le fichier attendu est <code>basic_pipelines/detection.py</code>.<br>
<img src="https://www.framboise314.fr/wp-content/uploads/2025/12/pi5_HAT2_31.jpg" alt="">Les programmes sont bien installés.</li>
<li><strong>Repérer le mode d’exécution prévu par le dépôt</strong> (script d’environnement, Docker, etc.) :<br> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>ls</span><span> </span><span>-</span><span>la</span><span> </span><span>~</span><span>/</span><span>hailo</span><span>-</span><span>rpi5</span><span>-</span><span>examples</span></p></div></td> </tr> </tbody></table> </div> <br>
&nbsp;</li>
</ol>
<p><img src="https://www.framboise314.fr/wp-content/uploads/2025/12/pi5_HAT2_32.jpg" alt=""></p>
<p>Cette étape permet de vérifier que tous les éléments nécessaires sont bien en place : les programmes de démonstration, la documentation associée et les scripts fournis par Hailo. On peut ainsi poursuivre les tests dans de bonnes conditions.</p>
<p>L’idée ici est d’identifier <em>ce que le dépôt fournit réellement</em> (script de setup, doc d’install),<br>
avant de lancer la moindre commande “magique”.</p>
<p>Une fois ces exemples en place, on pourra attaquer la suite (la première exécution sur <code>~/Videos/foule.mp4</code>)<br>
avec le bon script, dans le bon environnement — et mesurer la fluidité (FPS/latence) en scène chargée.</p>
<h2><span>Détection d’objets sur une vidéo de foule</span></h2>
<p>On commence volontairement par un test simple et parlant : une <strong>vidéo de foule</strong>. L’objectif n’est pas encore de faire de la pose ou du comptage, mais de vérifier deux points essentiels :</p>
<ul>
<li>le <strong>Hailo-10H est-il capable de traiter correctement un fichier vidéo</strong>, et pas uniquement un flux caméra&nbsp;?</li>
<li>le traitement reste-t-il <strong>fluide</strong> (FPS, latence) lorsque la scène devient dense&nbsp;?</li>
</ul>
<h3><span>Étape 1 : Préparer la vidéo de test</span></h3>
<p>Copiez votre fichier vidéo (au format MP4 de préférence) dans le dossier <code>Videos</code> de votre Raspberry Pi. Par exemple&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>~</span><span>/</span><span>Videos</span><span>/</span><span>foule</span><span>.</span><span>mp4</span></p></div></td> </tr> </tbody></table> </div> <p>La vidéo doit être <strong>encodée en H.264</strong>, sans piste audio (ou avec une piste ignorée). Le contenu exact de la vidéo importe peu à ce stade : l’important est qu’elle contienne plusieurs personnes en mouvement afin de solliciter réellement le pipeline.</p>
<h2><span>Étape 2 : Mettre en place l’environnement des exemples Hailo (RPi5)</span></h2>
<p>Pour les tests vidéo (détection, pose, segmentation…), nous n’allons pas utiliser le <em>Model Zoo</em> directement. Les démos “prêtes à lancer” sont fournies dans le dépôt <strong>hailo-rpi5-examples</strong>, qui s’appuie sur un <strong>virtualenv </strong>afin d’isoler les dépendances Python du système.<br>
Cette étape se fait <strong>une seule fois</strong> : on installe, puis on vérifie que l’environnement est bien opérationnel.</p>
<h3><span>Se placer dans le dépôt</span></h3> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>cd</span><span> </span><span>~</span><span>/</span><span>hailo</span><span>-</span><span>rpi5</span><span>-</span><span>examples</span></p></div></td> </tr> </tbody></table> </div> <h3><span>Lancer l’installation</span></h3> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>bash</span><span> </span><span>.</span><span>/</span><span>install</span><span>.</span><span>sh</span></p></div></td> </tr> </tbody></table> </div> <p>Cette étape est relativement longue : plusieurs dépendances Python et composants Hailo sont installés. Vous pouvez aller vous faire un café . À la fin, le script doit avoir créé (ou réutilisé) le virtualenv <code>venv_hailo_rpi_examples</code>.</p>
<h3><span>Création de l’arborescence attendue par les exemples</span></h3>
<p>Les scripts des pipelines Hailo s’attendent à trouver les modèles et certaines bibliothèques dans le répertoire <code>/usr/local/hailo</code>. Ce répertoire n’existe pas par défaut sur Raspberry Pi OS : il faut donc le créer.</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p></div> </td> <td><div><p><span>sudo </span><span>mkdir</span><span> </span><span>-</span><span>p</span><span> </span><span>/</span><span>usr</span><span>/</span><span>local</span><span>/</span><span>hailo</span><span>/</span><span>resources</span></p></div></td> </tr> </tbody></table> </div> <h3><span>Lien vers les modèles Hailo (.hef)</span></h3>
<p>Les modèles Hailo sont installés par le paquet <code>hailo-models</code> dans&nbsp;<code>/usr/share/hailo-models</code>, alors que les exemples les recherchent dans&nbsp;<code>/usr/local/hailo/resources/models</code>.<br>
On crée donc un lien symbolique pour rendre les modèles visibles au bon endroit.</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p></div> </td> <td><div><p><span>sudo </span><span>ln</span><span> </span><span>-</span><span>s</span><span> </span><span>/</span><span>usr</span><span>/</span><span>share</span><span>/</span><span>hailo</span><span>-</span><span>models</span><span> </span><span>/</span><span>usr</span><span>/</span><span>local</span><span>/</span><span>hailo</span><span>/</span><span>resources</span><span>/</span><span>models</span></p></div></td> </tr> </tbody></table> </div> <h3><span>Lien vers les bibliothèques de post-traitement (TAPPAS)</span></h3>
<p>Les bibliothèques de post-traitement Hailo (YOLO, cropper, overlay, etc.) sont fournies par&nbsp;<code>hailo-tappas-core</code> et installées dans&nbsp;<code>/usr/lib/aarch64-linux-gnu/hailo/tappas/post_processes</code>.<br>
Là encore, les exemples les attendent dans&nbsp;<code>/usr/local/hailo/resources/so</code>. On crée donc le lien symbolique correspondant.</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p><p>3</p></div> </td> <td><div><p><span>sudo </span><span>ln</span><span> </span><span>-</span><span>s</span><span> </span><span>/</span><span>usr</span><span>/</span><span>lib</span><span>/</span><span>aarch64</span><span>-</span><span>linux</span><span>-</span><span>gnu</span><span>/</span><span>hailo</span><span>/</span><span>tappas</span><span>/</span><span>post</span><span>_</span>processes<span> </span><span>\</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>/</span><span>usr</span><span>/</span><span>local</span><span>/</span><span>hailo</span><span>/</span><span>resources</span><span>/</span><span>so</span></p></div></td> </tr> </tbody></table> </div> <h3><span>Vérification de l’environnement</span></h3>
<p>À ce stade :</p>
<ul>
<li>l’environnement Python est actif,</li>
<li>les modèles Hailo sont accessibles,</li>
<li>les bibliothèques de post-traitement sont correctement référencées.</li>
</ul>
<p>Les scripts du dossier <code>basic_pipelines/</code> sont maintenant prêts à être utilisés<br>
pour analyser une vidéo locale avec l’accélérateur Hailo.</p>
<h3><span>Activer l’environnement de travail</span></h3> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p></div> </td> <td><div><p><span>source </span><span>setup_env</span><span>.</span><span>sh</span></p></div></td> </tr> </tbody></table> </div> <p><img src="https://www.framboise314.fr/wp-content/uploads/2025/12/pi5_HAT2_33.jpg" alt=""></p>
<p>Le prompt doit maintenant indiquer que l’environnement&nbsp;<code>venv_hailo_rpi_examples</code> est actif. Il faudra réactiver cet environnement à chaque fois que vous lancerez le terminal.</p>
<h3><span>Création du fichier de configuration (.env)</span></h3>
<p>Sur Raspberry Pi 5 avec AI HAT+ 2, le script d’installation ne remplit pas automatiquement le fichier <code>.env</code>. Il est donc nécessaire de le créer manuellement.</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p></div> </td> <td><div><p><span>nano</span><span> </span><span>.</span><span>env</span></p></div></td> </tr> </tbody></table> </div> <p>Y coller exactement le contenu suivant :</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p></div> </td> <td><div><p><span>host_arch</span><span>=</span><span>rpi</span></p><p><span>hailo_arch</span><span>=</span><span>hailo10h</span></p><p><span>hailort_version</span><span>=</span><span>5.1.1</span></p><p><span>tappas_version</span><span>=</span><span>5.1.0</span></p><p><span>server_url</span><span>=</span><span>http</span><span>:</span><span>//dev-public.hailo.ai/2025_01</span></p><p><span>resources_path</span><span>=</span><span>resources</span></p><p><span>virtual_env_name</span><span>=</span><span>venv_hailo_rpi_examples</span></p><p><span>tappas_variant</span><span>=</span><span>hailo</span><span>-</span><span>tappas</span><span>-</span><span>core</span></p><p><span>tappas_postproc_path</span><span>=</span><span>/</span><span>usr</span><span>/</span><span>lib</span><span>/</span><span>aarch64</span><span>-</span><span>linux</span><span>-</span><span>gnu</span><span>/</span><span>hailo</span><span>/</span><span>tappas</span><span>/</span><span>post</span><span>_</span>processes</p></div></td> </tr> </tbody></table> </div> <p>Enregistrer et quitter (<code>Ctrl+O</code>, <code>Entrée</code>, <code>Ctrl+X</code>).<br>
<span><em><strong>Nota :</strong> Certaines versions de la documentation Hailo mentionnent un serveur de téléchargement . Sur Raspberry Pi 5 avec AI HAT+ 2, ce téléchargement n’est pas requis, car les modèles et composants sont fournis par les paquets système</em>.</span></p>
<h2><span>Étape 3 : Vérifier les modèles Hailo installés localement (AI HAT+ 2)</span></h2>
<p>Sur Raspberry Pi 5 équipé de l’AI HAT+ 2 (Hailo-10H), les modèles d’inférence ne sont pas téléchargés depuis Internet lors de l’installation des exemples. Ils sont fournis directement par les paquets Debian officiels Hailo installés sur le système.<br>
Avant de lancer une démo de détection, il est donc indispensable de vérifier que les fichiers&nbsp;<code>.hef</code> (modèles compilés pour le NPU Hailo) sont bien présents localement.</p>
<h3><span>Emplacement des modèles Hailo</span></h3>
<p>Les modèles sont installés automatiquement dans le dossier système suivant&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>/</span><span>usr</span><span>/</span><span>share</span><span>/</span><span>hailo</span><span>-</span><span>models</span></p></div></td> </tr> </tbody></table> </div> <p>Depuis un terminal, listez le contenu de ce dossier&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>ls</span><span> </span><span>/</span><span>usr</span><span>/</span><span>share</span><span>/</span><span>hailo</span><span>-</span><span>models</span></p></div></td> </tr> </tbody></table> </div> <p>Vous devez obtenir une liste de fichiers <code>.hef</code>, par exemple&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p></div> </td> <td><div><p><span>yolov8m_h10</span><span>.</span><span>hef</span></p><p><span>yolov8s_h8</span><span>.</span><span>hef</span></p><p><span>yolov8m_pose_h10</span><span>.</span><span>hef</span></p><p><span>resnet_v1_50_h10</span><span>.</span><span>hef</span></p><p>…</p></div></td> </tr> </tbody></table> </div> <p>La présence de fichiers contenant <code>_h10</code> confirme que les modèles sont bien adaptés au NPU Hailo-10H de l’AI HAT+ 2.</p>
<h3><span>Modèle utilisé pour la détection d’objets</span></h3>
<p>Les scripts de détection fournis dans le dossier <code>basic_pipelines/</code> s’appuient sur des modèles YOLO optimisés pour Hailo. Pour l’AI HAT+ 2, la démo de détection de personnes utilise le modèle suivant&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>/</span><span>usr</span><span>/</span><span>share</span><span>/</span><span>hailo</span><span>-</span><span>models</span><span>/</span><span>yolov8m_h10</span><span>.</span><span>hef</span></p></div></td> </tr> </tbody></table> </div> <p>Aucune copie manuelle ni téléchargement supplémentaire n’est nécessaire : le script accède directement à ce modèle installé par le système.</p>
<h3><span>Vérification rapide (optionnelle)</span></h3>
<p>Pour vérifier explicitement que le modèle requis est bien présent&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>ls</span><span> </span><span>-</span><span>l</span><span> </span><span>/</span><span>usr</span><span>/</span><span>share</span><span>/</span><span>hailo</span><span>-</span><span>models</span><span>/</span><span>yolov8m_h10</span><span>.</span><span>hef</span></p></div></td> </tr> </tbody></table> </div> <p>Si le fichier existe, l’environnement est correctement configuré et prêt pour exécuter une première détection sur une vidéo locale.<br>
Nous pouvons maintenant passer à l’étape suivante : lancer une démo complète de détection d’objets sur une vidéo avec la chaîne Hailo.</p>
<h2><span>Étape 4 : Lancer une première détection de personnes sur la vidéo</span></h2>
<p>La vidéo de test est déjà en place (Étape 1) et l’environnement Hailo est correctement configuré (Étapes 2 et 3). Nous pouvons maintenant lancer une première démo complète de détection de personnes sur un fichier vidéo local.</p>
<h3><span>Activer l’environnement Hailo</span></h3>
<p>Rappel : À chaque nouvelle ouverture de terminal, l’environnement de travail doit être activé :</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p></div> </td> <td><div><p><span>cd</span><span> </span><span>~</span><span>/</span><span>hailo</span><span>-</span><span>rpi5</span><span>-</span><span>examples</span></p><p><span>source </span><span>setup_env</span><span>.</span><span>sh</span></p></div></td> </tr> </tbody></table> </div> <p>Le prompt doit maintenant indiquer que l’environnement&nbsp;<code>venv_hailo_rpi_examples</code> est actif.</p>
<h3><span>Lancer la détection vidéo</span></h3>
<p>Depuis le dossier <code>hailo-rpi5-examples</code>, exécutez la commande suivante&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>python </span><span>basic_pipelines</span><span>/</span><span>detection</span><span>.</span><span>py</span><span> </span><span>--</span><span>input</span><span> </span><span>~</span><span>/</span><span>Videos</span><span>/</span><span>foule_h264</span><span>.</span><span>mp4</span><span> </span><span>--</span><span>arch </span><span>hailo10h</span><span> </span><span>--</span><span>show</span><span>-</span><span>fps</span></p></div></td> </tr> </tbody></table> </div> <p>Une fenêtre vidéo s’ouvre alors automatiquement. Les personnes détectées sont entourées de boîtes, et un indicateur de performance (FPS) est affiché à l’écran.</p>
<h3><span>Résultat attendu</span></h3>
<ul>
<li>La vidéo est correctement lue</li>
<li>Les personnes sont détectées et encadrées</li>
<li>Le traitement reste fluide, même sur une scène chargée</li>
</ul>
<p>À ce stade, la chaîne complète est validée : lecture du fichier vidéo, inférence sur le NPU Hailo-10H, post-traitement TAPPAS et affichage en temps réel.</p>
<p>
<a href="https://www.framboise314.fr/wp-content/uploads/2025/12/video_demo_720p.mp4">https://www.framboise314.fr/wp-content/uploads/2025/12/video_demo_720p.mp4</a></p> <h2><span>Détection d’objets en direct avec une caméra USB sur Raspberry Pi 5 + AI HAT+ 2 (Hailo-10H)</span></h2>
<p>Après les tests sur fichier vidéo, place au <strong>temps réel</strong> avec une caméra USB. L’objectif est double : vérifier que la webcam est bien reconnue par le Raspberry Pi, puis lancer la <strong>détection&nbsp; d’objets en live</strong> via le pipeline Hailo (overlay + FPS).</p>
<h3><span>Vérifier que la caméra USB est détectée</span></h3>
<p>Branchez la caméra, puis listez les périphériques vidéo et l’identification V4L2&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p></div> </td> <td><div><p><span>ls</span><span> </span><span>-</span><span>l</span><span> </span><span>/</span><span>dev</span><span>/</span><span>video*</span></p><p><span>v4l2</span><span>-</span><span>ctl</span><span> </span><span>--</span><span>list</span><span>-</span><span>devices</span></p></div></td> </tr> </tbody></table> </div> <p>Vous devez voir une entrée de type <em>USB Live camera</em> avec au moins un <code>/dev/videoX</code> (dans mon cas <code>/dev/video0</code> et <code>/dev/video1</code>). Cela confirme que la caméra est bien visible côté système.</p>
<h3><span>Tester la caméra sous Raspberry Pi OS (sans IA)</span></h3>
<p>Avant d’introduire l’IA, il est important de vérifier que la caméra fonctionne correctement en affichage direct. Cela permet d’écarter tout problème matériel ou de pilote.</p>
<p>Lancez simplement l’affichage du flux vidéo brut&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p></div> </td> <td><div><p><span>ffplay</span><span> </span><span>/</span><span>dev</span><span>/</span><span>video0</span></p></div></td> </tr> </tbody></table> </div> <p>Une fenêtre doit s’ouvrir avec l’image de la caméra.</p>
<p><img src="https://www.framboise314.fr/wp-content/uploads/2025/12/pi5_HAT2_36.jpg" alt=""></p>
<p>Une légère latence (environ une demi-seconde) est normale à ce stade, car l’affichage se fait en logiciel, sans accélération particulière.</p>
<h3><span>Lancer la détection en direct (caméra USB)</span></h3>
<p>On se place dans le dépôt des exemples, on active l’environnement, puis on lance la démo en mode <strong>USB</strong> (détection + affichage du FPS)&nbsp;:</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p><p>3</p></div> </td> <td><div><p><span>cd</span><span> </span><span>~</span><span>/</span><span>hailo</span><span>-</span><span>rpi5</span><span>-</span><span>examples</span></p><p><span>source </span><span>setup_env</span><span>.</span><span>sh</span></p><p><span>python </span><span>basic_pipelines</span><span>/</span><span>detection</span><span>.</span><span>py</span><span> </span><span>--</span><span>input </span><span>usb</span><span> </span><span>--</span><span>arch </span><span>hailo10h</span><span> </span><span>--</span><span>show</span><span>-</span><span>fps</span></p></div></td> </tr> </tbody></table> </div> <p>Une fenêtre s’ouvre avec l’image de la caméra et les boîtes de détection (overlay). Sur une scène de bureau, la démo détecte non seulement une personne, mais aussi des objets courants (clavier, souris, etc.). La latence observée est d’environ <strong>0,5&nbsp;s</strong>, ce qui reste très exploitable pour de la démonstration et des tests.</p>
<p><img src="https://www.framboise314.fr/wp-content/uploads/2025/12/pi5_HAT2_35.jpg" alt=""></p>
<h2><span>Estimation de pose (squelette) en temps réel : état actuel sur Raspberry Pi 5 + AI HAT+ 2</span></h2>
<p>Après la détection d’objets, l’étape suivante logique consiste à tester l’estimation de pose (pose humaine), c’est-à-dire l’identification des articulations du corps (tête, épaules, coudes, hanches, genoux, etc.) et l’affichage d’un squelette en surimpression sur le flux vidéo.</p>
<p>Sur le papier, tout est réuni pour que cette démonstration fonctionne : le modèle&nbsp;<code>yolov8m_pose_h10.hef</code> est bien présent sur le système, fourni par les paquets Debian officiels (<code>/usr/share/hailo-models</code>), et le script&nbsp;<code>basic_pipelines/pose_estimation.py</code> est inclus dans le dépôt <em>hailo-rpi5-examples</em>.</p>
<p>Dans la pratique, sur Raspberry Pi 5 équipé de l’AI HAT+ 2 (Hailo-10H), le lancement de la démo de pose échoue systématiquement avec l’erreur suivante :</p> <div> <table> <tbody><tr> <td> <div><p>1</p><p>2</p><p>3</p></div> </td> <td><div><p><span>Cannot </span><span>load </span><span>symbol</span><span>:</span></p><p><span>libyolov8pose_postprocess</span><span>.</span><span>so</span><span>:</span><span> </span><span>undefined </span><span>symbol</span><span>:</span><span> </span><span>filter_letterbox</span></p><p><span>Erreur </span><span>de </span><span>segmentation</span></p></div></td> </tr> </tbody></table> </div> <p>Cette erreur indique que le module de post-traitement utilisé pour la pose (<code>libyolov8pose_postprocess.so</code>) appelle une fonction (<code>filter_letterbox</code>) qui n’est pas résolue dans la version actuelle des bibliothèques TAPPAS fournies pour le Raspberry Pi 5. Les vérifications effectuées montrent que ce symbole n’est pas exporté par&nbsp;<code>libgsthailotools.so</code> dans la version <code>hailo-tappas-core 5.1.0</code> disponible sur Raspberry Pi OS.</p>
<p>Il est important de souligner que ce problème ne provient ni de la configuration de l’utilisateur, ni du fichier vidéo, ni de la caméra USB. La détection d’objets fonctionne correctement dans le même environnement, ce qui confirme que le NPU Hailo-10H et la chaîne GStreamer sont opérationnels. Il s’agit ici d’une incompatibilité logicielle spécifique au pipeline d’estimation de pose dans l’état actuel des paquets fournis pour le Raspberry Pi 5.</p>
<p>Cet état de fait correspond à la situation <strong>au moment de la rédaction de cet article</strong>. L’AI HAT+ 2 étant encore en phase de lancement, il est très probable que les bibliothèques TAPPAS et les pipelines associés évoluent dans les prochaines semaines. Une mise à jour logicielle lors de la sortie officielle du produit, ou peu après, pourrait corriger cette limitation et rendre l’estimation de pose pleinement fonctionnelle sur Raspberry Pi 5.</p>
<p>Cette partie sera donc réévaluée et mise à jour dès qu’une version corrigée des paquets Hailo sera disponible.</p>
<h2><span>Sources</span></h2>
<p><a href="https://www.raspberrypi.com/products/ai-hat-plus-2/" target="_blank">https://www.raspberrypi.com/products/ai-hat-plus-2/</a></p>
<p><a href="https://pip-assets.raspberrypi.com/categories/1319-raspberry-pi-ai-hat-2/documents/RP-009655-MM-3-raspberry-pi-ai-hat-plus-2-product-brief.pdf" target="_blank">https://pip-assets.raspberrypi.com/categories/1319-raspberry-pi-ai-hat-2/documents/RP-009655-MM-3-raspberry-pi-ai-hat-plus-2-product-brief.pdf</a></p>
<p><a href="https://www.raspberrypi.com/documentation/accessories/ai-hat-plus.html" target="_blank">https://www.raspberrypi.com/documentation/accessories/ai-hat-plus.html</a></p>
<p><a href="https://hailo.ai/products/hailo-software/model-explorer/generative-ai/devices/hailo-10h/" target="_blank">https://hailo.ai/products/hailo-software/model-explorer/generative-ai/devices/hailo-10h/</a><br>
<a href="https://hailo.ai/products/ai-accelerators/hailo-10h-m-2-ai-acceleration-module" target="_blank">https://hailo.ai/products/ai-accelerators/hailo-10h-m-2-ai-acceleration-module</a></p>
<blockquote><p><a href="https://datascientest.com/docker-guide-complet">Docker : qu’est-ce que c’est et comment l’utiliser ?</a></p></blockquote> <p><strong>Les articles sur Framboise314 :</strong></p>
<blockquote><p><a href="https://www.framboise314.fr/raspberry-pi-ai-hat-plus-2-installation-pi5/">Raspberry Pi AI HAT+ 2 : présentation matérielle et installation sur Raspberry Pi 5</a></p></blockquote> <blockquote><p><a href="https://www.framboise314.fr/installer-raspberry-pi-ai-hat-plus-2-pi5-hailort-hailo-ollama/">Raspberry Pi AI HAT+ 2 : installer Hailo-10H et lancer un LLM local (Partie 1)</a></p></blockquote> <blockquote><p><a href="https://www.framboise314.fr/raspberry-pi-ai-hat-2-hailo-10h-video-partie-2/">Raspberry Pi AI HAT+ 2 : vision par ordinateur en vidéo avec Hailo-10H (Partie 2)</a></p></blockquote> </div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>