<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Arm &amp; ExecuTorch 0.7: Bringing Generative AI to the masses</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Arm &amp; ExecuTorch 0.7: Bringing Generative AI to the masses</h1>
  <div class="metadata">
<<<<<<< HEAD
    Source: Hugging Face Blog | Date: 8/13/2025 4:55:10 PM | Lang: EN |
=======
    Source: Hugging Face Blog | Date: 8/13/2025 2:55:10 PM | Lang: EN |
>>>>>>> 48d6193da6f49976a64b6a30483399bfb54b1b8d
    <a href="https://huggingface.co/blog/Arm/executorch-0-dot-7" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/sondhiArm"><img alt="EricSondhi's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YdisMmTuCDj0gVK2TDml7.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/gimmy87"><img alt="Gian Marco Iodice's avatar" src="https://huggingface.co/avatars/37014d030e51fd22f62fc97afeb32403.svg"></a> </span> </span></p> </div></div> <p> <a href="https://cdn-uploads.huggingface.co/production/uploads/6682a9252200824e2ddc667f/WyGi7MMqhNKvKTpIrULnc.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/6682a9252200824e2ddc667f/WyGi7MMqhNKvKTpIrULnc.png"></a></p>
<p>With Arm’s recent <a href="https://newsroom.arm.com/blog/arm-sme2-android-mobile-apps">SME2 announcement</a>, the role of <a href="https://www.arm.com/products/development-tools/embedded-and-software/kleidi-libraries">Arm KleidiAI</a> is increasingly clear as Arm’s AI accelerator layer powering the next wave of AI. By embedding into widely-used Edge AI frameworks like XNNPack, <a href="https://newsroom.arm.com/blog/kleidiai-integration-mediapipe">MediaPipe</a>, <a href="https://newsroom.arm.com/blog/arm-kleidi-alibaba-mnn-framework-multimodal-ai">MNN</a>, <a href="https://newsroom.arm.com/blog/arm-microsoft-kleidiai-onnx-runtime">ONNX Runtime</a>, and even llama.cpp, KleidiAI has delivered substantial <a href="https://newsroom.arm.com/blog/stability-ai-arm-kleidi-text-to-audio-generation">performance improvements</a> with no code changes required by developers. That foundation leads directly to the upcoming ExecuTorch 0.7 beta, where KleidiAI will be enabled by default—bringing automatic acceleration to devices built on the latest Arm CPU architecture, as well as a vast base of existing phones built on earlier generations.</p>
<p>Android and cross-platform developers—whether first- or third-party—gain instant access to KleidiAI AI performance optimizations via ExecuTorch and XNNPack. The result? Faster model startups, lower latency, leaner memory footprints—and no integration hurdles. What previously required custom tuning is now turn-key performance, ready out of the box. This efficiency unlocks new possibilities—not just for the latest high-end devices, but for a much broader range of hardware.</p>
<p>When we consider running Generative AI (GenAI) on mobile devices, it is easy to envision the latest flagship smartphones equipped with powerful CPUs, GPUs, and NPUs. But what if we told you that GenAI experiences—like running large language models (LLMs)—can also be brought to devices that are 3, 4, or even 5 years old? Or even to the Raspberry Pi 5?</p>
<p>Well, this is no longer just a vision, but a practical reality. Thanks to the Arm SDOT CPU feature, which has been available in Arm CPUs since 2015.</p>
<h2> <a href="#what-is-sdot"> <span></span> </a> <span> What is SDOT? </span>
</h2>
<p>The SDOT (Signed Dot Product) instruction, introduced in the Armv8.2 architecture and later CPUs, enables efficient dot product operations on vectors of 8-bit signed integers. The following image illustrates the behavior of one such SDOT instruction available on Arm CPUs:
<a href="https://cdn-uploads.huggingface.co/production/uploads/6682a9252200824e2ddc667f/guiinfFqKwgqUAz6UFS6R.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/6682a9252200824e2ddc667f/guiinfFqKwgqUAz6UFS6R.png"></a></p>
<p>As shown above, the instruction produces four 32-bit integer outputs, each resulting from the dot product of corresponding groups of four int8 elements from the left-hand side (LHS) and right-hand side (RHS) vector registers.</p>
<p>This instruction can be utilized to accelerate matrix multiplication routines—the core computational workload behind every LLM—when using Int8 or lower-bit precision formats, such as Int4. These operations typically involve numerous dot products between individual rows of the left-hand side matrix and corresponding columns of the right-hand side matrix.</p>
<p>The SDOT instruction is already widely supported across a diverse range of devices, opening the door for GenAI use cases to reach a significantly larger smartphone audience. As of today, Arm CPUs in approximately 3 billion Arm-based devices include this capability—enabling powerful on-device GenAI experiences for the majority of users. In fact, 72% of all devices now support this instruction.</p>
<p>Thanks to ExecuTorch, we are now enabling models like Llama 3.2 to run efficiently on the majority of Android devices as well as edge devices like the Raspberry Pi 5.</p>
<h2> <a href="#kleidiai--executorch-bringing-it-all-together"> <span></span> </a> <span> KleidiAI + ExecuTorch: Bringing it all together </span>
</h2>
<p>For the quantized Llama 3.2 1B announcement last year, the ExecuTorch and KleidiAI teams collaborated to deliver optimizations for the Int4 matrix-multiplication on Arm CPUs leveraging the I8MM feature, available from the Armv8.6 architecture onwards. As highlighted in a previous blog post, ExecuTorch with KleidiAI achieves over 20% higher prefill performance on the Galaxy S24+ compared to non-KleidiAI kernels.</p>
<p>This translates to more than 350 tokens per second during the prefill phase and over 40 tokens per second during the decode phase. This level of performance is sufficient to enable on-device tasks, such as summarizing unread messages, with a smooth user experience using only Arm CPUs. For context, summarizing around 50 unread messages typically involves processing approximately 600 tokens.</p>
<p>This year, the ExecuTorch and KleidiAI teams have focused on optimizing Int4 matrix multiplication performance by leveraging the SDOT instruction, aiming to broaden adoption.</p>
<p><a href="https://github.com/google/XNNPACK/pull/8604">Check out the XNNPack PR on GitHub</a></p>
<p>While LLM performance on Arm CPUs with only the SDOT extension may not match the latest flagship smartphones, it still enables impressive capabilities for on-device generative AI. In fact, in many scenarios, the decode phase is faster than the average human reading speed—highlighting that even older Arm CPUs can support practical and meaningful GenAI use cases.</p>
<p>For example, when combined with speech-to-text and text-to-speech models, a local LLM of this kind enables the creation of a fully private smart assistant that operates entirely offline, eliminating concerns about data privacy while still offering rich voice-based interactions. Such a device could seamlessly interact with your connected devices, ensuring users have peace of mind with their data.</p>
<p>Another compelling use case for running Llama 3.2 1B is context-aware text completion in local text editors. As you type, the model provides intelligent, real-time suggestions to streamline writing or coding workflows—all without requiring an internet connection.</p>
<p>These are just a few examples, and they only scratch the surface of what is possible with on-device GenAI.</p>
<h2> <a href="#conclusion-genai-for-everyone"> <span></span> </a> <span> Conclusion: GenAI for everyone </span>
</h2>
<p>With the combined power of SDOT, KleidiAI, and ExecuTorch, we are pushing the boundaries of what is possible. Bringing Generative AI beyond high-end flagship devices and making it accessible on billions of Arm-based devices already in use.</p>
<p>Now it is your turn—we are excited to see what you will create. To help you get started, check out Arm’s learning path, designed to guide you through developing your own applications with LLMs using ExecuTorch and KleidiAI.</p>
<p><a href="https://learn.arm.com/learning-paths/mobile-graphics-and-gaming/build-llama3-chat-android-app-using-executorch-and-xnnpack/">Build an Android chat app with Llama, KleidiAI, ExecuTorch, and XNNPACK</a></p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollStep(-1)">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollStep(1)">▼</button>
  </div>
  <script>
    function scrollStep(direction) {
      var step = Math.max(220, Math.round(window.innerHeight * 0.72));
      window.scrollBy({ top: direction * step, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up') scrollStep(-1);
      if (data.direction === 'down') scrollStep(1);
      if (data.direction === 'top') window.scrollTo({ top: 0, behavior: 'smooth' });
      if (data.direction === 'bottom') window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    });
  </script>
</body>
</html>