<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Community Evals: Because we're done trusting black-box leaderboards over the community</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.8; color: #e2e8f0; max-width: 800px; margin: 40px auto; padding: 0 20px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.5em; }
  .metadata { color: #94a3b8; font-size: 0.9em; margin-bottom: 2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 1em; }
  img { max-width: 100%; height: auto; border-radius: 8px; }
  a { color: #00d9ff; }
  p { margin-bottom: 1em; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 15px; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 15px; border-radius: 6px; overflow-x: auto; }

  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }

</style>
</head>
<body>
  <h1>Community Evals: Because we're done trusting black-box leaderboards over the community</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 2/4/2026 | Lang: EN |
    <a href="https://huggingface.co/blog/community-evals" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div>
					<p><a href="https://huggingface.co/blog">
							Back to Articles</a></p>

					
					
					
					<div><div>

<p><span><span><a href="https://huggingface.co/burtenshaw"><img alt="ben burtenshaw's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62d648291fa3e4e7ae3fa6e8/oatOwf8Xqe5eDbCSuYqCd.png" /></a>
				</span>
	</span></p>
			</div><div>

<p><span><span><a href="https://huggingface.co/SaylorTwift"><img alt="Nathan Habib's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1678663263366-63e0eea7af523c37e5a77966.jpeg" /></a>
				</span>
	</span></p>
			</div><div>

<p><span><span><a href="https://huggingface.co/kramp"><img alt="Bertrand Chevrier's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62d6c40ba71903fb4c1e3261/SQGmYShjtveXBf8HVpxPp.jpeg" /></a>
				</span>
	</span></p>
			</div><div>

<p><span><span><a href="https://huggingface.co/merve"><img alt="merve's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6141a88b3a0ec78603c9e784/DJsxSmWV39M33JFheLobC.jpeg" /></a>
				</span>
	</span></p>
			</div><div>

<p><span><span><a href="https://huggingface.co/davanstrien"><img alt="Daniel van Strien's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg" /></a>
				</span>
	</span></p>
			</div><div>

<p><span><span><a href="https://huggingface.co/nielsr"><img alt="Niels Rogge's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg" /></a>
				</span>
	</span></p>
			</div><div>

<p><span><span><a href="https://huggingface.co/julien-c"><img alt="Julien Chaumond's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg" /></a>
				</span>
	</span></p>
			</div></div>
					

					
<div><nav><ul><li><a href="#evaluation-is-broken">Evaluation is broken</a>
									<ul></ul>
								</li><li><a href="#what-were-shipping">What We're Shipping</a>
									<ul></ul>
								</li><li><a href="#why-this-matters">Why This Matters</a>
									<ul></ul>
								</li><li><a href="#get-started"><strong>Get Started</strong></a>
									<ul></ul>
								</li></ul></nav></div><p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/eval-results-blog/banner.png"><img alt="banner" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/eval-results-blog/banner.png" /></a></p>
<p><strong>TL;DR:</strong> Benchmark datasets on Hugging Face can now host leaderboards. Models store their own eval scores. Everything links together. The community can submit results via PR. Verified badges prove that the results can be reproduced.</p>
<h2>
	<a href="#evaluation-is-broken">
		<span></span>
	</a>
	<span>
		Evaluation is broken
	</span>
</h2>
<p>Let's be real about where we are with evals in 2026. MMLU is saturated above 91%. GSM8K hit 94%+. HumanEval is conquered. Yet some models that ace benchmarks still can't reliably browse the web, write production code, or handle multi-step tasks without hallucinating, based on usage reports. There is a clear gap between benchmark scores and real-world performance.</p>
<p>Furthermore, there is <em>another gap</em> within reported benchmark scores. Multiple sources report different results. From Model Cards, to papers, to evaluation platforms, there is no alignment in reported scores. The result is that the community lacks a single source of truth. </p>
<h2>
	<a href="#what-were-shipping">
		<span></span>
	</a>
	<span>
		What We're Shipping
	</span>
</h2>
<p><strong>Decentralized and transparent evaluation reporting.</strong></p>
<p>We are going to take evaluations on the Hugging Face Hub in a new direction by decentralizing reporting and allowing the entire community to openly report scores for benchmarks. At first, we will start with a shortlist of 4 benchmarks and over time we’ll expand to the most relevant benchmarks. </p>
<p><strong>For Benchmarks:</strong> Dataset repos can now register as benchmarks (<a href="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro">MMLU-Pro</a>, <a href="https://huggingface.co/datasets/Idavidrein/gpqa">GPQA</a>, <a href="https://huggingface.co/datasets/cais/hle">HLE</a> are already live). They automatically aggregate reported results from across the Hub and display leaderboards in the dataset card. The benchmark defines the eval spec via <code>eval.yaml</code>, based on the <a href="https://inspect.aisi.org.uk/">Inspect AI</a> format, so anyone can reproduce it. The reported results need to align with the task definition. </p>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/eval-results-blog/benchmark.png"><img alt="benchmark image" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/eval-results-blog/benchmark.png" /></a></p>
<p><strong>For Models:</strong> Eval scores live in <code>.eval_results/*.yaml</code> in the model repo. They appear on the model card and are fed into benchmark datasets. Both the model author’s results <strong>and open pull requests</strong> for results will be aggregated. Model authors will be able to close score PR and hide results. </p>
<p><strong>For the Community:</strong> Any user can submit evaluation results for any model via a PR. Results get shown as "community", without waiting for model authors to merge or close. The community can link to sources like a paper, Model Card, third-party evaluation platform, or <code>inspect</code> eval logs. The community can discuss scores like any PR. Since the Hub is Git based, there is a history of when evals were added, when changes were made, etc. The sources look like below.</p>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/eval-results-blog/model.png"><img alt="model image" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/eval-results-blog/model.png" /></a></p>
<p>To learn more about evaluation results, check out the <a href="https://huggingface.co/docs/hub/eval-results">docs</a>.</p>
<figure>
   
   <figcaption>Model scores in the Hub</figcaption>
</figure>

<h2>
	<a href="#why-this-matters">
		<span></span>
	</a>
	<span>
		Why This Matters
	</span>
</h2>
<p>Decentralizing evaluation will expose scores that already exist across the community in sources like model cards and papers. By exposing these scores, the community can build on top of them to aggregate, track, and understand scores across the field. Also, all scores will be exposed via Hub APIs, making it easy to aggregate and build curated leaderboards, dashboards, etc.</p>
<p>Community evals do not replace benchmarks so leaderboards and closed evals with published results are still crucial. However, we believe it's important to contribute to the field with open eval results based on reproducible eval specs. </p>
<p>This won't solve benchmark saturation or close the benchmark-reality gap. Nor will it stop training on test sets. But it makes the game visible by exposing what is evaluated, how, when, and by whom. </p>
<p>Mostly, we hope to make the Hub an active place to build and share reproducible benchmarks. Particularly focusing on new tasks and domains that challenge SOTA models more.</p>
<h2>
	<a href="#get-started">
		<span></span>
	</a>
	<span>
		<strong>Get Started</strong>
	</span>
</h2>
<p><strong>Add eval results:</strong> Publish the evals you conducted as YAML files in <code>.eval_results/</code> on any model repo.</p>
<p><strong>Check out the scores</strong> on the <a href="https://huggingface.co/datasets?benchmark=benchmark:official&amp;sort=trending">benchmark dataset</a>.</p>
<p><strong>Register a new benchmark:</strong> Add <code>eval.yaml</code> to your dataset repo and <a href="https://huggingface.co/spaces/OpenEvals/README/discussions/2">contact us to be included in the shortlist.</a></p>
<p><em>The feature is in beta. We're building in the open. <a href="https://huggingface.co/spaces/OpenEvals/README/discussions/1">Feedback welcome.</a></em></p>
</div></div>
  </div>
</body>
</html>