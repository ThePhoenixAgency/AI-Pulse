<!DOCTYPE html><html lang="fr"><head>
<meta charset="UTF-8">
<title>Google affirme qu'une attaque � utilis� plus de 100 000 instructions g�n�ratives pour tenter de cloner le chatbot IA Gemini, une extraction de mod�le pour obtenir la logique qui fait fonctionner le mod�le</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Google affirme qu'une attaque � utilis� plus de 100 000 instructions g�n�ratives pour tenter de cloner le chatbot IA Gemini, une extraction de mod�le pour obtenir la logique qui fait fonctionner le mod�le</h1>
  <div class="metadata">
    Source: Developpez.com | Date: 2/16/2026 12:54:00 PM | <a href="https://intelligence-artificielle.developpez.com/actu/380266/Google-affirme-qu-une-attaque-a-utilise-plus-de-100-000-instructions-generatives-pour-tenter-de-cloner-le-chatbot-IA-Gemini-une-extraction-de-modele-pour-obtenir-la-logique-qui-fait-fonctionner-le-modele/" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: FR
  </div>
  <div class="content"><p>Google a r�v�l� que son chatbot d'IA phare, Gemini, a �t� submerg� par des acteurs � � motivation commerciale � qui tentent de le cloner en lui envoyant des requ�tes � r�p�tition, parfois avec des milliers de requ�tes diff�rentes, dont une campagne qui a envoy� plus de 100 000 requ�tes � Gemini. Google a d�crit cette activit� comme une � extraction de mod�le �, dans laquelle des imitateurs potentiels sondent le syst�me � la recherche des mod�les et de la logique qui le font fonctionner. Les attaquants semblent vouloir utiliser ces informations pour cr�er ou renforcer leur propre IA, a-t-il d�clar�. La soci�t� pense que les coupables sont principalement des entreprises priv�es ou des chercheurs cherchant � obtenir un avantage concurrentiel.Gemini, anciennement Bard, est un assistant conversationnel d�velopp� par l'entreprise Google. Pour g�n�rer du texte, il se base sur une famille de grands mod�les de langage �galement appel�e Gemini, introduite au public le 7 d�cembre 2023. Gemini est l'acronyme de Generalized Multimodal Intelligence Network. Les mod�les se d�clinent en trois tailles : nano, pro et ultra. Gemini peut comprendre et interagir avec l'audio et la vid�o, et g�n�rer du texte (po�sie, scripts, pi�ces musicales, courriels, lettres, etc.), du code, des traductions (entre plus de 100 langues). Il peut produire plusieurs types de contenu cr�atif (images, dessins, sons, musique, vid�os�), aider des chercheurs en analysant des donn�es ou en g�n�rant des hypoth�ses. Gemini peut r�pondre aux questions de mani�re informative ou en produisant des cours personnalis�s, des jeux, des tutoriels, etc., avec les limites des IA (erreurs, biais, � hallucinations ��).Alors que la comp�tition autour de l�intelligence artificielle g�n�rative est souvent racont�e comme une succession de coups m�diatiques, de d�monstrations spectaculaires et d�annonces parfois pr�cipit�es, Google avance � un rythme plus feutr�. Avec Gemini, son mod�le d�IA unifi�, le groupe semble aujourd�hui r�colter les fruits d�une strat�gie plus structurelle que narrative. Sans d�clarer officiellement la victoire, Google appara�t de plus en plus comme un acteur central, voire dominant, dans la phase actuelle de la course � l�IA.R�cemment, Google a r�v�l� que son chatbot d'IA phare, Gemini, a �t� submerg� par des acteurs � � motivation commerciale � qui tentent de le cloner en lui envoyant des requ�tes � r�p�tition, parfois avec des milliers de requ�tes diff�rentes, dont une campagne qui a envoy� plus de 100 000 requ�tes � Gemini. Dans un rapport, Google a d�clar� �tre de plus en plus victime d'� attaques par distillation �, c'est-�-dire de questions r�p�t�es visant � amener un chatbot � r�v�ler son fonctionnement interne.Google a d�crit cette activit� comme une � extraction de mod�le �, dans laquelle des imitateurs potentiels sondent le syst�me � la recherche des mod�les et de la logique qui le font fonctionner. Les attaquants semblent vouloir utiliser ces informations pour cr�er ou renforcer leur propre IA, a-t-il d�clar�. La soci�t� pense que les coupables sont principalement des entreprises priv�es ou des chercheurs cherchant � obtenir un avantage concurrentiel. Un porte-parole a d�clar� que Google pensait que les attaques provenaient du monde entier, mais a refus� de donner plus de d�tails sur ce que l'on savait des suspects.
L'ampleur des attaques contre Gemini indique qu'elles sont tr�s probablement courantes ou le deviendront bient�t contre les outils d'IA personnalis�s des petites entreprises, a d�clar� John Hultquist, analyste en chef du groupe Threat Intelligence de Google. � Nous allons �tre le canari dans la mine de charbon pour beaucoup d'autres incidents �, a d�clar� Hultquist. Il a refus� de nommer les suspects. La soci�t� consid�re la distillation comme un vol de propri�t� intellectuelle, a-t-elle d�clar�.Les entreprises technologiques ont d�pens� des milliards de dollars pour d�velopper leurs chatbots IA, ou grands mod�les de langage, et consid�rent le fonctionnement interne de leurs meilleurs mod�les comme des informations exclusives extr�mement pr�cieuses. M�me si elles disposent de m�canismes pour tenter d'identifier les attaques par distillation et de bloquer les personnes qui les perp�trent, les principaux LLM sont intrins�quement vuln�rables � la distillation, car ils sont accessibles � tous sur Internet. OpenAI, la soci�t� � l'origine de ChatGPT, a accus� l'ann�e derni�re son rival chinois DeepSeek d'avoir men� des attaques de distillation afin d'am�liorer ses mod�les. Selon Google, bon nombre de ces attaques visaient � mettre au jour les algorithmes qui aident Gemini � � raisonner �, c'est-�-dire � d�cider comment traiter les informations. Hultquist a d�clar� qu'� mesure que de plus en plus d'entreprises con�oivent leurs propres LLM personnalis�s, entra�n�s sur des donn�es potentiellement sensibles, elles deviennent vuln�rables � des attaques similaires. � Imaginons que votre LLM ait �t� entra�n� sur 100 ans de r�flexions secr�tes sur votre fa�on de n�gocier. Th�oriquement, vous pourriez en distiller une partie �, a-t-il d�clar�.En 2026, Gemini est consid�r� comme le mod�le d'IA le plus populaire sur le march�. C'est en tout cas l'avis de certains sp�cialistes comme Geoffrey Hinton, le � parrain de l'IA �. Il a notamment d�clar� que Google est en train de rattraper OpenAI dans la course � l'intelligence artificielle (IA). Hinton a d�clar� : � Je pense qu'il est en fait plus surprenant que Google ait mis autant de temps � d�passer OpenAI. Je pense qu'� l'heure actuelle, ils commencent � le d�passer �.M�me les chiffres semblent aller dans ce sens. Lors de la pr�sentation de ses r�sultats financiers du quatri�me trimestre, Google a r�v�l� que son application d'assistant IA Gemini avait d�pass� les 750 millions d'utilisateurs actifs mensuels (MAU). Cela repr�sente une augmentation de 100 millions par rapport aux 650 millions de MAU du trimestre pr�c�dent. L'�cart qui la s�pare de son plus grand rival, ChatGPT, qui compte 810 millions d'utilisateurs actifs par mois, se r�duit. Voici un extrait du rapport de Google :GTIG AI Threat Tracker : distillation, exp�rimentation et int�gration (continue) de l'IA � des fins adversairesAu cours du dernier trimestre 2025, le Google Threat Intelligence Group (GTIG) a observ� que les acteurs malveillants int�graient de plus en plus l'intelligence artificielle (IA) pour acc�l�rer le cycle de vie des attaques, ce qui leur permettait de gagner en productivit� dans les domaines de la reconnaissance, de l'ing�nierie sociale et du d�veloppement de logiciels malveillants. Ce rapport fait suite � nos conclusions de novembre 2025 concernant les progr�s r�alis�s par les acteurs malveillants dans l'utilisation des outils d'IA.En identifiant ces indicateurs pr�coces et ces preuves de concept offensives, le GTIG vise � fournir aux d�fenseurs les informations n�cessaires pour anticiper la prochaine phase des menaces bas�es sur l'IA, contrecarrer de mani�re proactive les activit�s malveillantes et renforcer continuellement nos classificateurs et notre mod�le.R�sum�Google DeepMind et GTIG ont constat� une augmentation des tentatives d'extraction de mod�les ou � attaques par distillation �, une m�thode de vol de propri�t� intellectuelle qui enfreint les conditions d'utilisation de Google. Tout au long de ce rapport, nous avons r�pertori� les mesures que nous avons prises pour contrecarrer les activit�s malveillantes, notamment la d�tection, la perturbation et l'att�nuation des activit�s d'extraction de mod�les par Google. Bien que nous n'ayons pas observ� d'attaques directes contre des mod�les de pointe ou des produits d'IA g�n�rative de la part d'acteurs APT (Advanced Persistent Threat), nous avons observ� et att�nu� de fr�quentes attaques d'extraction de mod�les provenant d'entit�s du secteur priv� partout dans le monde et de chercheurs cherchant � cloner une logique propri�taire. Pour les acteurs malveillants soutenus par des gouvernements, les grands mod�les linguistiques (LLM) sont devenus des outils essentiels pour la recherche technique, le ciblage et la g�n�ration rapide d'app�ts de phishing nuanc�s. Ce rapport trimestriel met en �vidence la mani�re dont les acteurs malveillants de la R�publique populaire d�mocratique de Cor�e (RPDC), de l'Iran, de la R�publique populaire de Chine (RPC) et de la Russie ont mis en �uvre l'IA � la fin de l'ann�e 2025 et am�liore notre compr�hension de la mani�re dont l'utilisation abusive de l'IA g�n�rative se manifeste dans les campagnes que nous perturbons sur le terrain. Le GTIG n'a pas encore observ� d'acteurs APT ou d'op�rations d'information (IO) ayant atteint des capacit�s r�volutionnaires qui modifient fondamentalement le paysage des menaces.Ce rapport examine plus particuli�rement :- Les attaques par extraction de mod�les : les � attaques par distillation � sont en augmentation en tant que m�thode de vol de propri�t� intellectuelle depuis l'ann�e derni�re.- Les op�rations augment�es par l'IA : des �tudes de cas r�els d�montrent comment certains groupes rationalisent la reconnaissance et le phishing visant � �tablir des relations.- L'IA agentique : les acteurs malveillants commencent � s'int�resser au d�veloppement de capacit�s d'IA agentique pour soutenir le d�veloppement de logiciels malveillants et d'outils.- Les logiciels malveillants int�grant l'IA : Il existe de nouvelles familles de logiciels malveillants, telles que HONESTCUE, qui exp�rimentent l'utilisation de l'interface de programmation d'application (API) de Gemini pour g�n�rer du code permettant le t�l�chargement et l'ex�cution de logiciels malveillants de deuxi�me �tape.- �cosyst�me clandestin de � jailbreak � : des services malveillants tels que Xanthorox font leur apparition dans la clandestinit�, pr�tendant �tre des mod�les ind�pendants alors qu'ils s'appuient en r�alit� sur des API commerciales jailbreak�es et des serveurs MCP (Model Context Protocol) open source.Chez Google, nous nous engageons � d�velopper l'IA de mani�re audacieuse et responsable, ce qui signifie que nous prenons des mesures proactives pour perturber les activit�s malveillantes en d�sactivant les projets et les comptes associ�s aux acteurs malveillants, tout en am�liorant continuellement nos mod�les afin de les rendre moins vuln�rables aux abus. Nous partageons �galement de mani�re proactive les meilleures pratiques du secteur afin d'armer les d�fenseurs et de permettre une protection plus forte dans l'ensemble de l'�cosyst�me. Tout au long de ce rapport, nous mentionnons les mesures que nous avons prises pour contrecarrer les activit�s malveillantes, notamment la d�sactivation d'actifs et l'application de renseignements pour renforcer � la fois nos classificateurs et notre mod�le afin qu'il soit prot�g� contre toute utilisation abusive � l'avenir. Risques li�s aux mod�les directs : attaques perturbant l'extraction de mod�les� mesure que les organisations int�grent de plus en plus les LLM dans leurs op�rations principales, la logique propri�taire et la formation sp�cialis�e de ces mod�les sont devenues des cibles de grande valeur. Historiquement, les adversaires cherchant � voler des capacit�s de haute technologie utilisaient des op�rations d'intrusion informatiques conventionnelles pour compromettre les organisations et voler des donn�es contenant des secrets commerciaux. Pour de nombreuses technologies d'IA o� les LLM sont propos�s sous forme de services, cette approche n'est plus n�cessaire ; les acteurs peuvent utiliser un acc�s API l�gitime pour tenter de � cloner � certaines capacit�s des mod�les d'IA.Au cours de l'ann�e 2025, nous n'avons observ� aucune attaque directe contre des mod�les de pointe de la part d'acteurs APT ou d'op�rations d'information (IO) suivis. Cependant, nous avons observ� des attaques d'extraction de mod�les, �galement appel�es attaques de distillation, sur nos mod�les d'IA, afin d'obtenir des informations sur le raisonnement sous-jacent et les processus de cha�ne de pens�e d'un mod�le.
Que sont les attaques d'extraction de mod�les ? Les attaques d'extraction de mod�les (MEA) se produisent lorsqu'un adversaire utilise un acc�s l�gitime pour sonder syst�matiquement un mod�le d'apprentissage automatique mature afin d'extraire des informations utilis�es pour former un nouveau mod�le. Les adversaires qui se livrent � des MEA utilisent une technique appel�e distillation des connaissances (KD) pour extraire les informations d'un mod�le et transf�rer ces connaissances � un autre. C'est pourquoi les MEA sont souvent appel�es � attaques par distillation �.L'extraction de mod�les et la distillation des connaissances qui s'ensuit permettent � un attaquant d'acc�l�rer le d�veloppement de mod�les d'IA � un co�t nettement inf�rieur. Cette activit� constitue en fait une forme de vol de propri�t� intellectuelle (PI).La distillation des connaissances (KD) est une technique courante d'apprentissage automatique utilis�e pour former des mod�les � �tudiants � � partir de mod�les � enseignants � pr�existants. Cela implique souvent d'interroger le mod�le enseignant sur des probl�mes dans un domaine particulier, puis d'effectuer un r�glage fin supervis� (SFT) sur le r�sultat ou d'utiliser le r�sultat dans d'autres proc�dures de formation de mod�les pour produire le mod�le �tudiant. Il existe des utilisations l�gitimes de la distillation, et Google Cloud propose d�j� des offres pour effectuer la distillation. Cependant, la distillation � partir des mod�les Gemini de Google sans autorisation constitue une violation de nos conditions d'utilisation, et Google continue de d�velopper des techniques pour d�tecter et att�nuer ces tentatives.Faits marquants concernant les activit�s adverses augment�es par l'IAAu cours de l'ann�e �coul�e, nous avons constat� de mani�re constante que des attaquants soutenus par des gouvernements utilisaient Gemini � des fins abusives pour des t�ches de codage et de script, la collecte d'informations sur des cibles potentielles, la recherche de vuln�rabilit�s connues du public et la mise en �uvre d'activit�s post-compromission. Au quatri�me trimestre 2025, la compr�hension par le GTIG de la mani�re dont ces efforts se traduisent en op�rations concr�tes s'est am�lior�e, car nous avons constat� des liens directs et indirects entre l'utilisation abusive de Gemini par les acteurs malveillants et leurs activit�s dans le monde r�el.
Soutien � la reconnaissance et au d�veloppement des cibles Les acteurs APT ont utilis� Gemini pour soutenir plusieurs phases du cycle de vie des attaques, en mettant notamment l'accent sur la reconnaissance et le d�veloppement des cibles afin de faciliter la compromission initiale. Cette activit� souligne une �volution vers le phishing assist� par l'IA, o� la vitesse et la pr�cision des LLM permettent de contourner le travail manuel traditionnellement n�cessaire pour �tablir le profil des victimes. Au-del� de la g�n�ration de contenu pour les leurres de phishing, les LLM peuvent servir de multiplicateur de force strat�gique pendant la phase de reconnaissance d'une attaque, permettant aux acteurs malveillants de synth�tiser rapidement des renseignements open source (OSINT) afin de profiler des cibles de grande valeur, d'identifier les principaux d�cideurs dans les secteurs de la d�fense et de cartographier les hi�rarchies organisationnelles. En int�grant ces outils dans leur flux de travail, les acteurs malveillants peuvent passer de la reconnaissance initiale au ciblage actif � un rythme plus rapide et � une �chelle plus large. - UNC6418, un acteur malveillant non identifi�, a utilis� Gemini � mauvais escient pour mener des activit�s de collecte de renseignements cibl�es, en recherchant sp�cifiquement des identifiants de compte et des adresses e-mail sensibles. Peu apr�s, le GTIG a observ� que cet acteur malveillant...
La fin de cet article est r�serv�e aux abonn�s. Soutenez le Club Developpez.com en prenant un abonnement pour que nous puissions continuer � vous proposer des publications.</p></div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>

</body></html>