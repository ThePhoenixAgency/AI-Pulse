<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Easily Build and Share ROCm Kernels with Hugging Face</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Easily Build and Share ROCm Kernels with Hugging Face</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 11/17/2025 1:00:00 AM | Lang: EN |
    <a href="https://huggingface.co/blog/build-rocm-kernels" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/badaoui"><img alt="Abdennacer Badaoui's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/65baa31607366d903890bcf4/6M9WaawnvJ-2h5wSUic1I.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/daniehua"><img alt="Daniel Huang's avatar" src="https://huggingface.co/avatars/50b73ba39800032edb49909438bd8934.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/ColorsWind"><img alt="colorswind's avatar" src="https://huggingface.co/avatars/1183f4aa8259323fb97c2c95e0360e37.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/ftyghome"><img alt="Zesen Liu's avatar" src="https://huggingface.co/avatars/21f57115108a4f5db8734f0cded6682e.svg"></a> </span> </span></p> </div></div> <div><nav><ul><li><a href="#intoduction">Intoduction</a> <ul></ul> </li><li><a href="#build-steps">Build Steps</a> <ul><li><a href="#about-the-kernel">About the kernel</a> <ul></ul> </li><li><a href="#step-1-project-structure">Step 1: Project Structure</a> <ul></ul> </li><li><a href="#step-2-configuration-files-setup">Step 2: Configuration Files Setup</a> <ul></ul> </li><li><a href="#step-3-building-the-kernel">Step 3: Building the Kernel</a> <ul></ul> </li><li><a href="#step-4-uploading-the-kernel-to-the-hub">Step 4: Uploading the kernel to the Hub</a> <ul></ul> </li><li><a href="#step-5-lets-use-it-">Step 5: Let's use it :)</a> <ul></ul> </li></ul> </li><li><a href="#conclusion">Conclusion</a> <ul></ul> </li><li><a href="#related-libraries--hub">Related Libraries &amp; Hub</a> <ul></ul> </li></ul></nav></div><p><a href="https://huggingface.co/blog/assets/build-rocm-kernels/thumbnail.png"><img alt="Easily Build and Share ROCm Kernels with Hugging Face" src="https://huggingface.co/blog/assets/build-rocm-kernels/thumbnail.png"></a></p>
<h2> <a href="#intoduction"> <span></span> </a> <span> Intoduction </span>
</h2>
<p>Custom kernels are the backbone of high-performance deep learning, enabling GPU operations tailored precisely to your workload; whether that’s image processing, tensor transformations, or other compute-heavy tasks. But compiling these kernels for the right architectures, wiring all the build flags, and integrating them cleanly into PyTorch extensions can quickly become a mess of CMake/Nix, compiler errors, and ABI issues, which is not fun. Hugging Face’s <a href="https://github.com/huggingface/kernels"><strong>kernels</strong></a> library makes it easy to build (with <a href="https://github.com/huggingface/kernels/tree/main/builder"><strong>kernel-builder</strong></a>) and share these kernels with the <a href="https://huggingface.co/kernels-community"><strong>kernels-community</strong></a>, with support for multiple GPU and accelerator backends, including CUDA, ROCm, Metal, and XPU. This ensures your kernels are fast, portable, and seamlessly integrated with PyTorch.</p>
<p>In this guide, we focus exclusively on ROCm-compatible kernels and show how to build, test, and share them using <a href="https://github.com/huggingface/kernels/tree/main">kernels</a>. You’ll learn how to create kernels that run efficiently on AMD GPUs, along with best practices for reproducibility, packaging, and deployment.</p>
<p>This ROCm-specific walkthrough is a streamlined version of the original kernel-builder guide. If you’re looking for the broader CUDA-focused version, you can find it here: <a href="https://huggingface.co/blog/kernel-builder">A Guide to Building and Scaling Production-Ready CUDA Kernels</a>.</p>
<h2> <a href="#build-steps"> <span></span> </a> <span> Build Steps </span>
</h2>
<p>We will use the GEMM kernel from <a href="https://github.com/RadeonFlow/RadeonFlow_Kernels">RadeonFlow_Kernels</a> as an example. If you want to go straight to the guide, <a href="#step-1-project-structure">click here</a>.</p>
<h3> <a href="#about-the-kernel"> <span></span> </a> <span> About the kernel </span>
</h3>
<blockquote>
<p>This section was written by the <strong>RadeonFlow GEMM</strong> kernel authors to introduce the kernel.<br>Authors: <a href="https://huggingface.co/ColorsWind">ColorsWind</a>, <a href="https://huggingface.co/ftyghome">Zesen Liu</a>, and <a href="https://huggingface.co/jpy794">Andy</a></p>
</blockquote>
<p>The <strong>RadeonFlow GEMM</strong> kernel is a high-performance, FP8 block-wise matrix multiplication implementation optimized for the AMD Instinct MI300X GPU. GEMM (General Matrix Multiplication) is the core building block behind most deep learning workloads: given two matrices A and B, you compute their product C = A × B. Here it’s implemented in FP8, a low-precision floating-point format that trades a bit of accuracy for much higher throughput and lower memory bandwidth. This kernel was developed for the <a href="https://www.datamonsters.com/amd-developer-challenge-2025">AMD Developer Challenge 2025</a>, it was awarded the <strong>Grand Prize</strong> in <strong>June 2025</strong>, recognizing its excellence in performance and innovation on AMD hardware.</p>
<p>The kernel operates on quantized inputs using the <code>e4m3fnuz</code> floating-point format and applies per-block scaling to preserve accuracy during low-precision computation. The <code>e4m3fnuz</code> format is an FP8 variant with 4 exponent bits and 3 mantissa bits, designed to be efficient for neural network workloads. Because FP8 has a much smaller dynamic range than FP16/FP32, we apply per-block scaling factors (a_scale and b_scale) so that each block of values is rescaled into a numerically “comfortable” range before and after computation, which helps preserve accuracy despite the low precision. It takes the following arguments:</p>
<pre><code>(a, b, a_scale, b_scale, c)
</code></pre>
<p>where <code>a</code> and <code>b</code> are the input matrices, <code>a_scale</code> and <code>b_scale</code> are the scaling factors for <code>a</code> and <code>b</code> respectively,
and <code>c</code> is the output matrix:</p>
<ul>
<li><code>a</code> is K × M in e4m3fnuz</li>
<li><code>b</code> is K × N in e4m3fnuz</li>
<li><code>a_scale</code> is (K // 128) × M in fp32</li>
<li><code>b_scale</code> is (K // 128) × (N // 128) in fp32</li>
<li><code>c</code> is M × N in bf16</li>
</ul>
<p>The kernel is precompiled for specific matrix shapes and assumes a transposed memory layout (as required by the competition). To support additional shapes or alternative memory layouts, you must modify the kernel launcher.</p>
<p>So now that we have a high-performance ROCm kernel, the natural question is: how do we integrate it into a real PyTorch workflow and share it with others? That’s exactly what we’ll cover next, using <code>kernels</code> to structure, build, and publish the ROCm kernel.</p>
<blockquote>
<p>This is a fairly technical guide, but you can still follow it step by step without understanding every detail and everything will work fine. If you’re curious, you can always come back later to dig deeper into the concepts.</p>
</blockquote>
<h3> <a href="#step-1-project-structure"> <span></span> </a> <span> Step 1: Project Structure </span>
</h3>
<p>The Hugging Face Kernel Builder expects your files to be organized like this:</p>
<pre><code>gemm/
├── build.toml
├── gemm
│ └── gemm_kernel.h
├── flake.nix
└── torch-ext ├── torch_binding.cpp ├── torch_binding.h └── gemm └── __init__.py
</code></pre>
<ul>
<li><strong>build.toml</strong>: The project manifest; it’s the brain of the build process.</li>
<li><strong>gemm/</strong>: Your raw CUDA source code where the GPU magic happens.</li>
<li><strong>flake.nix</strong>: The key to a perfectly reproducible build environment.</li>
<li><strong>torch-ext/gemm/</strong>: The Python wrapper for the raw PyTorch operators</li>
</ul>
<p>Sometimes your project might depend on other files, like tests or helper scripts, and you can add them without any issues.
In our case, our project will be structured like this:</p>
<pre><code>gemm/
├── build.toml
├── gemm
│ ├── gemm_kernel.h
│ ├── gemm_kernel_legacy.h
│ ├── transpose_kernel.h
│ └── gemm_launcher.hip
├── include
│ ├── clangd_workaround.h
│ ├── gpu_libs.h
│ ├── gpu_types.h
│ └── timer.h
├── src/utils
│ ├── arithmetic.h
│ └── timer.hip
├── tests/checker
│ ├── checker.cpp
│ ├── metrics.h
│ └── checker.h
├── flake.nix
└── torch-ext ├── torch_binding.cpp ├── torch_binding.h └── gemm └── __init__.py
</code></pre>
<p>If you look at the original files of the gemm kernel in the RadeonFlow Kernels, they are HIP source files with <code>.cpp </code> extensions. As a first step, you need to change these extensions to either .h or .hip depending on their content and usage:</p>
<ul>
<li>Use <code>.h</code> for header files containing kernel declarations, inline functions, or template code that will be included in other files</li>
<li>Use <code>.hip</code> for implementation files containing HIP/GPU code that needs to be compiled separately (e.g., kernel launchers, device functions with complex implementations)</li>
</ul>
<p>In our example, <code>gemm_kernel.h</code>, <code>gemm_kernel_legacy.h</code>, and <code>transpose_kernel.h</code> are header files, while <code>gemm_launcher.hip</code> is a HIP implementation file. This naming convention helps the kernel-builder (<a href="https://github.com/huggingface/kernels/tree/main/builder"><code>kernels/builder</code></a>) correctly identify and compile each file type.</p>
<h3> <a href="#step-2-configuration-files-setup"> <span></span> </a> <span> Step 2: Configuration Files Setup </span>
</h3>
<h4> <a href="#the-buildtoml-manifest"> <span></span> </a> <span> The <code>build.toml</code> Manifest </span>
</h4>
<p>This file orchestrates the entire build. It tells the kernel-builder what to compile and how everything connects. </p>
<pre><code><span>[general]</span>
<span>name</span> = <span>"gemm"</span>
<span>universal</span> = <span>false</span> <span>[torch]</span>
<span>src</span> = [ <span>"torch-ext/torch_binding.cpp"</span>, <span>"torch-ext/torch_binding.h"</span>,
] <span>[kernel.gemm]</span>
<span>backend</span> = <span>"rocm"</span>
<span>rocm-archs</span> = [ <span>"gfx942"</span>,
] <span>depends</span> = [<span>"torch"</span>] <span>src</span> = [ <span>"include/clangd_workaround.h"</span>, <span>"include/gpu_libs.h"</span>, <span>"include/gpu_types.h"</span>, <span>"include/timer.h"</span>, <span>"gemm/gemm_kernel.h"</span>, <span>"gemm/gemm_kernel_legacy.h"</span>, <span>"gemm/gemm_launcher.hip"</span>, <span>"gemm/transpose_kernel.h"</span>, <span>"src/utils/arithmetic.h"</span>, <span>"src/utils/timer.hip"</span>, <span>"tests/checker/metrics.h"</span>,
] <span>include</span> = [<span>"include"</span>]
</code></pre>
<p><strong>general</strong></p>
<p>This section contains general project configuration settings.</p>
<ul>
<li><strong>name</strong> (required): The name of your project. This should match your kernel name and will be used for the Python package.</li>
<li><strong>universal</strong> (optional): the kernel is a universal kernel when set to <code>true</code>. A universal kernel is a pure Python package (no compiled files). Universal kernels do not use the other sections described below. A good example of a universal kernel is a Triton kernel. Default: <code>false</code></li>
</ul>
<p><strong>torch</strong></p>
<p>This section describes the Torch extension configuration. It defines the Python bindings that will expose your kernel to PyTorch.</p>
<ul>
<li><strong>src</strong> (required): A list of source files and headers for the PyTorch extension. In our case, this includes the C++ binding files that create the Python interface.</li>
</ul>
<p><strong>kernel.gemm</strong></p>
<p>Specification of a kernel named "gemm". You can define multiple kernel sections in the same build.toml file if you have multiple kernels.</p>
<ul>
<li><strong>backend</strong> (required): The compute backend for the kernel. We use "rocm" for AMD GPU support.</li>
<li><strong>rocm-archs</strong> (required for ROCm): A list of ROCm architectures that the kernel should be compiled for. "gfx942" targets the MI300 series GPUs.</li>
<li><strong>depends</strong> (required): A list of dependencies. We depend on "torch" to use PyTorch's tensor operations.</li>
<li><strong>include</strong> (optional): Include directories relative to the project root. This helps the compiler find header files.</li>
</ul>
<h4> <a href="#the-flakenix-reproducibility-file"> <span></span> </a> <span> The <code>flake.nix</code> Reproducibility File </span>
</h4>
<p>To ensure anyone can build your kernel on any machine, we use a flake.nix file. It locks the exact version of the kernel-builder and its dependencies. (You can just copy and paste this example and change the description)</p>
<pre><code>{ <span>description</span> = <span>"Flake for GEMM kernel"</span>; <span>inputs</span> = { kernel-builder.<span>url</span> = <span>"github:huggingface/kernels"</span>; }; <span>outputs</span> = { self, kernel-builder, }: kernel-builder.lib.genFlakeOutputs { <span>inherit</span> self; <span>path</span> = ./.; };
}
</code></pre>
<h4> <a href="#writing-the-kernel"> <span></span> </a> <span> Writing the Kernel </span>
</h4>
<p>Now for the GPU code. Inside <code>gemm/gemm_launcher.hip</code>, we define how the GEMM kernel is launched.
Depending on the configuration, we either call the new optimized <code>gemm/gemm_kernel</code> or fall back to the legacy implementation (<code>gemm/gemm_kernel_legacy</code>).</p>
<pre><code><span>// ... previous includes and definitions</span>
<span>extern</span> <span>"C"</span> <span>void</span> <span>run</span><span>(</span>
<span> <span>void</span> *a, <span>void</span> *b, <span>void</span> *as, <span>void</span> *bs, <span>void</span> *c,</span>
<span> <span>int</span> m, <span>int</span> n, <span>int</span> k,</span>
<span> PerfMetrics *metrics, hipStream_t job_stream0</span>
<span>)</span> { <span>const</span> __FP8_TYPE *a_ptr = static_cast&lt;<span>const</span> __FP8_TYPE *&gt;(a); <span>const</span> __FP8_TYPE *b_ptr = static_cast&lt;<span>const</span> __FP8_TYPE *&gt;(b); __BF16_TYPE *c_ptr = static_cast&lt;__BF16_TYPE *&gt;(c); <span>const</span> <span>float</span> *as_ptr = static_cast&lt;<span>const</span> <span>float</span> *&gt;(as); <span>const</span> <span>float</span> *bs_ptr = static_cast&lt;<span>const</span> <span>float</span> *&gt;(bs); KernelTimerScoped <span>timer</span><span>(timers, <span>2LL</span> * m * n * k,</span>
<span> metrics ? &amp;metrics-&gt;entries[<span>0</span>].time : nullptr,</span>
<span> metrics ? &amp;metrics-&gt;entries[<span>0</span>].gflops : nullptr, job_stream0)</span>; <span>// Dispatch GEMM to the fastest available implementation</span> <span>switch</span> (pack_shape(m, n, k)) { DISPATCH_GEMM(<span>1024</span>, <span>1536</span>, <span>7168</span>, <span>256</span>, <span>128</span>, <span>128</span>, <span>4</span>, <span>2</span>, <span>512</span>, <span>4</span>, <span>16</span>); DISPATCH_GEMM(<span>6144</span>, <span>7168</span>, <span>2304</span>, <span>256</span>, <span>128</span>, <span>128</span>, <span>4</span>, <span>2</span>, <span>512</span>, <span>1</span>, <span>16</span>); <span>default</span>: { <span>printf</span>(<span>"Error: Unsupported shape M=%d, K=%d, N=%d\n"</span>, m, k, n); <span>abort</span>(); } }
}
<span>// ...</span>
</code></pre>
<h4> <a href="#registering-a-native-pytorch-operator"> <span></span> </a> <span> Registering a Native PyTorch Operator </span>
</h4>
<p>This step is key. We’re not just making the function available in Python; we’re turning it into a native PyTorch operator. That means it becomes a first-class part of PyTorch itself, accessible through <code>torch.ops</code>.</p>
<p>The file <code>torch-ext/torch_binding.cpp</code> handles this registration. </p>
<pre><code><span>#<span>include</span> <span>&lt;torch/all.h&gt;</span></span>
<span>#<span>include</span> <span>&lt;torch/library.h&gt;</span></span>
<span>#<span>include</span> <span>&lt;hip/hip_runtime.h&gt;</span></span> <span>#<span>include</span> <span>"registration.h"</span></span>
<span>#<span>include</span> <span>"torch_binding.h"</span></span> <span>// Forward declaration of the C function from gemm_launcher.hip</span>
<span>extern</span> <span>"C"</span> { <span><span>struct</span> <span>PerfMetrics</span>;</span> <span>void</span> <span>run</span><span>(<span>void</span> *a, <span>void</span> *b, <span>void</span> *as, <span>void</span> *bs, <span>void</span> *c, <span>int</span> m, <span>int</span> n, <span>int</span> k, PerfMetrics *metrics, hipStream_t job_stream0)</span>;
} <span>void</span> <span>gemm</span><span>(torch::Tensor &amp;out, torch::Tensor <span>const</span> &amp;a, torch::Tensor <span>const</span> &amp;b, </span>
<span> torch::Tensor <span>const</span> &amp;as, torch::Tensor <span>const</span> &amp;bs)</span> { <span>// Validate tensor properties</span> TORCH_CHECK(a.device().is_cuda(), <span>"Input tensor a must be on GPU device"</span>); TORCH_CHECK(b.device().is_cuda(), <span>"Input tensor b must be on GPU device"</span>); TORCH_CHECK(as.device().is_cuda(), <span>"Scale tensor as must be on GPU device"</span>); TORCH_CHECK(bs.device().is_cuda(), <span>"Scale tensor bs must be on GPU device"</span>); TORCH_CHECK(out.device().is_cuda(), <span>"Output tensor out must be on GPU device"</span>); TORCH_CHECK(a.is_contiguous(), <span>"Input tensor a must be contiguous"</span>); TORCH_CHECK(b.is_contiguous(), <span>"Input tensor b must be contiguous"</span>); TORCH_CHECK(as.is_contiguous(), <span>"Scale tensor as must be contiguous"</span>); TORCH_CHECK(bs.is_contiguous(), <span>"Scale tensor bs must be contiguous"</span>); TORCH_CHECK(out.is_contiguous(), <span>"Output tensor out must be contiguous"</span>); <span>// Get matrix dimensions from tensor shapes</span> <span>// Assuming a is [M, K], b is [K, N], out is [M, N]</span> <span>int</span> M = a.size(<span>0</span>); <span>int</span> K = a.size(<span>1</span>); <span>int</span> N = b.size(<span>1</span>); TORCH_CHECK(b.size(<span>0</span>) == K, <span>"Matrix dimensions mismatch: a.size(1) != b.size(0)"</span>); TORCH_CHECK(out.size(<span>0</span>) == M, <span>"Output tensor dimension mismatch: out.size(0) != M"</span>); TORCH_CHECK(out.size(<span>1</span>) == N, <span>"Output tensor dimension mismatch: out.size(1) != N"</span>); <span>// Use default HIP stream (stream 0)</span> <span>const</span> hipStream_t stream = <span>0</span>; <span>// Call the C function</span> run(a.data_ptr(), b.data_ptr(), as.data_ptr(), bs.data_ptr(), out.data_ptr(), M, N, K, nullptr, stream);
} TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) { ops.def(<span>"gemm(Tensor! out, Tensor a, Tensor b, Tensor a_scale, Tensor b_scale) -&gt; ()"</span>); ops.impl(<span>"gemm"</span>, torch::kCUDA, &amp;gemm);
} REGISTER_EXTENSION(TORCH_EXTENSION_NAME)
</code></pre>
<p>The <code>torch_binding.h</code> file contains function declarations. For instance, the <code>gemm</code> kernel has the following declaration in <code>torch_binding.h</code>:</p>
<pre><code><span>#<span>pragma</span> once</span> <span>#<span>include</span> <span>&lt;torch/torch.h&gt;</span></span> <span>void</span> <span>gemm</span><span>(torch::Tensor &amp;out, torch::Tensor <span>const</span> &amp;a, torch::Tensor <span>const</span> &amp;b, </span>
<span> torch::Tensor <span>const</span> &amp;as, torch::Tensor <span>const</span> &amp;bs)</span>;
</code></pre>
<h4> <a href="#setting-up-the-__init__py-wrapper"> <span></span> </a> <span> Setting up the <code>__init__.py</code> wrapper </span>
</h4>
<p>In <code>torch-ext/gemm/</code> we need an <code>__init__.py</code> file to make this directory a Python package and to expose our custom operator in a user-friendly way. </p>
<pre><code><span>from</span> typing <span>import</span> <span>Optional</span>
<span>import</span> torch
<span>from</span> ._ops <span>import</span> ops <span>def</span> <span>gemm</span>(<span>a: torch.Tensor, b: torch.Tensor, as_: torch.Tensor, bs: torch.Tensor, </span>
<span> out: <span>Optional</span>[torch.Tensor] = <span>None</span></span>) -&gt; torch.Tensor: <span>if</span> out <span>is</span> <span>None</span>: <span># Create output tensor with appropriate shape and dtype</span> M, K = a.shape K_b, N = b.shape <span>assert</span> K == K_b, <span>f"Matrix dimension mismatch: A has <span>{K}</span> cols, B has <span>{K_b}</span> rows"</span> <span># Output should be BF16 type on the same device as inputs</span> out = torch.empty((M, N), dtype=torch.bfloat16, device=a.device) ops.gemm(out, a, b, as_, bs) <span>return</span> out
</code></pre>
<h3> <a href="#step-3-building-the-kernel"> <span></span> </a> <span> Step 3: Building the Kernel </span>
</h3>
<p>The kernel builder uses Nix for building kernels. You can build or run the kernels directly if you have Nix installed on your system. We recommend installing Nix in the following way:</p>
<ul>
<li><strong>Linux</strong>: use the <a href="https://nixos.org/download/">official Nix installer</a>.</li>
<li><strong>macOS</strong>: use the <a href="https://docs.determinate.systems/determinate-nix/">Determinate Nix installer</a>. In addition, Xcode 16.x is currently required to build kernels.</li>
</ul>
<h4> <a href="#getting-started-with-nix"> <span></span> </a> <span> Getting Started with Nix </span>
</h4>
<p>First of all, run this:</p>
<pre><code>nix flake update
</code></pre>
<p>This generates a <code>flake.lock</code> file that pins the kernel builder and all its transitive dependencies. Commit both <code>flake.nix</code> and <code>flake.lock</code> to your repository to ensure that kernel builds are reproducible.</p>
<p>Since the kernel builder depends on many packages (e.g., every supported PyTorch version), it is recommended to enable the Hugging Face cache to avoid expensive rebuilds:</p>
<pre><code><span># Install cachix and configure the cache</span>
cachix use huggingface
</code></pre>
<p>Or run it once without installing cachix permanently:</p>
<pre><code><span># Use cachix without installing it</span>
nix run nixpkgs<span>#cachix -- use huggingface</span>
</code></pre>
<h4> <a href="#building-kernels-with-nix"> <span></span> </a> <span> Building Kernels with Nix </span>
</h4>
<p>A kernel that has a <code>flake.nix</code> file can be built with the build-and-copy command:</p>
<pre><code><span>cd</span> Build_RadeonFlow_Kernels/gemm
nix build . -L
</code></pre>
<p>The compiled kernel will then be in the local <code>build/</code> directory.</p>
<h4> <a href="#development-shell-for-local-development"> <span></span> </a> <span> Development Shell for Local Development </span>
</h4>
<p>The kernel-builder provides shells for developing kernels. In such a shell, all required dependencies are available, as well as <code>build2cmake</code> for generating project files:</p>
<pre><code>$ nix develop
$ build2cmake generate-torch build.toml
$ cmake -B build-ext
$ cmake --build build-ext
</code></pre>
<p>If you want to test the kernel as a Python package, you can do so. <code>nix develop</code> will automatically create a virtual environment in <code>.venv</code> and activate it:</p>
<pre><code>$ nix develop
$ build2cmake generate-torch build.toml
$ pip install --no-build-isolation -e .
</code></pre>
<p>Development shells are available for every build configuration. For instance, you can get a Torch 2.7 development shell with ROCm 6.3 using:</p>
<pre><code>$ <span>rm</span> -rf .venv <span># Remove existing venv if any</span>
$ nix develop .<span>#devShells.torch27-cxx11-rocm63-x86_64-linux</span>
</code></pre>
<h3> <a href="#step-4-uploading-the-kernel-to-the-hub"> <span></span> </a> <span> Step 4: Uploading the kernel to the Hub </span>
</h3>
<p>Now that we built our kernel, we can test it and upload it to the Hub. </p>
<h4> <a href="#building-the-kernel-for-all-pytorch-and-rocm-versions"> <span></span> </a> <span> Building the Kernel for All PyTorch and ROCm Versions </span>
</h4>
<p>One small thing we'll want to do before we share is clean up all of the development artifacts that were generated during the build process to avoid uploading unnecessary files. </p>
<pre><code>build2cmake clean build.toml </code></pre>
<p>To build the kernel for all supported versions of PyTorch and ROCm, the kernel-builder tool automates the process: </p>
<pre><code><span># Outside of the dev shell, run the following command</span>
<span># if you are inside of the sandbox you can leave with `exit`</span>
nix build . -L
</code></pre>
<blockquote>
<p><strong>Note:</strong><br>This process may take a while, as it will build the kernel for all supported versions of PyTorch and ROCm.<br>The output will be in the <code>result</code> directory.</p>
</blockquote>
<p>The last step is to move the results into the expected build directory (this is where the kernels library will look for them). </p>
<pre><code><span>mkdir</span> -p build
rsync -av --delete --<span>chmod</span>=Du+w,Fu+w result/ build/
</code></pre>
<h4> <a href="#pushing-to-the-hugging-face-hub"> <span></span> </a> <span> Pushing to the Hugging Face Hub </span>
</h4>
<p>Pushing the build artifacts to the Hub will make it straightforward for other developers to use your kernel. We can use the <code>kernels upload</code> command for this: </p>
<pre><code>kernels upload &lt;path_to_kernel&gt; --repo_id hub-username/img2gray
</code></pre> You can also follow a standard git-based process for the upload. <p>First, create a new repo: </p>
<pre><code>hf repo create gemm
</code></pre>
<blockquote>
<p>Make sure you are logged in to the Hugging Face Hub using huggingface-cli login.</p>
</blockquote>
<p>Now, in your project directory, connect your project to the new repository and push your code:</p>
<pre><code><span># Initialize git and connect to the Hugging Face Hub</span>
git init
git remote add origin https://huggingface.co/&lt;your-username&gt;/gemm <span># Pull the changes (just the default .gitattributes file)</span>
git pull origin main
git xet install
git checkout -b main <span># Update to use Xet for the binary files</span>
git xet track <span>"*.so"</span> <span># Add and commit your changes (being careful to only include the necessary files</span>
<span># since our build2cmake command generated a lot of dev-specific files)</span>
git add \ build/ gemm/ include/ src/utils tests/checker \ torch-ext/torch_binding.cpp torch-ext/torch_binding.h torch-ext/gemm \ flake.nix flake.lock build.toml git commit -m <span>"feat: Created a compliant gemm kernel"</span>
git push -u origin main
</code></pre> <p>Fantastic! Your kernel is now on the Hugging Face Hub, ready for others to use and fully compliant with the kernels library. </p>
<h3> <a href="#step-5-lets-use-it-"> <span></span> </a> <span> Step 5: Let's use it :) </span>
</h3>
<p>With the <strong>kernels</strong> library, you don't "install" the kernel in the traditional sense. You load it directly from its Hub repository, which automatically registers the new operator.</p>
<pre><code><span>import</span> torch
<span>from</span> kernels <span>import</span> get_kernel <span># Load the kernel from the Hub</span>
gemm = get_kernel(<span>"kernels-community/gemm"</span>) <span># Matrix dimensions (must be supported - see gemm_launcher.cpp)</span>
M, N, K = <span>1024</span>, <span>1536</span>, <span>7168</span>
QUANT_SIZE = <span>128</span> <span># Setup device</span>
device = torch.device(<span>"cuda"</span>) <span># Create inputs - kernel expects A:(K,M), B:(K,N)</span>
A_fp32 = torch.randn(M, K, device=device)
B_fp32 = torch.randn(K, N, device=device) <span># Convert to FP8</span>
A_fp8 = A_fp32.to(torch.float8_e4m3fnuz)
B_fp8 = B_fp32.to(torch.float8_e4m3fnuz) <span># Create scale factors (uniform scaling)</span>
A_scale = torch.ones(K // QUANT_SIZE, M, device=device, dtype=torch.float32)
B_scale = torch.ones(K // QUANT_SIZE, N // QUANT_SIZE, device=device, dtype=torch.float32) C = torch.zeros(M, N, device=device, dtype=torch.bfloat16) <span># Use the kernel</span>
result = gemm.gemm(A_fp8, B_fp8, A_scale, B_scale, C)
</code></pre>
<p>That's it! Your ROCm kernel is now ready to use from the Hugging Face Hub.</p>
<h2> <a href="#conclusion"> <span></span> </a> <span> Conclusion </span>
</h2>
<p>Building and sharing ROCm kernels with the Hugging Face is now easier than ever. With a clean, reproducible workflow powered by Nix and seamless integration into PyTorch, developers can focus on optimizing performance rather than setup. Once built, your custom kernel can be shared on the Hugging Face Hub; making it instantly accessible to the community and usable across projects with just a few lines of code. </p>
<h2> <a href="#related-libraries--hub"> <span></span> </a> <span> Related Libraries &amp; Hub </span>
</h2>
<ul>
<li><a href="https://github.com/huggingface/kernels">kernels</a> – Library to build, manage and load kernels from the Hub. It contains the <a href="https://github.com/huggingface/kernels/tree/main/builder">kernel-builder</a> tooling.</li>
<li><a href="https://huggingface.co/kernels-community">Kernels Community Hub</a> – Share and discover kernels from the community.</li>
</ul>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>