<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Apple study looks into how people expect to interact with AI agents</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Apple study looks into how people expect to interact with AI agents</h1>
  <div class="metadata">
    Source: 9to5Mac | Date: 2/13/2026 12:15:29 AM | Lang: EN |
    <a href="https://9to5mac.com/2026/02/12/apple-study-looks-into-how-people-expect-to-interact-with-ai-agents/" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <figure> <img src="https://9to5mac.com/wp-content/uploads/sites/6/2025/07/machine-learning-research.jpg?quality=82&amp;strip=all&amp;w=1600" alt=""> </figure> <p>A team of Apple researchers set out to understand what real users expect from AI agents, and how they’d rather interact with them. Here’s what they found.</p> <h2>Apple explores UX trends for the era of AI agents</h2> <p>In the study, titled <a href="https://machinelearning.apple.com/research/mapping">Mapping the Design Space of User Experience for Computer Use Agents</a>, a team of four Apple researchers says that while the market has been investing heavily in the development and evaluation of AI agents, some aspects of the user experience have been overlooked: how users might want to interact with them, and what these interfaces should look like.</p> <p>To explore that, they divided the study into two phases: first, they identified the main UX patterns and design considerations that AI labs have been building into existing AI agents. Then, they tested and refined those ideas through hands-on user studies with an interesting method called Wizard of Oz.</p> <figure><img src="https://9to5mac.com/wp-content/uploads/sites/6/2026/02/ai-agent-study.jpg?quality=82&amp;strip=all&amp;w=1024" alt=""></figure> <p>By observing how those design patterns hold up in real-world user interactions, they were able to identify which current AI agent designs align with user expectations, and which fall short.</p> <h2>Phase 1: The taxonomy</h2> <p>The researchers looked into nine desktop, mobile, and web-based agents, including;</p> <ul>
<li>Claude Computer Use Tool</li> <li>Adept</li> <li>OpenAI Operator</li> <li>AIlice</li> <li>Magentic-UI</li> <li>UI-TARS</li> <li>Project Mariner</li> <li>TaxyAI</li> <li>AutoGLM</li>
</ul> <p>Then, they consulted with “8 practitioners who are designers, engineers, or researchers working in the domains of UX or AI at a large technology company,” which helped them map out a comprehensive taxonomy with four categories, 21 subcategories, and 55 example features covering the key UX considerations behind computer-using AI agents.</p> <p>The four main categories included:</p> <ul>
<li><strong>User Query</strong>: how users input commands</li> <li><strong>Explainability of Agent Activities</strong>: what information to present to the user about agent actions</li> <li><strong>User Control</strong>: how users can intervene</li> <li><strong>Mental Model &amp; Expectations</strong>: how to help users understand the agent’s capabilities</li>
</ul> <p>In essence, that framework spanned everything from aspects of the interface that let agents present their plans to users, to how they communicate their capabilities, surface errors, and allow users to step in when something goes wrong.</p> <p>With all of that at hand, they moved on to phase 2.</p> <h2>Phase 2: The Wizard-of-Oz study</h2> <p>The researchers recruited 20 users with prior experience with AI agents, and asked them to interact with an AI agent via a chat interface to perform either a vacation rental task or an online shopping task.</p> <figure><img src="https://9to5mac.com/wp-content/uploads/sites/6/2026/02/ai-agent-study-interface.jpg?quality=82&amp;strip=all&amp;w=1024" alt=""></figure> <p>From the study:</p> <blockquote>
<p>Participants were provided with a mock user chat interface through which they could interact with an “agent” played by the researcher. Meanwhile, the participant were also presented with the agent’s execution interface, where the researcher acted as the agent and interacted with the Ul on screen based on the participant’s command. On the user chat interface, participants could enter textual queries in natural language, which then appeared in the chat thread. Then, the “agent” began execution, where the researcher controlled the mouse and keyboard on their end to simulate the agent’s actions on the web page. When the researcher completed the task, they entered a shortcut key that posted a “task completed” message in the chat thread. During execution, participants could use an interrupt button to stop the agent, and a message “agent interrupted” would appear in the chat.</p>
</blockquote> <p>In other words, unbeknownst to the users, the AI agent was, in reality, a researcher sitting in the next room, who would read the text instructions and perform the requested task.</p> <p>For each task (vacation rental or online shopping), participants were requested to perform six functions with the help of the AI agent, some of which the agent would either purposely fail (such as getting stuck in a navigation loop) or make intentional mistakes (such as selecting something different from the user’s instruction).</p> <p>At the end of each session, the researchers asked participants to reflect on their experience and propose features or changes to improve the interaction.</p> <p>They also analyzed video recordings and chat logs from each session to identify recurring themes in user behavior, expectations, and pain points when interacting with the agent.</p> <h2>Main findings</h2> <p>Once all was said and done, the researchers found that users want visibility into what AI agents are doing, but not to micromanage every step, otherwise they could just perform the tasks themselves.</p> <p>They also concluded that users want different agent behaviors depending on whether they’re exploring options, or executing a familiar task. Likewise, user expectations change based on whether they’re familiar with the interface. The more unfamiliar they were, the more they wanted transparency, intermediate steps, explanations, and confirmation pauses (even in low-risk scenarios).</p> <p>They also found that people want more control when actions carry real consequences (such as making purchases, changing account or payment details, or contacting other people on their behalf), and also found that trust breaks down quickly when agents make silent assumptions or errors.</p> <p>For instance, when the agent encountered ambiguous choices on a page, or deviated from the original plan without clearly flagging it, participants instructed the system to pause and ask for clarification, rather than just pick something seemingly at random and move on.</p> <p>In that same vein, participants reported discomfort when the agent wasn’t transparent about making a particular choice, especially when that choice could lead to the wrong product being selected.</p> <p>All in all, this is an interesting study for app developers looking to adopt agentic capabilities on their apps, and you can read it in full <a href="https://machinelearning.apple.com/research/mapping">here</a>.</p> <h4>Accessory deals on Amazon</h4> <ul>
<li><strong><a href="https://www.amazon.com/Apple-Cancellation-Translation-Headphones-High-Fidelity/dp/B0FQFB8FMG?tag=marcmendes-20" target="_blank">AirPods Pro 3</a></strong></li> <li><a href="https://www.amazon.com/Apple-MX542LL-A-AirTag-Pack/dp/B0D54JZTHY/?tag=marcmendes-20" target="_blank">Apple AirTag 4 Pack</a></li> <li><a href="https://www.amazon.com/Beats-Charging-Durable-Tangle-Free-Compatible/dp/B0F1W7B5R1?tag=marcmendes-20" target="_blank">Beats USB-C to USB-C Woven Short Cable</a></li> <li><a href="https://www.amazon.com/gp/product/B0F6T6N2B1?tag=marcmendes-20" target="_blank">Wireless CarPlay adapter</a></li> <li><a href="https://amzn.to/3KmIQN7" target="_blank">Logitech MX Master 4</a></li>
</ul> <p><a target="_blank" href="https://google.com/preferences/source?q=https://9to5mac.com"> <img src="https://9to5mac.com/wp-content/themes/ninetofive/dist/images/google-preferred-source-badge-dark.png" alt="Add 9to5Mac as a preferred source on Google"> <img src="https://9to5mac.com/wp-content/themes/ninetofive/dist/images/google-preferred-source-badge-light.png" alt="Add 9to5Mac as a preferred source on Google"> </a> </p> </div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollStep(-1)">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollStep(1)">▼</button>
  </div>
  <script>
    function scrollStep(direction) {
      var step = Math.max(220, Math.round(window.innerHeight * 0.72));
      window.scrollBy({ top: direction * step, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up') scrollStep(-1);
      if (data.direction === 'down') scrollStep(1);
      if (data.direction === 'top') window.scrollTo({ top: 0, behavior: 'smooth' });
      if (data.direction === 'bottom') window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    });
  </script>
</body>
</html>