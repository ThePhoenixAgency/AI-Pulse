<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Why LLMs are plateauing – and what that means for software security</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.8; color: #e2e8f0; max-width: 800px; margin: 40px auto; padding: 0 20px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.5em; }
  .metadata { color: #94a3b8; font-size: 0.9em; margin-bottom: 2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 1em; }
  img { max-width: 100%; height: auto; border-radius: 8px; }
  a { color: #00d9ff; }
  p { margin-bottom: 1em; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 15px; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 15px; border-radius: 6px; overflow-x: auto; }

  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }

</style>
</head>
<body>
  <h1>Why LLMs are plateauing – and what that means for software security</h1>
  <p>Surprisingly, this heightened vulnerability risk was agnostic across the different model types, with no significant difference between the smaller and larger models.</p><p>Whilst the ability to generate syntactically correct code has improved dramatically, security remains stubbornly stagnant. Simply scaling models or updating training data is insufficient to meaningfully improve security outcomes.</p><p>The notable exception is OpenAI’s reasoning GPT-5 models, which take extra steps to think through problems before producing code. These models achieved substantially higher security pass rates of 70% and above, compared to 50-60% for previous generations.</p><p>However in contrast, GPT-5-chat, a non-reasoning variant, lagged at 52%, suggesting that reasoning alignment, not model scale, drives these gains. It’s possible OpenAI’s tuning examples here include high-quality secure code or explicitly teach models to reason about security trade-offs to achieve this higher rate.</p><p>Language-specific trends have also emerged. Many of the AI models perform much worse on Java code generation tasks than any other <a href="https://www.techradar.com/news/best-laptop-for-programming">coding</a> languages, with security pass rates at less than 30%, while Python, C# and JavaScript generally fall between 38% and 45%.</p><p>At the same time, newer models, especially reasoning-tuned ones, are performing better at generating secure C# and Java code, likely reflecting AI labs’ focus on major enterprise languages.</p><h2>Why is LLM security stagnating? </h2><p>The root of the problem lies in the nature of the training data, made up of public code samples scraped from the internet. As a result, the data contains both secure and insecure examples, including deliberately vulnerable projects like WebGoat – an insecure Java application used for security training.</p><p>The models then treat all these examples as legitimate ways to satisfy a coding request, learning patterns that don’t reliably distinguish safe from unsafe implementation.</p><p>With most LLMs training on this publicly available data, there are similar patterns in how they produce security risks. As the data remains largely unchanged over time, and is increasingly supplemented with synthetic and AI-generated code, model security performance has remained stagnant across generations of models.</p><p>This also helps explain why Java is particularly problematic. Java has a long history as a server-side implementation language, and predates widespread recognition of vulnerabilities like SQL injection.</p><p>Its training data must therefore contain many more security vulnerabilities than other languages like C# or <a href="https://www.techradar.com/best/best-ide-for-python">Python</a>, leading models to perform significantly worse on Java-specific tasks.</p><h2>The security blind spot in vibe coding</h2><p>These findings raise huge concerns for AI-assisted development and the growing popularity of vibe coding. While these practices accelerate productivity, developers rarely specify security constraints when prompting LLMs, which would dramatically improve the security of generated code.</p><p>For example, a developer might prompt a model to generate a database query without specifying whether it should construct using a prepared statement (safe) or string concatenation (unsafe).</p><p>This effectively leaves those decisions down to the LLMs which, as the findings show, choose incorrectly nearly half the time. Alarmingly, this issue shows little sign of improving.</p><p>And the risks are already surfacing in practice. A recent incident with an AI coding tool on the Replit platform caused the deletion of an entire live production <a href="https://www.techradar.com/best/best-database-software">database</a> during a code freeze – a clear warning of what can go wrong when AI-generated code is trusted without sufficient guardrails.</p><h2>The implications for developers and organizations</h2><p>Given these persistent shortfalls, relying on model improvements alone is not a viable security strategy. While newer reasoning models offer a clear advantage, security performance remains highly variable and even the best-performing models introduce vulnerabilities in nearly a third of cases.</p><p>AI coding assistants are powerful tools, but they cannot replace skilled developers or comprehensive security programs.</p><p>A layered approach to risk management is essential: maintaining continuous scanning and validation using static analysis (SAST) and software composition analysis (SCA), regardless of code origin, and proactive blocking of malicious dependencies are crucial to preventing vulnerabilities from reaching production pipelines.</p><p>AI-powered remediation tools also assist developers by providing real-time guidance and automated fixes, yet responsibility for secure implementation ultimately remains human.</p><p>AI coding assistants and agentic workflows represent the future of software development and will continue to evolve at a rapid pace. But while LLMs have become adept at generating functionally correct code, they continue to produce security vulnerabilities at a troublingly high rate – an issue that won’t be easy to fix.</p><p>The challenge for every organization is ensuring security evolves alongside these new capabilities. Addressing this requires security-specific training, reasoning alignment, and a recognition that security cannot be an afterthought if we want to prevent the accumulation of masses of security debt.</p><p>Until AI labs prioritize security in training and alignment processes, developers and security teams must treat AI-generated code as an inherently untrusted input – a principle that must be considered in day-to-day <a href="https://www.techradar.com/pro/best-vibe-coding-tools">vibe coding</a>.</p><p><em></em><a href="https://www.techradar.com/news/best-endpoint-security-software"><em>We've featured the best endpoint protection software.</em></a></p><p><em>This article was produced as part of TechRadarPro's Expert Insights channel where we feature the best and brightest minds in the technology industry today. The views expressed here are those of the author and are not necessarily those of TechRadarPro or Future plc. If you are interested in contributing find out more here: </em><a href="https://www.techradar.com/news/submit-your-story-to-techradar-pro" target="_blank"><em>https://www.techradar.com/news/submit-your-story-to-techradar-pro</em></a></p>
</div></div>
  </div>
</body>
</html>