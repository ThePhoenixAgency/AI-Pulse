<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>BigCodeArena: Judging code generations end to end with code executions</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>BigCodeArena: Judging code generations end to end with code executions</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 10/7/2025 11:37:25 AM | Lang: EN |
    <a href="https://huggingface.co/blog/bigcode/arena" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div> <p><span><span><a href="https://huggingface.co/terryyz"><img alt="Terry Yue Zhuo's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg"></a> </span> </span></p> </div> <div><nav><ul><li><a href="#motivation">Motivation</a> <ul></ul> </li><li><a href="#the-bigcodearena-platform">The BigCodeArena Platform</a> <ul><li><a href="#real-time-execution">Real-Time Execution</a> <ul></ul> </li><li><a href="#multi-language--framework-support">Multi-Language &amp; Framework Support</a> <ul></ul> </li><li><a href="#interactive-testing">Interactive Testing</a> <ul></ul> </li><li><a href="#multi-turn-conversations">Multi-Turn Conversations</a> <ul></ul> </li></ul> </li><li><a href="#what-weve-learned-5-months-of-community-evaluation">What We've Learned: 5 Months of Community Evaluation</a> <ul><li><a href="#programming-topics-in-the-wild">Programming Topics in the Wild</a> <ul></ul> </li><li><a href="#language-and-framework-popularity">Language and Framework Popularity</a> <ul></ul> </li><li><a href="#user-interaction-patterns">User Interaction Patterns</a> <ul></ul> </li><li><a href="#model-rankings-from-community-votes">Model Rankings from Community Votes</a> <ul></ul> </li></ul> </li><li><a href="#two-new-benchmarks-bigcodereward-and-autocodearena">Two New Benchmarks: BigCodeReward and AutoCodeArena</a> <ul><li><a href="#bigcodereward-evaluating-reward-models-for-code">BigCodeReward: Evaluating Reward Models for Code</a> <ul></ul> </li><li><a href="#autocodearena-automated-code-generation-benchmarks">AutoCodeArena: Automated Code Generation Benchmarks</a> <ul></ul> </li></ul> </li><li><a href="#try-it-yourself">Try It Yourself</a> <ul></ul> </li><li><a href="#open-source-everything">Open Source Everything</a> <ul></ul> </li><li><a href="#whats-next">What's Next?</a> <ul></ul> </li><li><a href="#conclusion">Conclusion</a> <ul></ul> </li><li><a href="#acknowledgements">Acknowledgements</a> <ul></ul> </li><li><a href="#citation">Citation</a> <ul></ul> </li></ul></nav></div><p>Evaluating the quality of AI-generated code is notoriously difficult. While humans can easily spot whether a piece of code "looks right," determining if it actually works correctly, handles edge cases properly, and produces the intended result requires running and testing it. This is why today, we're thrilled to announce <strong>BigCodeArena</strong> -- the first human-in-the-loop platform for evaluating code generation models through execution.</p>
<p>Inspired by LMArena for LLMs, we've built a platform that allows anyone to compare code generation models side-by-side, but with a crucial difference: <strong>you can actually run the code and see what it produces</strong>. Just submit a coding task, watch two different models generate solutions, execute both programs, and vote on which model produced better results. The outcomes are organized into a leaderboard that displays the community's highest-rated models.</p> <h2> <a href="#motivation"> <span></span> </a> <span> Motivation </span>
</h2>
<p>The field of code generation has long struggled with reliable evaluation methods. Traditional benchmarks like HumanEval test code against predefined test cases, but these represent only a tiny fraction of real-world programming tasks. Human evaluation platforms exist for general chatbots, but they fall short for code: reading raw source code and mentally simulating its execution is cognitively demanding and error-prone, especially for longer programs or complex UI applications.</p>
<p>Consider this scenario:</p>
<blockquote>
<p>You ask two AI models to build a responsive photo gallery website. Both generate code that looks syntactically correct. But which one is actually better? Without running the code, it's nearly impossible to tell. One might produce a beautiful, functional grid layout, while the other might have subtle bugs or poor styling that only become apparent when rendered in a browser.</p>
</blockquote>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/SspOHYTm-BT-zIlIV3Rdg.png"><img alt="bigcodearena-demo" src="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/SspOHYTm-BT-zIlIV3Rdg.png"></a></p>
<p>This observation led us to a key insight: <strong>execution feedback is essential for humans to judge code quality reliably.</strong> That's exactly what BigCodeArena provides.</p>
<h2> <a href="#the-bigcodearena-platform"> <span></span> </a> <span> The BigCodeArena Platform </span>
</h2>
<p>BigCodeArena extends the Chatbot Arena framework with powerful features specifically designed for code evaluation:</p>
<h3> <a href="#real-time-execution"> <span></span> </a> <span> Real-Time Execution </span>
</h3>
<p>Every code snippet generated by models is automatically executed in isolated sandbox environments. Whether it's a Python script, a React web app, a PyGame game, or a C++ algorithm, you can see the actual output, not just the source code.</p>
<h3> <a href="#multi-language--framework-support"> <span></span> </a> <span> Multi-Language &amp; Framework Support </span>
</h3>
<p>We currently support 10 languages (Python, JavaScript, TypeScript, HTML, C, C++, Java, Go, Rust, and Markdown) and 8 execution environments:</p>
<ul>
<li><strong>Web Frameworks</strong>: React, Vue, Core Web (vanilla HTML/CSS/JS)</li>
<li><strong>Python Frameworks</strong>: Streamlit, Gradio, PyGame</li>
<li><strong>Diagrams</strong>: Mermaid</li>
<li><strong>General Purpose Interpreters</strong>: Python and JavaScript code interpreters, plus compiled language runners</li>
</ul>
<h3> <a href="#interactive-testing"> <span></span> </a> <span> Interactive Testing </span>
</h3>
<p>Unlike static code comparison, you can actually interact with the generated applications:</p>
<ul>
<li>Click buttons and test UI elements in web apps</li>
<li>Play the games generated by models</li>
<li>Edit the code and re-run it to test modifications</li>
<li>View visual outputs like plots, charts, and diagrams</li>
</ul>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/2AvLXvIP1VE1moRw9BHS_.png"><img alt="bigcodearena" src="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/2AvLXvIP1VE1moRw9BHS_.png"></a></p>
<h3> <a href="#multi-turn-conversations"> <span></span> </a> <span> Multi-Turn Conversations </span>
</h3>
<p>Real programming isn't one-and-done. BigCodeArena supports multi-turn interactions, allowing you to refine requirements, ask for features to be added, or request bug fixes -- just like working with a real coding assistant.</p>
<h2> <a href="#what-weve-learned-5-months-of-community-evaluation"> <span></span> </a> <span> What We've Learned: 5 Months of Community Evaluation </span>
</h2>
<p>Since launching in February 2025, BigCodeArena has collected over <strong>14,000 conversations</strong> from more than 500 unique users, with <strong>4,700+ high-quality preference votes</strong> comparing 10 frontier LLMs.</p>
<h3> <a href="#programming-topics-in-the-wild"> <span></span> </a> <span> Programming Topics in the Wild </span>
</h3>
<p>Our users have explored remarkably diverse coding scenarios:</p>
<ul>
<li><strong>Web Design (36%)</strong>: Building responsive websites, interactive dashboards, and web applications</li>
<li><strong>Problem Solving (23%)</strong>: Algorithms, data structures, and computational challenges</li>
<li><strong>Game Development (16%)</strong>: Creating interactive games with physics, collision detection, and graphics</li>
<li><strong>Scientific Computing (14%)</strong>: Data analysis, visualization, and numerical simulations</li>
<li><strong>Creative Coding (8%)</strong>: Artistic visualizations, generative art, and experimental interfaces</li>
<li><strong>Diagram Creation (3%)</strong>: Flowcharts, system architectures, and data visualizations</li>
</ul>
<h3> <a href="#language-and-framework-popularity"> <span></span> </a> <span> Language and Framework Popularity </span>
</h3>
<p>Python dominates with over 4,000 conversations, followed by JavaScript/TypeScript (3,359), HTML (1,601), and C++ (642). Among frameworks, direct Python interpreters lead usage (6,000 sessions), with React (2,729), Core Web (1,574), Streamlit (1,254), and PyGame (1,087) also seeing heavy use.</p>
<h3> <a href="#user-interaction-patterns"> <span></span> </a> <span> User Interaction Patterns </span>
</h3>
<p>Most interactions are focused and efficient: 76% of conversations consist of just 2 turns (one request, one response), with a mean conversation length of 4.12 messages. However, the platform supports extended multi-turn debugging sessions when needed, with some conversations exceeding 10 turns as users refine complex applications.</p>
<h3> <a href="#model-rankings-from-community-votes"> <span></span> </a> <span> Model Rankings from Community Votes </span>
</h3>
<p>From our 14K conversations, we filtered for high-quality pairwise comparisons: conversations with at least two turns and actual code execution. This yielded <strong>4,731 voting samples</strong>, with each evaluated model receiving at least 700 votes. We aggregate these votes into <strong>Elo ratings</strong> using the Bradley-Terry model, which estimates the probability that one model beats another based on head-to-head comparisons.</p>
<p>To ensure robust rankings, we use 100 bootstrap resamples to construct 95% confidence intervals, so we can identify statistically significant performance differences between models.</p>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/rEnROVN53CxyMHsLh223J.jpeg"><img alt="elo_ratings_comparison_page-0001" src="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/rEnROVN53CxyMHsLh223J.jpeg"></a></p>
<p>We evaluate models under three settings to control for different factors:</p>
<ol>
<li><strong>All Data</strong>: Uses all pairwise comparisons regardless of execution environment or programming language</li>
<li><strong>Environment Matched</strong>: Only compares models when both were executed in the same sandbox (e.g., both in React or both in PyGame)</li>
<li><strong>Language Matched</strong>: Further restricts comparisons to the same programming language</li>
</ol>
<p>Rankings remain remarkably consistent across all three settings, revealing clear performance tiers:</p>
<p><strong>Top Tier</strong>: <strong>o3-mini</strong> and <strong>o1-mini</strong> consistently lead with the highest Elo ratings and tight confidence intervals. These models maintain top performance regardless of environment or language constraints, showing strong robustness across coding scenarios. <strong>Claude-3.5-Sonnet</strong> follows closely, particularly excelling when language is controlled.</p>
<p><strong>Mid Tier</strong>: <strong>GPT-4o</strong>, <strong>o1</strong>, and <strong>Gemini-2.0-Pro/Flash</strong> form a competitive middle tier. GPT-4o shows some sensitivity to language matching, suggesting room for improvement in multilingual consistency.</p>
<p><strong>Open Source Models</strong>: <strong>Qwen2.5</strong> variants and <strong>Llama-3.3-70B</strong> lag behind frontier proprietary models, highlighting the performance gap that remains between leading closed and open models.</p>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/5iB4XRwOBzjEdQM5tqVoU.jpeg"><img alt="combined_performance_heatmaps_page-0001" src="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/5iB4XRwOBzjEdQM5tqVoU.jpeg"></a></p>
<p><em>Figure: Overall win rate heatmaps (percentage of all pairwise comparisons won) of each model in the sessions across languages (<em>left</em>) and execution environments (<em>right</em>). For each category, we only keep models that appear in at least 3 conversation sessions.</em></p>
<h4> <a href="#performance-across-languages"> <span></span> </a> <span> Performance Across Languages </span>
</h4>
<p>Breaking down performance by programming language reveals interesting patterns:</p>
<ul>
<li>Top-tier models like <strong>o3-mini</strong> and <strong>o1-mini</strong> achieve dominant win rates in mainstream languages like Python, Java, and C++</li>
<li><strong>Gemini-2.0-Pro</strong> shows particular strength in Rust, achieving the highest win rate in that category</li>
<li>Different models exhibit distinct areas of expertise, with frontier models excelling in different niches</li>
<li>Open models like Qwen2.5 variants show inconsistent performance, particularly struggling with Rust and Go</li>
</ul>
<h4> <a href="#performance-across-execution-environments"> <span></span> </a> <span> Performance Across Execution Environments </span>
</h4>
<p>Analyzing win rates by execution environment reveals how models handle different runtime contexts:</p>
<p><strong>Robust Performers</strong>: <strong>o3-mini</strong> maintains consistently strong performance across React, Streamlit, Gradio, Core Web, and PyGame, demonstrating excellent environmental adaptability.</p>
<p><strong>Stable but Selective</strong>: <strong>Claude-3.5-Sonnet</strong> and <strong>Gemini-2.0-Flash</strong> show generally stable performance but with reduced win rates in complex UI-heavy environments like Vue and Mermaid.</p>
<p><strong>Framework-Specific Weaknesses</strong>: <strong>Qwen2.5</strong> models, while competitive in some web frameworks (Core Web, React), struggle significantly with interactive and visualization-oriented environments like PyGame, Vue, and Mermaid. These environments often require precise handling of control flow, graphics rendering, and package dependencies.</p>
<p>These results highlight an important insight: <strong>aggregate Elo scores don't tell the whole story</strong>. Some models remain brittle under specific runtime constraints, and execution environment matters significantly for real-world deployment.</p>
<h2> <a href="#two-new-benchmarks-bigcodereward-and-autocodearena"> <span></span> </a> <span> Two New Benchmarks: BigCodeReward and AutoCodeArena </span>
</h2>
<p>To advance research beyond crowdsourced evaluation, we're releasing two complementary benchmarks:</p>
<h3> <a href="#bigcodereward-evaluating-reward-models-for-code"> <span></span> </a> <span> BigCodeReward: Evaluating Reward Models for Code </span>
</h3>
<p>Building on our 4,700+ preference votes, BigCodeReward tests how well LLMs can judge code quality when acting as reward models. The key finding? <strong>Execution results dramatically improve judgment accuracy.</strong></p>
<p>When models can see execution outputs (screenshots of web apps, game visuals, program logs), their alignment with human preferences increases substantially:</p>
<ul>
<li>Claude-Sonnet-4: <strong>56.7% → 62.3%</strong> accuracy</li>
<li>GPT-4o: <strong>54.6% → 63.8%</strong> accuracy </li>
<li>Qwen2.5-VL-72B: <strong>58.7% → 66.2%</strong> accuracy</li>
</ul>
<p>This reinforces our core thesis: you can't reliably judge code without running it -- and this applies to both humans <em>and</em> AI judges.</p>
<h3> <a href="#autocodearena-automated-code-generation-benchmarks"> <span></span> </a> <span> AutoCodeArena: Automated Code Generation Benchmarks </span>
</h3>
<p>Inspired by Arena-Hard-Auto, AutoCodeArena provides a scalable way to evaluate new models without waiting for thousands of human votes. We carefully selected 600 representative prompts from our crowdsourced data, spanning all programming topics and frameworks.</p>
<p>Using automated LLM judges (Claude-3.7-Sonnet) to evaluate code execution results against a GPT-4.1 baseline, we can rapidly benchmark new models. This approach enables weekly leaderboard updates as new models are released.</p>
<p>Our automated benchmark evaluated 20+ cutting-edge models, including recently released systems:</p>
<p><strong>Top Performers:</strong></p>
<ol>
<li><strong>GPT-5</strong> -- Establishes new state-of-the-art by a significant margin</li>
<li><strong>Claude-Opus-4</strong> and <strong>Claude-Sonnet-4</strong> -- Strong second tier, excelling in reasoning-heavy tasks</li>
<li><strong>Qwen3-Coder</strong>, <strong>Kimi-K2</strong>, <strong>GLM-4.5</strong> -- Leading open models that narrow the gap with mid-tier proprietary systems</li>
</ol>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/NzyobCl-pYwgR-6ggufMe.jpeg"><img alt="overall_performance" src="https://cdn-uploads.huggingface.co/production/uploads/62b7fb545233925f253531c8/NzyobCl-pYwgR-6ggufMe.jpeg"></a></p>
<p><em>Figure: Win rates of recent LLMs on AutoCodeArena against a GPT-4.1 baseline, judged by Claude-3.7-Sonnet. The 50% mark represents parity with GPT-4.1. Models above this line outperform the baseline, while those below underperform. Error bars show 95% confidence intervals. Note: Claude-3.7-Sonnet is excluded from rankings to avoid self-judgment bias, and GPT-4.1 appears only as the reference baseline.</em></p>
<p>The results show that while proprietary models maintain an edge, open-source models are rapidly closing the gap, with some approaching GPT-4.1-level performance.</p>
<h2> <a href="#try-it-yourself"> <span></span> </a> <span> Try It Yourself </span>
</h2>
<p>BigCodeArena is open to everyone -- no account required! Visit <a href="https://huggingface.co/spaces/bigcode/arena">https://huggingface.co/spaces/bigcode/arena</a> to:</p>
<ul>
<li>Compare code from more recent frontier LLMs (e.g., Qwen3, DeepSeek-V3.X, and other proprietary models)</li>
<li>Test web apps, games, visualizations, and algorithms</li>
<li>See real execution results, not just source code</li>
<li>Vote on your preferences to help improve the leaderboard</li>
<li>Explore multi-turn coding conversations</li>
</ul>
<p>Whether you're building a React dashboard, creating a PyGame game, solving algorithmic challenges, or generating creative visualizations, BigCodeArena lets you see which models truly deliver.</p>
<h2> <a href="#open-source-everything"> <span></span> </a> <span> Open Source Everything </span>
</h2>
<p>Following the BigCode Project's commitment to transparency, we're releasing:</p>
<ul>
<li><strong>Codebase</strong>: Full evaluation pipelines and Gradio application source (<a href="https://github.com/bigcode-project/bigcodearena">GitHub</a>)</li>
<li><strong>Crowdsourced Data</strong>: 14K raw conversations and 4.7K preference votes (<a href="https://huggingface.co/collections/bigcode/bigcodearena-68cd3a196e5147cc45f8ea3d">HuggingFace Collection</a>)</li>
<li><strong>Benchmarks</strong>: BigCodeReward and AutoCodeArena datasets</li>
</ul>
<h2> <a href="#whats-next"> <span></span> </a> <span> What's Next? </span>
</h2>
<p>We envision BigCodeArena as a long-term project that evolves with the community:</p>
<ul>
<li><strong>Expanded Language Support</strong>: More programming languages and frameworks.</li>
<li><strong>Live Benchmarks</strong>: Continuously refreshed evaluation prompts to prevent overfitting </li>
<li><strong>Agent-Based Evaluation</strong>: Using AI agents to interact with web apps for deeper testing</li>
<li><strong>Better Reward Models</strong>: Advancing automated code quality assessment</li>
<li><strong>Community Contributions</strong>: We welcome new execution environments, evaluation criteria, and model additions. <a href="https://huggingface.co/spaces/bigcode/arena/discussions?new_pr=true">PRs</a> are always welcome!</li>
</ul>
<h2> <a href="#conclusion"> <span></span> </a> <span> Conclusion </span>
</h2>
<p>Evaluating code isn't like evaluating text -- you need to run it, test it, and interact with it. BigCodeArena makes this possible at scale, combining human judgment with real execution feedback to create the most reliable evaluation platform for code generation models.</p>
<p>Join us in building the future of code generation evaluation. Write a prompt, compare the models, and vote for your favorite. Your feedback helps the entire community understand which models truly deliver on the promise of AI-assisted programming.</p>
<p>We'd love to hear your feedback! Connect with us on <a href="https://github.com/bigcode-project/bigcodearena">GitHub</a>, join discussions in the <a href="https://huggingface.co/spaces/bigcode/arena/discussions">Hugging Face Space community tab</a>, or reach out to the BigCode Project at <code>contact@bigcode-project.org</code>.</p>
<h2> <a href="#acknowledgements"> <span></span> </a> <span> Acknowledgements </span>
</h2>
<p>We thank <a href="https://huggingface.co/lvwerra">Leandro von Werra</a> for his valuable suggestions and feedback on the blog.</p>
<h2> <a href="#citation"> <span></span> </a> <span> Citation </span>
</h2>
<pre><code>@article{zhuo2025bigcodearena, title={BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution}, author={Terry Yue Zhuo, Xiaolong Jin, Hange Liu, Juyong Jiang, Tianyang Liu, Chen Gong, Bhupesh Bishnoi, Vaisakhi Mishra, Marek Suppa, Noah Ziems, Saiteja Utpala, Ming Xu, Guangyu Song, Kaixin Li, Yuhan Cao, Bo Liu, Zheng Liu, Sabina Abdurakhmanova, Wenhao Yu, Mengzhao Jia, Jihan Yao, Kenneth Hamilton, Kumar Shridhar, Minh Chien Vu, Dingmin Wang, Jiawei Liu, Zijian Wang, Qian Liu, Binyuan Hui, Meg Risdal, Ahsen Khaliq, Atin Sood, Zhenchang Xing, Wasi Uddin Ahmad, John Grundy, David Lo, Banghua Zhu, Xiaoning Du, Torsten Scholak, Leandro von Werra}, year={2025}
}
</code></pre>
<hr>
<p><strong>Try BigCodeArena now</strong>: <a href="https://huggingface.co/spaces/bigcode/arena">Hugging Face Space</a></p>
<p><strong>Read the paper</strong>: <a href="https://huggingface.co/papers/2510.08697">Hugging Face</a></p>
<p><strong>Run the code</strong>: <a href="https://github.com/bigcode-project/bigcodearena">GitHub</a></p>
<p><strong>Explore the collection</strong>: <a href="https://huggingface.co/collections/bigcode/bigcodearena-68cd3a196e5147cc45f8ea3d">Hugging Face Collection</a></p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollStep(-1)">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollStep(1)">▼</button>
  </div>
  <script>
    function scrollStep(direction) {
      var step = Math.max(220, Math.round(window.innerHeight * 0.72));
      window.scrollBy({ top: direction * step, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up') scrollStep(-1);
      if (data.direction === 'down') scrollStep(1);
      if (data.direction === 'top') window.scrollTo({ top: 0, behavior: 'smooth' });
      if (data.direction === 'bottom') window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    });
  </script>
</body>
</html>