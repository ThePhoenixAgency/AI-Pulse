<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Kimina-Prover-RL</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>Kimina-Prover-RL</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 8/14/2025 12:13:01 PM | <a href="https://huggingface.co/blog/AI-MO/kimina-prover-rl" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/thibautbar"><img alt="Thibaut Barroyer's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/67c7666cd51b75fc80596316/5l5x9PRzLuPwuauN-ZXDW.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/TBUGTB"><img alt="Jonas Bayer's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PdtmLEqIkalWWw0tYND6o.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/mavi88"><img alt="Marina Vinyes's avatar" src="https://huggingface.co/avatars/7bfb5978f1c05d7e0a3b9dcdda90859b.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/mertunsal"><img alt="Mert Unsal's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/66775fab8c58c5a12ae629ac/rwaS-fPTO2be-UcqSL5y0.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/HaimingW"><img alt="Haiming Wang's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/65f9013b25145caed6c725a4/Qf0qqqwzU3xitBWkA4ZqC.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/XiaoHLim"><img alt="Xiaohan Lin's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/635e7a8b875f3e8c2756a4fe/CvATc3ZlDnYWyqq92ynep.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/MantasBaksys"><img alt="MantasBaksys's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/66e0354fa9f50756756f8635/9BqMWr7sB7i9r-uE-kO2h.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/ahhwuhu"><img alt="Junqi Liu's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/677f8f9b5b7a01fbc70c609e/bjpFv-Zq0o47mEQaXGwuI.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/dsantosmarco"><img alt="Marco Dos Santos's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/64146fc68ebec947b8c2fddc/tTTdYhcg5cp2U8LGZustC.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/floodsung"><img alt="Flood Sung's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6343d01a08c017b2c042305d/cmJrYkGs9RjDAKfMYCqdW.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/zhenzhe"><img alt="Ying's avatar" src="https://huggingface.co/avatars/f395bfa8e59f2fc80ccbef882473461e.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/hehepig166"><img alt="Zhu Zekai's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dWgbpoxTIpU9WUdlYiWBv.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/rookiemango"><img alt="lujianqiao's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/64d59835bcab729cb48e3258/cDTTOozta7zCkjkHmbPCQ.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/desaxce"><img alt="Hugues de Saxcé's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/650c5818fb7a51088736587b/u8FIlFoylRB3iSwQaBQQv.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/ebony59"><img alt="Ebony Zhang's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6437b938fac5ea753f1d0ba8/6xxQEmjKmrS1yPUZSoYQP.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/BoltonBailey"><img alt="Bolton Bailey's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/683f0dd9fde42c0912397f59/Kw0b5T64FGHMSn8YJ9RbK.webp"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/UnluckyOrangutan"><img alt="Frederick Pu's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/mnSJ7cP3UfzyA9qkqaRx-.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/evariste-liu"><img alt="Zhengying Liu's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6445f82f5691ca69b0df747a/lVILbKYiX6Aw9I97HlPo1.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/liyongsea"><img alt="LI Jia's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1645633727256-621660534b06824e1e5022ed.jpeg"></a> </span> </span></p> </div></div> <p><strong>A slimmed-down training pipeline from Kimina Prover, with core features and full compatibility with verl.</strong></p>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/67c7666cd51b75fc80596316/uUzRaw1J4qgQ8wAqPmnQq.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/67c7666cd51b75fc80596316/uUzRaw1J4qgQ8wAqPmnQq.png"></a></p>
<p>We are happy to introduce kimina-prover-rl, an open-source training pipeline for formal theorem proving in Lean 4, based on a structured reasoning-then-generation paradigm inspired by DeepSeek-R1.</p>
<p>This training pipelinee is a simplified version of the system we used to train <a href="https://huggingface.co/blog/AI-MO/kimina-prover">Kimina Prover</a>, preserving the key components of the system and offering full compatibility with the open-source Verl framework.</p>
<p>It is released as part of a <a href="https://github.com/project-numina/kimina-prover-rl/tree/main/recipe/kimina_prover_rl">fork of Verl</a> containing the complete training recipe in <code>recipe/kimina-prover-rl</code>, allowing anyone to reproduce our experiments or adapt the setup to their own models and datasets. All information to setup and launch the pipeline can be found in the <a href="https://github.com/project-numina/kimina-prover-rl/blob/main/recipe/kimina_prover_rl/README.md">README</a> of the recipe.</p>
<p>As a result of this training pipeline, we are releasing two models:</p>
<ul>
<li><strong><a href="https://huggingface.co/AI-MO/Kimina-Prover-RL-1.7B">AI-MO/Kimina-Prover-RL-1.7B</a></strong>, a 1.7B-parameter model that achieves <strong>76.63% Pass@32</strong> on the MiniF2F benchmark — setting a new state of the art for open-source models in this size category</li>
<li><strong><a href="https://huggingface.co/AI-MO/Kimina-Prover-RL-0.6B">AI-MO/Kimina-Prover-RL-0.6B</a></strong>, a 0.6B-parameter model that achieves <strong>71.30% Pass@32</strong> on the MiniF2F benchmark — also setting a new state of the art for open-source models in this size category.</li>
</ul>
<h2> <a href="#introduction"> </a> <span> Introduction </span>
</h2>
<p>kimina-prover-rl is a training pipeline designed to teach large language models to solve formal proof goals in Lean 4, using a two-stage output structure: a natural language reasoning trace followed by corresponding Lean code.</p>
<p>This paradigm, inspired by DeepSeek-R1, enables the model to separate planning from execution, promoting explainability, error recovery, and stronger generalization.</p>
<p>To train models under this reasoning framework, we apply GRPO — a reinforcement learning approach tailored for LLMs. This open-source version of the training pipeline of Kimina-prover is implemented using the RL library <a href="https://github.com/volcengine/verl">Verl</a>.</p>
<p>During the rollout phase of GRPO, the model generates N outputs for each prompt. A reward of 1 is assigned to any output whose Lean code is successfully verified by Lean using our <a href="https://github.com/project-numina/kimina-lean-server">kimina-lean-server</a>.</p>
<p>Two main features are added to this framework:</p>
<ul>
<li>A format checking reward to teach the model to structure its outputs</li>
<li>An error correction turn to encourage the model to learn from failure signals</li>
</ul>
<h2> <a href="#kimina-client"> </a> <span> Kimina-Client </span>
</h2>
<p>During training, a large number of Lean 4 proof candidates must be verified simultaneously. To handle this efficiently, we require a high-throughput verification system.</p>
<p>To meet this need, Numina and Kimi have developed an open-source server called <a href="https://github.com/project-numina/kimina-lean-server">kimina-lean-server</a>, which supports parallel proof checking at scale using Lean 4.</p>
<p>To simplify integration, we also provide <a href="https://www.piwheels.org/project/kimina-client/">kimina-client</a>, a lightweight Python package (available on PyPI) that offers a clean interface for interacting with the server’s API.</p>
<h2> <a href="#dataset"> </a> <span> Dataset </span>
</h2>
<p>We train using <a href="https://huggingface.co/datasets/AI-MO/Kimina-Prover-Promptset">Kimina-Prover-Promptset</a>, a curated subset of the <a href="https://huggingface.co/datasets/AI-MO/NuminaMath-LEAN">NuminaMath-LEAN</a> dataset.</p>
<p>For this training setup, we filter and preprocess the dataset as follows:</p>
<ul>
<li><strong>Remove easy problems</strong> with a historical win rate above <strong>0.5</strong> to only keep challenging statements in the dataset.</li>
<li><strong>Generate variants</strong> of existing problems to increase diversity using Gemini</li>
<li><strong>Duplicate hard problems</strong> to give them more weight during training</li>
</ul>
<p>The resulting dataset contains challenging, high-value problems for improving Lean 4 theorem proving models.</p>
<p>NuminaMath-LEAN-RL is also the dataset used to train <a href="https://huggingface.co/AI-MO/Kimina-Prover-RL-1.7B">AI-MO/Kimina-Prover-RL-1.7B</a> and <a href="https://huggingface.co/AI-MO/Kimina-Prover-RL-0.6B">AI-MO/Kimina-Prover-RL-0.6B</a>.</p>
<p>Example input format:</p>
<pre><code>Think about and solve the following problems step by step in Lean 4. # Problem:
Find all primes that are the difference of the fourth powers of two integers. # Formal Statement:
'''lean4
import Mathlib theorem number_theory_4487 : {p : ℕ | p.Prime ∧ ∃ a b, p = a ^ 4 - b ^ 4} = ∅ := by
'''
</code></pre>
<h2> <a href="#format-reward"> </a> <span> Format reward </span>
</h2>
<p>The core idea of our reasoning training pipeline is to structure the LLM output into two stages. One thinking block followed by one lean4 block:</p>
<ol>
<li>A reasoning block ( ... )</li>
<li>A Lean 4 code block</li>
</ol>
<pre><code>&lt;think&gt;
To prove the statement, we use induction on n.
The base case is trivial, and the inductive step follows by applying the hypothesis.
&lt;/think&gt; '''lean4
theorem my_thm : ∀ n, f n = g n := by induction n with | zero =&gt; simp | succ n ih =&gt; simp [ih]
'''
</code></pre>
<p>Each rollout is <strong>verified</strong> to ensure that this format is respected. If the output is malformed — e.g., missing the <code>&lt;think&gt;</code> block or misplacing the code — the model receives a <strong>zero reward</strong>, regardless of whether the proof is actually valid.</p>
<p>This enforces consistency and teaches the model to structure its outputs reliably.</p>
<p>In kimina-prover, these checks go beyond simply verifying the presence of <code>&lt;think&gt;</code> and lean4 blocks:</p>
<ul>
<li>Ensuring there is exactly one <code>&lt;think&gt;...&lt;/think&gt;</code> block and one lean4 code block per output.</li>
<li>Rejecting outputs with repetitive reasoning lines, which often indicate hallucinated or degenerate generations.</li>
<li>Checking that tactic blocks inside the thinking section are present in sufficient number and contain enough non-comment lines.</li>
<li>Applying thresholds on comment density (in both reasoning and Lean code), to penalize overly verbose or boilerplate outputs.</li>
<li>Comparing the semantic alignment between tactics described in blocks and the final Lean code using a matching score (e.g., Intersection-over-Union or subcode coverage).</li>
<li>Penalize unnecessarily long responses, encouraging the model to use tokens more efficiently while still giving complete replies</li>
</ul>
<p>Only generations that pass all these checks are considered well-formatted and can receive a reward. This structured filtering improves training stability and encourages clean reasoning.</p>
<h2> <a href="#error-correction"> </a> <span> Error correction </span>
</h2>
<p>To make training more informative, we have added an <strong>error correction mechanism</strong> that gives the model a chance to fix its own failed proofs.</p>
<p>When a rollout fails (e.g., due to a Lean error or incorrect proof), we:</p>
<ol>
<li>Store the full prompt, response, and Lean feedback.</li>
<li>Create a <strong>new training sample</strong> where the model is explicitly prompted to revise its previous reasoning/code.</li>
</ol>
<p>This encourages the model to learn from failure signals as lean feedback is provided during the training.</p>
<p>It also enables multi-turn interaction chains, where feedback from Lean is injected as part of the prompt, and the model is rewarded for successfully debugging its own output.</p>
<p>Because multi-turn responses can get long, we allow only one error-fix turn and cap the error message at a set number of tokens.</p>
<h2> <a href="#overview-of-the-pipeline"> </a> <span> Overview of the pipeline </span>
</h2>
<p>The work <strong>Understanding R1-Zero-Like Training: A Critical Perspective</strong> claims there's optimization bias in GRPO, that leads to artificially longer responses, especially for incorrect outputs.</p>
<p>We also noticed this behaviour during our experimentations and we used DrGPO for our optimization. DrGRPO aggregates token-level losses by normalizing with a global constant to eliminate length bias.</p>
<p>The configuration file provided in the repository is for a 8 GPUs setup.</p>
<p>The model that we are finetunning is the <strong><a href="https://huggingface.co/AI-MO/Kimina-Prover-Distill-1.7B">AI-MO/Kimina-Prover-Distill-1.7B</a></strong>. This model is a finetuned version of <strong><a href="https://huggingface.co/Qwen/Qwen3-1.7B">Qwen/Qwen3-1.7B</a></strong> with cold start data generated from our <strong><a href="https://huggingface.co/AI-MO/Kimina-Prover-72B">AI-MO/Kimina-Prover-72B</a></strong> model.</p>
<p>At every step, 256 samples are fetched in the training dataset. One out of two is an error correction sample. We generate 8 rollout per samples so 2048 generations. You can increase to 16 or 32 rollouts if you are using more than one node.</p>
<p>We evaluate the model every 5 training steps, using the best@8 metric from verl to have fast validation steps. You can increase to best@16 or 32 if you are using more than one node. We are evaluating the performance before and the after the error correction turn. For each failed reponse we allow the model to do one more tentative to fix its proof.</p>
<h2> <a href="#results"> </a> <span> Results </span>
</h2>
<p>After a few training steps, we observe a consistent improvement in performance. In this section we will discuss the training metrics after 48 hours of training on 8 H100 GPUs.</p>
<p>By step 85, the pipeline improves the model's accuracy by 4 points, reaching 70% for the best@8 metric and 74% after the error correction turn:</p>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/67c7666cd51b75fc80596316/pCXpFDfuoS87AN2iJJ4k7.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/67c7666cd51b75fc80596316/pCXpFDfuoS87AN2iJJ4k7.png"></a></p>
<p>In parallel, we observe that the number of format errors steadily decreases over the course of training, indicating that the model is learning to produce structurally valid outputs.</p>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/67c7666cd51b75fc80596316/SfhtLJ7VjGswmc6JRG8w4.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/67c7666cd51b75fc80596316/SfhtLJ7VjGswmc6JRG8w4.png"></a></p>
<p>Finally, and as expected under the DeepSeek-R1-style training setup, the average token length of the model’s outputs increases with training — a signal that the model is learning to reason in longer, more structured traces.</p>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/67c7666cd51b75fc80596316/AEavg6sbiAyQzpdYLpqiR.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/67c7666cd51b75fc80596316/AEavg6sbiAyQzpdYLpqiR.png"></a></p>
<p>After the training, we evaluated the model using pass@32 with and without error fixing. We were able to improve the performances of our 1.7B model by more than 3% at pass@32 on MiniF2F:</p>
<div> <table> <thead><tr>
<th>Model</th>
<th>Pass@32</th>
<th>Pass@32 with error fixing</th>
</tr> </thead><tbody><tr>
<td>AI-MO/Kimina-Prover-Distill-1.7B</td>
<td>72.95%</td>
<td>75.41%</td>
</tr>
<tr>
<td>AI-MO/Kimina-Prover-RL-1.7B</td>
<td>76.23%</td>
<td>77.87%</td>
</tr>
</tbody> </table>
</div>
<p>Using this training pipeline we also finetuned a 0.6B model, improving its performances by more than 2%.</p>
<div> <table> <thead><tr>
<th>Model</th>
<th>Pass@32</th>
</tr> </thead><tbody><tr>
<td>AI-MO/Kimina-Prover-Distill-0.6B</td>
<td>68.85%</td>
</tr>
<tr>
<td>AI-MO/Kimina-Prover-RL-0.6B</td>
<td>71.30%</td>
</tr>
</tbody> </table>
</div>
<h2> <a href="#conclusion"> </a> <span> Conclusion </span>
</h2>
<p>With <a href="https://github.com/project-numina/kimina-prover-rl/tree/main/recipe/kimina_prover_rl">Kimina-Prover-RL</a>, we provide a lightweight yet powerful reinforcement learning pipeline for training Lean 4 theorem provers.</p>
<p>By combining structured reasoning, format rewards, and error correction, we achieve state-of-the-art results for open-source models in the 0.6B–1.7B parameter range.</p>
<p>Alongside the models, we are also releasing a <a href="https://github.com/project-numina/kimina-prover-rl/blob/main/recipe/kimina_prover_rl">fork of Verl</a> containing the full training recipe in <code>recipe/kimina-prover-rl</code>, so the community can reproduce our results or adapt the pipeline to their own datasets and models.</p>
<p>We hope this release will serve as a solid foundation for the community to experiment with RL training in formal reasoning, and to push the limits of open-source automated theorem proving in Lean 4.</p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>