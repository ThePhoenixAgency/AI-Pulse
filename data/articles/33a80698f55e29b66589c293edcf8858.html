<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>GitHub - enjector/microgpt-c</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>GitHub - enjector/microgpt-c</h1>
  <div class="metadata">
    Source: Hacker News Show | Date: 2/17/2026 12:06:20 AM | <a href="https://github.com/enjector/microgpt-c" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><h1>MicroGPT-C</h1><a href="#microgpt-c"></a></div>
<p>A <strong>zero-dependency, pure C99</strong> implementation of a GPT-style character-level language model.</p>
<p>The algorithm faithfully matches <a href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95">Andrej Karpathy's </a></p><pre><a href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95"><code>microgpt.py</code></a></pre> — same architecture, same training loop, same sampling — but compiles to native code with optional compiler-driven SIMD auto-vectorisation for dramatically faster training and inference.<p></p>
<blockquote>
<p><strong>Train a GPT in 20 ms. Generate names in microseconds. No Python. No PyTorch. No GPU.</strong></p>
</blockquote>
<hr>
<div><h2>What Is This?</h2><a href="#what-is-this"></a></div>
<p>MicroGPT-C is a minimal, readable implementation of a GPT (Generative Pre-trained Transformer) — the same family of models behind ChatGPT, but stripped down to its essential algorithm. It trains a tiny character-level language model that learns to generate realistic human names from scratch.</p>
<p>The goal is <strong>education and experimentation</strong>: understand how attention, backpropagation, and the Adam optimiser actually work at the lowest level, without any framework abstractions.</p>
<table>
<thead>
<tr>
<th>Audience</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Students &amp; educators</strong></td>
<td>Study attention, softmax, Adam, and backprop in readable C — no framework magic</td>
</tr>
<tr>
<td><strong>Embedded / edge engineers</strong></td>
<td>Entire model fits in <strong>&lt; 50 KB</strong> RAM; runs on MCUs with no runtime dependencies</td>
</tr>
<tr>
<td><strong>Researchers</strong></td>
<td>Auditable baseline for quantisation, custom layers, or optimiser experiments</td>
</tr>
<tr>
<td><strong>Rapid prototypers</strong></td>
<td>Train → iterate in milliseconds; test tokenisers, vocabularies, data formats</td>
</tr>
</tbody>
</table>
<hr>
<div><h2>Quick Start</h2><a href="#quick-start"></a></div>
<div><pre><span><span>#</span> Linux / macOS</span>
chmod +x build.sh
./build.sh
./build/microgpt</pre></div>
<div><pre><span><span>::</span> Windows</span>
build.bat
build\Release\microgpt.exe</pre></div>
<p>The build automatically copies </p><pre><code>data/names.txt</code></pre> next to the executable.<p></p>
<hr>
<div><h2>Performance</h2><a href="#performance"></a></div>
<p>Measured on the same workload (1,000 training steps, 20 inference samples) — C vs the reference Python:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Python</th>
<th>C (fp64)</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Training time</strong></td>
<td>~93 s</td>
<td><strong>0.02 s</strong></td>
<td><strong>~4,600×</strong></td>
</tr>
<tr>
<td><strong>Training throughput</strong></td>
<td>~0.1 k tok/s</td>
<td><strong>~289 k tok/s</strong></td>
<td><strong>~2,800×</strong></td>
</tr>
<tr>
<td><strong>Steps/sec</strong></td>
<td>~11</td>
<td><strong>~40,000</strong></td>
<td><strong>~3,600×</strong></td>
</tr>
<tr>
<td><strong>Inference time</strong></td>
<td>~0.74 s</td>
<td><strong>&lt; 1 ms</strong></td>
<td><strong>~700×+</strong></td>
</tr>
<tr>
<td><strong>Inference rate</strong></td>
<td>~27 samples/s</td>
<td><strong>20,000 samples/s</strong></td>
<td><strong>~740×</strong></td>
</tr>
<tr>
<td><strong>Token throughput</strong></td>
<td>—</td>
<td><strong>109,000 tok/s</strong></td>
<td>—</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>INT8 quantised build:</strong> ~25% slower training than fp64 on this tiny model, but <strong>~8× smaller</strong> weight storage — ideal for constrained devices.</p>
</blockquote>
<hr>
<div><h2>Architecture</h2><a href="#architecture"></a></div>
<p>A single-layer, decoder-only Transformer following the GPT-2 design:</p>
<div><pre><code>Input → Token Embed + Pos Embed → RMSNorm → Self-Attention (4 heads, causal) → Residual → RMSNorm → MLP (fc1 → ReLU → fc2, 4× width) → Residual → Linear (lm_head) → Softmax → next-token probabilities
</code></pre></div>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Embedding dim</td>
<td>16</td>
</tr>
<tr>
<td>Attention heads</td>
<td>4</td>
</tr>
<tr>
<td>Layers</td>
<td>1</td>
</tr>
<tr>
<td>Context length</td>
<td>16</td>
</tr>
<tr>
<td>Total parameters</td>
<td>~4,600</td>
</tr>
<tr>
<td>Weight memory (fp64)</td>
<td>~37 KB</td>
</tr>
<tr>
<td>Weight memory (INT8)</td>
<td>~4.6 KB</td>
</tr>
<tr>
<td>Training memory</td>
<td>~144 KB</td>
</tr>
<tr>
<td>Inference memory</td>
<td>&lt; 50 KB</td>
</tr>
</tbody>
</table>
<p>Training uses the <strong>Adam</strong> optimiser with linear learning-rate decay (configurable in </p><pre><code>microgpt.h</code></pre>).<p></p>
<hr>
<div><h2>Build Options</h2><a href="#build-options"></a></div> <table>
<thead>
<tr>
<th>Platform</th>
<th>Standard</th>
<th>SIMD (faster)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux/macOS</td>
<td><pre><code>./build.sh</code></pre></td>
<td><pre><code>./build.sh --simd</code></pre></td>
</tr>
<tr>
<td>Windows</td>
<td><pre><code>build.bat</code></pre></td>
<td><pre><code>build.bat simd</code></pre></td>
</tr>
</tbody>
</table>
<div><h3>SIMD auto-vectorisation</h3><a href="#simd-auto-vectorisation"></a></div>
<p>The </p><pre><code>--simd</code></pre> flag enables compiler-driven <strong>auto-vectorisation</strong> of the core dot products, matrix multiplications, and normalisations. On x86-64 the compiler targets the best available instruction set (SSE4, AVX2, etc.) via <pre><code>-march=native</code></pre>; on MSVC it enables <pre><code>/arch:AVX2</code></pre>. This gives a measurable speed-up on larger models without any hand-written intrinsics — the compiler re-writes the scalar loops into SIMD instructions automatically.<p></p>
<div><pre><span><span>#</span> Linux / macOS — auto-detect best ISA</span>
./build.sh --simd <span><span>#</span> CMake directly</span>
cmake -DMICROGPT_SIMD=ON ..
cmake --build <span>.</span> --config Release</pre></div>
<div><h3>INT8 quantised build</h3><a href="#int8-quantised-build"></a></div>
<p>Weights are stored as 8-bit integers with per-matrix scales — the forward pass dequantises on the fly; Adam updates an fp64 master copy and requantises each step. This reduces weight storage by <strong>~8×</strong> (37 KB → 4.6 KB) at a small accuracy/speed trade-off.</p>
<table>
<thead>
<tr>
<th>Platform</th>
<th>Standard</th>
<th>SIMD</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux/macOS</td>
<td><pre><code>./build_quantised.sh</code></pre></td>
<td><pre><code>./build_quantised.sh --simd</code></pre></td>
</tr>
<tr>
<td>Windows</td>
<td><pre><code>build_quantised.bat</code></pre></td>
<td><pre><code>build_quantised.bat simd</code></pre></td>
</tr>
</tbody>
</table>
<div><h3>CMake directly</h3><a href="#cmake-directly"></a></div>
<div><pre>mkdir build <span>&amp;&amp;</span> <span>cd</span> build
cmake ..
cmake --build <span>.</span> --config Release <span><span>#</span> With INT8 quantisation</span>
cmake -DQUANTIZATION_INT8=ON .. <span><span>#</span> With SIMD auto-vectorisation</span>
cmake -DMICROGPT_SIMD=ON .. <span><span>#</span> Both</span>
cmake -DQUANTIZATION_INT8=ON -DMICROGPT_SIMD=ON ..</pre></div>
<hr>
<div><h2>Project Layout</h2><a href="#project-layout"></a></div>
<table>
<thead>
<tr>
<th>Path</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><pre><code>microgpt.h</code></pre></td>
<td>Model config, public API declarations</td>
</tr>
<tr>
<td><pre><code>microgpt.c</code></pre></td>
<td>Core engine: model, forward/backward, Adam, data loading</td>
</tr>
<tr>
<td><pre><code>main.c</code></pre></td>
<td>Entry point: load data → train → generate samples</td>
</tr>
<tr>
<td><pre><code>microgpt_amalgamated.c</code></pre></td>
<td><strong>Single-file build</strong> — same algorithm, no header needed</td>
</tr>
<tr>
<td><pre><code>data/names.txt</code></pre></td>
<td>Training data (one name per line, ~32k names)</td>
</tr>
<tr>
<td><pre><code>CMakeLists.txt</code></pre></td>
<td>CMake build (C99, Release, optional SIMD / INT8)</td>
</tr>
</tbody>
</table>
<hr>
<div><h2>Single-File Build</h2><a href="#single-file-build"></a></div>
<p></p><pre><code>microgpt_amalgamated.c</code></pre> is a self-contained single file containing the full GPT algorithm — data loading, training, and inference. No header file needed:<p></p>
<div><pre><span><span>#</span> Compile directly (no CMake required)</span>
cc -O2 -o microgpt microgpt_amalgamated.c -lm
cp data/names.txt <span>.</span> <span>&amp;&amp;</span> ./microgpt <span><span>#</span> Or via CMake</span>
cmake --build build --config Release --target microgpt_amalgamated
./build/microgpt_amalgamated</pre></div>
<hr>
<div><h2>Requirements</h2><a href="#requirements"></a></div>
<ul>
<li><strong>C99 compiler</strong> (GCC, Clang, MSVC)</li>
<li><strong>CMake 3.10+</strong></li>
<li>No other dependencies</li>
</ul>
<hr>
<div><h2>License</h2><a href="#license"></a></div>
<p>MIT — see <a href="/enjector/microgpt-c/blob/main/LICENSE">LICENSE</a> and source file headers.</p>
<p><strong>Author:</strong> Ajay Soni (<a href="mailto:ajay.soni@enjector.com">ajay.soni@enjector.com</a>), Enjector Software Ltd.</p>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>