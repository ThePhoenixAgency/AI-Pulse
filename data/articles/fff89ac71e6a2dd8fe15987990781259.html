<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Asynchronous Robot Inference: Decoupling Action Prediction and Execution</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Asynchronous Robot Inference: Decoupling Action Prediction and Execution</h1>
  <div class="metadata">
<<<<<<< HEAD
    Source: Hugging Face Blog | Date: 7/10/2025 2:00:00 AM | Lang: EN |
=======
    Source: Hugging Face Blog | Date: 7/10/2025 12:00:00 AM | Lang: EN |
>>>>>>> 48d6193da6f49976a64b6a30483399bfb54b1b8d
    <a href="https://huggingface.co/blog/async-robot-inference" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/fracapuano"><img alt="Francesco Capuano's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/imstevenpmwork"><img alt="Steven Palma's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CXvSv2l15uPkMQL_HBRDF.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/aractingi"><img alt="Michel Aractingi's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/668bd06dd58b51a628566d80/II7Yr5dT5ItMrpoMkQEy3.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/mshukor"><img alt="Mustafa Shukor's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62bdeedd01dc22b4d22a371e/ahbK9Ehurx1TgQAVw1TcS.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/danaaubakirova"><img alt="Dana Aubakirova's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/640e21ef3c82bd463ee5a76d/nVR1DFPAsiLw6Boys28Rb.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/AdilZtn"><img alt="Adil Zouitine's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/64c255b2254239173af0570a/xQtKvcQynqrIc52QgvICp.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/aliberts"><img alt="Simon Alibert's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/65fcb7f133a3d6f126772121/BvVbNqnlQgDr2f_9dm5Es.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/cadene"><img alt="Remi Cadene's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/62f857fbb9fda55613ce80d9/d7bRniKLmOt-iFN07k1Su.png"></a> </span> </span></p> </div></div> <p><strong>TL;DR</strong>
Robotic policies are increasingly bulky, and predict chunks of future actions rather than a single next action. This results in the robot being idle while awaiting new actions to perform, introducing noticeable lags at execution, and lacking responsiveness. Asynchronous inference tightens the control loop, removing lags at runtime and resulting in more adaptive control by decoupling action prediction from action execution. In this blog post, we cover the basics behind async inference, and how it can be used to improve the performance of robotic policies in the real-world.</p>
<h2> <a href="#table-of-contents"> </a> <span> Table of Contents </span>
</h2>
<ul>
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#async-inference-a-deep-dive">Async inference: a deep dive</a></li>
<li><a href="#1-why-sequential-inference-falls-short">1. Why sequential inference falls short</a></li>
<li><a href="#2-asynchronous-inference-in-a-nutshell">2. Asynchronous inference, in a nutshell</a></li>
<li><a href="#3-system-architecture">3. System Architecture</a><ul>
<li><a href="#robot-client">Robot Client</a></li>
<li><a href="#policy-server">Policy Server</a></li>
</ul>
</li>
<li><a href="#4-analyzing-async-inference">4. Analyzing async inference</a></li>
<li><a href="#5-using-async-in-your-setup">5. Using async in your setup</a></li>
<li><a href="#conclusions">Conclusions</a></li>
</ul>
<h2> <a href="#getting-started"> </a> <span> Getting started </span>
</h2>
<p>Get started with async inference by following <a href="https://huggingface.co/docs/lerobot/en/async">our tutorial</a>.</p> <p><em>Sequential inference (first) versus async inference (second)</em>. Allowing for replanning and a tighter control loop, async inference results in (1) attempts at recovery, and (2) a ~2x speedup in task completion. Sequential inference keeps acting out the current action chunk even after failure to grasp the object, while async inference can replan and act the new action chunk. Both setups use the same policy!</p>
<h2> <a href="#async-inference-a-deep-dive"> </a> <span> Async inference: a deep dive </span>
</h2>
<p>With async inference, we decouple action execution from action prediction. This is particularly relevant considering the tendency of currently popular models like [<a href="https://huggingface.co/papers/2304.13705">ACT</a>], [<a href="https://huggingface.co/papers/2406.09246">OpenVLA</a>], [<a href="https://huggingface.co/papers/2410.24164">PI0</a>], and [<a href="https://huggingface.co/papers/2506.01844">SmolVLA</a>] to be outputting chunks of actions <span><span>at:t+Ha_{t:t+H}</span></span> rather than single actions <span><span>ata_t</span></span> given an observation <span><span>oto_t</span></span>. Convince yourself of this by running all these models using <a href="https://huggingface.co/lerobot">LeRobot</a>.</p>
<p>Using chunks sequentially results in (1) lags at runtime, impacting task execution time and (2) lack of responsiveness, due to acting widely open-loop.
Asynchronous inference mitigates both these limitations by <strong>decoupling action prediction from action execution</strong>. We introduced asynchronous inference in SmolVLA, and found it to result in a ~2x speed-up in task completion time with comparable task success rate.</p>
<p>In particular, we design a 2-component system where policy inference and action execution are performed in two different processes, possibly on two different machines connected through the network:</p>
<ul>
<li>A <strong><code>PolicyServer</code></strong>, hosted on accelerated hardware and capable of running inference using more computational resources than the ones allocated on a real-world robot.</li>
<li>A <strong><code>RobotClient</code></strong> enqueues the received actions and executes them while the next chunk is being computed.</li>
</ul>
<p>Communication between <code>PolicyServer</code> and <code>RobotClient</code> relies on <strong>gRPC</strong>, which guarantees ~5× faster performance than a comparable REST API. The result of all of this is a robot that <em>never</em> waits for inference.</p>
<p> <img alt="Async inference scheme" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/async-inference/async_scheme.png">
</p> <p><i>Asynchronous inference</i>, highlighting: (1) The client sending the first observation for inference, receiving the first chunk shortly after; (2) The client sending another observation for processing while it has not yet exhausted the current chunk; (3) The client receiving an updated action chunk, which it aggregates with the remainders of the one it was previously executing.
</p> <hr>
<h2> <a href="#1-why-sequential-inference-falls-short"> </a> <span> 1. Why sequential inference falls short </span>
</h2>
<p>Suppose a policy <span><span>π \pi </span></span> maps the current observation <span><span>ot o_t </span></span> to a sequence of <span><span>H H </span></span> future actions.
Formally, <span><span>π:O ↦ A,At=(at,at+1,…at+H)=π(ot)\pi : \mathcal{O} \;\mapsto\; \mathcal{A}, \mathbf{A}_t = \begin{pmatrix} a_{t}, &amp; a_{t+1}, &amp; \dots &amp; a_{t+H} \end{pmatrix} = \pi(o_t)</span></span>.</p>
<p>A traditional control loop would therefore consist of the following steps:</p>
<ol>
<li>Capture <span><span>ot o_t </span></span>.</li>
<li>Run <span><span>π(ot) \pi(o_t) </span></span> to obtain <span><span>At=π(ot) \mathbf{A}_t = \pi(o_t) </span></span>.</li>
<li>Enqueue <span><span>At\mathbf{A_t} </span></span> and start acting popping actions from the queue.</li>
<li>If the queue is empty, wait for <span><span>At+H \mathbf{A}_{t+H} </span></span>, otherwise repeat step 3.</li>
</ol>
<p>During step 2 the robot is <strong>idle</strong>. The latency grows with the model size (and models tend to be increasingly bulky over time), and can quickly dominate interaction time (which is typically around 1/<code>fps</code>), as shown in the video below (coming from our <a href="https://discord.com/invite/ttk5CV6tUw">Discord community</a> ):</p> <p>This directly results in (1) reduced performance in terms of task completion time---the robot needs to be waiting for the next action chunk to be computed---and (2) reduced responsiveness, due to (2.1) acting widely open-loop while actions are available and (2.2) complete idleness while waiting for the next action chunk.</p>
<p><img alt="Sequential inference – idle periods highlighted" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/async-inference/sync.png"> <img alt="Time to select action – spikes indicate inference" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/async-inference/time_to_select_action.png">
</p> <p>(Left)<i>Sequential inference</i> with highlighted idle periods. (Right)<i>Time to select an action</i> showing spikes when inference is triggered due to local queue exhaustion (inference latency is around ~100ms---~3 frames at 30fps---using an ACT model on a 2021 MacBook Pro).</p> <hr>
<h2> <a href="#2-asynchronous-inference-in-a-nutshell"> </a> <span> 2. Asynchronous inference, in a nutshell </span>
</h2>
<p>Our system removes the idle period by overlapping computation and execution:</p>
<ol>
<li><code>RobotClient</code> streams the latest observation to <code>PolicyServer</code>.</li>
<li>While the server performs inference, the client executes the <strong>current queue</strong> of actions.</li>
<li>New actions arrive, are merged into the queue, and the loop continues.</li>
</ol>
<p>The key idea is that the robot already knows what to do for the next few timesteps, so it can keep moving while fresh actions are being computed on the server.</p>
<p> <img alt="Async inference diagram" src="https://github.com/user-attachments/assets/6f323660-52b4-4537-8bde-f9b70b7f1bc0">
</p>
<p><i>Asynchronous inference</i> overlaps in time the execution of the current action chunk with the computation of the next one, by decoupling these two processes, possibly running them on entirely distinct machines connected through the network.
</p> <p>This results in a tighter control loop, and a robot that never waits for inference. In turn, this results in ~2x speedup in task completion time with comparable task success rate, and more adaptive control coming from a tighter loop (see video below).</p> <hr>
<h2> <a href="#3system-architecture"> </a> <span> 3. System Architecture </span>
</h2>
<div> <table> <thead><tr>
<th>Component</th>
<th>Role</th>
<th>Technology</th>
</tr> </thead><tbody><tr>
<td><strong>RobotClient</strong></td>
<td>Runs on-board, streams observations, maintains an <strong>action queue</strong>, executes actions</td>
<td>Python, gRPC</td>
</tr>
<tr>
<td><strong>PolicyServer</strong></td>
<td>Hosts the policy, performs batched inference, sends action chunks back</td>
<td>Python, gRPC, possibly accelerated hardware (GPU/TPU)</td>
</tr>
</tbody> </table>
</div>
<p>Because gRPC is HTTP/2-based and uses protocol buffers, it achieves low-latency binary messaging and bidirectional streams out of the box, which in turn helps us maintain a tighter control loop and sub-100ms round-trip latency (on our local network, and hosting SmolVLA on a NVIDIA RTX 4090).</p>
<p>The <code>RobotClient</code> runs on-board, and streams observations to the <code>PolicyServer</code> through gRPC. The <code>PolicyServer</code> prepares the observations received for inference, and sends back to the <code>RobotClient</code> an action chunk.</p>
<h3> <a href="#robot-client"> </a> <span> Robot Client </span>
</h3>
<p> <img alt="From client perspective" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/async-inference/from_client_perspective.png">
</p>
<p><i>From the client's perspective</i>, observations are streamed to the server according to the local queue status. Incoming chunks are aggregated on overlapping portions with the currently available action queue.
</p> <p>The <code>RobotClient</code> maintains a local action queue and follows a simple yet effective strategy: <strong>send a new observation when the queue length drops below a configurable threshold</strong> (\(g\) in the SmolVLA paper, <code>chunk_size_threshold</code> in the code). This threshold value, expressed as a fraction of the maximum chunk size, acts as a trigger condition that balances computational load with responsiveness.</p>
<p> <img alt="Client to server" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/async-inference/client_to_server.png">
</p>
<p><i>The client streams observations to the server</i>, according to the local queue status.
</p> <p>From the client's perspective, the process unfolds as follows:</p>
<ol>
<li><p><strong>Queue monitoring</strong>: The client continuously monitors its action queue length against a <strong>chunk size threshold</strong> parameter. When the queue drops below this threshold, it signals that a new observation should be sent for processing.</p>
</li>
<li><p><strong>Observation streaming</strong>: Once the threshold condition is met, the client captures the current observation and streams it to the <code>PolicyServer</code> via gRPC. Crucially, <strong>observations are streamed rather than being sent via a unary RPC</strong> because they typically exceed the maximum message size of 4MB (multiple camera captures at high resolution result in this).</p>
</li>
<li><p><strong>Action chunk aggregation</strong>: When a new action chunk arrives from the server, the client merges it with any remaining actions in the current queue over the overlapping portion. This is where <strong>custom aggregators</strong> come into play, handling overlapping sections between the current and incoming chunks differently. As of now, we support flexibly aggregation between the chunks via the specification of a custom <code>aggregate_fn(chunk1: torch.Tensor, chunk2: torch.Tensor) -&gt; torch.Tensor</code> function, which is called for each overlapping timestep and can be user-provided.
The overlapping portions (shown in light blue in the diagram) require careful handling. We can design different aggregation strategies:</p>
<ul>
<li><strong>Replace</strong>: Simply replace overlapping actions with the newer predictions</li>
<li><strong>Weighted blend</strong>: Combine overlapping actions using temporal weights (closer actions get higher weight)</li>
</ul>
</li>
</ol>
<p>This system is highly configurable, as the chunk size threshold can be tuned based on network latency, model inference time, and desired responsiveness. A lower threshold means more frequent updates (and higher computational cost), while a higher threshold reduces communication overhead at the expense of potential queue starvation.
Lastly, we typically receive actions from <code>PolicyServer</code> in a thread, and perform them in another one. This keeps the client listening for incoming chunks in a separate thread, without blocking execution and always consuming the current chunk until a new one becomes fully available.</p>
<h3> <a href="#policy-server"> </a> <span> Policy Server </span>
</h3>
<p>Upon receiving observations from the <code>RobotClient</code>, the <code>PolicyServer</code> receives observations from the <code>RobotClient</code>, and performs the necessary observation cleaning to make received observations ready for inference. This process is illustrated in the image below:</p>
<p> <img alt="Server pipeline" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/async-inference/server_pipeline.png">
</p> </div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>