<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>GitHub - doramirdor/NadirClaw: Open-source LLM router that saves you money. Routes simple prompts to cheap/local models, complex ones to premium — automatically. OpenAI-compatible proxy.</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>GitHub - doramirdor/NadirClaw: Open-source LLM router that saves you money. Routes simple prompts to cheap/local models, complex ones to premium — automatically. OpenAI-compatible proxy.</h1>
  <div class="metadata">
    Source: Hacker News Show | Date: 2/17/2026 11:31:18 PM | <a href="https://github.com/doramirdor/NadirClaw" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><h1>NadirClaw</h1><a href="#nadirclaw"></a></div>
<p><a target="_blank" href="/doramirdor/NadirClaw/blob/main/logo_rb.png"><img alt="NadirClaw" src="/doramirdor/NadirClaw/raw/main/logo_rb.png"></a></p>
<p><a href="https://pypi.org/project/nadirclaw/"><img src="https://camo.githubusercontent.com/54dfb38a831e92d7b75aa439487e8a58c7a3bb911460d2434fc0c74ad07c36f7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6e61646972636c6177" alt="PyPI"></a>
<a href="https://github.com/doramirdor/NadirClaw/actions"><img src="https://github.com/doramirdor/NadirClaw/actions/workflows/ci.yml/badge.svg" alt="CI"></a>
<a href="https://pypi.org/project/nadirclaw/"><img src="https://camo.githubusercontent.com/322d1dfa8f3dc7e60a7b720073fbd2364bcc0bba7a751b7bd3a11f222f27f1f1/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6e61646972636c6177" alt="Python"></a>
<a href="/doramirdor/NadirClaw/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/6651979961213b222f3733a89e9fc68f3d7a9bb0577cdaa119c83f945dcb666a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f646f72616d6972646f722f4e61646972436c6177" alt="License"></a>
<a href="https://github.com/doramirdor/NadirClaw"><img src="https://camo.githubusercontent.com/796b75367b6fc1ce1e584a74ff9705a20626e7396a1d172a72b194a52677ece0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646f72616d6972646f722f4e61646972436c61773f7374796c653d736f6369616c" alt="GitHub stars"></a></p>
<p>Open-source LLM router that saves you money. Simple prompts go to cheap/local models, complex prompts go to premium models -- automatically.</p>
<p>NadirClaw sits between your AI tool and your LLM providers as an OpenAI-compatible proxy. It classifies every prompt in ~10ms and routes it to the right model. Works with any tool that speaks the OpenAI API: <a href="https://openclaw.dev">OpenClaw</a>, <a href="https://github.com/openai/codex">Codex</a>, Claude Code, Continue, Cursor, or plain </p><pre><code>curl</code></pre>.<p></p>
<blockquote>
<p><strong>How does NadirClaw compare to OpenRouter?</strong> See <a href="/doramirdor/NadirClaw/blob/main/docs/comparison.md">NadirClaw vs OpenRouter</a>.</p>
</blockquote>
<p> <a target="_blank" href="/doramirdor/NadirClaw/blob/main/docs/images/architecture.png"><img src="/doramirdor/NadirClaw/raw/main/docs/images/architecture.png" alt="NadirClaw Architecture"></a>
</p>
<p> <a target="_blank" href="/doramirdor/NadirClaw/blob/main/docs/images/nadirclaw_img.png"><img src="/doramirdor/NadirClaw/raw/main/docs/images/nadirclaw_img.png" alt="NadirClaw in action"></a>
</p>
<div><h2>Quick Start</h2><a href="#quick-start"></a></div>
<div><pre>pip install nadirclaw</pre></div>
<p>Or install from source:</p>
<div><pre>curl -fsSL https://raw.githubusercontent.com/doramirdor/NadirClaw/main/install.sh <span>|</span> sh</pre></div>
<p>Then run the interactive setup wizard:</p>
<div><pre>nadirclaw setup</pre></div>
<p>This guides you through selecting providers, entering API keys, and choosing models for each routing tier. Then start the router:</p>
<div><pre>nadirclaw serve --verbose</pre></div>
<p>That's it. NadirClaw starts on </p><pre><code>http://localhost:8856</code></pre> with sensible defaults (Gemini 3 Flash for simple, OpenAI Codex for complex). If you skip <pre><code>nadirclaw setup</code></pre>, the <pre><code>serve</code></pre> command will offer to run it on first launch.<p></p>
<div><h2>Features</h2><a href="#features"></a></div>
<ul>
<li><strong>Smart routing</strong> — classifies prompts in ~10ms using sentence embeddings</li>
<li><strong>Agentic task detection</strong> — auto-detects tool use, multi-step loops, and agent system prompts; forces complex model for agentic requests</li>
<li><strong>Reasoning detection</strong> — identifies prompts needing chain-of-thought and routes to reasoning-optimized models</li>
<li><strong>Routing profiles</strong> — <pre><code>auto</code></pre>, <pre><code>eco</code></pre>, <pre><code>premium</code></pre>, <pre><code>free</code></pre>, <pre><code>reasoning</code></pre> — choose your cost/quality strategy per request</li>
<li><strong>Model aliases</strong> — use short names like <pre><code>sonnet</code></pre>, <pre><code>flash</code></pre>, <pre><code>gpt4</code></pre> instead of full model IDs</li>
<li><strong>Session persistence</strong> — pins the model for multi-turn conversations so you don't bounce between models mid-thread</li>
<li><strong>Context-window filtering</strong> — auto-swaps to a model with a larger context window when your conversation is too long</li>
<li><strong>Rate limit fallback</strong> — if the primary model is rate-limited (429), automatically falls back to the other tier's model instead of failing</li>
<li><strong>Streaming support</strong> — full SSE streaming compatible with OpenClaw, Codex, and other streaming clients</li>
<li><strong>Native Gemini support</strong> — calls Gemini models directly via the Google GenAI SDK (not through LiteLLM)</li>
<li><strong>OAuth login</strong> — use your subscription with <pre><code>nadirclaw auth &lt;provider&gt; login</code></pre> (OpenAI, Anthropic, Google), no API key needed</li>
<li><strong>Multi-provider</strong> — supports Gemini, OpenAI, Anthropic, Ollama, and any LiteLLM-supported provider</li>
<li><strong>OpenAI-compatible API</strong> — drop-in replacement for any tool that speaks the OpenAI chat completions API</li>
<li><strong>Request reporting</strong> — <pre><code>nadirclaw report</code></pre> analyzes your JSONL logs with filters, latency stats, tier breakdown, and token usage</li>
<li><strong>Raw logging</strong> — optional <pre><code>--log-raw</code></pre> flag to capture full request/response content for debugging and replay</li>
<li><strong>OpenTelemetry tracing</strong> — optional distributed tracing with GenAI semantic conventions (<pre><code>pip install nadirclaw[telemetry]</code></pre>)</li>
</ul>
<div><h2>Prerequisites</h2><a href="#prerequisites"></a></div>
<ul>
<li><strong>Python 3.10+</strong></li>
<li><strong>git</strong></li>
<li><strong>At least one LLM provider:</strong>
<ul>
<li><a href="https://aistudio.google.com/apikey">Google Gemini API key</a> (free tier: 20 req/day)</li>
<li><a href="https://ollama.com">Ollama</a> running locally (free, no API key needed)</li>
<li><a href="https://console.anthropic.com/">Anthropic API key</a> for Claude models</li>
<li><a href="https://platform.openai.com/">OpenAI API key</a> for GPT models</li>
<li>Provider subscriptions via OAuth (<pre><code>nadirclaw auth openai login</code></pre>, <pre><code>nadirclaw auth anthropic login</code></pre>, <pre><code>nadirclaw auth antigravity login</code></pre>, <pre><code>nadirclaw auth gemini login</code></pre>)</li>
<li>Or any provider supported by <a href="https://docs.litellm.ai/docs/providers">LiteLLM</a></li>
</ul>
</li>
</ul>
<div><h2>Install</h2><a href="#install"></a></div> <div><pre>curl -fsSL https://raw.githubusercontent.com/doramirdor/NadirClaw/main/install.sh <span>|</span> sh</pre></div>
<p>This clones the repo to </p><pre><code>~/.nadirclaw</code></pre>, creates a virtual environment, installs dependencies, and adds <pre><code>nadirclaw</code></pre> to your PATH. Run it again to update.<p></p>
<div><h3>Manual install</h3><a href="#manual-install"></a></div>
<div><pre>git clone https://github.com/doramirdor/NadirClaw.git
<span>cd</span> NadirClaw
python3 -m venv venv
<span>source</span> venv/bin/activate
pip install -e <span>.</span></pre></div>
<div><h3>Uninstall</h3><a href="#uninstall"></a></div>
<div><pre>rm -rf <span>~</span>/.nadirclaw
sudo rm -f /usr/local/bin/nadirclaw</pre></div>
<div><h2>Configure</h2><a href="#configure"></a></div>
<div><h3>Environment File</h3><a href="#environment-file"></a></div>
<p>NadirClaw loads configuration from </p><pre><code>~/.nadirclaw/.env</code></pre>. Create or edit this file to set API keys and model preferences:<p></p>
<div><pre><span><span>#</span> ~/.nadirclaw/.env</span> <span><span>#</span> API keys (set the ones you use)</span>
GEMINI_API_KEY=AIza...
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-... <span><span>#</span> Model routing</span>
NADIRCLAW_SIMPLE_MODEL=gemini-3-flash-preview
NADIRCLAW_COMPLEX_MODEL=gemini-2.5-pro <span><span>#</span> Server</span>
NADIRCLAW_PORT=8856</pre></div>
<p>If </p><pre><code>~/.nadirclaw/.env</code></pre> does not exist, NadirClaw falls back to <pre><code>.env</code></pre> in the current directory.<p></p>
<div><h3>Authentication</h3><a href="#authentication"></a></div>
<p>NadirClaw supports multiple ways to provide LLM credentials, checked in this order:</p>
<ol>
<li><strong>OpenClaw stored token</strong> (<pre><code>~/.openclaw/agents/main/agent/auth-profiles.json</code></pre>)</li>
<li><strong>NadirClaw stored credential</strong> (<pre><code>~/.nadirclaw/credentials.json</code></pre>)</li>
<li><strong>Environment variable</strong> (<pre><code>GEMINI_API_KEY</code></pre>, <pre><code>ANTHROPIC_API_KEY</code></pre>, <pre><code>OPENAI_API_KEY</code></pre>, etc.)</li>
</ol> <div><pre><span><span>#</span> Add a Gemini API key</span>
nadirclaw auth add --provider google --key AIza... <span><span>#</span> Add any provider API key</span>
nadirclaw auth add --provider anthropic --key sk-ant-...
nadirclaw auth add --provider openai --key sk-... <span><span>#</span> Login with your OpenAI/ChatGPT subscription (OAuth, no API key needed)</span>
nadirclaw auth openai login <span><span>#</span> Login with your Anthropic/Claude subscription (OAuth, no API key needed)</span>
nadirclaw auth anthropic login <span><span>#</span> Login with Google Gemini (OAuth, opens browser)</span>
nadirclaw auth gemini login <span><span>#</span> Login with Google Antigravity (OAuth, opens browser)</span>
nadirclaw auth antigravity login <span><span>#</span> Store a Claude subscription token (from 'claude setup-token') - alternative to OAuth</span>
nadirclaw auth setup-token <span><span>#</span> Check what's configured</span>
nadirclaw auth status <span><span>#</span> Remove a credential</span>
nadirclaw auth remove google</pre></div>
<div><h4>Using environment variables</h4><a href="#using-environment-variables"></a></div>
<p>Set API keys in </p><pre><code>~/.nadirclaw/.env</code></pre>:<p></p>
<div><pre>GEMINI_API_KEY=AIza... <span><span>#</span> or GOOGLE_API_KEY</span>
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...</pre></div>
<div><h3>Model Configuration</h3><a href="#model-configuration"></a></div>
<p>Configure which model handles each tier:</p>
<div><pre>NADIRCLAW_SIMPLE_MODEL=gemini-3-flash-preview <span><span>#</span> cheap/free model</span>
NADIRCLAW_COMPLEX_MODEL=gemini-2.5-pro <span><span>#</span> premium model</span>
NADIRCLAW_REASONING_MODEL=o3 <span><span>#</span> reasoning tasks (optional, defaults to complex)</span>
NADIRCLAW_FREE_MODEL=ollama/llama3.1:8b <span><span>#</span> free fallback (optional, defaults to simple)</span></pre></div>
<div><h3>Example Setups</h3><a href="#example-setups"></a></div>
<table>
<thead>
<tr>
<th>Setup</th>
<th>Simple Model</th>
<th>Complex Model</th>
<th>API Keys Needed</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gemini + Gemini</strong></td>
<td><pre><code>gemini-2.5-flash</code></pre></td>
<td><pre><code>gemini-2.5-pro</code></pre></td>
<td><pre><code>GEMINI_API_KEY</code></pre></td>
</tr>
<tr>
<td><strong>Gemini + Claude</strong></td>
<td><pre><code>gemini-2.5-flash</code></pre></td>
<td><pre><code>claude-sonnet-4-5-20250929</code></pre></td>
<td><pre><code>GEMINI_API_KEY</code></pre> + <pre><code>ANTHROPIC_API_KEY</code></pre></td>
</tr>
<tr>
<td><strong>Claude + Ollama</strong></td>
<td><pre><code>ollama/llama3.1:8b</code></pre></td>
<td><pre><code>claude-sonnet-4-5-20250929</code></pre></td>
<td><pre><code>ANTHROPIC_API_KEY</code></pre></td>
</tr>
<tr>
<td><strong>Claude + Claude</strong></td>
<td><pre><code>claude-haiku-4-5-20251001</code></pre></td>
<td><pre><code>claude-sonnet-4-5-20250929</code></pre></td>
<td><pre><code>ANTHROPIC_API_KEY</code></pre></td>
</tr>
<tr>
<td><strong>OpenAI + Ollama</strong></td>
<td><pre><code>ollama/llama3.1:8b</code></pre></td>
<td><pre><code>gpt-4.1</code></pre></td>
<td><pre><code>OPENAI_API_KEY</code></pre></td>
</tr>
<tr>
<td><strong>OpenAI + OpenAI</strong></td>
<td><pre><code>gpt-4.1-mini</code></pre></td>
<td><pre><code>gpt-4.1</code></pre></td>
<td><pre><code>OPENAI_API_KEY</code></pre></td>
</tr>
<tr>
<td><strong>OpenAI Codex</strong></td>
<td><pre><code>gemini-2.5-flash</code></pre></td>
<td><pre><code>openai-codex/gpt-5.3-codex</code></pre></td>
<td><pre><code>GEMINI_API_KEY</code></pre> + OAuth login</td>
</tr>
<tr>
<td><strong>Fully local</strong></td>
<td><pre><code>ollama/llama3.1:8b</code></pre></td>
<td><pre><code>ollama/qwen3:32b</code></pre></td>
<td>None</td>
</tr>
</tbody>
</table>
<p>Gemini models are called natively via the Google GenAI SDK. All other models go through <a href="https://docs.litellm.ai/docs/providers">LiteLLM</a>, which supports 100+ providers.</p>
<div><h2>Usage with Gemini</h2><a href="#usage-with-gemini"></a></div>
<p>Gemini is the default simple model. NadirClaw calls Gemini natively via the Google GenAI SDK for best performance.</p>
<div><pre><span><span>#</span> Set your Gemini API key</span>
nadirclaw auth add --provider google --key AIza... <span><span>#</span> Or set in ~/.nadirclaw/.env</span>
<span>echo</span> <span><span>"</span>GEMINI_API_KEY=AIza...<span>"</span></span> <span>&gt;&gt;</span> <span>~</span>/.nadirclaw/.env <span><span>#</span> Start the router</span>
nadirclaw serve --verbose</pre></div>
<div><h3>Rate Limit Fallback</h3><a href="#rate-limit-fallback"></a></div>
<p>If the primary model hits a 429 rate limit, NadirClaw automatically retries once, then falls back to the other tier's model. For example, if </p><pre><code>gemini-3-flash-preview</code></pre> is exhausted, NadirClaw will try <pre><code>gemini-2.5-pro</code></pre> (or whatever your complex model is). If both models are rate-limited, it returns a friendly error message instead of crashing.<p></p>
<div><h2>Usage with Ollama</h2><a href="#usage-with-ollama"></a></div>
<p>If you're running <a href="https://ollama.com">Ollama</a> locally, NadirClaw works out of the box with no API keys:</p>
<div><pre><span><span>#</span> Fully local setup -- no API keys, no cost</span>
NADIRCLAW_SIMPLE_MODEL=ollama/llama3.1:8b \
NADIRCLAW_COMPLEX_MODEL=ollama/qwen3:32b \
nadirclaw serve --verbose</pre></div>
<p>Or mix local + cloud:</p>
<div><pre>nadirclaw serve \ --simple-model ollama/llama3.1:8b \ --complex-model claude-sonnet-4-20250514 \ --verbose</pre></div> <table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Good For</th>
</tr>
</thead>
<tbody>
<tr>
<td><pre><code>llama3.1:8b</code></pre></td>
<td>4.7 GB</td>
<td>Simple tier (fast, good enough)</td>
</tr>
<tr>
<td><pre><code>qwen3:32b</code></pre></td>
<td>19 GB</td>
<td>Complex tier (local, no API cost)</td>
</tr>
<tr>
<td><pre><code>qwen3-coder</code></pre></td>
<td>19 GB</td>
<td>Code-heavy complex tier</td>
</tr>
<tr>
<td><pre><code>deepseek-r1:14b</code></pre></td>
<td>9 GB</td>
<td>Reasoning-heavy complex tier</td>
</tr>
</tbody>
</table>
<div><h2>Usage with OpenClaw</h2><a href="#usage-with-openclaw"></a></div>
<p><a href="https://openclaw.dev">OpenClaw</a> is a personal AI assistant that bridges messaging services to AI coding agents. NadirClaw integrates as a model provider so OpenClaw's requests are automatically routed to the right model.</p>
<div><h3>Quick Setup</h3><a href="#quick-setup"></a></div>
<div><pre><span><span>#</span> Auto-configure OpenClaw to use NadirClaw</span>
nadirclaw openclaw onboard <span><span>#</span> Start the router</span>
nadirclaw serve</pre></div>
<p>This writes NadirClaw as a provider in </p><pre><code>~/.openclaw/openclaw.json</code></pre> with model <pre><code>nadirclaw/auto</code></pre>. If OpenClaw is already running, it will auto-reload the config -- no restart needed.<p></p>
<div><h3>Configure Only (Without Launching)</h3><a href="#configure-only-without-launching"></a></div>
<div><pre>nadirclaw openclaw onboard
<span><span>#</span> Then start NadirClaw separately when ready:</span>
nadirclaw serve</pre></div>
<div><h3>What It Does</h3><a href="#what-it-does"></a></div>
<p></p><pre><code>nadirclaw openclaw onboard</code></pre> adds this to your OpenClaw config:<p></p>
<div><pre>{ <span>"models"</span>: { <span>"providers"</span>: { <span>"nadirclaw"</span>: { <span>"baseUrl"</span>: <span><span>"</span>http://localhost:8856/v1<span>"</span></span>, <span>"apiKey"</span>: <span><span>"</span>local<span>"</span></span>, <span>"api"</span>: <span><span>"</span>openai-completions<span>"</span></span>, <span>"models"</span>: [{ <span>"id"</span>: <span><span>"</span>auto<span>"</span></span>, <span>"name"</span>: <span><span>"</span>auto<span>"</span></span> }] } } }, <span>"agents"</span>: { <span>"defaults"</span>: { <span>"model"</span>: { <span>"primary"</span>: <span><span>"</span>nadirclaw/auto<span>"</span></span> } } }
}</pre></div>
<p>NadirClaw supports the SSE streaming format that OpenClaw expects (</p><pre><code>stream: true</code></pre>), handling multi-modal content and tool definitions in system prompts.<p></p>
<div><h2>Usage with Codex</h2><a href="#usage-with-codex"></a></div>
<p><a href="https://github.com/openai/codex">Codex</a> is OpenAI's CLI coding agent. NadirClaw integrates as a custom model provider.</p>
<div><pre><span><span>#</span> Auto-configure Codex</span>
nadirclaw codex onboard <span><span>#</span> Start the router</span>
nadirclaw serve</pre></div>
<p>This writes </p><pre><code>~/.codex/config.toml</code></pre>:<p></p>
<div><pre><span>model_provider</span> = <span><span>"</span>nadirclaw<span>"</span></span> [<span>model_providers</span>.<span>nadirclaw</span>]
<span>base_url</span> = <span><span>"</span>http://localhost:8856/v1<span>"</span></span>
<span>api_key</span> = <span><span>"</span>local<span>"</span></span></pre></div>
<div><h3>OpenAI Subscription (OAuth)</h3><a href="#openai-subscription-oauth"></a></div>
<p>To use your ChatGPT subscription instead of an API key:</p>
<div><pre><span><span>#</span> Login with your OpenAI account (opens browser)</span>
nadirclaw auth openai login <span><span>#</span> NadirClaw will auto-refresh the token when it expires</span></pre></div>
<p>This delegates to the Codex CLI for the OAuth flow and stores the credentials in </p><pre><code>~/.nadirclaw/credentials.json</code></pre>. Tokens are automatically refreshed when they expire.<p></p>
<div><h2>Usage with Any OpenAI-Compatible Tool</h2><a href="#usage-with-any-openai-compatible-tool"></a></div>
<p>NadirClaw exposes a standard OpenAI-compatible API. Point any tool at it:</p>
<div><pre><span><span>#</span> Base URL</span>
http://localhost:8856/v1 <span><span>#</span> Model</span>
model: <span><span>"</span>auto<span>"</span></span> <span><span>#</span> or omit -- NadirClaw picks the best model</span></pre></div>
<div><h3>Example: curl</h3><a href="#example-curl"></a></div>
<div><pre>curl http://localhost:8856/v1/chat/completions \ -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \ -d <span><span>'</span>{</span>
<span> "messages": [{"role": "user", "content": "What is 2+2?"}]</span>
<span> }<span>'</span></span></pre></div>
<div><h3>Example: curl (streaming)</h3><a href="#example-curl-streaming"></a></div>
<div><pre>curl http://localhost:8856/v1/chat/completions \ -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \ -d <span><span>'</span>{</span>
<span> "messages": [{"role": "user", "content": "What is 2+2?"}],</span>
<span> "stream": true</span>
<span> }<span>'</span></span></pre></div>
<div><h3>Example: Python (openai SDK)</h3><a href="#example-python-openai-sdk"></a></div>
<div><pre><span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span> <span>client</span> <span>=</span> <span>OpenAI</span>( <span>base_url</span><span>=</span><span>"http://localhost:8856/v1"</span>, <span>api_key</span><span>=</span><span>"local"</span>, <span># NadirClaw doesn't require auth by default</span>
) <span>response</span> <span>=</span> <span>client</span>.<span>chat</span>.<span>completions</span>.<span>create</span>( <span>model</span><span>=</span><span>"auto"</span>, <span>messages</span><span>=</span>[{<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"What is 2+2?"</span>}],
)
<span>print</span>(<span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>)</pre></div>
<div><h2>Routing Profiles</h2><a href="#routing-profiles"></a></div>
<p>Choose your routing strategy by setting the model field:</p>
<table>
<thead>
<tr>
<th>Profile</th>
<th>Model Field</th>
<th>Strategy</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>auto</strong></td>
<td><pre><code>auto</code></pre> or omit</td>
<td>Smart routing (default)</td>
<td>Best overall balance</td>
</tr>
<tr>
<td><strong>eco</strong></td>
<td><pre><code>eco</code></pre></td>
<td>Always use simple model</td>
<td>Maximum savings</td>
</tr>
<tr>
<td><strong>premium</strong></td>
<td><pre><code>premium</code></pre></td>
<td>Always use complex model</td>
<td>Best quality</td>
</tr>
<tr>
<td><strong>free</strong></td>
<td><pre><code>free</code></pre></td>
<td>Use free fallback model</td>
<td>Zero cost</td>
</tr>
<tr>
<td><strong>reasoning</strong></td>
<td><pre><code>reasoning</code></pre></td>
<td>Use reasoning model</td>
<td>Chain-of-thought tasks</td>
</tr>
</tbody>
</table>
<div><pre><span><span>#</span> Use profiles via the model field</span>
curl http://localhost:8856/v1/chat/completions \ -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \ -d <span><span>'</span>{"model": "eco", "messages": [{"role": "user", "content": "Hello"}]}<span>'</span></span> <span><span>#</span> Also works with nadirclaw/ prefix</span>
<span><span>#</span> model: "nadirclaw/eco", "nadirclaw/premium", etc.</span></pre></div>
<div><h2>Model Aliases</h2><a href="#model-aliases"></a></div>
<p>Use short names instead of full model IDs:</p>
<table>
<thead>
<tr>
<th>Alias</th>
<th>Resolves To</th>
</tr>
</thead>
<tbody>
<tr>
<td><pre><code>sonnet</code></pre></td>
<td><pre><code>claude-sonnet-4-5-20250929</code></pre></td>
</tr>
<tr>
<td><pre><code>opus</code></pre></td>
<td><pre><code>claude-opus-4-6-20250918</code></pre></td>
</tr>
<tr>
<td><pre><code>haiku</code></pre></td>
<td><pre><code>claude-haiku-4-5-20251001</code></pre></td>
</tr>
<tr>
<td><pre><code>gpt4</code></pre></td>
<td><pre><code>gpt-4.1</code></pre></td>
</tr>
<tr>
<td><pre><code>gpt5</code></pre></td>
<td><pre><code>gpt-5.2</code></pre></td>
</tr>
<tr>
<td><pre><code>flash</code></pre></td>
<td><pre><code>gemini-2.5-flash</code></pre></td>
</tr>
<tr>
<td><pre><code>gemini-pro</code></pre></td>
<td><pre><code>gemini-2.5-pro</code></pre></td>
</tr>
<tr>
<td><pre><code>deepseek</code></pre></td>
<td><pre><code>deepseek/deepseek-chat</code></pre></td>
</tr>
<tr>
<td><pre><code>deepseek-r1</code></pre></td>
<td><pre><code>deepseek/deepseek-reasoner</code></pre></td>
</tr>
<tr>
<td><pre><code>llama</code></pre></td>
<td><pre><code>ollama/llama3.1:8b</code></pre></td>
</tr>
</tbody>
</table>
<div><pre><span><span>#</span> Use an alias as the model</span>
curl http://localhost:8856/v1/chat/completions \ -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \ -d <span><span>'</span>{"model": "sonnet", "messages": [{"role": "user", "content": "Hello"}]}<span>'</span></span></pre></div>
<div><h2>Routing Intelligence</h2><a href="#routing-intelligence"></a></div>
<p> <a target="_blank" href="/doramirdor/NadirClaw/blob/main/docs/images/routing-flow.png"><img src="/doramirdor/NadirClaw/raw/main/docs/images/routing-flow.png" alt="Routing flow"></a>
</p>
<p>Beyond basic simple/complex classification, NadirClaw applies routing modifiers that can override the base decision:</p>
<div><h3>Agentic Task Detection</h3><a href="#agentic-task-detection"></a></div>
<p>NadirClaw detects agentic requests (coding agents, multi-step tool use) and forces them to the complex model, even if the individual message looks simple. Signals:</p>
<ul>
<li>Tool definitions in the request (<pre><code>tools</code></pre> array)</li>
<li>Tool-role messages (active tool execution loop)</li>
<li>Assistant→tool→assistant cycles (multi-step execution)</li>
<li>Agent-like system prompts ("you are a coding agent", "you can execute commands")</li>
<li>Long system prompts (&gt;500 chars, typical of agent instructions)</li>
<li>Deep conversations (&gt;10 messages)</li>
</ul>
<p>This prevents a message like "now add tests" from being routed to the cheap model when it's part of an ongoing agentic refactoring session.</p>
<div><h3>Reasoning Detection</h3><a href="#reasoning-detection"></a></div>
<p>Prompts with 2+ reasoning markers are routed to the reasoning model (or complex model if no reasoning model is configured):</p>
<ul>
<li>"step by step", "think through", "chain of thought"</li>
<li>"prove that", "derive the", "mathematically show"</li>
<li>"analyze the tradeoffs", "compare and contrast"</li>
<li>"critically analyze", "evaluate whether"</li>
</ul>
<div><h3>Session Persistence</h3><a href="#session-persistence"></a></div>
<p>Once a conversation is routed to a model, subsequent messages in the same session reuse that model. This prevents jarring mid-conversation model switches. Sessions are keyed by system prompt + first user message, with a 30-minute TTL.</p>
<div><h3>Context Window Filtering</h3><a href="#context-window-filtering"></a></div>
<p>If the estimated token count of a request exceeds a model's context window, NadirClaw automatically swaps to a model with a larger context. For example, a 150k-token conversation targeting </p><pre><code>gpt-4o</code></pre> (128k context) will be redirected to <pre><code>gemini-2.5-pro</code></pre> (1M context).<p></p>
<div><h2>CLI Reference</h2><a href="#cli-reference"></a></div>
<div><pre>nadirclaw setup <span><span>#</span> Interactive setup wizard (providers, keys, models)</span>
nadirclaw serve <span><span>#</span> Start the router server</span>
nadirclaw serve --log-raw <span><span>#</span> Start with full request/response logging</span>
nadirclaw classify <span><span>#</span> Classify a prompt (no server needed)</span>
nadirclaw report <span><span>#</span> Show a summary report of request logs</span>
nadirclaw report --since 24h <span><span>#</span> Report for the last 24 hours</span>
nadirclaw status <span><span>#</span> Show config, credentials, and server status</span>
nadirclaw auth add <span><span>#</span> Add an API key for any provider</span>
nadirclaw auth status <span><span>#</span> Show configured credentials (masked)</span>
nadirclaw auth remove <span><span>#</span> Remove a stored credential</span>
nadirclaw auth setup-token <span><span>#</span> Store a Claude subscription token (alternative to OAuth)</span>
nadirclaw auth openai login <span><span>#</span> Login with OpenAI subscription (OAuth)</span>
nadirclaw auth openai <span>logout</span> <span><span>#</span> Remove stored OpenAI OAuth credential</span>
nadirclaw auth anthropic login <span><span>#</span> Login with Anthropic/Claude subscription (OAuth)</span>
nadirclaw auth anthropic <span>logout</span> <span><span>#</span> Remove stored Anthropic OAuth credential</span>
nadirclaw auth antigravity login <span><span>#</span> Login with Google Antigravity (OAuth, opens browser)</span>
nadirclaw auth antigravity <span>logout</span> <span><span>#</span> Remove stored Antigravity OAuth credential</span>
nadirclaw auth gemini login <span><span>#</span> Login with Google Gemini (OAuth, opens browser)</span>
nadirclaw auth gemini <span>logout</span> <span><span>#</span> Remove stored Gemini OAuth credential</span>
nadirclaw codex onboard <span><span>#</span> Configure Codex integration</span>
nadirclaw openclaw onboard <span><span>#</span> Configure OpenClaw integration</span>
nadirclaw build-centroids <span><span>#</span> Regenerate centroid vectors from prototypes</span></pre></div>
<div><h3><pre><code>nadirclaw serve</code></pre></h3><a href="#nadirclaw-serve"></a></div>
<div><pre>nadirclaw serve [OPTIONS] Options: --port INTEGER Port to listen on (default: 8856) --simple-model TEXT Model <span>for</span> simple prompts --complex-model TEXT Model <span>for</span> complex prompts --models TEXT Comma-separated model list (legacy) --token TEXT Auth token --verbose Enable debug logging --log-raw Log full raw requests and responses to JSONL</pre></div>
<div><h3><pre><code>nadirclaw report</code></pre></h3><a href="#nadirclaw-report"></a></div>
<p> <a target="_blank" href="/doramirdor/NadirClaw/blob/main/docs/images/report.png"><img src="/doramirdor/NadirClaw/raw/main/docs/images/report.png" alt="nadirclaw report output"></a>
</p>
<p>Analyze request logs and print a summary report:</p>
<div><pre>nadirclaw report <span><span>#</span> full report</span>
nadirclaw report --since 24h <span><span>#</span> last 24 hours</span>
nadirclaw report --since 7d <span><span>#</span> last 7 days</span>
nadirclaw report --since 2025-02-01 <span><span>#</span> since a specific date</span>
nadirclaw report --model gemini <span><span>#</span> filter by model name</span>
nadirclaw report --format json <span><span>#</span> machine-readable JSON output</span>
nadirclaw report --export report.txt <span><span>#</span> save to file</span></pre></div>
<p>Example output:</p>
<div><pre><code>NadirClaw Report
==================================================
Total requests: 147
From: 2026-02-14T08:12:03+00:00
To: 2026-02-14T22:47:19+00:00 Requests by Type
------------------------------ classify 12 completion 135 Tier Distribution
------------------------------ complex 41 (31.1%) direct 8 (6.1%) simple 83 (62.9%) Model Usage
------------------------------------------------------------ Model Reqs Tokens gemini-3-flash-preview 83 48210 openai-codex/gpt-5.3-codex 41 127840 claude-sonnet-4-20250514 8 31500 Latency (ms)
---------------------------------------- classifier avg=12 p50=11 p95=24 total avg=847 p50=620 p95=2340 Token Usage
------------------------------ Prompt: 138420 Completion: 69130 Total: 207550 Fallbacks: 3 Errors: 2 Streaming requests: 47 Requests with tools: 18 (54 tools total)
</code></pre></div>
<div><h3><pre><code>nadirclaw classify</code></pre></h3><a href="#nadirclaw-classify"></a></div>
<p>Classify a prompt locally without running the server. Useful for testing your setup:</p>
<div><pre>$ nadirclaw classify <span><span>"</span>What is 2+2?<span>"</span></span>
Tier: simple
Confidence: 0.2848
Score: 0.0000
Model: gemini-3-flash-preview $ nadirclaw classify <span><span>"</span>Design a distributed system for real-time trading<span>"</span></span>
Tier: complex
Confidence: 0.1843
Score: 1.0000
Model: gemini-2.5-pro</pre></div>
<div><h3><pre><code>nadirclaw status</code></pre></h3><a href="#nadirclaw-status"></a></div>
<div><pre>$ nadirclaw status
NadirClaw Status
----------------------------------------
Simple model: gemini-3-flash-preview
Complex model: gemini-2.5-pro
Tier config: explicit (env vars)
Port: 8856
Threshold: 0.06
Log dir: /Users/you/.nadirclaw/logs
Token: nadir-<span>***</span> Server: RUNNING (ok)</pre></div>
<div><h2>How It Works</h2><a href="#how-it-works"></a></div>
<p>Most LLM usage doesn't need a premium model. NadirClaw routes each prompt to the right tier automatically:</p>
<p> <a target="_blank" href="/doramirdor/NadirClaw/blob/main/docs/images/usage-distribution.png"><img src="/doramirdor/NadirClaw/raw/main/docs/images/usage-distribution.png" alt="Typical LLM usage distribution"></a>
</p>
<p>NadirClaw uses a binary complexity classifier based on sentence embeddings:</p>
<ol>
<li>
<p><strong>Pre-computed centroids</strong>: Ships two tiny centroid vectors (~1.5 KB each) derived from ~170 seed prompts. These are pre-computed and included in the package — no training step required.</p>
</li>
<li>
<p><strong>Classification</strong>: For each incoming prompt, computes its embedding using <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">all-MiniLM-L6-v2</a> (~80 MB, downloaded once on first use) and measures cosine similarity to both centroids. If the prompt is closer to the complex centroid, it routes to your complex model; otherwise to your simple model.</p>
</li>
<li>
<p><strong>Borderline handling</strong>: When confidence is below the threshold (default 0.06), the classifier defaults to complex -- it's cheaper to over-serve a simple prompt than to under-serve a complex one.</p>
</li>
<li>
<p><strong>Routing modifiers</strong>: After classification, NadirClaw applies intelligent overrides:</p>
<ul>
<li><strong>Agentic detection</strong> — if tool definitions, tool-role messages, or agent system prompts are detected, forces the complex model</li>
<li><strong>Reasoning detection</strong> — if 2+ reasoning markers are found, routes to the reasoning model</li>
<li><strong>Context window check</strong> — if the conversation exceeds the model's context window, swaps to a model that fits</li>
<li><strong>Session persistence</strong> — reuses the same model for follow-up messages in the same conversation</li>
</ul>
</li>
<li>
<p><strong>Dispatch</strong>: Calls the selected model via the appropriate backend:</p>
<ul>
<li><strong>Gemini models</strong> — called natively via the <a href="https://github.com/googleapis/python-genai">Google GenAI SDK</a> for best performance</li>
<li><strong>All other models</strong> — called via <a href="https://docs.litellm.ai">LiteLLM</a>, which provides a unified interface to 100+ providers</li>
</ul>
</li>
<li>
<p><strong>Rate limit fallback</strong>: If the selected model returns a 429 rate limit error, NadirClaw retries once, then automatically falls back to the other tier's model. If both are rate-limited, it returns a user-friendly error message.</p>
</li>
</ol>
<p>Classification takes ~10ms on a warm encoder. The first request takes ~2-3 seconds to load the embedding model.</p>
<div><h2>API Endpoints</h2><a href="#api-endpoints"></a></div>
<p>Auth is disabled by default (local-only). Set </p><pre><code>NADIRCLAW_AUTH_TOKEN</code></pre> to require a bearer token.<p></p>
<table>
<thead>
<tr>
<th>Endpoint</th>
<th>Method</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><pre><code>/v1/chat/completions</code></pre></td>
<td>POST</td>
<td>OpenAI-compatible completions with auto routing (supports <pre><code>stream: true</code></pre>)</td>
</tr>
<tr>
<td><pre><code>/v1/classify</code></pre></td>
<td>POST</td>
<td>Classify a prompt without calling an LLM</td>
</tr>
<tr>
<td><pre><code>/v1/classify/batch</code></pre></td>
<td>POST</td>
<td>Classify multiple prompts at once</td>
</tr>
<tr>
<td><pre><code>/v1/models</code></pre></td>
<td>GET</td>
<td>List available models</td>
</tr>
<tr>
<td><pre><code>/v1/logs</code></pre></td>
<td>GET</td>
<td>View recent request logs</td>
</tr>
<tr>
<td><pre><code>/health</code></pre></td>
<td>GET</td>
<td>Health check (no auth required)</td>
</tr>
</tbody>
</table>
<div><h2>Configuration Reference</h2><a href="#configuration-reference"></a></div>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><pre><code>NADIRCLAW_SIMPLE_MODEL</code></pre></td>
<td><pre><code>gemini-3-flash-preview</code></pre></td>
<td>Model for simple prompts</td>
</tr>
<tr>
<td><pre><code>NADIRCLAW_COMPLEX_MODEL</code></pre></td>
<td><pre><code>openai-codex/gpt-5.3-codex</code></pre></td>
<td>Model for complex prompts</td>
</tr>
<tr>
<td><pre><code>NADIRCLAW_REASONING_MODEL</code></pre></td>
<td><em>(falls back to complex)</em></td>
<td>Model for reasoning tasks</td>
</tr>
<tr>
<td><pre><code>NADIRCLAW_FREE_MODEL</code></pre></td>
<td><em>(falls back to simple)</em></td>
<td>Free fallback model</td>
</tr>
<tr>
<td><pre><code>NADIRCLAW_AUTH_TOKEN</code></pre></td>
<td><em>(empty — auth disabled)</em></td>
<td>Set to require a bearer token</td>
</tr>
<tr>
<td><pre><code>GEMINI_API_KEY</code></pre></td>
<td>--</td>
<td>Google Gemini API key (also accepts <pre><code>GOOGLE_API_KEY</code></pre>)</td>
</tr>
<tr>
<td><pre><code>ANTHROPIC_API_KEY</code></pre></td>
<td>--</td>
<td>Anthropic API key</td>
</tr>
<tr>
<td><pre><code>OPENAI_API_KEY</code></pre></td>
<td>--</td>
<td>OpenAI API key</td>
</tr>
<tr>
<td><pre><code>OLLAMA_API_BASE</code></pre></td>
<td><pre><code>http://localhost:11434</code></pre></td>
<td>Ollama base URL</td>
</tr>
<tr>
<td><pre><code>NADIRCLAW_CONFIDENCE_THRESHOLD</code></pre></td>
<td><pre><code>0.06</code></pre></td>
<td>Classification threshold (lower = more complex)</td>
</tr>
<tr>
<td><pre><code>NADIRCLAW_PORT</code></pre></td>
<td><pre><code>8856</code></pre></td>
<td>Server port</td>
</tr>
<tr>
<td><pre><code>NADIRCLAW_LOG_DIR</code></pre></td>
<td><pre><code>~/.nadirclaw/logs</code></pre></td>
<td>Log directory</td>
</tr>
<tr>
<td><pre><code>NADIRCLAW_LOG_RAW</code></pre></td>
<td><pre><code>false</code></pre></td>
<td>Log full raw requests and responses (<pre><code>true</code></pre>/<pre><code>false</code></pre>)</td>
</tr>
<tr>
<td><pre><code>NADIRCLAW_MODELS</code></pre></td>
<td><pre><code>openai-codex/gpt-5.3-codex,gemini-3-flash-preview</code></pre></td>
<td>Legacy model list (fallback if tier vars not set)</td>
</tr>
<tr>
<td><pre><code>OTEL_EXPORTER_OTLP_ENDPOINT</code></pre></td>
<td><em>(empty — disabled)</em></td>
<td>OpenTelemetry collector endpoint (enables tracing)</td>
</tr>
</tbody>
</table>
<div><h2>OpenTelemetry (Optional)</h2><a href="#opentelemetry-optional"></a></div>
<p>NadirClaw supports optional distributed tracing via OpenTelemetry. Install the extras and set an OTLP endpoint:</p>
<div><pre>pip install nadirclaw[telemetry] <span><span>#</span> Export to a local collector (e.g. Jaeger, Grafana Tempo)</span>
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 nadirclaw serve</pre></div>
<p>When enabled, NadirClaw emits spans for:</p>
<ul>
<li><strong><pre><code>smart_route_analysis</code></pre></strong> — classifier decision with tier and selected model</li>
<li><strong><pre><code>dispatch_model</code></pre></strong> — individual LLM provider call</li>
<li><strong><pre><code>chat_completion</code></pre></strong> — full request lifecycle</li>
</ul>
<p>Spans include <a href="https://opentelemetry.io/docs/specs/semconv/gen-ai/">GenAI semantic conventions</a> (</p><pre><code>gen_ai.request.model</code></pre>, <pre><code>gen_ai.usage.input_tokens</code></pre>, <pre><code>gen_ai.usage.output_tokens</code></pre>) plus custom <pre><code>nadirclaw.*</code></pre> attributes for routing metadata.<p></p>
<p>If the telemetry packages are not installed or </p><pre><code>OTEL_EXPORTER_OTLP_ENDPOINT</code></pre> is not set, all tracing is a no-op with zero overhead.<p></p>
<div><h2>Project Structure</h2><a href="#project-structure"></a></div>
<div><pre><code>nadirclaw/ __init__.py # Package version cli.py # CLI commands (setup, serve, classify, report, status, auth, codex, openclaw) setup.py # Interactive setup wizard (provider selection, credentials, model config) server.py # FastAPI server with OpenAI-compatible API + streaming classifier.py # Binary complexity classifier (sentence embeddings) credentials.py # Credential storage, resolution chain, and OAuth token refresh encoder.py # Shared SentenceTransformer singleton oauth.py # OAuth login flows (OpenAI, Anthropic, Gemini, Antigravity) routing.py # Routing intelligence (agentic, reasoning, profiles, aliases, sessions) report.py # Log parsing and report generation telemetry.py # Optional OpenTelemetry integration (no-op without packages) auth.py # Bearer token / API key authentication settings.py # Environment-based configuration (reads ~/.nadirclaw/.env) prototypes.py # Seed prompts for centroid generation simple_centroid.npy # Pre-computed simple centroid vector complex_centroid.npy # Pre-computed complex centroid vector
</code></pre></div>
<div><h2>License</h2><a href="#license"></a></div>
<p>MIT</p>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>