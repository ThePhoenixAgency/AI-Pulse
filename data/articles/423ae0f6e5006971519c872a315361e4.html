<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 1/5/2026 10:56:51 PM | <a href="https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/tsungyi"><img alt="Tsung-Yi Lin's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/63b738acbd2d1535227daa4c/dbPQFvHwC-Cf-ssMGYUo6.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/debrajsinha"><img alt="Debraj Sinha's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/689a6b88b0d1d056f147c631/TaRXOL2s8rNJmmhYDt1bv.jpeg"></a> </span> </span></p> </div></div> <div><nav><ul><li><a href="#nvidia-cosmos-reason-2-reasoning-vision-language-model-for-physical-ai">NVIDIA Cosmos Reason 2: Reasoning Vision Language Model for Physical AI</a> <ul><li><a href="#-key-highlights"> Key Highlights</a> <ul></ul> </li><li><a href="#-popular-use-cases"> Popular Use Cases</a> <ul></ul> </li></ul> </li><li><a href="#other-models-from-the-cosmos-family">Other Models From The Cosmos Family:</a> <ul><li><a href="#-cosmos-predict-25"><strong> Cosmos Predict 2.5</strong></a> <ul></ul> </li></ul> </li><li><a href="#resources">Resources</a> <ul></ul> </li></ul></nav></div> <p>NVIDIA today released <a href="https://huggingface.co/nvidia/Cosmos-Reason2-8B">Cosmos Reason 2</a>, the latest advancement in open, reasoning vision language models for physical AI. Cosmos Reason 2 surpasses its previous version in accuracy and tops the <a href="https://huggingface.co/spaces/shi-labs/physical-ai-bench-leaderboard">Physical AI Bench</a> and <a href="https://huggingface.co/spaces/facebook/physical_reasoning_leaderboard">Physical Reasoning</a> leaderboards as the #1 open model for visual understanding.</p>
<h2> <a href="#nvidia-cosmos-reason-2-reasoning-vision-language-model-for-physical-ai"> <span></span> </a> <span> NVIDIA Cosmos Reason 2: Reasoning Vision Language Model for Physical AI </span>
</h2>
<p>Since their introduction, <a href="https://www.nvidia.com/en-us/glossary/vision-language-models/">vision-language models</a> have rapidly improved at tasks like object and pattern recognition in images. But they still struggle with tasks humans find natural, like planning several steps ahead, dealing with uncertainty or adapting to new situations. Cosmos Reason is designed to close this gap by giving robots and AI agents stronger common sense and reasoning to solve complex problems step by step.</p>
<p>Cosmos Reason 2 is a state-of-the-art, open reasoning vision-language model (VLM) that enables robots and AI agents to see, understand, plan, and act in the physical world like humans. It uses common sense, physics, and prior knowledge to recognize how objects move across space and time to handle complex tasks, adapt to new situations, and figure out how to solve problems step by step. </p>
<h3> <a href="#-key-highlights"> <span></span> </a> <span> Key Highlights </span>
</h3>
<ul>
<li><p>Improved spatio-temporal understanding and timestamp precision.</p>
</li>
<li><p>Optimized performance with flexible deployment options from edge to cloud with 2B and 8B parameters model sizes.</p>
</li>
<li><p>Support for expanded set of spatial understanding and visual perception capabilities ‚Äî 2D/3D point localization, bounding box coordinates, trajectory data, and OCR support. </p>
</li>
<li><p>Improved long-context understanding with 256K input tokens, up from 16K with Cosmos Reason 1.</p>
</li>
<li><p>Adaptable to multiple use cases with easy-to-use <a href="https://nvidia-cosmos.github.io/cosmos-cookbook/index.html">Cosmos Cookbook recipes</a>.</p>
</li>
</ul>
<h3> <a href="#-popular-use-cases"> <span></span> </a> <span> Popular Use Cases </span>
</h3>
<ul>
<li><p><strong>Video analytics AI agents</strong> ‚Äî These agents can extract valuable insights from massive volumes of video data to optimize processes. Cosmos Reason 2 builds on the capabilities of Cosmos Reason 1 and now provides OCR support, as well as 2D/3D point localization and a set of mark understanding. </p>
<figure> <figcaption>Example of how Cosmos Reason can understand text embedded within a video to determine the condition of the road during a rainstorm. </figcaption>
</figure> <p>Developers can jumpstart development of video analytics AI agents by using the <a href="https://build.nvidia.com/nvidia/video-search-and-summarization">NVIDIA blueprint for video search and summarization (VSS)</a> with Cosmos Reason as the VLM.</p>
<p><a href="https://salesforce.com/blog/the-new-frontier-of-physical-ai-how-salesforce-and-nvidia-turn-robots-into-enterprise-agents/">Salesforce</a> is transforming workplace safety and compliance by analyzing video footage captured by Cobalt robots with Agentforce and VSS blueprint with Cosmos Reason as the VLM. </p>
</li>
<li><p><strong>Data annotation and critique</strong> ‚Äî Enable developers to automate high-quality annotation and critique of massive, diverse training datasets. Cosmos Reason provides time stamps and detailed descriptions for real or synthetically generated training videos. </p>
<figure> <img alt="Data annotation and critique example" src="https://cdn-uploads.huggingface.co/production/uploads/63b738acbd2d1535227daa4c/N1iod6-BikvBPFe9LUoi5.png"> <figcaption>Example of a sample prompt to generate detailed, time-stamped captions for a race car video.</figcaption>
</figure> <p><a href="https://nvidia-cosmos.github.io/cosmos-cookbook/recipes/post_training/reason2/video_caption_vqa/post_training.html">Uber is exploring Cosmos Reason 2</a> to deliver accurate, searchable video captions for autonomous vehicle (AV) training data, enabling efficient identification of critical driving scenarios. This <a href="https://nvidia-cosmos.github.io/cosmos-cookbook/recipes/post_training/reason2/video_caption_vqa/post_training.html">co-authored Reason 2 for AV Video Captioning and VQA recipe</a> demonstrates how to fine-tune and evaluate Cosmos Reason 2-8B on annotated AV videos. Across multiple evaluation metrics, measurable improvements were achieved: BLEU scores improved 10.6% (0.113 ‚Üí 0.125), MCQ-based VQA gained 0.67 percentage points (80.18% ‚Üí 80.85%), and LingoQA increased 13.8% (63.2% ‚Üí 77.0%). These gains demonstrate effective domain adaptation for AV applications. </p>
</li>
<li><p><strong>Robot planning and reasoning</strong> ‚Äî Act as the brain for deliberate, methodical decision-making in a robot vision language action (VLA) model. Cosmos Reason 2 now provides trajectory coordinates in addition to determining next steps. </p> <figure> <figcaption>Example of the prompt and JSON output from Cosmos Reason 2 to provide the steps and trajectory the robot gripper needs to take to move the painter‚Äôs tape into the basket.</figcaption> </figure> <p><a href="https://encord.com/blog/data-agents">Encord</a> provides native support for Cosmos Reason 2 in its <a href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fencord.com%2Fdata-agents%2F&amp;data=05%7C02%7Ckrumley%40nvidia.com%7C42a86cea3d0c445e973208de44e9bea2%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C639023968198962586%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C40000%7C%7C%7C&amp;sdata=%2Fqw0guXgke6MRaqZShEkwhyYu%2FohI314%2Fju3A4%2F5pao%3D&amp;reserved=0">Data Agent</a> library and AI data platform, enabling developers to leverage Cosmos Reason 2 as a VLA for robotics and other physical AI use cases.</p>
</li>
</ul>
<p>Companies like Hitachi, <a href="https://www.milestonesys.com/company/news/press-releases/milestone-launches-vision-language-model/">Milestone</a> and <a href="https://www.vastdata.com/blog/vast-nvidia-cosmos-reason-smart-cities">VAST Data</a> are using Cosmos Reason to advance robotics, autonomous driving, and video analytics AI agents for traffic and workplace safety.</p>
<p>Try <a href="https://build.nvidia.com/nvidia/cosmos-reason2-8b">Cosmos Reason 2 on build.nvidia.com</a> and experience the latest features with sample prompts for generating bounding boxes and robot trajectories. Upload your own videos and images for further analysis. </p>
<p>Download Cosmos Reason 2 models (<a href="https://huggingface.co/nvidia/Cosmos-Reason2-2B">2B</a> and <a href="https://huggingface.co/nvidia/Cosmos-Reason2-8B">8B</a>) on Hugging Face or <a href="https://nvidia-cosmos.github.io/cosmos-cookbook/getting_started/brev/reason2/reason2_on_brev.html">use Cosmos Reason 2 in the cloud</a>. The model will be available soon on Amazon Web Services, Google Cloud and Microsoft Azure. To get started, check out <a href="https://docs.nvidia.com/cosmos/latest/reason2/index.html">Cosmos Reason 2 documentation</a> and the <a href="https://nvidia-cosmos.github.io/cosmos-cookbook/index.html">Cosmos Cookbook</a>. </p>
<h2> <a href="#other-models-from-the-cosmos-family"> <span></span> </a> <span> Other Models From The Cosmos Family: </span>
</h2>
<h3> <a href="#-cosmos-predict-25"> <span></span> </a> <span> <strong> Cosmos Predict 2.5</strong> </span>
</h3>
<p>Cosmos Predict is a generative AI model that predicts future states of the physical world as video, based on text, image, or video inputs.</p>
<ul>
<li><a href="https://huggingface.co/spaces/shi-labs/physical-ai-bench-leaderboard">Physical AI Bench</a> leader for quality, accuracy and overall consistency. </li>
<li>Up to 30 seconds of physically and temporally consistent clip per generation. </li>
<li>Supports multiple framerates and resolution. </li>
<li>Pre-trained on 200 million clips. </li>
<li>Available as 2B and 14B pre-trained models and various 2B post-trained models for multiview, action conditioning and autonomous vehicle training.</li>
</ul>
<p><a href="https://huggingface.co/nvidia/Cosmos-Predict2.5-2B">Check out model card</a>&gt;&gt;</p>
<p> <strong> Cosmos Transfer 2.5</strong></p>
<p>Cosmos Transfer is our lightest multicontrol model built for video to world style transfer. </p>
<ul>
<li>Scale a single simulation or spatial video across various environments and lighting conditions. </li>
<li>Improved prompt adherence and physics alignment. </li>
<li>Use with <a href="https://developer.nvidia.com/isaac/sim">NVIDIA Isaac Sim</a> or <a href="https://developer.nvidia.com/blog/accelerating-av-simulation-with-neural-reconstruction-and-world-foundation-models/">NVIDIA Omniverse NuRec</a> for simulation to real transformation.</li>
</ul>
<p><a href="https://huggingface.co/nvidia/Cosmos-Transfer2.5-2B">Check out model card&gt;&gt;</a></p>
<p><strong> NVIDIA GR00T N1.6</strong></p>
<p><a href="https://research.nvidia.com/labs/gear/gr00t-n1_6/">NVIDIA GR00T N1.6</a> is an open reasoning vision language action (VLA) model, purpose-built for humanoid robots, that unlocks full body control and uses NVIDIA Cosmos Reason for better reasoning and contextual understanding.</p>
<h2> <a href="#resources"> <span></span> </a> <span> Resources </span>
</h2>
<p> Watch a demo of Cosmos ‚Üí <a href="https://youtu.be/iWs-2TD5Dcc">https://youtu.be/iWs-2TD5Dcc</a></p>
<p>üèª Read the Cosmos Cookbook ‚Üí <a href="https://nvda.ws/4qevli8">https://nvda.ws/4qevli8</a></p>
<p> Explore Models &amp; Datasets ‚Üí <a href="https://github.com/nvidia-cosmos">https://github.com/nvidia-cosmos</a></p>
<p> Try Cosmos Models in our Hosted Catalog ‚Üí <a href="https://nvda.ws/3Yg0Dcx">https://nvda.ws/3Yg0Dcx</a></p>
<p> Join the Cosmos Community ‚Üí <a href="https://discord.gg/u23rXTHSC9">https://discord.gg/u23rXTHSC9</a></p>
<p> Contribute to the Cosmos Cookbook ‚Üí <a href="https://nvda.ws/4aQcBkk">https://nvda.ws/4aQcBkk</a></p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">‚ñ≤</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">‚ñº</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>