<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>We don’t have to have unsupervised killer robots</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>We don’t have to have unsupervised killer robots</h1>
  <div class="metadata">
    Source: The Verge | Date: 2/27/2026 4:18:26 PM | <a href="https://www.theverge.com/ai-artificial-intelligence/885963/anthropic-dod-pentagon-tech-workers-ai-labs-react" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><div><p>It’s the day of the Pentagon’s looming ultimatum for Anthropic: allow the US military <a href="https://www.theverge.com/ai-artificial-intelligence/883456/anthropic-pentagon-department-of-defense-negotiations">unchecked access</a> to its technology, including for mass surveillance and fully autonomous lethal weapons, or potentially be designated a “supply chain risk” and potentially lose hundreds of billions of dollars in contracts. Amid the intensifying public statements and threats, tech workers across the industry are looking at their own companies’ government and military contracts wondering what kind of future they’re helping to build.</p><p>While the Department of Defense has spent weeks negotiating with Anthropic over removing its guardrails, including allowing the US military to use Anthropic’s AI kill targets with no human oversight, OpenAI and xAI had <a href="https://www.washingtonpost.com/technology/2026/02/22/pentagon-anthropic-ai-dispute/">reportedly</a> already agreed to such terms, although OpenAI is <a href="https://www.washingtonpost.com/technology/2026/02/22/pentagon-anthropic-ai-dispute/">reportedly</a> attempting to adopt the same red lines in the agreements as Anthropic. The overall situation has left employees at some companies with defense contracts feeling betrayed. “When I joined the tech industry, I thought tech was about making people’s lives easier,” an Amazon Web Services employee told <em>The Verge</em>, “but now it seems like it’s all about making it easier to surveil and deport and kill people.”</p><p>In conversations with <em>The Verge</em>, current and former employees from OpenAI, xAI, Amazon, Microsoft, and Google expressed similar feelings about the changing moral landscape of their companies. Organized groups representing 700,000 tech workers at Amazon, Google, Microsoft, and more have <a href="https://medium.com/@notechforapartheid/jointstatement-5561f1572e46">signed a letter</a> demanding that the companies reject the Pentagon’s demands. But many saw little chance of their employers — whether they’re directly embroiled in this conflict or not — questioning the government or pushing back.</p><p>“From their perspective, they’d love to keep making money and not have to talk about it,” said a software engineer from Microsoft.</p><p>So far, Anthropic has stood its ground. Anthropic CEO Dario Amodei put out a <a href="https://www.anthropic.com/news/statement-department-of-war">statement</a> on Thursday that the Pentagon’s “threats do not change our position: we cannot in good conscience accede to their request.” But he has stated that he is not at all opposed to lethal autonomous weapons sometime in the future, just that the technology was not reliable enough “today.” Amodei even offered to partner with the DoD on “R&amp;D to improve the reliability of these systems, but they have not accepted this offer,” he wrote in the statement.</p><p>In the past few years, however, major tech companies have <a href="https://www.theverge.com/podcast/784865/ai-safety-military-defense-openai-anthropic-ethics">loosened their rules or changed their mission statements</a> to expand into lucrative government or military contracts. In 2024, OpenAI removed a ban on “military and warfare” use cases from its terms of service; after that, it signed a deal with autonomous weapons maker Anduril and then its DoD contract, and just this week, Anthropic <a href="https://www.anthropic.com/news/responsible-scaling-policy-v3">changed its oft-touted responsible scaling policy</a>, dropping its longtime safety pledge in order to ensure it stayed competitive in the AI race. Big Tech players like Amazon, Google, and Microsoft have also allowed defense and intelligence agencies to use their AI products, including <a href="https://www.theverge.com/ai-artificial-intelligence/876558/tech-workers-ice-resistance-google-microsoft-clear-abbott">some agreeing to work with ICE</a> despite growing outcry from the public and employees alike.</p><p>In past years, tech workers’ resistance to partnerships and deals they deem harmful to society at large sometimes led to big change. In 2018, for instance, thousands of Google employees successfully pressured the company to end its “<a href="https://www.nytimes.com/2018/06/01/technology/google-pentagon-project-maven.html">Project Maven</a>” partnership with the Pentagon, and Microsoft workers presented leadership with an anti-ICE <a href="https://www.nytimes.com/2018/06/19/technology/tech-companies-immigration-border.html">petition</a> signed by <a href="https://www.cnbc.com/2018/07/27/microsoft-employees-question-ceo--satya-nadella-over-companys-contrac.html">about 500</a> Microsoft employees, though Microsoft <a href="https://www.972mag.com/ice-microsoft-azure-leaked-files/">still works with</a> the agency. In 2020, after the murder of George Floyd, tech companies made public statements about and financial commitments supporting the Black Lives Matter movement. But in recent months, the industry has seen a very different reality: a culture of fear and silence, especially amid cooperation with the Trump administration and ICE, tech workers <a href="https://www.theverge.com/ai-artificial-intelligence/876558/tech-workers-ice-resistance-google-microsoft-clear-abbott">recently told <em>The Verge</em></a>.</p><p>Companies have followed in the footsteps of longtime surveillance and military tech partnerships, who have only become more hawkish. That includes the Peter Thiel-cofounded Palantir, whose CEO Alex Karp recently <a href="https://x.com/MmisterNobody/status/2023780218650194110?s=20">stated</a> to shareholders that “Palantir is here to disrupt and make the institutions we partner with the very best in the world, and, when it’s necessary, to scare enemies and on occasion kill them. And we hope you’re in favor of that.” (Protect Democracy, a nonprofit, recently put out an <a href="https://protectdemocracy.org/work/anthropic-sign-on/">open letter</a> calling for Congressional oversight of the Department of Defense’s demands for unrestricted use of AI. )</p><p>OpenAI, Google, Microsoft, xAI, and Amazon did not immediately respond to requests for comment.</p><p>A former xAI employee told <em>The Verge</em>, “Everyone is actually working on killer robots at this point,” adding that he believes everyone will follow in the footsteps of Palantir, Anduril, and xAI, since the government sentiment is that if a company doesn’t acquiesce, it’s “against the benefits of the country, in a sense.” He said there’s a “big push for working with the military, and the trend is it’s cool to do it… You’re a patriot if you do it.”</p><p>A Google employee called the situation a “dominance display from Hegseth that is disgusting.” He added, “Over and over AI is presenting us with choices about who we want to be and what kind of society and future we want to have. And they’re coming at us fast and with, really, the least thoughtful and least principled leaders in power that we could imagine. I can only thank Anthropic for insisting on the decent path and using their leverage — that they are indispensable — to chart a course toward a humane world and a humane future.”</p><p>The AWS employee told <em>The Verge</em> that “boundaries have definitely eroded in terms of the customers big tech is willing to court” and that there’s “a deliberate whitewashing of the implications of new lucrative deals.” She recalled recently receiving an email from an AWS executive touting a more than $580 million contract with the US Air Force, among other partnerships, as a sign of Amazon’s AI successes, with no acknowledgment of the broader scope or harms involved.</p><p>“If the government is hell-bent on pursuing technologies like this, they should have to build them themselves, and be answerable for those decisions,” she said.</p><p>The erosion may have extended to internal culture as well — normalizing the idea that companies should always be watching. The AWS employee said that she and her colleagues are tracked on how much they’re using AI for their jobs, how often they’re working from the office, and more. “I can see myself and my coworkers getting more desensitized to surveillance on ourselves at work, and I’m worried that means we’re obeying, complying, and giving up too much in advance,” she said.</p><p>An OpenAI employee said the general feeling within the AI industry over the last few weeks “has reopened the door to more discussion… about the values and the future of the technology.” The employee said that the Pentagon-Anthropic situation, the recent ICE headlines, and the fast advancement of AI have been some of the main factors opening up those discussions internally.</p><p>Even so, people who are immigrants or in more vulnerable positions are more afraid to speak, the OpenAI employee said.</p><p>Anthropic, the former xAI employee said, seems like it’s in a position where it can say no and still stay afloat. Its focus on enterprise rather than consumer AI business may make it more sustainable even without government contracts, offering it some leverage. A software engineer at Microsoft said of Anthropic, speaking generally, “I was surprised to see them stand on some form of principle. I don’t know how long it’ll last.”</p><p>“Will it last?” seems to be the question on everyone’s lips. The Pentagon has already <a href="https://www.axios.com/2026/02/25/anthropic-pentagon-blacklist-claude">reportedly</a> asked two major defense contractors, Boeing and Lockheed Martin, to provide information about their reliance on Anthropic’s Claude, as it moves to potentially designate Anthropic a “supply chain risk,” a classification usually reserved for <a href="https://www.theverge.com/ai-artificial-intelligence/883456/anthropic-pentagon-department-of-defense-negotiations">threats to national security</a> and rarely, if ever, assigned to a US company. It also <a href="https://www.axios.com/2026/02/24/anthropic-pentagon-claude-hegseth-dario">reportedly</a> may be considering invoking the Defense Production Act to attempt to force Anthropic to comply with its request.</p><p>Just like with any other AI company, if Anthropic folds, the Microsoft employee said, there’s little chance of it or others pulling back on killer robots and surveillance. “Once you’re in the door with the Department of Defense or whatever we’re calling it now… I think it’s probably hard for them to actually have the oversight they claim. It’s just going to be lucrative to basically give themselves permission to do the thing that makes the most money.”</p><p>In Microsoft’s own case, he said he doesn’t expect the company to adhere to any sort of ethical principles. The company has worked extensively with the Israeli Defense Forces, including for mass surveillance of Palestinians and dissidents, <a href="https://www.theverge.com/ai-artificial-intelligence/761731/pro-palestinian-protests-microsoft-headquarters-redmond-washington-no-azure-tech-for-apartheid">despite employee protest</a>. (It said it <a href="https://www.nbcnews.com/tech/tech-news/microsoft-ends-israel-military-units-access-cloud-service-rcna233723">ended some parts</a> of the partnership last year.)</p><p>Another Microsoft employee told <em>The Verge</em> that although “Microsoft holds a Responsible AI ‘commitment,’… they are currently attempting to play both sides for the sake of profit rather than meaningfully commitment to Responsible AI.”</p><p>But this is nothing new, one AI startup employee said. In her eyes, the boundaries have often been “fuzzy, especially within AI,” about what kinds of things companies are willing to let their technology power. “A lot of it has been going on beneath the surface for as long as AI has been around.”</p><p>The AWS employee emphasized that “we need cross-tech solidarity and a coherent, worker-led vision for AI now more than ever.”</p><p>“The safeguards that Anthropic is trying to keep in place are no mass surveillance of Americans and no fully autonomous weapons, which just means that they want a human in the loop if the machine is going to kill somebody,” she added. “Even if this technology were perfect — which it isn’t — I think most Americans don’t want machines that kill people without human oversight running around in an America that’s become an AI-powered mass surveillance state.”</p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li><span><span><span></span><span>Hayden Field</span></span></span></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul></div></div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>