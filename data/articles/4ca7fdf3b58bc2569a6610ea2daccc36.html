<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>GitHub - pydantic/monty at console.dev</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.8; color: #e2e8f0; max-width: 800px; margin: 40px auto; padding: 0 20px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.5em; }
  .metadata { color: #94a3b8; font-size: 0.9em; margin-bottom: 2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 1em; }
  img { max-width: 100%; height: auto; border-radius: 8px; }
  a { color: #00d9ff; }
  p { margin-bottom: 1em; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 15px; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 15px; border-radius: 6px; overflow-x: auto; }

  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }

</style>
</head>
<body>
  <h1>GitHub - pydantic/monty at console.dev</h1>
  <div class="metadata">
    Source: Console.dev | Date: 2/12/2026 | Lang: EN |
    <a href="https://github.com/pydantic/monty?ref=console.dev" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div><article><div>
  <p></p><h2>Monty</h2><a href="#monty"></a><p></p>
</div>
<div>
  <p></p><h3>A minimal, secure Python interpreter written in Rust for use by AI.</h3><a href="#a-minimal-secure-python-interpreter-written-in-rust-for-use-by-ai"></a><p></p>
</div>
<p><a href="https://github.com/pydantic/monty/actions/workflows/ci.yml?query=branch%3Amain"><img src="https://github.com/pydantic/monty/actions/workflows/ci.yml/badge.svg" alt="CI" /></a>
  <a href="https://codspeed.io/pydantic/monty?utm_source=badge"><img src="https://camo.githubusercontent.com/8dad61a3ac354a42b12bb7a2c0a672c496faf5b24408d56c247e324f9c860078/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6453706565642d506572666f726d616e6365253230547261636b65642d626c75653f6c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423361575230614430694d5459694947686c6157646f644430694d54596949485a705a58644362336739496a41674d4341784e6941784e6949675a6d6c7362443069626d39755a53496765473173626e4d39496d6830644841364c79393364336375647a4d7562334a6e4c7a49774d44417663335a6e496a3438634746306143426b50534a4e4f434177544441674f45773449444532544445324944684d4f434177576949675a6d6c736244306964326870644755694c7a34384c334e325a7a343d" alt="Codspeed" /></a>
  <a href="https://codecov.io/gh/pydantic/monty"><img src="https://camo.githubusercontent.com/cc1e0d740b6425e467165c0952310c87025558eed6f6a2f412527f2ec8bfb756/68747470733a2f2f636f6465636f762e696f2f67682f707964616e7469632f6d6f6e74792f67726170682f62616467652e7376673f746f6b656e3d48583452445158354f47" alt="Coverage" /></a>
  <a href="https://pypi.python.org/pypi/pydantic-monty"><img src="https://camo.githubusercontent.com/26cfb4e82aaef72860dcd1f28b7f433e4c1881b52cfccefada4819015ff1b792/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f707964616e7469632d6d6f6e74792e737667" alt="PyPI" /></a>
  <a href="https://github.com/pydantic/monty"><img src="https://camo.githubusercontent.com/7c7bb07017a92fc1e7dad4fb66ef4a2537ee1ae9f6a227040358d52356adc6c9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f707964616e7469632d6d6f6e74792e737667" alt="versions" /></a>
  <a href="https://github.com/pydantic/monty/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/da94340d95ce34e16e779627d15b70e3674eebcb8db26a7024cf83a0857594f5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f707964616e7469632f6d6f6e74792e7376673f763d32" alt="license" /></a>
  <a href="https://logfire.pydantic.dev/docs/join-slack/"><img src="https://camo.githubusercontent.com/baa9b9b02e5dd6a293c1c23eabdcddca96d92fce408d515f1d49b678888eacd7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f536c61636b2d4a6f696e253230536c61636b2d3441313534423f6c6f676f3d736c61636b" alt="Join Slack" /></a>
</p>
<hr />
<p><strong>Experimental</strong> - This project is still in development, and not ready for the prime time.</p>
<p>A minimal, secure Python interpreter written in Rust for use by AI.</p>
<p>Monty avoids the cost, latency, complexity and general faff of using a full container based sandbox for running LLM generated code.</p>
<p>Instead, it lets you safely run Python code written by an LLM embedded in your agent, with startup times measured in single digit microseconds not hundreds of milliseconds.</p>
<p>What Monty <strong>can</strong> do:</p>
<ul>
<li>Run a reasonable subset of Python code - enough for your agent to express what it wants to do</li>
<li>Completely block access to the host environment: filesystem, env variables and network access are all implemented via external function calls the developer can control</li>
<li>Call functions on the host - only functions you give it access to</li>
<li>Run typechecking - monty supports full modern python type hints and comes with <a href="https://docs.astral.sh/ty/">ty</a> included in a single binary to run typechecking</li>
<li>Be snapshotted to bytes at external function calls, meaning you can store the interpreter state in a file or database, and resume later</li>
<li>Startup extremely fast (&lt;1Î¼s to go from code to execution result), and has runtime performance that is similar to CPython (generally between 5x faster and 5x slower)</li>
<li>Be called from Rust, Python, or Javascript - because Monty has no dependencies on cpython, you can use it anywhere you can run Rust</li>
<li>Control resource usage - Monty can track memory usage, allocations, stack depth, and execution time and cancel execution if it exceeds preset limits</li>
<li>Collect stdout and stderr and return it to the caller</li>
<li>Run async or sync code on the host via async or sync code on the host</li>
</ul>
<p>What Monty <strong>cannot</strong> do:</p>
<ul>
<li>Use the standard library (except a few select modules: <code>sys</code>, <code>typing</code>, <code>asyncio</code>, <code>dataclasses</code> (soon), <code>json</code> (soon))</li>
<li>Use third party libraries (like Pydantic), support for external python library is not a goal</li>
<li>define classes (support should come soon)</li>
<li>use match statements (again, support should come soon)</li>
</ul>
<hr />
<p>In short, Monty is extremely limited and designed for <strong>one</strong> use case:</p>
<p><strong>To run code written by agents.</strong></p>
<p>For motivation on why you might want to do this, see:</p>
<ul>
<li><a href="https://blog.cloudflare.com/code-mode/">Codemode</a> from Cloudflare</li>
<li><a href="https://platform.claude.com/docs/en/agents-and-tools/tool-use/programmatic-tool-calling">Programmatic Tool Calling</a> from Anthropic</li>
<li><a href="https://www.anthropic.com/engineering/code-execution-with-mcp">Code Execution with MCP</a> from Anthropic</li>
<li><a href="https://github.com/huggingface/smolagents">Smol Agents</a> from Hugging Face</li>
</ul>
<p>In very simple terms, the idea of all the above is that LLMs can work faster, cheaper and more reliably if they're asked to write Python (or Javascript) code, instead of relying on traditional tool calling. Monty makes that possible without the complexity of a sandbox or risk of running code directly on the host.</p>
<p><strong>Note:</strong> Monty will (soon) be used to implement <code>codemode</code> in <a href="https://github.com/pydantic/pydantic-ai">Pydantic AI</a></p>
<p></p><h2>Usage</h2><a href="#usage"></a><p></p>
<p>Monty can be called from Python, JavaScript/TypeScript or Rust.</p>
<p></p><h3>Python</h3><a href="#python"></a><p></p>
<p>To install:</p>
<div><pre>uv add pydantic-monty</pre></div>
<p>(Or <code>pip install pydantic-monty</code> for the boomers)</p>
<p>Usage:</p>
<div><pre><span>from</span> <span>typing</span> <span>import</span> <span>Any</span>

<span>import</span> <span>pydantic_monty</span>

<span>code</span> <span>=</span> <span>"""</span>
<span>async def agent(prompt: str, messages: Messages):</span>
<span>    while True:</span>
<span>        print(f'messages so far: {messages}')</span>
<span>        output = await call_llm(prompt, messages)</span>
<span>        if isinstance(output, str):</span>
<span>            return output</span>
<span>        messages.extend(output)</span>
<span></span>
<span>await agent(prompt, [])</span>
<span>"""</span>

<span>type_definitions</span> <span>=</span> <span>"""</span>
<span>from typing import Any</span>
<span></span>
<span>Messages = list[dict[str, Any]]</span>
<span></span>
<span>async def call_llm(prompt: str, messages: Messages) -&gt; str | Messages:</span>
<span>    raise NotImplementedError()</span>
<span></span>
<span>prompt: str = ''</span>
<span>"""</span>

<span>m</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(
    <span>code</span>,
    <span>inputs</span><span>=</span>[<span>'prompt'</span>],
    <span>external_functions</span><span>=</span>[<span>'call_llm'</span>],
    <span>script_name</span><span>=</span><span>'agent.py'</span>,
    <span>type_check</span><span>=</span><span>True</span>,
    <span>type_check_stubs</span><span>=</span><span>type_definitions</span>,
)


<span>Messages</span> <span>=</span> <span>list</span>[<span>dict</span>[<span>str</span>, <span>Any</span>]]


<span>async</span> <span>def</span> <span>call_llm</span>(<span>prompt</span>: <span>str</span>, <span>messages</span>: <span>Messages</span>) <span>-&gt;</span> <span>str</span> <span>|</span> <span>Messages</span>:
    <span>if</span> <span>len</span>(<span>messages</span>) <span>&lt;</span> <span>2</span>:
        <span>return</span> [{<span>'role'</span>: <span>'system'</span>, <span>'content'</span>: <span>'example response'</span>}]
    <span>else</span>:
        <span>return</span> <span>f'example output, message count <span><span>{</span><span>len</span>(<span>messages</span>)<span>}</span></span>'</span>


<span>async</span> <span>def</span> <span>main</span>():
    <span>output</span> <span>=</span> <span>await</span> <span>pydantic_monty</span>.<span>run_monty_async</span>(
        <span>m</span>,
        <span>inputs</span><span>=</span>{<span>'prompt'</span>: <span>'testing'</span>},
        <span>external_functions</span><span>=</span>{<span>'call_llm'</span>: <span>call_llm</span>},
    )
    <span>print</span>(<span>output</span>)
    <span>#&gt; example output, message count 2</span>


<span>if</span> <span>__name__</span> <span>==</span> <span>'__main__'</span>:
    <span>import</span> <span>asyncio</span>

    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p></p><h4>Iterative Execution with External Functions</h4><a href="#iterative-execution-with-external-functions"></a><p></p>
<p>Use <code>start()</code> and <code>resume()</code> to handle external function calls iteratively,
giving you control over each call:</p>
<div><pre><span>import</span> <span>pydantic_monty</span>

<span>code</span> <span>=</span> <span>"""</span>
<span>data = fetch(url)</span>
<span>len(data)</span>
<span>"""</span>

<span>m</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>code</span>, <span>inputs</span><span>=</span>[<span>'url'</span>], <span>external_functions</span><span>=</span>[<span>'fetch'</span>])

<span># Start execution - pauses when fetch() is called</span>
<span>result</span> <span>=</span> <span>m</span>.<span>start</span>(<span>inputs</span><span>=</span>{<span>'url'</span>: <span>'https://example.com'</span>})

<span>print</span>(<span>type</span>(<span>result</span>))
<span>#&gt; &lt;class 'pydantic_monty.MontySnapshot'&gt;</span>
<span>print</span>(<span>result</span>.<span>function_name</span>)  <span># fetch</span>
<span>#&gt; fetch</span>
<span>print</span>(<span>result</span>.<span>args</span>)
<span>#&gt; ('https://example.com',)</span>

<span># Perform the actual fetch, then resume with the result</span>
<span>result</span> <span>=</span> <span>result</span>.<span>resume</span>(<span>return_value</span><span>=</span><span>'hello world'</span>)

<span>print</span>(<span>type</span>(<span>result</span>))
<span>#&gt; &lt;class 'pydantic_monty.MontyComplete'&gt;</span>
<span>print</span>(<span>result</span>.<span>output</span>)
<span>#&gt; 11</span></pre></div>
<p></p><h4>Serialization</h4><a href="#serialization"></a><p></p>
<p>Both <code>Monty</code> and <code>MontySnapshot</code> can be serialized to bytes and restored later.
This allows caching parsed code or suspending execution across process boundaries:</p>
<div><pre><span>import</span> <span>pydantic_monty</span>

<span># Serialize parsed code to avoid re-parsing</span>
<span>m</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>'x + 1'</span>, <span>inputs</span><span>=</span>[<span>'x'</span>])
<span>data</span> <span>=</span> <span>m</span>.<span>dump</span>()

<span># Later, restore and run</span>
<span>m2</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>.<span>load</span>(<span>data</span>)
<span>print</span>(<span>m2</span>.<span>run</span>(<span>inputs</span><span>=</span>{<span>'x'</span>: <span>41</span>}))
<span>#&gt; 42</span>

<span># Serialize execution state mid-flight</span>
<span>m</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>'fetch(url)'</span>, <span>inputs</span><span>=</span>[<span>'url'</span>], <span>external_functions</span><span>=</span>[<span>'fetch'</span>])
<span>progress</span> <span>=</span> <span>m</span>.<span>start</span>(<span>inputs</span><span>=</span>{<span>'url'</span>: <span>'https://example.com'</span>})
<span>state</span> <span>=</span> <span>progress</span>.<span>dump</span>()

<span># Later, restore and resume (e.g., in a different process)</span>
<span>progress2</span> <span>=</span> <span>pydantic_monty</span>.<span>MontySnapshot</span>.<span>load</span>(<span>state</span>)
<span>result</span> <span>=</span> <span>progress2</span>.<span>resume</span>(<span>return_value</span><span>=</span><span>'response data'</span>)
<span>print</span>(<span>result</span>.<span>output</span>)
<span>#&gt; response data</span></pre></div>
<p></p><h3>Rust</h3><a href="#rust"></a><p></p>
<div><pre><span>use</span> monty<span>::</span><span>{</span><span>MontyRun</span><span>,</span> <span>MontyObject</span><span>,</span> <span>NoLimitTracker</span><span>,</span> <span>StdPrint</span><span>}</span><span>;</span>

<span>let</span> code = <span>r#"</span>
<span>def fib(n):</span>
<span>    if n &lt;= 1:</span>
<span>        return n</span>
<span>    return fib(n - 1) + fib(n - 2)</span>
<span></span>
<span>fib(x)</span>
<span>"#</span><span>;</span>

<span>let</span> runner = <span>MontyRun</span><span>::</span><span>new</span><span>(</span>code<span>.</span><span>to_owned</span><span>(</span><span>)</span><span>,</span> <span>"fib.py"</span><span>,</span> <span>vec</span><span>!</span><span>[</span><span>"x"</span><span>.</span>to_owned<span>(</span><span>)</span><span>]</span><span>,</span> <span>vec</span><span>!</span><span>[</span><span>]</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>let</span> result = runner<span>.</span><span>run</span><span>(</span><span>vec</span><span>!</span><span>[</span><span>MontyObject</span><span>::</span><span>Int</span><span>(</span><span>10</span><span>)</span><span>]</span><span>,</span> <span>NoLimitTracker</span><span>,</span> <span>&amp;</span><span>mut</span> <span>StdPrint</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>assert_eq</span><span>!</span><span>(</span>result<span>,</span> <span>MontyObject</span><span>::</span><span>Int</span><span>(</span><span>55</span><span>)</span><span>)</span><span>;</span></pre></div>
<p></p><h4>Serialization</h4><a href="#serialization-1"></a><p></p>
<p><code>MontyRun</code> and <code>RunProgress</code> can be serialized using the <code>dump()</code> and <code>load()</code> methods:</p>
<div><pre><span>use</span> monty<span>::</span><span>{</span><span>MontyRun</span><span>,</span> <span>MontyObject</span><span>,</span> <span>NoLimitTracker</span><span>,</span> <span>StdPrint</span><span>}</span><span>;</span>

<span>// Serialize parsed code</span>
<span>let</span> runner = <span>MontyRun</span><span>::</span><span>new</span><span>(</span><span>"x + 1"</span><span>.</span><span>to_owned</span><span>(</span><span>)</span><span>,</span> <span>"main.py"</span><span>,</span> <span>vec</span><span>!</span><span>[</span><span>"x"</span><span>.</span>to_owned<span>(</span><span>)</span><span>]</span><span>,</span> <span>vec</span><span>!</span><span>[</span><span>]</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>let</span> bytes = runner<span>.</span><span>dump</span><span>(</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>

<span>// Later, restore and run</span>
<span>let</span> runner2 = <span>MontyRun</span><span>::</span><span>load</span><span>(</span><span>&amp;</span>bytes<span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>let</span> result = runner2<span>.</span><span>run</span><span>(</span><span>vec</span><span>!</span><span>[</span><span>MontyObject</span><span>::</span><span>Int</span><span>(</span><span>41</span><span>)</span><span>]</span><span>,</span> <span>NoLimitTracker</span><span>,</span> <span>&amp;</span><span>mut</span> <span>StdPrint</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>assert_eq</span><span>!</span><span>(</span>result<span>,</span> <span>MontyObject</span><span>::</span><span>Int</span><span>(</span><span>42</span><span>)</span><span>)</span><span>;</span></pre></div>
<p></p><h2>PydanticAI Integration</h2><a href="#pydanticai-integration"></a><p></p>
<p>Monty will power code-mode in
<a href="https://github.com/pydantic/pydantic-ai">Pydantic AI</a>. Instead of making
sequential tool calls, the LLM writes Python code that calls your tools
as functions and Monty executes it safely.</p>
<div><pre><span>from</span> <span>pydantic_ai</span> <span>import</span> <span>Agent</span>
<span>from</span> <span>pydantic_ai</span>.<span>toolsets</span>.<span>code_mode</span> <span>import</span> <span>CodeModeToolset</span>
<span>from</span> <span>pydantic_ai</span>.<span>toolsets</span>.<span>function</span> <span>import</span> <span>FunctionToolset</span>
<span>from</span> <span>typing_extensions</span> <span>import</span> <span>TypedDict</span>


<span>class</span> <span>WeatherResult</span>(<span>TypedDict</span>):
    <span>city</span>: <span>str</span>
    <span>temp_c</span>: <span>float</span>
    <span>conditions</span>: <span>str</span>


<span>toolset</span> <span>=</span> <span>FunctionToolset</span>()


<span>@<span>toolset</span>.<span>tool</span></span>
<span>def</span> <span>get_weather</span>(<span>city</span>: <span>str</span>) <span>-&gt;</span> <span>WeatherResult</span>:
    <span>"""Get current weather for a city."""</span>
    <span># your real implementation here</span>
    <span>return</span> {<span>'city'</span>: <span>city</span>, <span>'temp_c'</span>: <span>18</span>, <span>'conditions'</span>: <span>'partly cloudy'</span>}


<span>@<span>toolset</span>.<span>tool</span></span>
<span>def</span> <span>get_population</span>(<span>city</span>: <span>str</span>) <span>-&gt;</span> <span>int</span>:
    <span>"""Get the population of a city."""</span>
    <span>return</span> {<span>'london'</span>: <span>9_000_000</span>, <span>'paris'</span>: <span>2_100_000</span>, <span>'tokyo'</span>: <span>14_000_000</span>}.<span>get</span>(
        <span>city</span>.<span>lower</span>(), <span>0</span>
    )


<span>toolset</span> <span>=</span> <span>CodeModeToolset</span>(<span>toolset</span>)

<span>agent</span> <span>=</span> <span>Agent</span>(
    <span>'anthropic:claude-sonnet-4-5'</span>,
    <span>toolsets</span><span>=</span>[<span>toolset</span>],
)

<span>result</span> <span>=</span> <span>agent</span>.<span>run_sync</span>(
    <span>'Compare the weather and population of London, Paris, and Tokyo.'</span>
)
<span>print</span>(<span>result</span>.<span>output</span>)</pre></div>
<p></p><h2>Alternatives</h2><a href="#alternatives"></a><p></p>
<p>There are generally two responses when you show people Monty:</p>
<ol>
<li>Oh my god, this solves so many problems, I want it.</li>
<li>Why not X?</li>
</ol>
<p>Where X is some alternative technology. Oddly often these responses are combined, suggesting people have not yet found an alternative that works for them, but are incredulous that there's really no good alternative to creating an entire Python implementation from scratch.</p>
<p>I'll try to run through the most obvious alternatives, and why there aren't right for what we wanted.</p>
<p>NOTE: all these technologies are impressive and have widespread uses, this commentary on their limitations for our use case should not be seen as a criticism. Most of these solutions were not conceived with the goal of providing an LLM sandbox, which is why they're not necessary great at it.</p>
<table>
<thead>
<tr>
<th>Tech</th>
<th>Language completeness</th>
<th>Security</th>
<th>Start latency</th>
<th>FOSS</th>
<th>Setup complexity</th>
<th>File mounting</th>
<th>Snapshotting</th>
</tr>
</thead>
<tbody>
<tr>
<td>Monty</td>
<td>partial</td>
<td>strict</td>
<td>0.06ms</td>
<td>free / OSS</td>
<td>easy</td>
<td>easy</td>
<td>easy</td>
</tr>
<tr>
<td>Docker</td>
<td>full</td>
<td>good</td>
<td>195ms</td>
<td>free / OSS</td>
<td>intermediate</td>
<td>easy</td>
<td>intermediate</td>
</tr>
<tr>
<td>Pyodide</td>
<td>full</td>
<td>poor</td>
<td>2800ms</td>
<td>free / OSS</td>
<td>intermediate</td>
<td>easy</td>
<td>hard</td>
</tr>
<tr>
<td>starlark-rust</td>
<td>very limited</td>
<td>good</td>
<td>1.7ms</td>
<td>free / OSS</td>
<td>easy</td>
<td>not available?</td>
<td>impossible?</td>
</tr>
<tr>
<td>WASI / Wasmer</td>
<td>partial, almost full</td>
<td>strict</td>
<td>66ms</td>
<td>free *</td>
<td>intermediate</td>
<td>easy</td>
<td>intermediate</td>
</tr>
<tr>
<td>sandboxing service</td>
<td>full</td>
<td>strict</td>
<td>1033ms</td>
<td>not free</td>
<td>intermediate</td>
<td>hard</td>
<td>intermediate</td>
</tr>
<tr>
<td>YOLO Python</td>
<td>full</td>
<td>non-existent</td>
<td>0.1ms / 30ms</td>
<td>free / OSS</td>
<td>easy</td>
<td>easy / scary</td>
<td>hard</td>
</tr>
</tbody>
</table>
<p>See <a href="https://github.com/pydantic/monty/blob/main/scripts/startup_performance.py">./scripts/startup_performance.py</a> for the script used to calculate the startup performance numbers.</p>
<p>Details on each row below:</p>
<p></p><h3>Monty</h3><a href="#monty-1"></a><p></p>
<ul>
<li><strong>Language completeness</strong>: No classes (yet), limited stdlib, no third-party libraries</li>
<li><strong>Security</strong>: Explicitly controlled filesystem, network, and env access, strict limits on execution time and memory usage</li>
<li><strong>Start latency</strong>: Starts in microseconds</li>
<li><strong>Setup complexity</strong>: just <code>pip install pydantic-monty</code> or <code>npm install @pydantic/monty</code>, ~4.5MB download</li>
<li><strong>File mounting</strong>: Strictly controlled, see <a href="https://github.com/pydantic/monty/pull/85">#85</a></li>
<li><strong>Snapshotting</strong>: Monty's pause and resume functionality with <code>dump()</code> and <code>load()</code> makes it trivial to pause, resume and fork execution</li>
</ul>
<p></p><h3>Docker</h3><a href="#docker"></a><p></p>
<ul>
<li><strong>Language completeness</strong>: Full CPython with any library</li>
<li><strong>Security</strong>: Process and filesystem isolation, network policies, but container escapes exist, memory limitation is possible</li>
<li><strong>Start latency</strong>: Container startup overhead (~195ms measured)</li>
<li><strong>Setup complexity</strong>: Requires Docker daemon, container images, orchestration, <code>python:3.14-alpine</code> is 50MB - docker can't be installed from PyPI</li>
<li><strong>File mounting</strong>: Volume mounts work well</li>
<li><strong>Snapshotting</strong>: Possible with durable execution solutions like Temporal, or snapshotting an image and saving it as a Docker image.</li>
</ul>
<p></p><h3>Pyodide</h3><a href="#pyodide"></a><p></p>
<ul>
<li><strong>Language completeness</strong>: Full CPython compiled to WASM, almost all libraries available</li>
<li><strong>Security</strong>: Relies on browser/WASM sandbox - not designed for server-side isolation, python code can run arbitrary code in the JS runtime, only deno allows isolation, memory limits are hard/impossible to enforce with deno</li>
<li><strong>Start latency</strong>: WASM runtime loading is slow (~2800ms cold start)</li>
<li><strong>Setup complexity</strong>: Need to load WASM runtime, handle async initialization, pyodide NPM package is ~12MB, deno is ~50MB - Pyodide can't be called with just PyPI packages</li>
<li><strong>File mounting</strong>: Virtual filesystem via browser APIs</li>
<li><strong>Snapshotting</strong>: Possible with durable execution solutions like Temporal presumably, but hard</li>
</ul>
<p></p><h3>starlark-rust</h3><a href="#starlark-rust"></a><p></p>
<p>See <a href="https://github.com/facebook/starlark-rust">starlark-rust</a>.</p>
<ul>
<li><strong>Language completeness</strong>: Configuration language, not Python - no classes, exceptions, async</li>
<li><strong>Security</strong>: Deterministic and hermetic by design</li>
<li><strong>Start latency</strong>: runs embedded in the process like Monty, hence impressive startup time</li>
<li><strong>Setup complexity</strong>: Usable in python via <a href="https://github.com/inducer/starlark-pyo3">starlark-pyo3</a></li>
<li><strong>File mounting</strong>: No file handling by design AFAIK?</li>
<li><strong>Snapshotting</strong>: Impossible AFAIK?</li>
</ul>
<p></p><h3>WASI / Wasmer</h3><a href="#wasi--wasmer"></a><p></p>
<p>Running Python in WebAssembly via <a href="https://wasmer.io/">Wasmer</a>.</p>
<ul>
<li><strong>Language completeness</strong>: Full CPython, pure Python external packages work via mounting, external packages with C bindings don't work</li>
<li><strong>Security</strong>: In principle WebAssembly should provide strong sandboxing guarantees.</li>
<li><strong>Start latency</strong>: The <a href="https://pypi.org/project/wasmer/">wasmer</a> python package hasn't been updated for 3 years and I couldn't find docs on calling Python in wasmer from Python, so I called it via subprocess. Start latency was 66ms.</li>
<li><strong>Setup complexity</strong>: wasmer download is 100mb, the "python/python" package is 50mb.</li>
<li><strong>FOSS</strong>: I marked this as "free *" since the cost is zero but not everything seems to be open source. As of 2026-02-10 the <a href="https://wasmer.io/python/python"><code>python/python</code> wasmer package</a> package has no readme, no license, no source link and no indication of how it's built, the recently uploaded versions show size as "0B" although the download is ~50MB - the build process for the Python binary is not clear and transparent. <em>(If I'm wrong here, please create an issue to correct correct me)</em></li>
<li><strong>File mounting</strong>: Supported</li>
<li><strong>Snapshotting</strong>: Supported via journaling</li>
</ul>
<p></p><h3>sandboxing service</h3><a href="#sandboxing-service"></a><p></p>
<p>Services like <a href="https://daytona.io/">Daytona</a>, <a href="https://e2b.dev/">E2B</a>, <a href="https://modal.com/">Modal</a>.</p>
<p>There are similar challenges, more setup complexity but lower network latency for setting up your own sandbox setup with k8s.</p>
<ul>
<li><strong>Language completeness</strong>: Full CPython with any library</li>
<li><strong>Security</strong>: Professionally managed container isolation</li>
<li><strong>Start latency</strong>: Network round-trip and container startup time. I got ~1s cold start time with Daytona EU from London, Daytona advertise sub 90ms latency, presumably that's for an existing container, not clear if it includes network latency</li>
<li><strong>FOSS</strong>: Pay per execution or compute time, some implementations are open source</li>
<li><strong>Setup complexity</strong>: API integration, auth tokens - fine for startups but generally a non-start for enterprises</li>
<li><strong>File mounting</strong>: Upload/download via API calls</li>
<li><strong>Snapshotting</strong>: Possible with durable execution solutions like Temporal, also the services offer some solutions for this, I think based con docker containers</li>
</ul>
<p></p><h3>YOLO Python</h3><a href="#yolo-python"></a><p></p>
<p>Running Python directly via <code>exec()</code> (~0.1ms) or subprocess (~30ms).</p>
<ul>
<li><strong>Language completeness</strong>: Full CPython with any library</li>
<li><strong>Security</strong>: None - full filesystem, network, env vars, system commands</li>
<li><strong>Start latency</strong>: Near-zero for <code>exec()</code>, ~30ms for subprocess</li>
<li><strong>Setup complexity</strong>: None</li>
<li><strong>File mounting</strong>: Direct filesystem access (that's the problem)</li>
<li><strong>Snapshotting</strong>: Possible with durable execution solutions like Temporal</li>
</ul>
</article></div></div>
  </div>
</body>
</html>