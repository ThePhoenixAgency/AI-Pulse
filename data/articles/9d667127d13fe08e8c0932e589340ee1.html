<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>LeRobot v0.4.0: Supercharging OSS Robot Learning</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>LeRobot v0.4.0: Supercharging OSS Robot Learning</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 10/24/2025 2:00:00 AM | Lang: EN |
    <a href="https://huggingface.co/blog/lerobot-release-v040" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/imstevenpmwork"><img alt="Steven Palma's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CXvSv2l15uPkMQL_HBRDF.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/aractingi"><img alt="Michel Aractingi's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/668bd06dd58b51a628566d80/II7Yr5dT5ItMrpoMkQEy3.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/pepijn223"><img alt="Pepijn Kooijmans's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/65f9d37113336392bad1e49c/B0Fxwconnu7lvtjBz4Ruq.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/CarolinePascal"><img alt="Caroline Pascal's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/67d7dea1786ddcb3af5a44b3/gEgXTH4oO91GIzjHR-yrb.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/jadechoghari"><img alt="Jade Choghari's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/65eab3c432efd8afebc5aee9/7kIg3Dpd2Sdwv-tgFOvtn.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/fracapuano"><img alt="Francesco Capuano's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/AdilZtn"><img alt="Adil Zouitine's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/64c255b2254239173af0570a/xQtKvcQynqrIc52QgvICp.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/nepyope"><img alt="Martino Russi's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/631365ad289cf15634c6f600/bRndkmck1CZFKJr5U-p4A.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/thomwolf"><img alt="Thomas Wolf's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg"></a> </span> </span></p> </div></div> <div><nav><ul><li><a href="#tldr">TL;DR</a> <ul></ul> </li><li><a href="#table-of-contents">Table-of-Contents</a> <ul></ul> </li><li><a href="#datasets-ready-for-the-next-wave-of-large-scale-robot-learning">Datasets: Ready for the Next Wave of Large-Scale Robot Learning</a> <ul><li><a href="#whats-new-in-datasets-v30">What's New in Datasets v3.0?</a> <ul></ul> </li><li><a href="#new-feature-dataset-editing-tools">New Feature: Dataset Editing Tools!</a> <ul></ul> </li></ul> </li><li><a href="#simulation-environments-expanding-your-training-grounds">Simulation Environments: Expanding Your Training Grounds</a> <ul><li><a href="#libero-support">LIBERO Support</a> <ul></ul> </li><li><a href="#meta-world-integration">Meta-World Integration</a> <ul></ul> </li></ul> </li><li><a href="#codebase-powerful-tools-for-everyone">Codebase: Powerful Tools For Everyone</a> <ul><li><a href="#the-new-pipeline-for-data-processing">The New Pipeline for Data Processing</a> <ul></ul> </li><li><a href="#multi-gpu-training-made-easy">Multi-GPU Training Made Easy</a> <ul></ul> </li></ul> </li><li><a href="#policies-unleashing-open-world-generalization">Policies: Unleashing Open-World Generalization</a> <ul><li><a href="#pi0-and-pi05">PI0 and PI0.5</a> <ul></ul> </li><li><a href="#gr00t-n15">GR00T N1.5</a> <ul></ul> </li></ul> </li><li><a href="#robots-a-new-era-of-hardware-integration-with-the-plugin-system">Robots: A New Era of Hardware Integration with the Plugin System</a> <ul><li><a href="#key-benefits">Key Benefits</a> <ul></ul> </li><li><a href="#reachy-2-integration">Reachy 2 Integration</a> <ul></ul> </li><li><a href="#phone-integration">Phone Integration</a> <ul></ul> </li></ul> </li><li><a href="#the-hugging-face-robot-learning-course">The Hugging Face Robot Learning Course</a> <ul><li><a href="#deep-dive-the-modern-robot-learning-tutorial">Deep Dive: The Modern Robot Learning Tutorial</a> <ul></ul> </li></ul> </li><li><a href="#final-thoughts-from-the-team">Final thoughts from the team</a> <ul></ul> </li></ul></nav></div><p>We're thrilled to announce a series of significant advancements across LeRobot, designed to make open-source robot learning more powerful, scalable, and user-friendly than ever before! From revamped datasets to versatile editing tools, new simulation environments, and a groundbreaking plugin system for hardware, LeRobot is continuously evolving to meet the demands of cutting-edge embodied AI.</p>
<h2> <a href="#tldr"> <span></span> </a> <span> TL;DR </span>
</h2>
<p>LeRobot v0.4.0 delivers a major upgrade for open-source robotics, introducing scalable Datasets v3.0, powerful new VLA models like PI0.5 and GR00T N1.5, and a new plugin system for easier hardware integration. The release also adds support for LIBERO and Meta-World simulations, simplified multi-GPU training, and a new Hugging Face Robot Learning Course.</p>
<h2> <a href="#table-of-contents"> <span></span> </a> <span> Table-of-Contents </span>
</h2>
<ul>
<li><a href="#lerobot-v040-super-charging-oss-robotics-learning">LeRobot v0.4.0: Supercharging OSS Robot Learning</a><ul>
<li><a href="#tldr">TL;DR</a></li>
<li><a href="#table-of-contents">Table-of-Contents</a></li>
<li><a href="#datasets-ready-for-the-next-wave-of-large-scale-robot-learning">Datasets: Ready for the Next Wave of Large-Scale Robot Learning</a><ul>
<li><a href="#whats-new-in-datasets-v30">What's New in Datasets v3.0?</a></li>
<li><a href="#new-feature-dataset-editing-tools">New Feature: Dataset Editing Tools!</a></li>
</ul>
</li>
<li><a href="#simulation-environments-expanding-your-training-grounds">Simulation Environments: Expanding Your Training Grounds</a><ul>
<li><a href="#libero-support">LIBERO Support</a></li>
<li><a href="#meta-world-integration">Meta-World Integration</a></li>
</ul>
</li>
<li><a href="#codebase-powerful-tools-for-everyone">Codebase: Powerful Tools For Everyone</a><ul>
<li><a href="#the-new-pipeline-for-data-processing">The New Pipeline for Data Processing</a></li>
<li><a href="#multi-gpu-training-made-easy">Multi-GPU Training Made Easy</a></li>
</ul>
</li>
<li><a href="#policies-unleashing-open-world-generalization">Policies: Unleashing Open-World Generalization</a><ul>
<li><a href="#pi0-and-pi05">PI0 and PI0.5</a></li>
<li><a href="#gr00t-n15">GR00T N1.5</a></li>
</ul>
</li>
<li><a href="#robots-a-new-era-of-hardware-integration-with-the-plugin-system">Robots: A New Era of Hardware Integration with the Plugin System</a><ul>
<li><a href="#key-benefits">Key Benefits</a></li>
<li><a href="#reachy-2-integration">Reachy 2 Integration</a></li>
<li><a href="#phone-integration">Phone Integration</a></li>
</ul>
</li>
<li><a href="#the-hugging-face-robot-learning-course">The Hugging Face Robot Learning Course</a><ul>
<li><a href="#deep-dive-the-modern-robot-learning-tutorial">Deep Dive: The Modern Robot Learning Tutorial</a></li>
</ul>
</li>
<li><a href="#final-thoughts-from-the-team">Final thoughts from the team</a></li>
</ul>
</li>
</ul>
<h2> <a href="#datasets-ready-for-the-next-wave-of-large-scale-robot-learning"> <span></span> </a> <span> Datasets: Ready for the Next Wave of Large-Scale Robot Learning </span>
</h2>
<p>We've completely overhauled our dataset infrastructure with <strong>LeRobotDataset v3.0</strong>, featuring a new chunked episode format and streaming capabilities. This is a game-changer for handling massive datasets like <a href="https://huggingface.co/collections/lerobot/open-x-embodiment">OXE</a> (Open X Embodiment) and <a href="https://huggingface.co/datasets/lerobot/droid_1.0.1">Droid</a>, bringing unparalleled efficiency and scalability.</p>
<h3> <a href="#whats-new-in-datasets-v30"> <span></span> </a> <span> What's New in Datasets v3.0? </span>
</h3>
<ul>
<li>Chunked Episodes for Massive Scale: Our new format supports datasets at the OXE-level (&gt; 400GB), enabling unprecedented scalability.</li>
<li>Efficient Video Storage + Streaming: Enjoy faster loading times and seamless streaming of video data.</li>
<li>Unified Parquet Metadata: Say goodbye to scattered JSONs! All episode metadata is now stored in unified, structured Parquet files for easier management and access.</li>
<li>Faster Loading &amp; Better Performance: Experience significantly reduced dataset initialization times and more efficient memory usage.</li>
</ul>
<p>We've also provided a conversion script to easily migrate your existing v2.1 datasets to the new v3.0 format, ensuring a smooth transition. Read more about it in our previous <a href="https://huggingface.co/blog/lerobot-datasets-v3">blog post</a>. Open-source robotics keeps leveling up!</p>
<h3> <a href="#new-feature-dataset-editing-tools"> <span></span> </a> <span> New Feature: Dataset Editing Tools! </span>
</h3>
<p>Working with LeRobot datasets just got a whole lot easier! We've introduced a powerful set of utilities for flexible dataset editing.</p>
<p>With our new <code>lerobot-edit-dataset</code> CLI, you can now:</p>
<ul>
<li>Delete specific episodes from existing datasets.</li>
<li>Split datasets by fractions or episode indices.</li>
<li>Add or remove features with ease.</li>
<li>Merge multiple datasets into one unified set.</li>
</ul>
<pre><code><span># Merge multiple datasets into a single dataset.</span>
lerobot-edit-dataset \ --repo_id lerobot/pusht_merged \ --operation.type merge \ --operation.repo_ids <span>"['lerobot/pusht_train', 'lerobot/pusht_val']"</span> <span># Delete episodes and save to a new dataset (preserves original dataset)</span>
lerobot-edit-dataset \ --repo_id lerobot/pusht \ --new_repo_id lerobot/pusht_after_deletion \ --operation.type delete_episodes \ --operation.episode_indices <span>"[0, 2, 5]"</span>
</code></pre>
<p>These tools streamline your workflow, allowing you to curate and optimize your robot datasets like never before. Check out the <a href="https://huggingface.co/docs/lerobot/using_dataset_tools">docs</a> for more details!</p>
<h2> <a href="#simulation-environments-expanding-your-training-grounds"> <span></span> </a> <span> Simulation Environments: Expanding Your Training Grounds </span>
</h2>
<p>We're continuously expanding LeRobot's simulation capabilities to provide richer and more diverse training environments for your robotic policies.</p>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lerobot-blog/release-v0.4.0/lerobot-libero-groot-v040.gif"><img alt="libero-demo" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lerobot-blog/release-v0.4.0/lerobot-libero-groot-v040.gif"></a></p>
<h3> <a href="#libero-support"> <span></span> </a> <span> LIBERO Support </span>
</h3>
<p>LeRobot now officially supports <a href="https://libero-project.github.io/intro.html">LIBERO</a>, one of the largest open benchmarks for Vision-Language-Action (VLA) policies, boasting over 130 tasks! This is a huge step toward building the go-to evaluation hub for VLAs, enabling easy integration and a unified setup for evaluating any VLA policy.</p>
<p>Check out the <a href="https://huggingface.co/datasets/HuggingFaceVLA/libero">LIBERO dataset</a> and our <a href="https://huggingface.co/docs/lerobot/en/libero">docs</a> to get started!</p>
<h3> <a href="#meta-world-integration"> <span></span> </a> <span> Meta-World Integration </span>
</h3>
<p>We've integrated <a href="https://meta-world.github.io/">Meta-World</a>, a premier benchmark for testing multi-task and generalization abilities in robotic manipulation, featuring over 50 diverse manipulation tasks. This integration, along with our standardized use of <code>gymnasium ≥ 1.0.0</code> and <code>mujoco ≥ 3.0.0</code>, ensures deterministic seeding and a robust simulation foundation.</p>
<p>Train your policies with the <a href="https://huggingface.co/datasets/lerobot/metaworld_mt50">Meta-World dataset</a> today!</p>
<h2> <a href="#codebase-powerful-tools-for-everyone"> <span></span> </a> <span> Codebase: Powerful Tools For Everyone </span>
</h2>
<p>We're making robot control more flexible and accessible, enabling new possibilities for data collection and model training.</p>
<h3> <a href="#the-new-pipeline-for-data-processing"> <span></span> </a> <span> The New Pipeline for Data Processing </span>
</h3>
<p>Getting data from a robot to a model (and back!) is tricky. Raw sensor data, joint positions, and language instructions don't match what AI models expect. Models need normalized, batched tensors on the right device, while your robot hardware needs specific action commands.</p>
<p>We're excited to introduce <strong>Processors</strong>: a new, modular pipeline that acts as a universal translator for your data. Think of it as an assembly line where each <code>ProcessorStep</code> handles one specific job—like normalizing, tokenizing text, or moving data to the GPU.</p>
<p>You can chain these steps together into a powerful pipeline to perfectly manage your data flow. We've even created two distinct types to make life easier:</p>
<ul>
<li><code>PolicyProcessorPipeline</code>: Built for models. It expertly handles batched tensors for high-performance training and inference.</li>
<li><code>RobotProcessorPipeline</code>: Built for hardware. It processes individual data points (like a single observation or action) for real-time robot control.</li>
</ul>
<pre><code><span># Get environment state</span>
obs = robot.get_observation() <span># Rename, Batch, Normalize, Tokenize, Move Device ... </span>
obs_processed = preprocess(obs) <span># Run inference</span>
action = model.select_action(obs_processed) <span># Unnormalize, Move Device ...</span>
action_processed = postprocess(action) <span># Execute action</span>
robot.send_action(action_processed)
</code></pre>
<p>This system makes it simple to connect any policy to any robot, ensuring your data is always in the perfect format for every step of the way. Learn more about it in our <a href="https://huggingface.co/docs/lerobot/introduction_processors">Introduction to Processors documentation</a>.</p>
<h3> <a href="#multi-gpu-training-made-easy"> <span></span> </a> <span> Multi-GPU Training Made Easy </span>
</h3>
<p>Training large robot policies just got a lot faster! We've integrated <a href="https://github.com/huggingface/accelerate">Accelerate</a> directly into our training pipeline, making it incredibly simple to scale your experiments across multiple GPUs with just <strong>one command</strong>:</p>
<pre><code>accelerate launch \ --multi_gpu \ --num_processes=<span>$NUM_GPUs</span> \ $(<span>which</span> lerobot-train) \ --dataset.repo_id=<span>${HF_USER}</span>/my_dataset \ --policy.repo_id=<span>${HF_USER}</span>/my_trained_policy \ --policy.type=<span>$POLICY_TYPE</span> \ <span># ... More training configuration flags</span>
</code></pre>
<p>Whether you're fine-tuning a policy or running large-scale experiments, LeRobot now handles all the complexities of distributed training for you. This means you can drastically reduce training time, cutting it in half with 2 GPUs, down to a third with 3 GPUs, and beyond.</p>
<p>Check out the <a href="https://huggingface.co/docs/lerobot/multi_gpu_training">documentation</a> to accelerate your robot learning!</p>
<h2> <a href="#policies-unleashing-open-world-generalization"> <span></span> </a> <span> Policies: Unleashing Open-World Generalization </span>
</h2>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lerobot-blog/release-v0.4.0/lerobot-libero-groot2-v040.gif"><img alt="groot-demo" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lerobot-blog/release-v0.4.0/lerobot-libero-groot2-v040.gif"></a></p>
<h3> <a href="#pi0-and-pi05"> <span></span> </a> <span> PI0 and PI0.5 </span>
</h3>
<p>In a major milestone for open-source robotics, we've integrated <strong>pi0</strong> and <strong>pi0.5</strong> policies by Physical Intelligence into LeRobot! These Vision-Language-Action (VLA) models represent a significant leap towards addressing open-world generalization in robotics. But what makes π₀.₅ revolutionary?</p>
<ul>
<li>Open-World Generalization: Designed to adapt to entirely new environments and situations, generalizing across physical, semantic, and environmental levels.</li>
<li>Co-training on Heterogeneous Data: Learns from a diverse mix of multimodal web data, verbal instructions, subtask commands, and multi-environment robot data.</li>
<li>Physical Intelligence Collaboration: Huge thanks to the <a href="https://huggingface.co/physical-intelligence">Physical Intelligence team</a> for their groundbreaking work!</li>
</ul>
<p>You can find the models on the Hugging Face Hub: <a href="https://huggingface.co/lerobot/pi05_base">pi0.5_base</a>, <a href="https://huggingface.co/lerobot/pi0_base">pi0_base</a>, and their Libero-tuned counterparts. For more details, checkout the <a href="https://www.physicalintelligence.company/blog/pi05">Physical Intelligence Reasearch</a></p>
<h3> <a href="#gr00t-n15"> <span></span> </a> <span> GR00T N1.5 </span>
</h3>
<p>In another exciting development, we've integrated <strong>NVIDIA's GR00T N1.5</strong> into LeRobot, thanks to a fantastic collaboration with the NVIDIA robotics team! This open foundation model is a powerhouse for generalized robot reasoning and skills. As a cross-embodiment model, it takes multimodal input (like language and images) to perform complex manipulation tasks in diverse environments, marking another major leap in generalized robotics. But what makes GR00T N1.5 a game-changer?</p>
<ul>
<li>Generalized Reasoning &amp; Skills: Designed as a cross-embodiment foundation model, GR00T N1.5 excels at generalized reasoning and manipulation tasks, with improved language-following ability.</li>
<li>Expansive Heterogeneous Training: It learns from a massive dataset combining real captured humanoid data, synthetic data generated by NVIDIA Isaac GR00T Blueprint, and internet-scale video data.</li>
<li>NVIDIA Collaboration: We're thrilled to partner with the <a href="https://huggingface.co/nvidia">NVIDIA team</a> to bring this state-of-the-art model to the open-source LeRobot community!</li>
</ul>
<p>You can find the model on the Hugging Face Hub: <a href="https://huggingface.co/nvidia/GR00T-N1.5-3B">GR00T-N1.5-3B</a>. For more details, check out the <a href="https://research.nvidia.com/labs/gear/gr00t-n1_5/">NVIDIA research page</a> and the <a href="https://github.com/NVIDIA/Isaac-GR00T">official GitHub repository</a>.</p>
<p>The native integration of these policies in <code>lerobot</code> is a huge step forward in making robot learning as open and reproducible as it can be. Try them out today, share your runs, and let's push forward the frontier of embodied AI together!</p>
<h2> <a href="#robots-a-new-era-of-hardware-integration-with-the-plugin-system"> <span></span> </a> <span> Robots: A New Era of Hardware Integration with the Plugin System </span>
</h2>
<p>Big news for hardware enthusiasts! We've launched a brand-new plugin system to revolutionize how you integrate third-party hardware with LeRobot. Now, connecting any robot, camera, or teleoperator is as simple as a <code>pip install</code>, eliminating the need to modify the core library.</p>
<h3> <a href="#key-benefits"> <span></span> </a> <span> Key Benefits </span>
</h3>
<ul>
<li>Extensibility: Develop and integrate custom hardware in separate Python packages.</li>
<li>Scalability: Supports a growing ecosystem of devices without bloating the core library.</li>
<li>Community-Friendly: Lowers the barrier to entry for community contributions, fostering a more collaborative environment.</li>
</ul>
<p>Learn how to create your own plugin in our <a href="https://huggingface.co/docs/lerobot/integrate_hardware#using-your-own-lerobot-devices-">documentation</a>.</p>
<pre><code>pip install lerobot_teleoperator_my_awesome_teleop
lerobot-teleoperate --teleop.type=my_awesome_teleop
</code></pre>
<h3> <a href="#reachy-2-integration"> <span></span> </a> <span> Reachy 2 Integration </span>
</h3>
<p>Thanks to our new plugin system, we've also added <a href="https://www.pollen-robotics.com/reachy/">Reachy 2</a> from Pollen Robotics to LeRobot! Reachy 2 is available for both real robot control and simulation, enabling you to experiment with teleoperation and autonomous demos right away.</p>
<h3> <a href="#phone-integration"> <span></span> </a> <span> Phone Integration </span>
</h3>
<p>Thanks to our powerful new pipeline system, you can now teleoperate your follower arm <strong>right from your phone</strong> (iOS/Android). The phone acts as a teleoperator device, and our <code>RobotProcessor</code> pipeline handles all the transformations, allowing you to drive robots in different action spaces (like end-effector space) with ease. <a href="https://github.com/huggingface/lerobot/tree/main/examples/phone_to_so100">Check out the examples!</a></p>
<h2> <a href="#the-hugging-face-robot-learning-course"> <span></span> </a> <span> The Hugging Face Robot Learning Course </span>
</h2>
<p>We're launching a comprehensive, self-paced, and entirely <strong>open-source course</strong> designed to make robot learning accessible to everyone! If you're curious about how real-world robots learn, this is the perfect place to start.</p>
<p>In this course, you’ll learn how to:</p>
<ul>
<li>Understand the fundamentals of classical robotics.</li>
<li>Use generative models for imitation learning (VAEs, diffusion, etc.).</li>
<li>Apply Reinforcement Learning to real-world robots.</li>
<li>Explore the latest generalist robot policies like PI0 and SmolVLA.</li>
</ul>
<p>Join the <a href="https://huggingface.co/robotics-course">Hugging Face Robotics organization</a> to follow along and start your journey!</p>
<h3> <a href="#deep-dive-the-modern-robot-learning-tutorial"> <span></span> </a> <span> Deep Dive: The Modern Robot Learning Tutorial </span>
</h3>
<p>For those who want to go deeper, we've also published a <strong>hands-on tutorial</strong> on the most recent advancements in robotics. This guide provides self-contained explanations, re-derives modern techniques from first principles, and includes ready-to-use code examples using LeRobot and Hugging Face.</p>
<p>The tutorial itself is hosted in a <a href="https://huggingface.co/spaces/lerobot/robot-learning-tutorial">Space</a> and it features practical examples using LeRobot, with all models and datasets on the Hugging Hub. You can also check out <a href="https://huggingface.co/papers/2510.12403">our paper</a> for a detailed overview.</p>
<h2> <a href="#final-thoughts-from-the-team"> <span></span> </a> <span> Final thoughts from the team </span>
</h2>
<p>Beyond these major features, this release is packed with numerous bug fixes, documentation improvements, updated dependencies, more examples and better infrastructure to make your experience with LeRobot smoother and more reliable.</p>
<p>We want to extend a huge <strong>thank you to everyone in the community</strong> for your invaluable contributions, feedback, and support. We're incredibly excited about the future of open-source robotics and can't wait to work with you on what's next!</p>
<p>Stay tuned for more to come Get started <a href="https://github.com/huggingface/lerobot">here</a>!
– The LeRobot team </p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>