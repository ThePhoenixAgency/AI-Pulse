<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>GitHub - moonshine-ai/moonshine: Fast and accurate automatic speech recognition (ASR) for edge devices</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>GitHub - moonshine-ai/moonshine: Fast and accurate automatic speech recognition (ASR) for edge devices</h1>
  <div class="metadata">
    Source: GitHub Trending | Date: 2/16/2026 10:42:17 PM | <a href="https://github.com/moonshine-ai/moonshine" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <p><a target="_blank" href="/moonshine-ai/moonshine/blob/main/images/logo.png"><img src="/moonshine-ai/moonshine/raw/main/images/logo.png" alt="Moonshine Voice Logo"></a></p>
<div><h1>Moonshine Voice</h1><a href="#moonshine-voice"></a></div>
<p><strong>Voice Interfaces for Everyone</strong></p>
<ul>
<li><a href="#quickstart">Quickstart</a></li>
<li><a href="#when-should-you-choose-moonshine-over-whisper">When should you choose Moonshine over Whisper?</a></li>
<li><a href="#using-the-library">Using the Library</a></li>
<li><a href="#models">Models</a></li>
<li><a href="#api-reference">API Reference</a></li>
<li><a href="#support">Support</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#license">License</a></li>
</ul>
<p><a href="https://moonshine.ai">Moonshine</a> Voice is an open source AI toolkit for developers building real-time voice applications.</p>
<ul>
<li>Everything runs on-device, so it's fast, private, and you don't need an account, credit card, or API keys.</li>
<li>The framework and models are optimized for live streaming applications, offering low latency responses by doing a lot of the work while the user is still talking.</li>
<li>All models are based on our <a href="https://arxiv.org/abs/2602.12241">cutting edge research</a> and trained from scratch, so we can offer <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">higher accuracy than Whisper Large V3 at the top end</a>, down to tiny 26MB models for constrained deployments.</li>
<li>It's easy to integrate across platforms, with the same library running on <a href="#python">Python</a>, <a href="#ios">iOS</a>, <a href="#android">Android</a>, <a href="#macos">MacOS</a>, <a href="#linux">Linux</a>, <a href="#windows">Windows</a>, <a href="#raspberry-pi">Raspberry Pis</a>, <a href="https://www.linkedin.com/posts/petewarden_most-of-the-recent-news-about-ai-seems-to-activity-7384664255242932224-v6Mr/">IoT devices</a>, and wearables.</li>
<li>Batteries are included. Its high-level APIs offer complete solutions for common tasks like transcription, speaker identification (diarization) and command recognition, so you don't need to be an expert to build a voice application.</li>
<li>It supports multiple languages, including English, Spanish, Mandarin, Japanese, Korean, Vietnamese, Ukrainian, and Arabic.</li>
</ul>
<div><h2>Quickstart</h2><a href="#quickstart"></a></div>
<p><a href="https://discord.gg/27qp9zSRXF">Join our community on Discord to get live support</a>.</p>
<div><h3>Python</h3><a href="#python"></a></div>
<div><pre>pip install moonshine-voice
python -m moonshine_voice.mic_transcriber --language en</pre></div>
<p>Listens to the microphone and prints updates to the transcript as they come in.</p>
<div><pre>python -m moonshine_voice.intent_recognizer</pre></div>
<p>Listens for user-defined action phrases, like "Turn on the lights", using semantic matching so natural language variations are recognized. For more, check out <a href="https://bit.ly/moonshine-colab">our "Getting Started" Colab notebook</a> and <a href="https://www.youtube.com/watch?v=WH-AGvHmtoM">video</a>.</p>
<div><h3>iOS</h3><a href="#ios"></a></div>
<p>Download <a href="https://github.com/moonshine-ai/moonshine/releases/latest/download/ios-examples.tar.gz">github.com/moonshine-ai/moonshine/releases/latest/download/ios-examples.tar.gz</a>, extract it, and then open the </p><pre><code>Transcriber/Transcriber.xcodeproj</code></pre> project in Xcode.<p></p>
<div><h3>Android</h3><a href="#android"></a></div>
<p>Download <a href="https://github.com/moonshine-ai/moonshine/releases/latest/download/android-examples.tar.gz">github.com/moonshine-ai/moonshine/releases/latest/download/android-examples.tar.gz</a>, extract it, and then open the </p><pre><code>Transcriber</code></pre> folder in Android Studio.<p></p>
<div><h3>Linux</h3><a href="#linux"></a></div>
<p><a href="https://github.com/moonshine-ai/moonshine/archive/refs/heads/main.zip">Download</a> or </p><pre><code>git clone</code></pre> this repository and then run:<p></p>
<div><pre><span>cd</span> core
mkdir build
cmake ..
cmake --build <span>.</span>
./moonshine-cpp-test</pre></div>
<div><h3>MacOS</h3><a href="#macos"></a></div>
<p>Download <a href="https://github.com/moonshine-ai/moonshine/releases/latest/download/macos-examples.tar.gz">github.com/moonshine-ai/moonshine/releases/latest/download/macos-examples.tar.gz</a>, extract it, and then open the </p><pre><code>MicTranscription/MicTranscription.xcodeproj</code></pre> project in Xcode.<p></p>
<div><h3>Windows</h3><a href="#windows"></a></div>
<p>Download <a href="https://github.com/moonshine-ai/moonshine/releases/latest/download/windows-examples.tar.gz">github.com/moonshine-ai/moonshine/releases/latest/download/windows-examples.tar.gz</a>, extract it, and then open the </p><pre><code>cli-transcriber\cli-transcriber.vcxproj</code></pre> project in Visual Studio.<p></p>
<p><a href="#python">Install Moonshine in Python</a> for model downloading.</p>
<p>In the terminal:</p>
<div><pre>pip install moonshine-voice
<span>cd</span> examples\windows\cli-transcriber
.\download-lib.bat
msbuild cli-transcriber.sln /p:Configuration=Release /p:Platform=x64
python -m moonshine_voice.download --language en
x64\Release\cli-transcriber.exe --model-path <span>&lt;</span>path from the download command<span>&gt;</span> --model-arch <span>&lt;</span>number from the download command<span>&gt;</span></pre></div>
<div><h3>Raspberry Pi</h3><a href="#raspberry-pi"></a></div>
<p>You'll need a USB microphone plugged in to get audio input, but the Python pip package has been optimized for the Pi, so you can run:</p>
<div><pre> sudo pip install --break-system-packages moonshine-voice python -m moonshine_voice.mic_transcriber --language en</pre></div>
<p>I've recorded <a href="https://www.youtube.com/watch?v=NNcqx1wFxl0">a screencast on YouTube</a> to help you get started, and you can also download <a href="https://github.com/moonshine-ai/moonshine/releases/latest/download/raspberry-pi-examples.tar.gz">github.com/moonshine-ai/moonshine/releases/latest/download/raspberry-pi-examples.tar.gz</a> for some fun, Pi-specific examples. <a href="/moonshine-ai/moonshine/blob/main/examples/raspberry-pi/my-dalek/README.md">The README</a> has information about using a virtual environment for the Python install if you don't want to use </p><pre><code>--break-system-packages</code></pre>.<p></p>
<div><h2>When should you choose Moonshine over Whisper?</h2><a href="#when-should-you-choose-moonshine-over-whisper"></a></div>
<p>TL;DR - When you're working with live speech.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>WER</th>
<th># Parameters</th>
<th>MacBook Pro</th>
<th>Linux x86</th>
</tr>
</thead>
<tbody>
<tr>
<td>Moonshine Medium Streaming</td>
<td>6.65%</td>
<td>245 million</td>
<td>258ms</td>
<td>347ms</td>
</tr>
<tr>
<td>Whisper Large v3</td>
<td>7.44%</td>
<td>1.5 billion</td>
<td>11,286ms</td>
<td>16,919ms</td>
</tr>
<tr>
<td>Moonshine Small Streaming</td>
<td>7.84%</td>
<td>123 million</td>
<td>148ms</td>
<td>201ms</td>
</tr>
<tr>
<td>Whisper Small</td>
<td>8.59%</td>
<td>244 million</td>
<td>1940ms</td>
<td>3,425ms</td>
</tr>
<tr>
<td>Moonshine Tiny Streaming</td>
<td>12.00%</td>
<td>34 million</td>
<td>50ms</td>
<td>76ms</td>
</tr>
<tr>
<td>Whisper Tiny</td>
<td>12.81%</td>
<td>39 million</td>
<td>277ms</td>
<td>1,141ms</td>
</tr>
</tbody>
</table>
<p><em>See <a href="#benchmarks">benchmarks</a> for how these numbers were measured.</em></p>
<p><a href="/moonshine-ai/moonshine/blob/main">OpenAI's release of their Whisper family of models</a> was a massive step forward for open-source speech to text. They offered a range of sizes, allowing developers to trade off compute and storage space against accuracy to fit their applications. Their biggest models, like Large v3, also gave accuracy scores that were higher than anything available outside of large tech companies like Google or Apple. At Moonshine we were early and enthusiastic adopters of Whisper, and we still remain big fans of the models and the great frameworks like <a href="https://github.com/SYSTRAN/faster-whisper">FasterWhisper</a> and others that have been built around them.</p>
<p>However, as we built applications that needed a live voice interface we found we needed features that weren't available through Whisper:</p>
<ul>
<li><strong>Whisper always operates on a 30-second input window</strong>. This isn't an issue when you're processing audio in large batches, you can usually just look ahead in the file and find a 30-second-ish chunk of speech to apply it to. Voice interfaces can't look ahead to create larger chunks from their input stream, and phrases are seldom longer than five to ten seconds. This means there's a lot of wasted computation encoding zero padding in the encoder and decoder, which means longer latency in returning results. Since one of the most important requirements for any interface is responsiveness, usually defined as latency below 200ms, this hurts the user experience even on platforms that have compute to spare, and makes it unusable on more constrained devices.</li>
<li><strong>Whisper doesn't cache anything</strong>. Another common requirement for voice interfaces is that they display feedback as the user is talking, so that they know the app is listening and understanding them. This means calling the speech to text model repeatedly over time as a sentence is spoken. Most of the audio input is the same, with only a short addition to the end. Even though a lot of the input is constant, Whisper starts from scratch every time, doing a lot of redundant work on audio that it has seen before. Like the fixed input window, this unnecessary latency impairs the user experience.</li>
<li><strong>Whisper supports a lot of languages poorly</strong>. Whisper's multilingual support is an incredible feat of engineering, and demonstrated a single model could handle many languages, and even offer translations. This chart from OpenAI (<a href="https://cdn.openai.com/papers/whisper.pdf">raw data in Appendix D-2.4</a>) shows the drop-off in Word Error Rate (WER) with the very largest 1.5 billion parameter model.</li>
</ul>
<p><a target="_blank" href="/moonshine-ai/moonshine/blob/main/images/lang-chart.png"><img src="/moonshine-ai/moonshine/raw/main/images/lang-chart.png" alt="Language Chart"></a></p>
<p>82 languages are listed, but only 33 have sub-20% WER (what we consider usable). For the Base model size commonly used on edge devices, only 5 languages are under 20% WER. Asian languages like Korean and Japanese stand out as the native tongue of large markets with a lot of tech innovation, but Whisper doesn't offer good enough accuracy to use in most applications The proprietary in-house versions of Whisper that are available through OpenAI's cloud API seem to offer better accuracy, but aren't available as open models.</p>
<ul>
<li><strong>Fragmented edge support</strong>. A fantastic ecosystem has grown up around Whisper, there are a lot of mature frameworks you can use to deploy the models. However these often tend to be focused on desktop-class machines and operating systems. There are projects you can use across edge platforms like iOS, Android, or Raspberry Pi OS, but they tend to have different interfaces, capabilities, and levels of optimization. This made building applications that need to run on a variety of devices unnecessarily difficult.</li>
</ul>
<p>All these limitations drove us to create our own family of models that better meet the needs of live voice interfaces. It took us some time since the combined size of the open speech datasets available is tiny compared to the amount of web-derived text data, but after extensive data-gathering work, we were able to release <a href="https://arxiv.org/abs/2410.15608">the first generation of Moonshine models</a>. These removed the fixed-input window limitation along with some other architectural improvements, and gave significantly lower latency than Whisper in live speech applications, often running 5x faster or more.</p>
<p>However we kept encountering applications that needed even lower latencies on even more constrained platforms. We also wanted to offer higher accuracy than the Base-equivalent that was the top end of the initial models. That led us to this second generation of Moonshine models, which offer:</p>
<ul>
<li><strong>Flexible input windows</strong>. You can supply any length of audio (though we recommend staying below around 30 seconds) and the model will only spend compute on that input, no zero-padding required. This gives us a significant latency boost.</li>
<li><strong>Caching for streaming</strong>. Our models now support incremental addition of audio over time, and they cache the input encoding and part of the decoder's state so that we're able to skip even more of the compute, driving latency down dramatically.</li>
<li><strong>Language-specific models</strong>. We have gathered data and trained models for multiple languages, including Arabic, Japanese, Korean, Spanish, Ukrainian, Vietnamese, and Chinese. As we discuss in our <a href="https://arxiv.org/abs/2509.02523">Flavors of Moonshine paper</a>, we've found that we can get much higher accuracy for the same size and compute if we restrict a model to focus on just one language, compared to training one model across many.</li>
<li><strong>Cross-platform library support</strong>. We're building applications ourselves, and needed to be able to deploy these models across Linux, MacOS, Windows, iOS, and Android, as well as use them from languages like Python, Swift, Java, and C++. To support this we architected a portable C++ core library that handles all of the processing, uses OnnxRuntime for good performance across systems, and then built native interfaces for all the required high-level languages. This allows developers to learn one API, and then deploy it almost anywhere they want to run.</li>
<li><strong>Better accuracy than Whisper V3 Large</strong>. On <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">HuggingFace's OpenASR leaderboard</a>, our newest streaming model for English, Medium Streaming, achieves a lower word-error rate than the most-accurate Whisper model from OpenAI. This is despite Moonshine's version using 250 million parameters, versus Large v3's 1.5 billion, making it much easier to deploy on the edge.</li>
</ul>
<p>Hopefully this gives you a good idea of how Moonshine compares to Whisper. If you're working with GPUs in the cloud on data in bulk where throughput is most important then Whisper (or Nvidia alternatives like Parakeet) offer advantages like batch processing, but we believe we can't be beat for live speech. We've built the framework and models we wished we'd had when we first started building applications with voice interfaces, so if you're working with live voice inputs, <a href="#quickstart">give Moonshine a try</a>.</p>
<div><h2>Using the Library</h2><a href="#using-the-library"></a></div>
<p>The Moonshine API is designed to take care of the details around capturing and transcribing live speech, giving application developers a high-level API focused on actionable events. I'll use Python to illustrate how it works, but the API is consistent across all the supported languages.</p>
<ul>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#concepts">Concepts</a></li>
<li><a href="#getting-started-with-transcription">Getting Started with Transcription</a>
<ul>
<li><a href="#transcription-event-flow">Transcription Event Flow</a></li>
</ul>
</li>
<li><a href="#getting-started-with-command-recognition">Getting Started with Command Recognition</a></li>
<li><a href="#examples">Examples</a></li>
<li><a href="#adding-the-library-to-your-own-app">Adding the Library to your own App</a></li>
<li><a href="#python-1">Python</a></li>
<li><a href="#ios-or-macos">iOS or MacOS</a></li>
<li><a href="#android-1">Android</a></li>
<li><a href="#windowsc">Windows</a></li>
<li><a href="#debugging">Debugging</a>
<ul>
<li><a href="#console-logs">Console Logs</a></li>
<li><a href="#input-saving">Input Saving</a></li>
<li><a href="#api-call-logging">API Call Logging</a></li>
</ul>
</li>
<li><a href="#building-from-source">Building from Source</a>
<ul>
<li><a href="#cmake">Cmake</a></li>
<li><a href="#language-bindings">Language Bindings</a></li>
<li><a href="#porting">Porting</a></li>
</ul>
</li>
<li><a href="#downloading-models">Downloading Models</a></li>
<li><a href="#benchmarking">Benchmarking</a></li>
</ul>
<div><h3>Architecture</h3><a href="#architecture"></a></div>
<p>Our goal is to build a framework that any developer can pick up and use, even with no previous experience of speech technologies. We've abstracted away a lot of the unnecessary details and provide a simple interface that lets you focus on building your application, and that's reflected in our system architecture.</p>
<p>The basic flow is:</p>
<ul>
<li>Create a <pre><code>Transcriber</code></pre> or <pre><code>IntentRecognizer</code></pre> object, depending on whether you want the text that's spoken, or just to know that a user has requested an action.</li>
<li>Attach an <pre><code>EventListener</code></pre> that gets called when important things occur, like the end of a phrase or an action being triggered, so your application can respond.</li>
</ul>
<p>Traditionally, adding a voice interface to an application or product required integrating a lot of different libraries to handle all the processing that's needed to capture audio and turn it into something actionable. The main steps involved are microphone capture, voice activity detection (to break a continuous stream of audio into sections of speech), speech to text, speaker identification, and intent recognition. Each of these steps typically involved a different framework, which greatly increased the complexity of integrating, optimizing, and maintaining these dependencies.</p>
<p>Moonshine Voice includes all of these stages in a single library, and abstracts away everything but the essential information your application needs to respond to user speech, whether you want to transcribe it or trigger actions.</p>
<p><a target="_blank" href="/moonshine-ai/moonshine/blob/main/images/moonshine-voice-architecture.png"><img src="/moonshine-ai/moonshine/raw/main/images/moonshine-voice-architecture.png" alt="Moonshine Voice Architecture"></a></p>
<p>Most developers should be able to treat the library as a black box that tells them when something interesting has happened, using our event-based classes to implement application logic. Of course the framework is fully open source, so speech experts can dive as deep under the hood as they'd like, but it's not necessary to use it.</p>
<div><h3>Concepts</h3><a href="#concepts"></a></div>
<p>A <a href="/moonshine-ai/moonshine/blob/main/python/src/moonshine_voice/transcriber.py#L66"><strong>Transcriber</strong></a> takes in audio input and turns any speech into text. This is the first object you'll need to create to use Moonshine, and you'll give it a path to <a href="#downloading-models">the models you've downloaded</a>.</p>
<p>A <a href="/moonshine-ai/moonshine/blob/main/python/src/moonshine_voice/mic_transcriber.py#L10"><strong>MicTranscriber</strong></a> is a helper class based on the general transcriber that takes care of connecting to a microphone using your platform's built-in support (for example sounddevice in Python) and then feeding the audio in as it's captured.</p>
<p>A <a href="/moonshine-ai/moonshine/blob/main/python/src/moonshine_voice/transcriber.py#L297"><strong>Stream</strong></a> is a handler for audio input. The reason streams exist is because you may want to process multiple audio inputs at once, and a transcriber can support those through multiple streams, without duplicating the model resources. If you only have one input, the transcriber class includes the same methods (start/stop/add_audio) as a stream, and you can use that interface instead and forget about streams.</p>
<p>A <a href="/moonshine-ai/moonshine/blob/main/python/src/moonshine_voice/moonshine_api.py#L51"><strong>TranscriptLine</strong></a> is a data structure holding information about one line in the transcript. When someone is speaking, the library waits for short pauses (where punctuation might go in written language) and starts a new line. These aren't exactly sentences, since a speech pause isn't a sure sign of the end of a sentence, but this does break the spoken audio into segments that can be considered phrases. A line includes state such as whether the line has just started, is still being spoken, or is complete, along with its start time and duration.</p>
<p>A <a href="/moonshine-ai/moonshine/blob/main/python/src/moonshine_voice/moonshine_api.py#67"><strong>Transcript</strong></a> is a list of lines in time order holding information about what text has already been recognized, along with other state like when it was captured.</p>
<p>A <a href="/moonshine-ai/moonshine/blob/main/python/src/moonshine_voice/transcriber.py#L22"><strong>TranscriptEvent</strong></a> contains information about changes to the transcript. Events include a new line being started, the text in a line being updated, and a line being completed. The event object includes the transcript line it's referring to as a member, holding the latest state of that line.</p>
<p>A <a href="/moonshine-ai/moonshine/blob/main/python/src/moonshine_voice/transcriber.py#L266"><strong>TranscriptEventListener</strong></a> is a protocol that allows app-defined functions to be called when transcript events happen. This is the main way that most applications interact with the results of the transcription. When live speech is happening, applications usually need to respond or display results as new speech is recognized, and this approach allows you to handle those changes in a similar way to events from traditional user interfaces like touch screen gestures or mouse clicks on buttons.</p>
<p>An <a href="/moonshine-ai/moonshine/blob/main/python/src/moonshine_voice/intent_recognizer.py#L44"><strong>IntentRecognizer</strong></a> is a type of TranscriptEventListener that allows you to invoke different callback functions when preprogrammed intents are detected. This is useful for building voice command recognition features.</p>
<div><h3>Getting Started with Transcription</h3><a href="#getting-started-with-transcription"></a></div>
<p>We have <a href="#examples">examples</a> for most platforms so as a first step I recommend checking out what we have for the systems you're targeting.</p>
<p>Next, you'll need to <a href="#adding-the-library-to-your-own-app">add the library to your project</a>. We aim to provide pre-built binaries for all major platforms using their native package managers. On Python this means a pip install, for Android it's a Maven package, and for MacOS and iOS we provide a Swift package through SPM.</p>
<p>The transcriber needs access to the files for the model you're using, so after <a href="#downloading-models">downloading them</a> you'll need to place them somewhere the application can find them, and make a note of the path. This usually means adding them as resources in your IDE if you're planning to distribute the app, or you can use hard-wired paths if you're just experimenting. The download script gives you the location of the models and their architecture type on your drive after it completes.</p>
<p>Now you can try creating a transcriber. Here's what that looks like in Python:</p>
<div><pre><span>transcriber</span> <span>=</span> <span>Transcriber</span>(<span>model_path</span><span>=</span><span>model_path</span>, <span>model_arch</span><span>=</span><span>model_arch</span>)</pre></div>
<p>If the model isn't found, or if there's any other error, this will throw an exception with information about the problem. You can also check the console for logs from the core library, these are printed to </p><pre><code>stderr</code></pre> or your system's equivalent.<p></p>
<p>Now we'll create a listener that contains the app logic that you want triggered when the transcript updates, and attach it to your transcriber:</p>
<div><pre><span>class</span> <span>TestListener</span>(<span>TranscriptEventListener</span>): <span>def</span> <span>on_line_started</span>(<span>self</span>, <span>event</span>): <span>print</span>(<span>f"Line started: <span><span>{</span><span>event</span>.<span>line</span>.<span>text</span><span>}</span></span>"</span>) <span>def</span> <span>on_line_text_changed</span>(<span>self</span>, <span>event</span>): <span>print</span>(<span>f"Line text changed: <span><span>{</span><span>event</span>.<span>line</span>.<span>text</span><span>}</span></span>"</span>) <span>def</span> <span>on_line_completed</span>(<span>self</span>, <span>event</span>): <span>print</span>(<span>f"Line completed: <span><span>{</span><span>event</span>.<span>line</span>.<span>text</span><span>}</span></span>"</span>) <span>transcriber</span>.<span>add_listener</span>(<span>listener</span>)</pre></div>
<p>The transcriber needs some audio data to work with. If you want to try it with the microphone you can update your transcriber creation line to use a MicTranscriber instead, but if you want to start with a .wav file for testing purposes here's how you feed that in:</p>
<div><pre> <span>audio_data</span>, <span>sample_rate</span> <span>=</span> <span>load_wav_file</span>(<span>wav_path</span>) <span>transcriber</span>.<span>start</span>() <span># Loop through the audio data in chunks to simulate live streaming</span> <span># from a microphone or other source.</span> <span>chunk_duration</span> <span>=</span> <span>0.1</span> <span>chunk_size</span> <span>=</span> <span>int</span>(<span>chunk_duration</span> <span>*</span> <span>sample_rate</span>) <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>0</span>, <span>len</span>(<span>audio_data</span>), <span>chunk_size</span>): <span>chunk</span> <span>=</span> <span>audio_data</span>[<span>i</span>: <span>i</span> <span>+</span> <span>chunk_size</span>] <span>transcriber</span>.<span>add_audio</span>(<span>chunk</span>, <span>sample_rate</span>) <span>transcriber</span>.<span>stop</span>()</pre></div>
<p>The important things to notice here are:</p>
<ul>
<li>We create an array of mono audio data from a wav file, using the convenience <pre><code>load_wav_file()</code></pre> function that's part of the Moonshine library.</li>
<li>We start the transcriber to activate its processing code.</li>
<li>The loop adds audio in chunks. These chunks can be any length and any sample rate, the library takes care of all the housekeeping.</li>
<li>As audio is added, the event listener you added will be called, giving information about the latest speech.</li>
</ul>
<p>In a real application you'd be calling </p><pre><code>add_audio()</code></pre> from an audio handler that's receiving it from your source. Since the library can handle arbitrary durations and sample rates, just make sure it's mono and otherwise feed it in as-is.<p></p>
<p>The transcriber analyses the speech at a default interval of every 500ms of input. You can change this with the </p><pre><code>update_interval</code></pre> argument to the transcriber constructor. For streaming models most of the work is done as the audio is being added, and it's automatically done at the end of a phrase, so changing this won't usually affect the workload or latency massively.<p></p>
<p>The key takeaway is that you usually don't need to worry about the transcript data structure itself, the event system tells you when something important happens. You can manually trigger a transcript update by calling </p><pre><code>update_transcription()</code></pre> which returns a transcript object with all of the information about the current session if you do need to examine the state.<p></p>
<p>By calling </p><pre><code>start()</code></pre> and <pre><code>stop()</code></pre> on a transcriber (or stream) we're beginning and ending a session. Each session has one transcript document associated with it, and it is started fresh on every <pre><code>start()</code></pre> call, so you should make copies of any data you need from the transcript object before that.<p></p>
<p>The transcriber class also offers a simpler </p><pre><code>transcribe_without_streaming()</code></pre> method, for when you have an array of data from the past that you just want to analyse, such as a file or recording.<p></p>
<p>We also offer a specialization of the base </p><pre><code>Transcriber</code></pre> class called <pre><code>MicTranscriber</code></pre>. How this is implemented will depend on the language and platform, but it should provide a transcriber that's automatically attached to the main microphone on the system. This makes it straightforward to start transcribing speech from that common source, since it supports all of the same listener callbacks as the base class.<p></p>
<div><h4>Transcription Event Flow</h4><a href="#transcription-event-flow"></a></div>
<p>The main communication channel between the library and your application is through events that are passed to any listener functions you have registered. There are four major event types:</p>
<ul>
<li><pre><code>LineStarted</code></pre>. This is sent to listeners when the beginning of a new speech segment is detected. It may or may not contain any text, but since it's dispatched near the start of an utterance, that text is likely to change over time.</li>
<li><pre><code>LineUpdated</code></pre>. Called whenever any of the information about a line changes, including the duration, audio data, and text.</li>
<li><pre><code>LineTextChanged</code></pre>. Called only when the text associated with a line is updated. This is a subset of <pre><code>LineUpdated</code></pre> that focuses on the common need to refresh the text shown to users as often as possible to keep the experience interactive.</li>
<li><pre><code>LineCompleted</code></pre>. Sent when we detect that someone has paused speaking, and we've ended the current segment. The line data structure has the final values for the text, duration, and speaker ID.</li>
</ul>
<p>We offer some guarantees about these events:</p>
<ul>
<li><pre><code>LineStarted</code></pre> is always called exactly once for any segment.</li>
<li><pre><code>LineCompleted</code></pre> is always called exactly once after <pre><code>LineStarted</code></pre> for any segment.</li>
<li><pre><code>LineUpdated</code></pre> and <pre><code>LineTextChanged</code></pre> will only ever be called after the <pre><code>LineStarted</code></pre> and before the <pre><code>LineCompleted</code></pre> events for a segment.</li>
<li>Those update events are not guaranteed to be called (and in practice can be disabled by setting <pre><code>update_interval</code></pre> to a very large value).</li>
<li>There will only be one line active at any one time for any given stream.</li>
<li>Once <pre><code>LineCompleted</code></pre> has been called, the library will never alter that line's data again.</li>
<li>If <pre><code>stop()</code></pre> is called on a transcriber or stream, any active lines will have <pre><code>LineCompleted</code></pre> called.</li>
<li>Each line has a 64-bit <pre><code>lineId</code></pre> that is designed to be unique enough to avoid collisions.</li>
<li>This <pre><code>lineId</code></pre> remains the same for the line over time, from the first <pre><code>LineStarted</code></pre> event onwards.</li>
</ul>
<div><h3>Getting Started with Command Recognition</h3><a href="#getting-started-with-command-recognition"></a></div>
<p>If you want your application to respond when users talk, you need to understand what they're saying. The previous generation of voice interfaces could only recognize speech that was phrased in exactly the form they expected. For example "Alexa, turn on living-room lights" might work, but "Alexa, lights on in the living room please" might not. The general problem of figuring out what a user wants from natural speech is known as intent recognition. There have been decades of research into this area, but the rise of transformer-based LLMs has given us new tools. We have integrated some of these advances into Moonshine Voice's command recognition API.</p>
<p>The basic idea is that your application registers some general actions you're interested in, like "Turn the lights on" or "Move left", and then Moonshine sends an event when the user says something that matches the meaning of those phrases. It works a lot like a graphical user interface - you define a button (action) and an event callback that is triggered when the user presses that button.</p>
<p>To give it a try for yourself, run this built-in example:</p>
<div><pre>python -m moonshine_voice.intent_recognizer</pre></div>
<p>This will present you with a menu of command phrases, and then start listening to the microphone. If you say something that's a variant on one of the phrases you'll see a "triggered" log message telling you which action was matched, along with how confident the system is in the match.</p>
<div><pre> Let there be light.
<span><span>'</span>TURN ON THE LIGHTS<span>'</span></span> triggered by <span><span>'</span>Let there be light.<span>'</span></span> with 76% confidence</pre></div>
<p>To show that you can modify these at run time, try supplying your own list of phrases as a comma-separated string argument to </p><pre><code>--intents</code></pre>.<p></p>
<div><pre>python -m moonshine_voice.intent_recognizer --intents <span><span>"</span>Turn left, turn right, go backwards, go forward<span>"</span></span></pre></div>
<p>This could be the core command set to control a robot's movement for example. It's worth spending a bit of time experimenting with different wordings of the command phrases, and different variations on the user side, to get a feel for how the system works.</p>
<p>Under the hood this is all accomplished using two main classes. We've met the </p><pre><code>MicTranscriber</code></pre> above, the new addition is <pre><code>IntentRecognizer</code></pre>. This listens to the results of the transcriber, fuzzily matches completed lines against any intents that have been registered with it, and calls back the client-supplied code.<p></p>
<p>The fuzzy matching uses a sentence-embedding model based on Gemma300m, so the first step is downloading it and getting the path:</p>
<div><pre><span>embedding_model_path</span>, <span>embedding_model_arch</span> <span>=</span> <span>get_embedding_model</span>( <span>args</span>.<span>embedding_model</span>, <span>args</span>.<span>quantization</span>
)</pre></div>
<p>Once we have the model's location, we create an </p><pre><code>IntentRecognizer</code></pre> using that path. The only other argument is the <pre><code>threshold</code></pre> we use for fuzzy matching. It's between 0 and 1, with low numbers producing more matches but at the cost of less accuracy, and vice versa for high values.<p></p>
<div><pre><span>intent_recognizer</span> <span>=</span> <span>IntentRecognizer</span>( <span>model_path</span><span>=</span><span>embedding_model_path</span>, <span>model_arch</span><span>=</span><span>embedding_model_arch</span>, <span>model_variant</span><span>=</span><span>args</span>.<span>quantization</span>, <span>threshold</span><span>=</span><span>args</span>.<span>threshold</span>,
)</pre></div>
<p>Next we tell the recognizer what kinds of phrases to listen out for, and what to do when there's a match.</p>
<div><pre><span>def</span> <span>on_intent_triggered_on</span>(<span>trigger</span>: <span>str</span>, <span>utterance</span>: <span>str</span>, <span>similarity</span>: <span>float</span>): <span>print</span>(<span>f"<span>\n</span>'<span><span>{</span><span>trigger</span>.<span>upper</span>()<span>}</span></span>' triggered by '<span><span>{</span><span>utterance</span><span>}</span></span>' with <span><span>{</span><span>similarity</span>:.0%<span>}</span></span> confidence"</span>) <span>for</span> <span>intent</span> <span>in</span> <span>intents</span>: <span>intent_recognizer</span>.<span>register_intent</span>(<span>intent</span>, <span>on_intent_triggered_on</span>)</pre></div>
<p>The recognizer supports the transcript event listener interface, so the final stage is adding it as a listener to the </p><pre><code>MicTranscriber</code></pre>.<p></p>
<div><pre><span>mic_transcriber</span>.<span>add_listener</span>(<span>intent_recognizer</span>)</pre></div>
<p>Once you start the transcriber, it will listen out for any variations on the supplied phrases, and call </p><pre><code>on_intent_triggered_on()</code></pre> whenever there's a match.<p></p>
<p>The current intent recognition is designed for full-sentence matching, which works well for straightforward commands, but we will be expanding into more advanced "slot filling" techniques in the future, to handle extracting the quantity from "I want ten bananas" for example.</p>
<div><h3>Examples</h3><a href="#examples"></a></div>
<p>The <a href="/moonshine-ai/moonshine/blob/main/examples"></a></p><pre><a href="/moonshine-ai/moonshine/blob/main/examples"><code>examples</code></a></pre> folder has code samples organized by platform. We offer these for <a href="/moonshine-ai/moonshine/blob/main/examples/android">Android</a>, <a href="/moonshine-ai/moonshine/blob/main/examples/c++">portable C++</a>, <a href="/moonshine-ai/moonshine/blob/main/examples/ios">iOS</a>, <a href="/moonshine-ai/moonshine/blob/main/examples/macos">MacOS</a>, <a href="/moonshine-ai/moonshine/blob/main/examples/python">Python</a>, and <a href="/moonshine-ai/moonshine/blob/main/examples/windows">Windows</a>. We have tried to use the most common build system for each platform, so Android uses Android Studio and Maven, iOS and MacOS use Xcode and Swift, while Windows uses Visual Studio.<p></p>
<p>The examples usually include one minimal project that just creates a transcriber and then feeds it data from a WAV file, and another that's pulling audio from a microphone using the platform's default framework for accessing audio devices.</p>
<div><h3>Adding the Library to your own App</h3><a href="#adding-the-library-to-your-own-app"></a></div>
<p>We distribute the library through the most widely-used package managers for each platform. Here's how you can use these to add the framework to an existing project on different systems.</p>
<div><h4>Python</h4><a href="#python-1"></a></div>
<p>The Python package is <a href="https://pypi.org/project/moonshine-voice/">hosted on PyPi</a>, so all you should need to do to install it is </p><pre><code>pip install moonshine-voice</code></pre>, and then <pre><code>import moonshine_voice</code></pre> in your project.<p></p>
<div><h4>iOS or MacOS</h4><a href="#ios-or-macos"></a></div>
<p>For iOS we use the Swift Package Manager, with <a href="https://github.com/moonshine-ai/moonshine-swift/">an auto-updated GitHub repository</a> holding each version. To use this right-click on the file view sidebar in Xcode and choose "Add Package Dependencies..." from the menu. A dialog should open up, paste </p><pre><code>https://github.com/moonshine-ai/moonshine-swift/</code></pre> into the top search box and you should see <pre><code>moonshine-swift</code></pre>. Select it and choose "Add Package", and it should be added to your project. You should now be able to <pre><code>import MoonshineVoice</code></pre> and use the library. You will need to add any model files you use to your app bundle and ensure they're copied during the deployment phase, so they can be accessed on-device.<p></p>
<p>For reference purposes you can find Xcode projects with these changes applied in <a href="/moonshine-ai/moonshine/blob/main/examples/ios/Transcriber"></a></p><pre><a href="/moonshine-ai/moonshine/blob/main/examples/ios/Transcriber"><code>examples/ios/Transcriber</code></a></pre> and <a href="/moonshine-ai/moonshine/blob/main/examples/macos/BasicTranscription"><pre><code>examples/macos/BasicTranscription</code></pre></a>.<p></p>
<div><h4>Android</h4><a href="#android-1"></a></div>
<p>On Android we publish <a href="https://mvnrepository.com/artifact/ai.moonshine/moonshine-voice">the package to Maven</a>. To include it in your project using Android Studio and Gradle, first add the version number you want to the </p><pre><code>gradle/libs.versions.toml</code></pre> file by inserting a line in the <pre><code>[versions]</code></pre> section, for example <pre><code>moonshineVoice = "0.0.48"</code></pre>. Then in the <pre><code>[libraries]</code></pre> part, add a reference to the package: <pre><code>moonshine-voice = { group = "ai.moonshine", name = "moonshine-voice", version.ref = "moonshineVoice" }</code></pre>.<p></p>
<p>Finally, in your </p><pre><code>app/build.gradle.kts</code></pre> add the library to the <pre><code>dependencies</code></pre> list: <pre><code>implementation(libs.moonshine.voice)</code></pre>. You can find a working example of all these changes in [<pre><code>examples/android/Transcriber</code></pre>].<p></p>
<div><h4>Windows/C++</h4><a href="#windowsc"></a></div>
<p>We couldn't find a single package manager that is used by most Windows developers, so instead we've made the raw library and headers available as a download. The script in <a href="/moonshine-ai/moonshine/blob/main/examples/windows/cli-transcriber/download-lib.bat"></a></p><pre><a href="/moonshine-ai/moonshine/blob/main/examples/windows/cli-transcriber/download-lib.bat"><code>examples/windows/cli-transcriber/download-lib.bat</code></a></pre> will fetch these for you. You'll see an <p></p><pre><code>include</code></pre> folder that you should add to the include search paths in your project settings, and a <pre><code>lib</code></pre> directory that you should add to the include search paths. Then add all of the library files in the <pre><code>lib</code></pre> folder to your project's linker dependencies.<p></p>
<pre><code>#include "moonshine-cpp.h"</code></pre> to access Moonshine from your C++ code. If you want to see an example of all these changes together, take a look at <a href="/moonshine-ai/moonshine/blob/main/examples/windows/cli-transcriber"><pre><code>examples/windows/cli-transcriber</code></pre></a>.<p></p>
<div><h3>Debugging</h3><a href="#debugging"></a></div>
<div><h4>Console Logs</h4><a href="#console-logs"></a></div>
<p>The library is designed to help you understand what's going wrong when you hit an issue. If something isn't working as expected, the first place to look is the console for log messages. Whenever there's a failure point or an exception within the core library, you should see a message that adds more information about what went wrong. Your language bindings should also recognize when the core library has returned an error and raise an appropriate exception, but sometimes the logs can be helpful because they contain more details.</p>
<div><h4>Input Saving</h4><a href="#input-saving"></a></div>
<p>If no errors are being reported but the quality of the transcription isn't what you expect, it's worth ruling out an issue with the audio data that the transcriber is receiving. To make this easier, you can pass in the </p><pre><code>save_input_wav_path</code></pre> option when you create a transcriber. That will save any audio received into .wav files in the folder you specify. Here's a Python example:<p></p>
<div><pre><span>python</span> <span>-</span><span>m</span> <span>moonshine_voice</span>.<span>transcriber</span> <span>-</span><span>-</span><span>options</span><span>=</span><span>'save_input_wav_path=.'</span></pre></div>
<p>This will run test audio through a transcriber, and write out the audio it has received into an </p><pre><code>input_1.wav</code></pre> file in the current directory. If you're running multiple streams, you'll see <pre><code>input_2.wav</code></pre>, etc for each additional one. These wavs only contain the audio data from the latest session, and are overwritten after each one is started. Listening to these files should help you confirm that the input you're providing is as you expect it, and not distorted or corrupted.<p></p>
<div><h4>API Call Logging</h4><a href="#api-call-logging"></a></div>
<p>If you're running into errors it can be hard to keep track of the timeline of your interactions with the library. The </p><pre><code>log_api_calls</code></pre> option will print out the underlying API calls that have been triggered to the console, so you can investigate any ordering or timing issues.<p></p>
<div><pre><span>uv</span> <span>run</span> <span>-</span><span>m</span> <span>moonshine_voice</span>.<span>transcriber</span> <span>-</span><span>-</span><span>options</span><span>=</span><span>'log_api_calls=true'</span></pre></div>
<div><h3>Building from Source</h3><a href="#building-from-source"></a></div>
<p>If you want to debug into the library internals, or add instrumentation to help understand its operation, or add improvements or customizations, all of the source is available for you to build it for yourself.</p>
<div><h4>Cmake</h4><a href="#cmake"></a></div>
<p>The core engine of the library is contained in the </p><pre><code>core</code></pre> folder of this repo. It's written in C++ with a C interface for easy integration with other languages. We use cmake to build on all our platforms, and so the easiest way to get started is something like this:<p></p>
<div><pre><span>cd</span> core
mkdir -p build
<span>cd</span> build
cmake ..
cmake --build <span>.</span></pre></div>
<p>After that completes you should have a set of binary executables you can run on your own system. These executables are all unit tests, and expect to be run from the </p><pre><code>test-assets</code></pre> folder. You can run the build and test process in one step using the <a href="/moonshine-ai/moonshine/blob/main/scripts/run-core-tests.sh"><pre><code>scripts/run-core-tests.sh</code></pre></a>, or <a href="/moonshine-ai/moonshine/blob/main/scripts/run-core-tests.bat"><pre><code>scripts/run-core-tests.bat</code></pre></a> for Windows. All tests should compile and run without any errors.<p></p>
<div><h4>Language Bindings</h4><a href="#language-bindings"></a></div>
<p>There are various scripts for building for different platforms and languages, but to see examples of how to build for all of the supported systems you should look at <a href="/moonshine-ai/moonshine/blob/main/scripts/build-all-platforms.sh"></a></p><pre><a href="/moonshine-ai/moonshine/blob/main/scripts/build-all-platforms.sh"><code>scripts/build-all-platforms.sh</code></a></pre>. This is the script we call for every release, and it builds all of the artifacts we upload to the various package manager systems.<p></p>
<p>The different platforms and languages have a layer on top of the C interfaces to enable idiomatic use of the library within the different environments. The major systems have their own top-level folders in this repo, for example: <a href="/moonshine-ai/moonshine/blob/main/python"></a></p><pre><a href="/moonshine-ai/moonshine/blob/main/python"><code>python</code></a></pre>, <a href="/moonshine-ai/moonshine/blob/main/android"><pre><code>android</code></pre></a>, and <a href="/moonshine-ai/moonshine/blob/main/swift"><pre><code>swift</code></pre></a> for iOS and MacOS. This is where you'll find the code that calls the underlying core library routines, and handles the event system for each platform.<p></p>
<div><h4>Porting</h4><a href="#porting"></a></div>
<p>If you have a device that isn't supported, you can try <a href="#cmake">building using cmake</a> on your system. The only major dependency that the C++ core library has is <a href="https://github.com/microsoft/onnxruntime">the Onnx Runtime</a>. We include <a href="/moonshine-ai/moonshine/blob/main/core/third-party/onnxruntime/lib">pre-built binary library files</a> for all our supported systems, but you'll need to find or build your own version if the libraries we offer don't cover your use case.</p>
<p>If you want to call this library from a language we don't support, then you should take a look at <a href="/moonshine-ai/moonshine/blob/main/core/moonshine-c-api.h">the C interface bindings</a>. Most languages have some way to call into C functions, so you can use these and the binding examples for other languages to guide your implementation.</p>
<div><h3>Downloading Models</h3><a href="#downloading-models"></a></div>
<p>The easiest way to get the model files is using the Python module. After <a href="#python">installing it</a> run the downloader like this:</p>
<div><pre>python -m moonshine_voice.download --language en</pre></div>
<p>You can use either the two-letter code or the English name for the </p><pre><code>language</code></pre> argument. If you want to see which languages are supported by your current version they're <a href="#available-models">listed below</a>, or you can supply a bogus language as the argument to this command:<p></p>
<div><pre>python -m moonshine_voice.download --language foo</pre></div>
<p>You can also optionally request a specific model architecture using the </p><pre><code>model-arch</code></pre> flag, chosen from the numbers in <a href="/moonshine-ai/moonshine/blob/main/core/moonshine-c-api.h">moonshine-c-api.h</a>. If no architecture is set, the script will load the highest-quality model available.<p></p>
<p>The download script will log the location of the downloaded model files and the model architecture, for example:</p>
<div><pre>encoder_model.ort: 100%<span>|</span><span>|</span> 29.9M/29.9M [00:<span>00&lt;</span>00:00, 34.5MB/s]
decoder_model_merged.ort: 100%<span>|</span><span>|</span> 104M/104M [00:<span>02&lt;</span>00:00, 52.6MB/s]
tokenizer.bin: 100%<span>|</span><span>|</span> 244k/244k [00:<span>00&lt;</span>00:00, 1.44MB/s]
Model download url: https://download.moonshine.ai/model/base-en/quantized/base-en
Model components: [<span><span>'</span>encoder_model.ort<span>'</span></span>, <span><span>'</span>decoder_model_merged.ort<span>'</span></span>, <span><span>'</span>tokenizer.bin<span>'</span></span>]
Model arch: 1
Downloaded model path: /Users/petewarden/Library/Caches/moonshine_voice/download.moonshine.ai/model/base-en/quantized/base-en</pre></div>
<p>The last two lines tell you which model architecture is being used, and where the model files are on disk. By default it uses your user cache directory, which is </p><pre><code>~/Library/Caches/moonshine_voice</code></pre> on MacOS, but you can use a different location by setting the <pre><code>MOONSHINE_VOICE_CACHE</code></pre> environment variable before running the script.<p></p>
<div><h3>Benchmarks</h3><a href="#benchmarks"></a></div>
<p>The core library includes a benchmarking tool that simulates processing live audio by loading a .wav audio file and feeding it in chunks to the model. To run it:</p>
<div><pre><code>cd core
md build
cd build
cmake ..
cmake --build . --config Release
./benchmark
</code></pre></div>
<p>This will report the absolute time taken to process the audio, what percentage of the audio file's duration that is, and the average latency for a response.</p>
<p>The percentage is helpful because it approximates how much of a compute load the model will be on your hardware. For example, if it shows 20% then that means the speech processing will take a fifth of the compute time when running in your application, leaving 80% for the rest of your code.</p>
<p>The latency metric needs a bit of explanation. What most applications care about is how soon they are notified about a phrase after the user has finished talking, since this determines how fast the product can respond. As with any user interface, the time between speech ending and the app doing something determines how responsive the voice interface feels, with a goal of keeping it below 200ms. The latency figure logged here is the average time between when the library determines the user has stopped talking and the delivery of the final transcript of that phrase to the client. This is where streaming models have the most impact, since they do a lot of their work upfront, while speech is still happening, so they can usually finish very quickly.</p>
<p>By default the benchmark binary uses the Tiny English model that's embedded in the framework, but you can pass in the </p><pre><code>--model-path</code></pre> and <pre><code>--model-arch</code></pre> parameters to choose <a href="#downloading-models">one that you've downloaded</a>.<p></p>
<p>You can also choose how often the transcript should be updated using the </p><pre><code>--transcription-interval</code></pre> argument. This defaults to 0.5 seconds, but the right value will depend on how fast your application needs updates. Longer intervals reduce the compute required a bit, at the cost of slower updates.<p></p>
<div><h4>Whisper Comparisons</h4><a href="#whisper-comparisons"></a></div>
<p>For platforms that support Python, you can run the <a href="/moonshine-ai/moonshine/blob/main/scripts/run-benchmarks.py"></a></p><pre><a href="/moonshine-ai/moonshine/blob/main/scripts/run-benchmarks.py"><code>scripts/run-benchmarks.py</code></a></pre> script which will evaluate similar metrics, with the advantage that it can also download the models so you don't need to worry about path handling.<p></p>
<p>It also evaluates equivalent Whisper models. This is a pretty opinionated benchmark that looks at the latency and total compute cost
of the two families of models in a situation that is representative of many common
real-time voice applications' requirements:</p>
<ul>
<li>Speech needs to be responded to as quickly as possible once a user completes a phrase.</li>
<li>The phrases are of durations between a range of one to ten seconds.</li>
</ul>
<pre><code>These are very different requirements from bulk offline processing scenarios, where the
overall throughput of the system is more important, and so the latency on a single
segment of speech is less important than the overall throughput of the system. This
allows optimizations like batch processing.</code></pre>
<pre><code>We are not claiming that Whisper is not a great model for offline processing, but we
do want to highlight the advantages we that Moonshine offers for live speech
applications with real-time latency requirements.</code></pre>
<p>The experimental setup is as follows:</p>
<ul>
<li>We use the two_cities.wav audio file as a test case, since it has a mix of short
and long phrases. You can vary this by passing in your own audio file with the
--wav_path argument.</li>
<li>We use the Moonshine Tiny, Base, Tiny Streaming, Small Streaming, and Medium
Streaming models.</li>
<li>We compare these to the Whisper Tiny, Base, Small, and Large v3 models. Since the
Moonshine Medium Streaming model achieves lower WER than Whisper Large v3 we compare
those two, otherwise we compare each with their namesake.</li>
<li>We use the Moonshine VAD segmenter to split the audio into phrases, and feed each
phrase to Whisper for transcription.</li>
<li>Response latency for both models is measured as the time between a phrase being
identified as complete by the VAD segmenter and the transcribed text being returned.
For Whisper this means the full transcription time, but since the Moonshine models
are streaming we can do a lot of the work while speech is still happening, so the
latency is much lower.</li>
<li>We measure the total compute cost of the models by totalling the duration of the
audio processing times for each model, and then expressing that as a percentage of the
total audio duration. This is the inverse of the commonly used real-time factor (RTF)
metric, but it reflects the compute load required for a real-time application.</li>
<li>We're using faster-whisper for Whisper, since that seems to provide the best
cross-platform performance. We're also sticking with the CPU, since most applications
can't rely on GPU or NPU acceleration being present on all the platforms they target.
We know there are a lot of great GPU/NPU-accelerated Whisper implementations out there,
but these aren't portable enough to be useful for the applications we care about.</li>
</ul>
<div><h2>Models</h2><a href="#models"></a></div>
<p>Moonshine Voice is based on a family of speech to text models created by the team at Moonshine AI. If you want to download models to use with the framework, you can use <a href="#downloading-models">the Python package to access them</a>. This section contains more information about the history and characteristics of the models we offer.</p>
<ul>
<li><a href="#papers">Papers</a></li>
<li><a href="#available-models">Available Models</a></li>
<li><a href="#domain-customization">Domain Customization</a></li>
<li><a href="#quantization">Quantization</a></li>
<li><a href="#huggingface">HuggingFace</a></li>
</ul>
<div><h3>Papers</h3><a href="#papers"></a></div>
<p>These research papers are a good resource for understanding the architectures and performance strategies behind the models:</p>
<ul>
<li><a href="https://arxiv.org/abs/2410.15608"><strong>Moonshine: Speech Recognition for Live Transcription and Voice Commands</strong></a>: Describes the first-generation model architecture, which enabled flexible-duration input windows, improving on Whisper's fixed 30 second requirement.</li>
<li><a href="https://arxiv.org/abs/2509.02523"><strong>Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices</strong></a>: How we improved accuracy for non-English languages by training mono-lingual models.</li>
<li><a href="https://arxiv.org/abs/2602.12241"><strong>Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech
Applications</strong></a>: Introduces our approach to streaming, and the advantages it offers for live voice applications.</li>
</ul>
<div><h3>Available Models</h3><a href="#available-models"></a></div>
<p>Here are the models currently available. See <a href="#downloading-models">Downloading Models</a> for how to obtain them. This library uses the Onnx model format, converted to the memory-mappable OnnxRuntime (</p><pre><code>.ort</code></pre>) flatbuffer encoding. For <pre><code>safetensor</code></pre> versions, see the <a href="#huggingface">HuggingFace</a> section.<p></p>
<table>
<thead>
<tr>
<th>Language</th>
<th>Architecture</th>
<th># Parameters</th>
<th>WER/CER</th>
</tr>
</thead>
<tbody>
<tr>
<td>English</td>
<td>Tiny</td>
<td>26 million</td>
<td>12.66%</td>
</tr>
<tr>
<td>English</td>
<td>Tiny Streaming</td>
<td>34 million</td>
<td>12.00%</td>
</tr>
<tr>
<td>English</td>
<td>Base</td>
<td>58 million</td>
<td>10.07%</td>
</tr>
<tr>
<td>English</td>
<td>Small Streaming</td>
<td>123 million</td>
<td>7.84%</td>
</tr>
<tr>
<td>English</td>
<td>Medium Streaming</td>
<td>245 million</td>
<td>6.65%</td>
</tr>
<tr>
<td>Arabic</td>
<td>Base</td>
<td>58 million</td>
<td>5.63%</td>
</tr>
<tr>
<td>Japanese</td>
<td>Base</td>
<td>58 million</td>
<td>13.62%</td>
</tr>
<tr>
<td>Korean</td>
<td>Tiny</td>
<td>26 million</td>
<td>6.46%</td>
</tr>
<tr>
<td>Mandarin</td>
<td>Base</td>
<td>58 million</td>
<td>25.76%</td>
</tr>
<tr>
<td>Spanish</td>
<td>Base</td>
<td>58 million</td>
<td>4.33%</td>
</tr>
<tr>
<td>Ukrainian</td>
<td>Base</td>
<td>58 million</td>
<td>14.55%</td>
</tr>
<tr>
<td>Vietnamese</td>
<td>Base</td>
<td>58 million</td>
<td>8.82%</td>
</tr>
</tbody>
</table>
<p>The English evaluations were done using the <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">HuggingFace OpenASR Leaderboard</a> datasets and methodology. The other languages were evaluated using the FLEURS dataset and the <a href="/moonshine-ai/moonshine/blob/main/scripts/eval-model-accuracy.py"></a></p><pre><a href="/moonshine-ai/moonshine/blob/main/scripts/eval-model-accuracy.py"><code>scripts/eval-model-accuracy</code></a></pre> script, with the character or word error rate chosen per language.<p></p>
<p>One common issue to watch out for if you're using models that don't use the Latin alphabet (so any languages except English and Spanish) is that you'll need to set the <a href="#transcriber-options"></a></p><pre><a href="#transcriber-options"><code>max_tokens_per_second</code></a></pre><a href="#transcriber-options"> option</a> to 13.0 when you create the transcriber. This is because the most common pattern for hallucinations is endlessly repeating the last few words, and our heuristic to detect this is to check if there's an unusually high number of tokens for the duration of a segment. Unfortunately the base number of tokens per second for non-Latin languages is much higher than for English, thanks to how we're tokenizing, so you have to manually set the threshold higher to avoid cutting off valid outputs.<p></p>
<div><h3>Domain Customization</h3><a href="#domain-customization"></a></div>
<p>It's often useful to be able to calibrate a speech to text model towards certain words that you're expecting to hear in your application, whether it's technical terms, slang, or a particular dialect or accent. <a href="mailto:contact@moonshine.ai">Moonshine AI offers full retraining using our internal dataset for customization as a commercial service</a> and we do hope to support free lighter-weight approaches in the future. You can find a community project working on this at <a href="https://github.com/pierre-cheneau/finetune-moonshine-asr">github.com/pierre-cheneau/finetune-moonshine-asr</a>.</p>
<div><h3>Quantization</h3><a href="#quantization"></a></div>
<p>We typically quantize our models to eight-bit weights across the board, and eight-bit calculations for heavy operations like MatMul. This is all post-training quantization, using a combination of OnnxRuntime's tools and <a href="https://pypi.org/project/onnx-shrink-ray/">my Onnx Shrink Ray utility</a>. The only anomaly in the process is the treatment of the frontend, which uses convolution layers to generate features, which produces results similar to the more traditional MEL spectrogram preprocessing, but in a learned way with standard ML operations. The inputs to this initial stage correspond to 16-bit signed integers from the raw audio data (though they're encoded as floats) so we've found it necessary to leave the convolution operations in at least B16 float precision.</p>
<p>You can see the options we use for the conversions in <a href="/moonshine-ai/moonshine/blob/main/scripts/quantize-streaming-model.sh">scripts/quantize-streaming-model.sh</a>.</p>
<div><h3>HuggingFace</h3><a href="#huggingface"></a></div>
<p>We have </p><pre><code>safetensors</code></pre> versions of the models linked from our organization on HF, <a href="https://huggingface.co/UsefulSensors/models">huggingface.co/UsefulSensors/models</a>. The organization name is from an earlier incarnation of the company, when we were focused on supplying complete voice interface solutions integrated onto a low-cost chip with a built-in microphone. These are all floating-point checkpoints exported from our training pipeline<p></p>
<div><h2>API Reference</h2><a href="#api-reference"></a></div>
<p>This documentation covers the Python API, but the same functions and classes are present in all the other supported languages, just with native adaptations (for example CamelCase). You should be able to use this as a reference for all platforms the library runs on.</p>
<ul>
<li><a href="#data-structures">Data Structures</a>
<ul>
<li><a href="#transcriberline">TranscriberLine</a></li>
<li><a href="#transcript">Transcript</a></li>
<li><a href="#transcriptevent">TranscriptEvent</a></li>
<li><a href="#intentmatch">IntentMatch</a></li>
</ul>
</li>
<li><a href="#classes">Classes</a>
<ul>
<li><a href="#transcriber">Transcriber</a></li>
<li><a href="#mictranscriber">MicTranscriber</a></li>
<li><a href="#stream">Stream</a></li>
<li><a href="#transcripteventlistener">TranscriptEventListener</a></li>
<li><a href="#intentrecognizer">IntentRecognizer</a></li>
</ul>
</li>
</ul>
<div><h3>Data Structures</h3><a href="#data-structures"></a></div>
<div><h4>TranscriberLine</h4><a href="#transcriberline"></a></div>
<p>Represents a single "line" or speech segment in a transcript. It includes information about the timing, speaker, and text content of the utterance, as well as state such as whether the speech is ongoing or done. If you're building an application that involves transcription, this data structure has all of the information available about each line of speech. Be aware that each line can be updated multiple times with new text and other information as the user keeps speaking.</p>
<ul>
<li>
<p></p><pre><code>text</code></pre>: A string containing the UTF-8 encoded text that has been extracted from the audio of this segment.<p></p>
</li>
<li>
<p></p><pre><code>start_time</code></pre>: A float value representing the time in seconds since the start of the current session that the current utterance was first detected.<p></p>
</li>
<li>
<p></p><pre><code>duration</code></pre>: A float that represents the duration in seconds of the current utterance.<p></p>
</li>
<li>
<p></p><pre><code>line_id</code></pre>: An unsigned 64-bit integer that represents a line in a collision-resistant way, for use in storage and ensuring the application can keep track of lines as they change over time. See <a href="#transcription-event-flow">Transcription Event Flow</a> for more details.<p></p>
</li>
<li>
<p></p><pre><code>is_complete</code></pre>: A boolean that is false until the segment has been completed, and true for the remainder of the line's lifetime.<p></p>
</li>
<li>
<p></p><pre><code>is_updated</code></pre>: A boolean that's true if any information about the line has changed since the last time the transcript was updated. Since the transcript will be periodically updated internally by the library as you add audio chunks, you can't rely on polling this to detect changes. You should rely on the event/listener flow to catch modifications instead. This applies to all of the booleans below too.<p></p>
</li>
<li>
<p></p><pre><code>is_new</code></pre>: A boolean indicating whether the line has been added to the transcript by the last update call.<p></p>
</li>
<li>
<p></p><pre><code>has_text_changed</code></pre>: A boolean that's set if the contents of the line's text was modified by the last transcript update. If this is set, <pre><code>is_updated</code></pre> will always be set too, but if other properties of the line (for example the duration or the audio data) have changed but the text remains the same, then <pre><code>is_updated</code></pre> can be true while <pre><code>has_text_changed</code></pre> is false.<p></p>
</li>
<li>
<p></p><pre><code>has_speaker_id</code></pre>: Whether a speaker has been identified for this line. Unless the <pre><code>identify_speakers</code></pre> option passed to the Transcriber is set to false, this will always be true by the time the line is complete, and potentially it may be set earlier. The speaker identification process is still experimental, so the current accuracy may not be reliable enough for some applications.<p></p>
</li>
<li>
<p></p><pre><code>speaker_id</code></pre>: A unique-ish unsigned 64-bit integer that is designed for storage or used to identify the same speaker across multiple sessions.<p></p>
</li>
<li>
<p></p><pre><code>speaker_index</code></pre>: An integer that represents the order in which the speaker appeared in the transcript, to make it easy to give speakers default names like "Speaker 1:", etc.<p></p>
</li>
<li>
<p></p><pre><code>audio_data</code></pre>: An array of 32-bit floats representing the raw audio data that the line is based on, as 16KHz mono PCM data between 0.0 and 1.0. This can be useful for further processing (for example to drive a visual indicator or to feed into a specialized speech to text model after the line is complete).<p></p>
</li>
</ul> <p>A Transcript contains a list of TranscriberLines, arranged in descending time order. The transcript is reset at every </p><pre><code>Transcriber.start()</code></pre> call, so if you need to retain information from it, you should make explicit copies. Most applications won't work with this structure, since all of the same information is available through event callbacks.<p></p>
<div><h4>TranscriptEvent</h4><a href="#transcriptevent"></a></div>
<p>Contains information about a change to the transcript. It has four subclasses, which are explained in more detail in <a href="#transcription-event-flow">the transcription event flow section</a>. Most of the information is contained in the </p><pre><code>line</code></pre> member, but there's also a <pre><code>stream_handle</code></pre> that your application can use to tell the source of a line if you're running multiple streams.<p></p>
<div><h4>IntentMatch</h4><a href="#intentmatch"></a></div>
<p>This event is sent to any listeners you have registered when an </p><pre><code>IntentRecognizer</code></pre> finds a match to a command you've specified.<p></p>
<ul>
<li><pre><code>trigger_phrase</code></pre>: The string representing the canonical command, exactly as you registered it with the recognizer.</li>
<li><pre><code>utterance</code></pre>: The text of the utterance that triggered the match.</li>
<li><pre><code>similarity</code></pre>: A float value that reflects how confident the recognizer is that the utterance has the same meaning as the command, with zero being the least confident and one the most.</li>
</ul>
<div><h3>Classes</h3><a href="#classes"></a></div>
<div><h4>Transcriber</h4><a href="#transcriber"></a></div>
<p>Handles the speech to text pipeline.</p>
<ul>
<li>
<p><a></a></p><pre><code>__init__()</code></pre>: Loads and initializes the transcriber.<p></p>
<ul>
<li><pre><code>model_path</code></pre>: The path to the directory holding the component model files needed for the complete flow. Note that this is a path to the <strong>folder</strong>, not an individual <strong>file</strong>. You can download and get a path to a cached version of the standard models using the <a href="#downloading-models">download_model()</a> function.</li>
<li><pre><code>model_arch</code></pre>: The architecture of the model to load, from the selection defined in <pre><code>ModelArch</code></pre>.</li>
<li><pre><code>update_interval</code></pre>: By default the transcriber will periodically run text transcription as new audio data is fed, so that update events can be triggered. This value is how often the speech to text model should be run. You can set this to a large duration to suppress updates between a line starting and ending, but because the streaming models do a lot of their work before the final speech to text stage, this may not reduce overall latency by much.</li>
<li><a></a><pre><code>options</code></pre>: These are flags that affect how the transcription process works inside the library, often enabling performance optimizations or debug logging. They are passed as a dictionary mapping strings to strings, even if the values are to be interpreted as numbers - for example <pre><code>{"max_tokens_per_second", "15"}</code></pre>.
<ul>
<li><pre><code>skip_transcription</code></pre>: If you only want the voice-activity detection and segmentation, but want to do further processing in your app, you can set this to "true" and then use the <pre><code>audioData</code></pre> array in each line.</li>
<li><pre><code>max_tokens_per_second</code></pre>: The models occassionally get caught in an infinite decoder loop, where the same words are repeated over and over again. As a heuristic to catch this we compare the number of tokens in the current run to the duration of the audio, and if there seem to be too many tokens we truncate the decoding. By default this is set to 6.5, but for non-English languages where the models produce a lot more raw tokens per second, you may want to bump this to 13.0.</li>
<li><pre><code>transcription_interval</code></pre>: How often to run transcription, in seconds.</li>
<li><pre><code>vad_threshold</code></pre>: Controls the sensitivity of the initial voice-activity detection stage that decides how to break raw audio into segments. This defaults to 0.5, with lower values creating longer segments, potentially with more background noise sections, and higher values breaking up speech into smaller chunks, at the risk of losing some actual speech by clipping.</li>
<li><pre><code>save_input_wav_path</code></pre>: One of the most common causes of poor transcription quality is incorrect conversion or corruption of the audio that's fed into the pipeline. If you set this option to a folder path, the transcriber will save out exactly what it has received as 16KHz mono WAV files, so you can ensure that your input audio is as you expect.</li>
<li><pre><code>log_api_calls</code></pre>: Another debugging option, turning this on causes all calls to the C API entry points in the library to write out information on their arguments to stderr or the console each time they're run.</li>
<li><pre><code>log_ort_runs</code></pre>: Prints information about the ONNXRuntime inference runs and how long they take.</li>
<li><pre><code>vad_window_duration</code></pre>: The VAD runs every 30ms, but to get higher-confidence values we average the results over time. This value is the time in seconds to average over. The default is 0.5s, shorter durations will spot speech faster at the cost of lower accuracy, higher values may increase accuracy, but at the cost of missing shorter utterances.</li>
<li><pre><code>vad_look_behind_sample_count</code></pre>: Because we're averaging over time, the mean VAD signal will lag behind the initial speech detection. To compensate for that, when speech is detected we pull in some of the audio immediately before the average passed the threshold. This value is the number of samples to prepend, and defaults to 8192 (all at 16KHz).</li>
<li><pre><code>vad_max_segment_duration</code></pre>: It can be hard to find gaps in rapid-fire speech, but a lot of applications want their text in chunks that aren't endless. This option sets the longest duration a line can be before it's marked as complete and a new segment is started. The default is 15 seconds, and to increase the chance that a natural break is found, the <pre><code>vad_threshold</code></pre> is linearly decreased over time from two thirds of the maximum duration until the maximum is reached.</li>
<li><pre><code>identify_speakers</code></pre>: A boolean that controls whether to run the speaker identification stage in the pipeline.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a></a></p><pre><code>transcribe_without_streaming()</code></pre>: A convenience function to extract text from a non-live audio source, such as a file. We optimize for streaming use cases, so you're probably better off using libraries that specialize in bulk, batched transcription if you use this a lot and have performance constraints. This will still call any registered event listeners as it processes the lines, so this can be useful to test your application using pre-recorded files, or to easily integrate offline audio sources.<p></p>
<ul>
<li><pre><code>audio_data</code></pre>: An array of 32-bit float values, representing mono PCM audio between -1.0 and 1.0, to be analyzed for speech.</li>
<li><pre><code>sample_rate</code></pre>: The number of samples per second. The library uses this to convert to its working rate (16KHz) internally.</li>
<li><pre><code>flags</code></pre>: Integer, currently unused.</li>
</ul>
</li>
<li>
<p><a></a></p><pre><code>start()</code></pre>: Begins a new transcription session. You need to call this after you've created the <pre><code>Transcriber</code></pre> and before you add any audio.<p></p>
</li>
<li>
<p><a></a></p><pre><code>stop()</code></pre>: Ends a transcription session. If a speech segment was still active, it's marked as complete and the appropriate event handlers are called.<p></p>
</li>
<li>
<p><a></a></p><pre><code>add_audio()</code></pre>: Call this every time you have a new chunk of audio from your input, to begin processing. The size and sample rate of the audio should be whatever's natural for your source, since the library will handle all conversions.<p></p>
<ul>
<li><pre><code>audio_data</code></pre>: Array of 32-bit floats representing a mono PCM chunk of audio.</li>
<li><pre><code>sample_rate</code></pre>: How many samples per second are present in the input audio. The library uses this to convert the data to its preferred rate.</li>
</ul>
</li>
<li>
<p><a></a></p><pre><code>update_transcription</code></pre>: The transcript is usually updated periodically as audio data is added, but if you need to trigger one yourself, for example when a user presses refresh, or want access to the complete transcript, you can call this manually.<p></p>
<ul>
<li><pre><code>flags</code></pre>: Integer holding flags that are combined using bitwise or (<pre><code>|</code></pre>).
<ul>
<li><pre><code>MOONSHINE_FLAG_FORCE_UPDATE</code></pre>: By default the transcriber returns a cached version of the transcript if less than 200ms of new audio has come in since the last transcription, but by setting this you can ensure that a transcription happens regardless.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a></a></p><pre><code>create_stream()</code></pre>: If your application is taking audio input from multiple sources, for example a microphone and system audio, then you'll want to create multiple streams on a single transcriber to avoid loading multiple copies of the models. Each stream has its own transcript, and line events are tagged with the stream handle they came from. You don't need to worry about this if you only need to deal with a single input though, just use the <pre><code>Transcriber</code></pre> class's <pre><code>start()</code></pre>, <pre><code>stop()</code></pre>, etc. This function returns <pre><code>Stream</code></pre> class object.<p></p>
<ul>
<li><pre><code>flags</code></pre>: Integer, reserved for future expansion.</li>
<li><pre><code>update_interval</code></pre>: Period in seconds between transcription updates.</li>
</ul>
</li>
<li>
<p><a></a></p><pre><code>add_listener()</code></pre>: Registers a callable object with the transcriber. This object will be called back as audio is fed in and text is extracted.<p></p>
<ul>
<li><pre><code>listener</code></pre>: This is often a subclass of <pre><code>TranscriptEventListener</code></pre>, but can be a plain function. It defines what code is called when a speech event happens.</li>
</ul>
</li>
<li>
<p><a></a></p><pre><code>remove_listener()</code></pre>: Deletes a listener so that it no longer receives events.<p></p>
<ul>
<li><pre><code>listener</code></pre>: An object you previously passed into <pre><code>add_listener()</code></pre>.</li>
</ul>
</li>
<li>
<p><a></a></p><pre><code>remove_all_listeners()</code></pre>: Deletes all registered listeners so than none of them receive events anymore.<p></p>
</li>
</ul>
<div><h4>MicTranscriber</h4><a href="#mictranscriber"></a></div>
<p>This class supports the []</p><pre><code>start()</code></pre>](#transcriber-start), <a href="#transcriber-stop"><pre><code>stop()</code></pre></a> and listener functions of <a href="#transcriber"><pre><code>Transcriber</code></pre></a>, but internally creates and attaches to the system's microphone input, so you don't need to call <a href="#transcriber-add-audio"><pre><code>add_audio()</code></pre></a> yourself. In Python this uses the <a href="/moonshine-ai/moonshine/blob/main"><pre><code>sounddevice</code></pre> library</a>, but in other languages the class uses the native audio API under the hood.<p></p>
<div><h4>Stream</h4><a href="#stream"></a></div>
<p>The access point for when you need to feed multiple audio inputs into a single transcriber. Supports <a href="#transcriber-start"></a></p><pre><a href="#transcriber-start"><code>start()</code></a></pre>, <a href="#transcriber-stop"><pre><code>stop()</code></pre></a>, <a href="#transcriber-add-audio"><pre><code>add_audio()</code></pre></a>, <a href="#transcriber-update-transcription"><pre><code>update_transcription()</code></pre></a>, <a href="#transcriber-add-listener"><pre><code>add_listener()</code></pre></a>, <a href="#transcriber-remove-listener"><pre><code>remove_listener()</code></pre></a>, and <a href="#transcriber-remove-all-listeners"><pre><code>remove_all_listeners()</code></pre></a> as documented in the <a href="#transcriber"><pre><code>Transcriber</code></pre></a> class.<p></p>
<div><h4>TranscriptEventListener</h4><a href="#transcripteventlistener"></a></div>
<p>A convenience class to derive from to create your own listener code. Override any or all of </p><pre><code>on_line_started()</code></pre>, <pre><code>on_line_updated()</code></pre>, <pre><code>on_line_text_changed()</code></pre>, and <pre><code>on_line_completed()</code></pre>, and they'll be called back when the corresponding event occurs.<p></p>
<div><h4>IntentRecognizer</h4><a href="#intentrecognizer"></a></div>
<p>A specialized kind of event listener that you add as a listener to a </p><pre><code>Transcriber</code></pre>, and it then analyzes the transcription results to determine if any of the specified commands have been spoken, using natural-language fuzzy matching.<p></p> <div><h2>Support</h2><a href="#support"></a></div>
<p>Our primary support channel is <a href="https://discord.gg/27qp9zSRXF">the Moonshine Discord</a>. We make our best efforts to respond to questions there, and other channels like <a href="https://github.com/moonshine-ai/moonshine/issues">GitHub issues</a>. We also offer paid support for commercial customers who need porting or acceleration on other platforms, model customization, more languages, or any other services, please <a href="mailto:contact@moonshine.ai">get in touch</a>.</p>
<div><h2>Roadmap</h2><a href="#roadmap"></a></div>
<p>This library is in active development, and we aim to implement:</p>
<ul>
<li>Binary size reduction for mobile deployment.</li>
<li>More languages.</li>
<li>More streaming models.</li>
<li>Improved speaker identification.</li>
<li>Lightweight domain customization.</li>
</ul>
<div><h2>Acknowledgements</h2><a href="#acknowledgements"></a></div>
<p>We're grateful to:</p>
<ul>
<li>Lambda and Stephen Balaban for supporting our model training through <a href="https://lambda.ai/research">their foundational model grants</a>.</li>
<li>The ONNX Runtime community for building <a href="https://github.com/microsoft/onnxruntime">a fast, cross-platform inference engine</a>.</li>
<li><a href="https://github.com/snakers4">Alexander Veysov</a> for the great <a href="https://github.com/snakers4/silero-vad">Silero Voice Activity Detector</a>.</li>
<li><a href="https://github.com/onqtam">Viktor Kirilov</a> for <a href="https://github.com/doctest/doctest">his fantastic DocTest C++ testing framework</a>.</li>
<li><a href="https://github.com/nemtrif">Nemanja Trifunovic</a> for <a href="https://github.com/nemtrif/utfcpp">his very helpful UTF8 CPP library</a>.</li>
<li>The <a href="https://www.pyannote.ai/">Pyannote team</a> for making available their speaker embedding model.</li>
</ul>
<div><h2>License</h2><a href="#license"></a></div>
<p>This code, apart from the source in </p><pre><code>core/third-party</code></pre>, is licensed under the MIT License, see LICENSE in this repository.<p></p>
<p>The English-language models are also released under the MIT License. Models for other languages are released under the <a href="https://moonshine.ai">Moonshine Community License</a>, which is a non-commercial license.</p>
<p>The code in </p><pre><code>core/third-party</code></pre> is licensed according to the terms of the open source projects it originates from, with details in a LICENSE file in each subfolder.<p></p>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()"></button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()"></button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>