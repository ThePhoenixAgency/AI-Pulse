<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Building a Universal Memory Layer for AI Agents: Architecture Patterns for Scalable State Management</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>Building a Universal Memory Layer for AI Agents: Architecture Patterns for Scalable State Management</h1>
  <div class="metadata">
    Source: Dev.to Open Source | Date: 2/18/2026 6:46:43 PM | <a href="https://dev.to/varun_pratapbhardwaj_b13/building-a-universal-memory-layer-for-ai-agents-architecture-patterns-for-scalable-state-management-4g8" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><div> <p>Every time an AI agent completes a task, it forgets everything. The conversation context vanishes. The user preferences it inferred are gone. The multi-step reasoning chain it constructed dissolves. If you have built anything with LLM-based agents, you have hit this wall: agents are stateless by default, and making them stateful is an unsolved architecture problem for most teams.</p> <p>This is not the same challenge as caching database queries or managing user sessions. Agent memory requires storing heterogeneous data (facts, episodes, preferences, tool outputs), retrieving it with semantic understanding, and sharing it across agents that may run on different models or frameworks. The patterns you need come from cognitive science as much as from distributed systems.</p> <blockquote>
<p><strong>What You Will Learn</strong></p> <ul>
<li>The difference between episodic, semantic, and procedural memory in the context of AI agents</li>
<li>How to design a write/read pipeline from agent actions through to context window injection</li>
<li>Concrete retrieval strategies: vector search, BM25, and hybrid ranking with reciprocal rank fusion</li>
<li>Architecture tradeoffs between local-first and cloud-based memory stores</li>
<li>How to make memory interoperable across multi-agent systems (OpenAI, Claude, Gemini, open-source)</li>
<li>When traditional caching or database patterns break down for agent state</li>
</ul>
</blockquote> <h2> <a name="conceptual-foundation-why-agent-memory-is-different" href="#conceptual-foundation-why-agent-memory-is-different"> </a> Conceptual Foundation: Why Agent Memory Is Different
</h2> <p>Traditional application state management assumes structured data with known schemas. You store a user record, query it by ID, maybe cache it in Redis. The data model is fixed at design time.</p> <p>Agent memory breaks these assumptions in three ways. First, the data is unstructured and heterogeneous — a memory might be a conversation snippet, a JSON tool result, an inferred user preference, or a reasoning trace. Second, retrieval must be semantic — you cannot query agent memory purely by key; you need to find memories that are <em>relevant</em> to the current context, even if they share no lexical overlap. Third, the consumer of this memory is a language model with a finite context window, so you must rank and compress memories before injection.</p> <p>Cognitive science provides a useful taxonomy that maps well to engineering requirements. Human memory is broadly divided into three systems, and agent memory benefits from the same decomposition.</p> <p><strong>Episodic memory</strong> stores specific events and experiences with temporal context. For an agent, this means conversation turns, tool invocations, and their results — the "what happened" log. Episodic memory is append-only and timestamped.</p> <p><strong>Semantic memory</strong> stores general knowledge and facts extracted from experiences. When an agent learns that "the user prefers Python over JavaScript" across multiple conversations, that is a semantic memory. It is distilled, deduplicated, and updated over time.</p> <p><strong>Procedural memory</strong> stores learned behaviors and patterns — how to accomplish recurring tasks. In agent systems, this might be stored as successful tool-call sequences, prompt templates that worked well, or workflow graphs.</p> <blockquote>
<p><strong>Do Not Conflate Agent Memory with RAG</strong></p> <p>Retrieval-Augmented Generation (RAG) retrieves from a static knowledge base. Agent memory retrieves from a <em>dynamic, agent-generated</em> store that grows with every interaction. The write path matters as much as the read path. If your architecture only handles reads from a pre-indexed corpus, you do not have agent memory — you have document search.</p>
</blockquote> <h2> <a name="how-it-works-the-memory-writeread-pipeline" href="#how-it-works-the-memory-writeread-pipeline"> </a> How It Works: The Memory Write/Read Pipeline
</h2> <p>The core architecture has two pipelines: a write path that processes agent outputs into structured memory stores, and a read path that retrieves and ranks relevant memories for context injection.<br>
</p> <div>
<pre><code>graph TD A["Agent Action / Output"] --&gt; B["Episodic Buffer"] B --&gt; C["Memory Processor"] C --&gt; D["Episodic Store (raw events, timestamped)"] C --&gt; E["Semantic Store (extracted facts, entities)"] C --&gt; F["Procedural Store (workflow patterns)"] G["New Agent Query / Task"] --&gt; H["Hybrid Retriever"] D --&gt; H E --&gt; H F --&gt; H H --&gt; I["Vector Search (embedding similarity)"] H --&gt; J["BM25 (keyword match)"] I --&gt; K["Reciprocal Rank Fusion"] J --&gt; K K --&gt; L["Context Window Injection"] L --&gt; M["Agent LLM Call"]
</code></pre>
<div>
<p> Enter fullscreen mode Exit fullscreen mode </p>
</div>
</div> <h3> <a name="the-write-path" href="#the-write-path"> </a> The Write Path
</h3> <p>When an agent completes an action — a conversation turn, a tool call, a reasoning step — the raw event enters an <strong>episodic buffer</strong>. This buffer is a short-term holding area, analogous to working memory. A memory processor then performs three operations:</p> <ol>
<li>
<strong>Store the raw episode</strong> with metadata (timestamp, agent ID, session ID, tool used, token count).</li>
<li>
<strong>Extract semantic facts</strong> using an LLM or rule-based extractor. For example, from the conversation "I moved to Berlin last year," extract the fact <code>{entity: "user", attribute: "location", value: "Berlin", confidence: 0.9}</code>.</li>
<li>
<strong>Detect procedural patterns</strong> by comparing the current action sequence against stored workflows. If a three-step tool-call pattern recurs, store it as a reusable procedure.</li>
</ol> <h3> <a name="the-read-path" href="#the-read-path"> </a> The Read Path
</h3> <p>When an agent needs context for a new task, the hybrid retriever queries all three stores simultaneously. This is where things get interesting — and where naive approaches fail.</p> <p>A pure vector search finds semantically similar memories but misses exact keyword matches. A pure BM25 keyword search finds lexical matches but misses paraphrased or conceptually related memories. You need both.</p> <h2> <a name="practical-implementation-building-the-memory-layer" href="#practical-implementation-building-the-memory-layer"> </a> Practical Implementation: Building the Memory Layer
</h2> <p>Let us build this step by step. We will use Python with commonly available libraries.</p> <p><strong>1. Define the Memory Schema</strong></p> <p>Every memory entry needs a consistent schema regardless of which store it lives in. Here is a minimal but extensible data class:<br>
</p> <div>
<pre><code><span>from</span> <span>dataclasses</span> <span>import</span> <span>dataclass</span><span>,</span> <span>field</span>
<span>from</span> <span>datetime</span> <span>import</span> <span>datetime</span>
<span>from</span> <span>typing</span> <span>import</span> <span>Optional</span>
<span>import</span> <span>uuid</span> <span>@dataclass</span>
<span>class</span> <span>MemoryEntry</span><span>:</span> <span>content</span><span>:</span> <span>str</span> <span># The raw text content
</span> <span>memory_type</span><span>:</span> <span>str</span> <span># "episodic", "semantic", "procedural"
</span> <span>embedding</span><span>:</span> <span>Optional</span><span>[</span><span>list</span><span>[</span><span>float</span><span>]]</span> <span>=</span> <span>None</span> <span># Vector embedding, computed async
</span> <span>metadata</span><span>:</span> <span>dict</span> <span>=</span> <span>field</span><span>(</span><span>default_factory</span><span>=</span><span>dict</span><span>)</span> <span>memory_id</span><span>:</span> <span>str</span> <span>=</span> <span>field</span><span>(</span><span>default_factory</span><span>=</span><span>lambda</span><span>:</span> <span>str</span><span>(</span><span>uuid</span><span>.</span><span>uuid4</span><span>()))</span> <span>created_at</span><span>:</span> <span>datetime</span> <span>=</span> <span>field</span><span>(</span><span>default_factory</span><span>=</span><span>datetime</span><span>.</span><span>utcnow</span><span>)</span> <span>agent_id</span><span>:</span> <span>Optional</span><span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span> <span># Which agent wrote this
</span> <span>session_id</span><span>:</span> <span>Optional</span><span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span> <span># Conversation/task session
</span> <span>trust_score</span><span>:</span> <span>float</span> <span>=</span> <span>1.0</span> <span># For multi-agent: how reliable is this memory
</span></code></pre>
<div>
<p> Enter fullscreen mode Exit fullscreen mode </p>
</div>
</div> <p>The <code>trust_score</code> field is important for multi-agent systems. When Agent A writes a memory and Agent B reads it, B needs a way to assess reliability. More on this later.</p> <p><strong>2. Implement the Write Path with Semantic Extraction</strong></p> <p>The write path takes raw agent output and produces both episodic and semantic memories:<br>
</p> <div>
<pre><code><span>import</span> <span>json</span>
<span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span> <span>client</span> <span>=</span> <span>OpenAI</span><span>()</span> <span>EXTRACTION_PROMPT</span> <span>=</span> <span>"""</span><span>Extract structured facts from this agent interaction.
Return a JSON array of objects with keys: entity, attribute, value, confidence (0-1).
Only extract facts that are explicitly stated or strongly implied. Interaction:
{content} JSON output:</span><span>"""</span> <span>def</span> <span>write_memory</span><span>(</span><span>content</span><span>:</span> <span>str</span><span>,</span> <span>agent_id</span><span>:</span> <span>str</span><span>,</span> <span>session_id</span><span>:</span> <span>str</span><span>,</span> <span>store</span><span>)</span> <span>-&gt;</span> <span>list</span><span>[</span><span>MemoryEntry</span><span>]:</span> <span>"""</span><span>Process agent output into episodic + semantic memories.</span><span>"""</span> <span>memories</span> <span>=</span> <span>[]</span> <span># 1. Always store the raw episode
</span> <span>episodic</span> <span>=</span> <span>MemoryEntry</span><span>(</span> <span>content</span><span>=</span><span>content</span><span>,</span> <span>memory_type</span><span>=</span><span>"</span><span>episodic</span><span>"</span><span>,</span> <span>agent_id</span><span>=</span><span>agent_id</span><span>,</span> <span>session_id</span><span>=</span><span>session_id</span><span>,</span> <span>metadata</span><span>=</span><span>{</span><span>"</span><span>raw</span><span>"</span><span>:</span> <span>True</span><span>}</span> <span>)</span> <span>memories</span><span>.</span><span>append</span><span>(</span><span>episodic</span><span>)</span> <span># 2. Extract semantic facts via LLM
</span> <span>try</span><span>:</span> <span>response</span> <span>=</span> <span>client</span><span>.</span><span>chat</span><span>.</span><span>completions</span><span>.</span><span>create</span><span>(</span> <span>model</span><span>=</span><span>"</span><span>gpt-4o-mini</span><span>"</span><span>,</span> <span># Use a fast, cheap model for extraction
</span> <span>messages</span><span>=</span><span>[{</span><span>"</span><span>role</span><span>"</span><span>:</span> <span>"</span><span>user</span><span>"</span><span>,</span> <span>"</span><span>content</span><span>"</span><span>:</span> <span>EXTRACTION_PROMPT</span><span>.</span><span>format</span><span>(</span><span>content</span><span>=</span><span>content</span><span>)}],</span> <span>temperature</span><span>=</span><span>0.0</span><span>,</span> <span>response_format</span><span>=</span><span>{</span><span>"</span><span>type</span><span>"</span><span>:</span> <span>"</span><span>json_object</span><span>"</span><span>}</span> <span>)</span> <span>facts</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>response</span><span>.</span><span>choices</span><span>[</span><span>0</span><span>].</span><span>message</span><span>.</span><span>content</span><span>).</span><span>get</span><span>(</span><span>"</span><span>facts</span><span>"</span><span>,</span> <span>[])</span> <span>for</span> <span>fact</span> <span>in</span> <span>facts</span><span>:</span> <span>semantic</span> <span>=</span> <span>MemoryEntry</span><span>(</span> <span>content</span><span>=</span><span>f</span><span>"</span><span>{</span><span>fact</span><span>[</span><span>'</span><span>entity</span><span>'</span><span>]</span><span>}</span><span>: </span><span>{</span><span>fact</span><span>[</span><span>'</span><span>attribute</span><span>'</span><span>]</span><span>}</span><span> = </span><span>{</span><span>fact</span><span>[</span><span>'</span><span>value</span><span>'</span><span>]</span><span>}</span><span>"</span><span>,</span> <span>memory_type</span><span>=</span><span>"</span><span>semantic</span><span>"</span><span>,</span> <span>agent_id</span><span>=</span><span>agent_id</span><span>,</span> <span>session_id</span><span>=</span><span>session_id</span><span>,</span> <span>metadata</span><span>=</span><span>{</span><span>"</span><span>confidence</span><span>"</span><span>:</span> <span>fact</span><span>.</span><span>get</span><span>(</span><span>"</span><span>confidence</span><span>"</span><span>,</span> <span>0.5</span><span>),</span> <span>**</span><span>fact</span><span>},</span> <span>trust_score</span><span>=</span><span>fact</span><span>.</span><span>get</span><span>(</span><span>"</span><span>confidence</span><span>"</span><span>,</span> <span>0.5</span><span>)</span> <span>)</span> <span>memories</span><span>.</span><span>append</span><span>(</span><span>semantic</span><span>)</span> <span>except</span> <span>Exception</span> <span>as</span> <span>e</span><span>:</span> <span># Semantic extraction is best-effort; never block the write path
</span> <span>print</span><span>(</span><span>f</span><span>"</span><span>Extraction failed: </span><span>{</span><span>e</span><span>}</span><span>"</span><span>)</span> <span># 3. Compute embeddings for all memories
</span> <span>texts</span> <span>=</span> <span>[</span><span>m</span><span>.</span><span>content</span> <span>for</span> <span>m</span> <span>in</span> <span>memories</span><span>]</span> <span>embedding_response</span> <span>=</span> <span>client</span><span>.</span><span>embeddings</span><span>.</span><span>create</span><span>(</span> <span>model</span><span>=</span><span>"</span><span>text-embedding-3-small</span><span>"</span><span>,</span> <span>input</span><span>=</span><span>texts</span> <span>)</span> <span>for</span> <span>i</span><span>,</span> <span>m</span> <span>in</span> <span>enumerate</span><span>(</span><span>memories</span><span>):</span> <span>m</span><span>.</span><span>embedding</span> <span>=</span> <span>embedding_response</span><span>.</span><span>data</span><span>[</span><span>i</span><span>].</span><span>embedding</span> <span># 4. Persist to store
</span> <span>store</span><span>.</span><span>upsert</span><span>(</span><span>memories</span><span>)</span> <span>return</span> <span>memories</span>
</code></pre>
<div>
<p> Enter fullscreen mode Exit fullscreen mode </p>
</div>
</div> <p>Note the deliberate choice to use a small, fast model (<code>gpt-4o-mini</code>) for extraction rather than the full reasoning model. The extraction step runs on every write, so latency and cost compound quickly. This gets tricky because you are balancing extraction quality against write throughput — in production, you may want to run a higher-quality extraction asynchronously and update the semantic store later.</p> <p><strong>3. Implement Hybrid Retrieval with Reciprocal Rank Fusion</strong></p> <p>This is the core of the read path. We combine vector similarity search with BM25 keyword search using Reciprocal Rank Fusion (RRF), a simple but effective rank aggregation method.<br>
</p> <div>
<pre><code><span>import</span> <span>math</span>
<span>from</span> <span>collections</span> <span>import</span> <span>defaultdict</span>
<span>from</span> <span>rank_bm25</span> <span>import</span> <span>BM25Okapi</span> <span># pip install rank-bm25
</span><span>import</span> <span>numpy</span> <span>as</span> <span>np</span> <span>def</span> <span>cosine_similarity</span><span>(</span><span>a</span><span>:</span> <span>list</span><span>[</span><span>float</span><span>],</span> <span>b</span><span>:</span> <span>list</span><span>[</span><span>float</span><span>])</span> <span>-&gt;</span> <span>float</span><span>:</span> <span>a</span><span>,</span> <span>b</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>a</span><span>),</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>b</span><span>)</span> <span>return</span> <span>float</span><span>(</span><span>np</span><span>.</span><span>dot</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>/</span> <span>(</span><span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>a</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>b</span><span>)</span> <span>+</span> <span>1e-10</span><span>))</span> <span>def</span> <span>hybrid_retrieve</span><span>(</span> <span>query</span><span>:</span> <span>str</span><span>,</span> <span>memories</span><span>:</span> <span>list</span><span>[</span><span>MemoryEntry</span><span>],</span> <span>query_embedding</span><span>:</span> <span>list</span><span>[</span><span>float</span><span>],</span> <span>top_k</span><span>:</span> <span>int</span> <span>=</span> <span>10</span><span>,</span> <span>rrf_k</span><span>:</span> <span>int</span> <span>=</span> <span>60</span> <span># RRF constant, controls how much lower ranks are penalized
</span><span>)</span> <span>-&gt;</span> <span>list</span><span>[</span><span>MemoryEntry</span><span>]:</span> <span>"""</span><span> Hybrid retrieval: combine vector search + BM25 using Reciprocal Rank Fusion. RRF score for a document d = sum over rankers r of: 1 / (k + rank_r(d)) </span><span>"""</span> <span># --- Vector search ranking ---
</span> <span>vec_scores</span> <span>=</span> <span>[]</span> <span>for</span> <span>m</span> <span>in</span> <span>memories</span><span>:</span> <span>if</span> <span>m</span><span>.</span><span>embedding</span> <span>is</span> <span>not</span> <span>None</span><span>:</span> <span>sim</span> <span>=</span> <span>cosine_similarity</span><span>(</span><span>query_embedding</span><span>,</span> <span>m</span><span>.</span><span>embedding</span><span>)</span> <span>vec_scores</span><span>.</span><span>append</span><span>((</span><span>m</span><span>.</span><span>memory_id</span><span>,</span> <span>sim</span><span>))</span> <span>vec_scores</span><span>.</span><span>sort</span><span>(</span><span>key</span><span>=</span><span>lambda</span> <span>x</span><span>:</span> <span>x</span><span>[</span><span>1</span><span>],</span> <span>reverse</span><span>=</span><span>True</span><span>)</span> <span>vec_ranks</span> <span>=</span> <span>{</span><span>mid</span><span>:</span> <span>rank</span> <span>+</span> <span>1</span> <span>for</span> <span>rank</span><span>,</span> <span>(</span><span>mid</span><span>,</span> <span>_</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>vec_scores</span><span>)}</span> <span># --- BM25 keyword ranking ---
</span> <span>tokenized_corpus</span> <span>=</span> <span>[</span><span>m</span><span>.</span><span>content</span><span>.</span><span>lower</span><span>().</span><span>split</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>memories</span><span>]</span> <span>bm25</span> <span>=</span> <span>BM25Okapi</span><span>(</span><span>tokenized_corpus</span><span>)</span> <span>bm25_scores</span> <span>=</span> <span>bm25</span><span>.</span><span>get_scores</span><span>(</span><span>query</span><span>.</span><span>lower</span><span>().</span><span>split</span><span>())</span> <span>bm25_ranked</span> <span>=</span> <span>sorted</span><span>(</span> <span>[(</span><span>memories</span><span>[</span><span>i</span><span>].</span><span>memory_id</span><span>,</span> <span>score</span><span>)</span> <span>for</span> <span>i</span><span>,</span> <span>score</span> <span>in</span> <span>enumerate</span><span>(</span><span>bm25_scores</span><span>)],</span> <span>key</span><span>=</span><span>lambda</span> <span>x</span><span>:</span> <span>x</span><span>[</span><span>1</span><span>],</span> <span>reverse</span><span>=</span><span>True</span> <span>)</span> <span>bm25_ranks</span> <span>=</span> <span>{</span><span>mid</span><span>:</span> <span>rank</span> <span>+</span> <span>1</span> <span>for</span> <span>rank</span><span>,</span> <span>(</span><span>mid</span><span>,</span> <span>_</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>bm25_ranked</span><span>)}</span> <span># --- Reciprocal Rank Fusion ---
</span> <span>rrf_scores</span> <span>=</span> <span>defaultdict</span><span>(</span><span>float</span><span>)</span> <span>for</span> <span>mid</span> <span>in</span> <span>vec_ranks</span><span>:</span> <span>rrf_scores</span><span>[</span><span>mid</span><span>]</span> <span>+=</span> <span>1.0</span> <span>/</span> <span>(</span><span>rrf_k</span> <span>+</span> <span>vec_ranks</span><span>[</span><span>mid</span><span>])</span> <span>for</span> <span>mid</span> <span>in</span> <span>bm25_ranks</span><span>:</span> <span>rrf_scores</span><span>[</span><span>mid</span><span>]</span> <span>+=</span> <span>1.0</span> <span>/</span> <span>(</span><span>rrf_k</span> <span>+</span> <span>bm25_ranks</span><span>[</span><span>mid</span><span>])</span> <span># Sort by fused score
</span> <span>sorted_ids</span> <span>=</span> <span>sorted</span><span>(</span><span>rrf_scores</span><span>.</span><span>keys</span><span>(),</span> <span>key</span><span>=</span><span>lambda</span> <span>mid</span><span>:</span> <span>rrf_scores</span><span>[</span><span>mid</span><span>],</span> <span>reverse</span><span>=</span><span>True</span><span>)</span> <span># Map back to MemoryEntry objects
</span> <span>id_to_memory</span> <span>=</span> <span>{</span><span>m</span><span>.</span><span>memory_id</span><span>:</span> <span>m</span> <span>for</span> <span>m</span> <span>in</span> <span>memories</span><span>}</span> <span>return</span> <span>[</span><span>id_to_memory</span><span>[</span><span>mid</span><span>]</span> <span>for</span> <span>mid</span> <span>in</span> <span>sorted_ids</span><span>[:</span><span>top_k</span><span>]]</span>
</code></pre>
<div>
<p> Enter fullscreen mode Exit fullscreen mode </p>
</div>
</div> <p>The <code>rrf_k</code> parameter (typically 60) controls how aggressively lower-ranked results are penalized. A smaller <code>rrf_k</code> amplifies the difference between ranks; a larger value smooths it out. The RRF formula — <code>1 / (k + rank)</code> — is elegant because it requires no score normalization between the two rankers. BM25 scores and cosine similarities live on completely different scales, but RRF only uses ordinal ranks.</p> <p><strong>4. Inject Memories into the Agent Context Window</strong></p> <p>The final step is formatting retrieved memories for the LLM. This is where you must be ruthless about token budgets:<br>
</p> <div>
<pre><code><span>def</span> <span>build_memory_context</span><span>(</span> <span>memories</span><span>:</span> <span>list</span><span>[</span><span>MemoryEntry</span><span>],</span> <span>max_tokens</span><span>:</span> <span>int</span> <span>=</span> <span>2000</span> <span># Reserve this much of the context window for memory
</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span> <span>"""</span><span>Format retrieved memories for context injection, respecting token limits.</span><span>"""</span> <span>sections</span> <span>=</span> <span>{</span><span>"</span><span>semantic</span><span>"</span><span>:</span> <span>[],</span> <span>"</span><span>episodic</span><span>"</span><span>:</span> <span>[],</span> <span>"</span><span>procedural</span><span>"</span><span>:</span> <span>[]}</span> <span>for</span> <span>m</span> <span>in</span> <span>memories</span><span>:</span> <span>sections</span><span>[</span><span>m</span><span>.</span><span>memory_type</span><span>].</span><span>append</span><span>(</span><span>m</span><span>)</span> <span>parts</span> <span>=</span> <span>[]</span> <span># Semantic memories first — they are the most compressed and informative
</span> <span>if</span> <span>sections</span><span>[</span><span>"</span><span>semantic</span><span>"</span><span>]:</span> <span>facts</span> <span>=</span> <span>"</span><span>
</span><span>"</span><span>.</span><span>join</span><span>(</span><span>f</span><span>"</span><span>- </span><span>{</span><span>m</span><span>.</span><span>content</span><span>}</span><span> (confidence: </span><span>{</span><span>m</span><span>.</span><span>trust_score</span><span>:</span><span>.</span><span>1</span><span>f</span><span>}</span><span>)</span><span>"</span> <span>for</span> <span>m</span> <span>in</span> <span>sections</span><span>[</span><span>"</span><span>semantic</span><span>"</span><span>])</span> <span>parts</span><span>.</span><span>append</span><span>(</span><span>f</span><span>"</span><span>## Known Facts
</span><span>{</span><span>facts</span><span>}</span><span>"</span><span>)</span> <span># Then episodic — recent events for temporal context
</span> <span>if</span> <span>sections</span><span>[</span><span>"</span><span>episodic</span><span>"</span><span>]:</span> <span>episodes</span> <span>=</span> <span>"</span><span>
</span><span>"</span><span>.</span><span>join</span><span>(</span> <span>f</span><span>"</span><span>- [</span><span>{</span><span>m</span><span>.</span><span>created_at</span><span>.</span><span>strftime</span><span>(</span><span>'</span><span>%Y-%m-%d %H</span><span>:</span><span>%</span><span>M</span><span>'</span><span>)</span><span>}</span><span>] </span><span>{</span><span>m</span><span>.</span><span>content</span><span>[</span><span>:</span><span>200</span><span>]</span><span>}</span><span>"</span> <span>for</span> <span>m</span> <span>in</span> <span>sorted</span><span>(</span><span>sections</span><span>[</span><span>"</span><span>episodic</span><span>"</span><span>],</span> <span>key</span><span>=</span><span>lambda</span> <span>x</span><span>:</span> <span>x</span><span>.</span><span>created_at</span><span>,</span> <span>reverse</span><span>=</span><span>True</span><span>)</span> <span>)</span> <span>parts</span><span>.</span><span>append</span><span>(</span><span>f</span><span>"</span><span>## Recent Events
</span><span>{</span><span>episodes</span><span>}</span><span>"</span><span>)</span> <span># Procedural last — only include if relevant
</span> <span>if</span> <span>sections</span><span>[</span><span>"</span><span>procedural</span><span>"</span><span>]:</span> <span>procedures</span> <span>=</span> <span>"</span><span>
</span><span>"</span><span>.</span><span>join</span><span>(</span><span>f</span><span>"</span><span>- </span><span>{</span><span>m</span><span>.</span><span>content</span><span>}</span><span>"</span> <span>for</span> <span>m</span> <span>in</span> <span>sections</span><span>[</span><span>"</span><span>procedural</span><span>"</span><span>])</span> <span>parts</span><span>.</span><span>append</span><span>(</span><span>f</span><span>"</span><span>## Known Procedures
</span><span>{</span><span>procedures</span><span>}</span><span>"</span><span>)</span> <span>context</span> <span>=</span> <span>"</span><span> </span><span>"</span><span>.</span><span>join</span><span>(</span><span>parts</span><span>)</span> <span># Rough token estimate: 1 token ≈ 4 characters for English text
</span> <span>estimated_tokens</span> <span>=</span> <span>len</span><span>(</span><span>context</span><span>)</span> <span>//</span> <span>4</span> <span>if</span> <span>estimated_tokens</span> <span>&gt;</span> <span>max_tokens</span><span>:</span> <span># Truncate from the bottom (procedural, then episodic, then semantic)
</span> <span>target_chars</span> <span>=</span> <span>max_tokens</span> <span>*</span> <span>4</span> <span>context</span> <span>=</span> <span>context</span><span>[:</span><span>target_chars</span><span>]</span> <span>+</span> <span>"</span><span>
[Memory truncated due to context limits]</span><span>"</span> <span>return</span> <span>context</span>
</code></pre>
<div>
<p> Enter fullscreen mode Exit fullscreen mode </p>
</div>
</div> <p>The ordering matters: semantic facts first (dense, high-value), then episodic events (temporal context), then procedural knowledge. If you need to truncate, you lose the least critical memories last.</p> <h2> <a name="multiagent-interoperability-and-trust-scoring" href="#multiagent-interoperability-and-trust-scoring"> </a> Multi-Agent Interoperability and Trust Scoring
</h2> <p>When multiple agents share a memory store, you face a new challenge: not all memories are equally trustworthy. Agent A might extract a fact incorrectly. Agent B, running on a different model, might interpret the same conversation differently. Without trust signals, your memory store accumulates noise.</p> <p>A practical trust scoring model considers three factors:<br>
</p> <div>
<pre><code><span>def</span> <span>compute_trust_score</span><span>(</span><span>memory</span><span>:</span> <span>MemoryEntry</span><span>,</span> <span>reading_agent_id</span><span>:</span> <span>str</span><span>,</span> <span>store</span><span>)</span> <span>-&gt;</span> <span>float</span><span>:</span> <span>"""</span><span> Compute trust score for a memory based on: 1. Source agent reliability (historical accuracy) 2. Corroboration (do other memories support this?) 3. Recency decay (older uncorroborated facts lose trust) </span><span>"""</span> <span># Factor 1: Source reliability — track per-agent accuracy over time
</span> <span>source_reliability</span> <span>=</span> <span>store</span><span>.</span><span>get_agent_reliability</span><span>(</span><span>memory</span><span>.</span><span>agent_id</span><span>)</span> <span># 0.0 to 1.0
</span> <span># Factor 2: Corroboration — how many other memories support this fact?
</span> <span>similar</span> <span>=</span> <span>store</span><span>.</span><span>find_similar</span><span>(</span><span>memory</span><span>.</span><span>content</span><span>,</span> <span>threshold</span><span>=</span><span>0.85</span><span>,</span> <span>exclude_id</span><span>=</span><span>memory</span><span>.</span><span>memory_id</span><span>)</span> <span>unique_agents</span> <span>=</span> <span>set</span><span>(</span><span>m</span><span>.</span><span>agent_id</span> <span>for</span> <span>m</span> <span>in</span> <span>similar</span> <span>if</span> <span>m</span><span>.</span><span>agent_id</span> <span>!=</span> <span>memory</span><span>.</span><span>agent_id</span><span>)</span> <span>corroboration</span> <span>=</span> <span>min</span><span>(</span><span>len</span><span>(</span><span>unique_agents</span><span>)</span> <span>/</span> <span>3.0</span><span>,</span> <span>1.0</span><span>)</span> <span># Cap at 3 independent sources
</span> <span># Factor 3: Recency — exponential decay with half-life of 30 days
</span> <span>age_days</span> <span>=</span> <span>(</span><span>datetime</span><span>.</span><span>utcnow</span><span>()</span> <span>-</span> <span>memory</span><span>.</span><span>created_at</span><span>).</span><span>days</span> <span>recency</span> <span>=</span> <span>math</span><span>.</span><span>exp</span><span>(</span><span>-</span><span>0.693</span> <span>*</span> <span>age_days</span> <span>/</span> <span>30</span><span>)</span> <span># 0.693 = ln(2)
</span> <span># Weighted combination
</span> <span>score</span> <span>=</span> <span>0.4</span> <span>*</span> <span>source_reliability</span> <span>+</span> <span>0.35</span> <span>*</span> <span>corroboration</span> <span>+</span> <span>0.25</span> <span>*</span> <span>recency</span> <span>return</span> <span>round</span><span>(</span><span>score</span><span>,</span> <span>3</span><span>)</span>
</code></pre>
<div>
<p> Enter fullscreen mode Exit fullscreen mode </p>
</div>
</div> <p>This approach lets agents that consistently produce accurate memories gain influence over the shared store, while poorly calibrated agents see their contributions naturally downweighted.</p> <blockquote>
<p><strong>Interoperability Across Model Providers</strong></p> <p>For memory to work across OpenAI, Anthropic, Google, and open-source models, the memory layer must be model-agnostic. This means storing memories as plain text with metadata — not as model-specific embeddings. Re-embed at read time using whatever model the reading agent prefers, or maintain multiple embedding indexes. The <code>MemoryEntry</code> schema above stores content as text first, embeddings second.</p>
</blockquote> <h2> <a name="realworld-considerations-tradeoffs-and-failure-modes" href="#realworld-considerations-tradeoffs-and-failure-modes"> </a> Real-World Considerations: Tradeoffs and Failure Modes
</h2> <div><table>
<thead>
<tr>
<th>Dimension</th>
<th>Local-First Memory</th>
<th>Cloud-Based Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>Latency</td>
<td>Sub-millisecond reads from local SQLite/vector store</td>
<td>10-100ms network round-trip per query</td>
</tr>
<tr>
<td>Privacy</td>
<td>Data never leaves the device</td>
<td>Requires encryption, access controls, compliance</td>
</tr>
<tr>
<td>Multi-device sync</td>
<td>Requires conflict resolution (CRDTs or similar)</td>
<td>Centralized, no conflicts</td>
</tr>
<tr>
<td>Storage limits</td>
<td>Bounded by local disk</td>
<td>Effectively unbounded</td>
</tr>
<tr>
<td>Multi-agent sharing</td>
<td>Harder — need sync protocol</td>
<td>Natural — shared data plane</td>
</tr>
<tr>
<td>Offline capability</td>
<td>Full functionality</td>
<td>Degraded or none</td>
</tr>
</tbody>
</table></div> <p><strong>When local-first wins:</strong> Privacy-sensitive applications, single-user desktop agents, edge deployments, or any scenario where latency matters more than cross-device availability. You can use SQLite with the <code>sqlite-vss</code> extension for vector search on a single machine — no infrastructure needed.</p> <p><strong>When cloud wins:</strong> Multi-agent systems where agents run on different machines, team collaboration scenarios, or when you need centralized governance and audit logs.</p> <p><strong>Failure modes to watch for:</strong></p> <ul>
<li>
<strong>Memory bloat.</strong> Without a consolidation strategy, episodic memory grows linearly with every interaction. You need a background process that merges old episodes into semantic summaries and prunes raw events. Think of it like log rotation.</li>
<li>
<strong>Embedding drift.</strong> If you update your embedding model, old vectors become incompatible with new ones. Either re-embed your entire store (expensive) or maintain a model version tag on each embedding and re-embed at query time for mismatched versions.</li>
<li>
<strong>Hallucinated extractions.</strong> The LLM-based semantic extraction step will sometimes produce incorrect facts. The trust scoring and corroboration mechanisms help, but you should also expose a way for users to correct or delete memories.</li>
</ul> <blockquote>
<p><strong>Do Not Store Secrets in Agent Memory</strong></p> <p>Agent memory stores are designed for broad retrieval — they are the opposite of access-controlled secret stores. Never allow agents to write API keys, passwords, PII, or other sensitive data into the memory layer without explicit redaction. Add a pre-write filter that detects and strips sensitive patterns before persistence.</p>
</blockquote> <h2> <a name="seeing-this-in-practice" href="#seeing-this-in-practice"> </a> Seeing This in Practice
</h2> <p>The architecture described above — episodic and semantic stores, hybrid retrieval with RRF, trust scoring across agents, local-first storage with knowledge graph relationships — is implemented in <a href="https://github.com/superlocalmemory" target="_blank">SuperLocalMemory</a>. You can inspect how it handles multi-agent trust scoring and shared memory across different AI tools (OpenAI, Claude, Gemini) using a local-first architecture that combines vector search with BM25 hybrid retrieval and a knowledge graph layer.</p> <p>A minimal example of querying the memory layer:<br>
</p> <div>
<pre><code><span># Clone and explore the reference implementation</span>
git clone https://github.com/superlocalmemory/superlocalmemory.git
<span>cd </span>superlocalmemory <span># The memory store exposes a simple API for writes and hybrid reads</span>
python <span>-c</span> <span>"
from slm import MemoryStore store = MemoryStore(path='./my_agent_memory.db') # Write a memory from agent interaction
store.write( content='User prefers concise answers with code examples', agent_id='assistant-v1', session_id='session-42', memory_type='semantic'
) # Hybrid retrieval for a new query
results = store.retrieve( query='What format does the user like for responses?', top_k=5, strategy='hybrid' # combines vector + BM25
) for r in results: print(f'[{r.memory_type}] {r.content} (trust: {r.trust_score})')
"</span>
</code></pre>
<div>
<p> Enter fullscreen mode Exit fullscreen mode </p>
</div>
</div> <p>The codebase is a useful reference for seeing how the write pipeline, extraction, embedding, and hybrid retrieval fit together in a working system.</p> <h2> <a name="further-reading-and-sources" href="#further-reading-and-sources"> </a> Further Reading and Sources
</h2> <ul>
<li>
<a href="https://arxiv.org/abs/2512.09458v1" target="_blank">Architectures for Building Agentic AI</a> by Nowaczyk (2025). Argues that reliability in agentic systems is an architectural property. The paper's component breakdown — goal manager, planner, memory, verifiers — aligns with the architecture in this post.</li>
<li>
<a href="https://arxiv.org/abs/2510.09567v1" target="_blank">Safe, Untrusted, "Proof-Carrying" AI Agents</a> by Tagliabue and Greco (2025). Discusses trust and governance in agentic data workflows — relevant to the trust scoring section.</li>
<li>
<a href="https://arxiv.org/abs/2501.02842v1" target="_blank">Foundations of GenIR</a> by Ai, Zhan, and Liu (2025). Covers how generative AI models reshape information retrieval — the theoretical underpinning for why hybrid retrieval matters.</li>
<li>
<a href="https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf" target="_blank">Reciprocal Rank Fusion</a> by Cormack, Clarke, and Butt (2009). The original RRF paper — short, practical, and still the most commonly used rank fusion method.</li>
<li>
<a href="https://en.wikipedia.org/wiki/Okapi_BM25" target="_blank">BM25 — The Okapi weighting scheme</a>. Wikipedia provides a solid primer on the BM25 scoring function if you want to understand the math behind the keyword retrieval component.</li>
<li>
<a href="https://github.com/asg017/sqlite-vss" target="_blank">sqlite-vss</a>. SQLite extension for vector similarity search — useful for local-first memory architectures without requiring a separate vector database.</li>
</ul> <blockquote>
<p><strong>Key Takeaways</strong></p> <ul>
<li>Agent memory is not caching. It requires unstructured storage, semantic retrieval, and dynamic writes — a fundamentally different architecture from traditional state management.</li>
<li>Decompose memory into <strong>episodic</strong> (events), <strong>semantic</strong> (facts), and <strong>procedural</strong> (workflows). Each store has different write patterns, retention policies, and retrieval characteristics.</li>
<li>Use <strong>hybrid retrieval</strong> (vector search + BM25 with Reciprocal Rank Fusion) to avoid the blind spots of either approach alone. RRF is simple to implement and does not require score normalization.</li>
<li>For multi-agent systems, implement <strong>trust scoring</strong> based on source reliability, corroboration, and recency. Without it, shared memory stores accumulate noise.</li>
<li>Choose <strong>local-first</strong> for privacy and latency; choose <strong>cloud-based</strong> for multi-agent coordination and unlimited storage. Many production systems will use both with a sync layer.</li>
<li>Always budget for <strong>memory consolidation</strong> (merging episodes into semantic summaries) and <strong>embedding migration</strong> (handling model version changes). These operational concerns are easy to overlook but critical at scale.</li>
</ul>
</blockquote> </div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>