<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>How to Build a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac for Healthcare</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>How to Build a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac for Healthcare</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 10/28/2025 8:42:35 PM | <a href="https://huggingface.co/blog/nvidia/nvidia-isaac-for-healthcare" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div> <p><span><span><a href="https://huggingface.co/asawareeb"><img alt="Asawaree's avatar" src="https://huggingface.co/avatars/c1eb1024a4c6bd1296884ff17b0e0cbf.svg"></a> </span> </span></p> </div> <h2> <div><nav><ul><li><a href="#a-hands-on-guide-to-collecting-data-training-policies-and-deploying-autonomous-medical-robotics-workflows-on-real-hardware">A hands-on guide to collecting data, training policies, and deploying autonomous medical robotics workflows on real hardware</a> <ul></ul> </li><li><a href="#so-arm-starter-workflow-building-an-embodied-surgical-assistant">SO-ARM Starter Workflow; Building an Embodied Surgical Assistant</a> <ul><li><a href="#technical-implementation">Technical Implementation</a> <ul></ul> </li><li><a href="#sim-to-real-mixed-training-approach">Sim-to-Real Mixed Training Approach</a> <ul></ul> </li><li><a href="#hardware-requirements">Hardware Requirements</a> <ul></ul> </li><li><a href="#data-collection-implementation">Data Collection Implementation</a> <ul></ul> </li><li><a href="#simulation-teleoperation-controls">Simulation Teleoperation Controls</a> <ul></ul> </li><li><a href="#model-training-pipeline">Model Training Pipeline</a> <ul></ul> </li></ul> </li><li><a href="#end-to-end-sim-collecttraineval-pipelines">End-to-End Sim Collect–Train–Eval Pipelines</a> <ul><li><a href="#generate-synthetic-data-in-simulation">Generate Synthetic Data in Simulation</a> <ul></ul> </li><li><a href="#train-and-evaluate-policies">Train and Evaluate Policies</a> <ul></ul> </li><li><a href="#convert-models-to-tensorrt">Convert Models to TensorRT</a> <ul></ul> </li></ul> </li><li><a href="#getting-started">Getting Started</a> <ul></ul> </li><li><a href="#resources">Resources</a> <ul></ul> </li></ul></nav></div> <a href="#a-hands-on-guide-to-collecting-data-training-policies-and-deploying-autonomous-medical-robotics-workflows-on-real-hardware"> <span></span> </a> <span> A hands-on guide to collecting data, training policies, and deploying autonomous medical robotics workflows on real hardware </span>
</h2>
<p>Simulation has been a cornerstone in medical imaging to address the data gap. However, in healthcare robotics until now, it's often been too slow, siloed, or difficult to translate into real-world systems. That’s now changing. With new advances in GPU-accelerated simulation and digital twins, developers can design, test, and validate robotic workflows entirely in virtual environments - reducing prototyping time from months to days, improving model accuracy, and enabling safer, faster innovation before a single device reaches the operating room.</p>
<p>That's why NVIDIA introduced Isaac for Healthcare earlier this year, a developer framework for AI healthcare robotics, that enables developers in solving these challenges via integrated data collection, training, and evaluation pipelines that work across both simulation and hardware. Specifically, the Isaac for Healthcare v0.4 release provides users with an end-to-end <a href="https://github.com/isaac-for-healthcare/i4h-workflows/blob/main/workflows/so_arm_starter/README.md">SO-ARM based starter workflow</a> and <a href="https://github.com/isaac-for-healthcare/i4h-workflows/blob/main/tutorials/assets/bring_your_own_or/README.md">the bring your own operating room tutorial</a>. The SO-ARM starter workflow lowers the barrier for MedTech developers to experience the full workflow from simulation to training to deployment and start building and validating autonomously on real hardware right away.</p>
<p>In this post, we'll walk through the starter workflow and its technical implementation details to help you build a surgical assistant robot in less time than ever imaginable before.</p>
<h2> <a href="#so-arm-starter-workflow-building-an-embodied-surgical-assistant"> <span></span> </a> <span> SO-ARM Starter Workflow; Building an Embodied Surgical Assistant </span>
</h2>
<p>The SO-ARM starter workflow introduces a new way to explore surgical assistance tasks, and provides developers with a complete end-to-end pipeline for autonomous surgical assistance:</p>
<ul>
<li>Collect real-world and synthetic data with <a href="https://huggingface.co/docs/lerobot/en/so101">SO-ARM</a> using LeRobot </li>
<li>Post-train GR00T N1.5, evaluate in <a href="https://isaac-sim.github.io/IsaacLab/main/index.html">Isaac Lab</a>, then deploy to hardware</li>
</ul>
<p>This workflow gives developers a safe, repeatable environment to train and refine assistive skills before moving into the Operating Room. </p>
<h3> <a href="#technical-implementation"> <span></span> </a> <span> Technical Implementation </span>
</h3>
<p>The workflow implements a three-stage pipeline that integrates simulation and real hardware: </p>
<ol>
<li><strong>Data Collection</strong>: Mixed simulation and real-world teleoperation demonstrations using SO-101 and LeRobot </li>
<li><strong>Model Training</strong>: Post-training GR00T N1.5 on combined datasets with dual-camera vision </li>
<li><strong>Policy Deployment</strong>: Real-time inference on physical hardware with RTI DDS communication</li>
</ol>
<p>Notably, over 93% of the data used for policy training was generated synthetically in simulation, underscoring the strength of simulation in bridging the robotic data gap. </p>
<h3> <a href="#sim-to-real-mixed-training-approach"> <span></span> </a> <span> Sim-to-Real Mixed Training Approach </span>
</h3>
<p>The workflow combines simulation and real-world data to address the fundamental challenge that training robots in the real world is expensive and limited, while pure simulation often fails to capture real-world complexities. The approach uses approximately 70 simulation episodes for diverse scenarios and environmental variations, combined with 10-20 real-world episodes for authenticity and grounding. This mixed training creates policies that generalize beyond either domain alone.</p>
<h3> <a href="#hardware-requirements"> <span></span> </a> <span> Hardware Requirements </span>
</h3>
<p>The workflow requires:</p>
<ul>
<li><strong>GPU</strong>: RT Core-enabled architecture (Ampere or later) with ≥30GB VRAM for GR00T N1.5 inference </li>
<li><strong>SO-ARM101 Follower</strong>: 6-DOF precision manipulator with dual-camera vision (wrist and room). The SO-ARM101 features WOWROBO vision components, including a wrist-mounted camera with a 3D-printed adapter. </li>
<li><strong>SO-ARM101 Leader</strong>: 6-DOF Teleoperation interface for expert demonstration collection</li>
</ul>
<p>Notably, developers could run all the simulation, training and deployment (3 computers needed for physical AI) on one <a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/">DGX Spark</a>.</p>
<h3> <a href="#data-collection-implementation"> <span></span> </a> <span> Data Collection Implementation </span>
</h3> <p>For real-world data collection with SO-ARM101 hardware or any other version supported in LeRobot:</p>
<pre><code>python /path/to/lerobot-record \ --robot.type=so101_follower \ --robot.port=&lt;follower_port_id&gt; \ --robot.cameras="{wrist: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}, room: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30}}" \ --robot.id=so101_follower_arm \ --teleop.type=so101_leader \ --teleop.port=&lt;leader_port_id&gt; \ --teleop.id=so101_leader_arm \ --dataset.repo_id=&lt;user&gt;/surgical_assistance/surgical_assistance \ --dataset.num_episodes=15 \ --dataset.single_task="Prepare and hand surgical instruments to surgeon"
</code></pre> <p>For simulation-based data collection:</p>
<pre><code># With keyboard teleoperation python -m simulation.environments.teleoperation_record \ --enable_cameras \ --record \ --dataset_path=/path/to/save/dataset.hdf5 \ --teleop_device=keyboard # With SO-ARM101 leader arm python -m simulation.environments.teleoperation_record \ --port=&lt;your_leader_arm_port_id&gt; \ --enable_cameras \ --record \ --dataset_path=/path/to/save/dataset.hdf5
</code></pre>
<h3> <a href="#simulation-teleoperation-controls"> <span></span> </a> <span> Simulation Teleoperation Controls </span>
</h3>
<p>For users without physical SO-ARM101 hardware, the workflow provides keyboard-based teleoperation with the following joint controls:</p>
<ul>
<li>Joint 1 (shoulder_pan): Q (+) / U (-) </li>
<li>Joint 2 (shoulder_lift): W (+) / I (-) </li>
<li>Joint 3 (elbow_flex): E (+) / O (-) </li>
<li>Joint 4 (wrist_flex): A (+) / J (-) </li>
<li>Joint 5 (wrist_roll): S (+) / K (-) </li>
<li>Joint 6 (gripper): D (+) / L (-) </li>
<li>R Key: Reset recording environment </li>
<li>N Key: Mark episode as successful</li>
</ul>
<h3> <a href="#model-training-pipeline"> <span></span> </a> <span> Model Training Pipeline </span>
</h3>
<p>After collecting both simulation and real-world data, convert and combine datasets for training:</p>
<pre><code># Convert simulation data to LeRobot format python -m training.hdf5_to_lerobot \ --repo_id=surgical_assistance_dataset \ --hdf5_path=/path/to/your/sim_dataset.hdf5 \ --task_description="Autonomous surgical instrument handling and preparation" # Post-train GR00T N1.5 on mixed dataset python -m training.gr00t_n1_5.train \ --dataset_path /path/to/your/surgical_assistance_dataset \ --output_dir /path/to/surgical_checkpoints \ --data_config so100_dualcam </code></pre>
<p>The trained model processes natural language instructions such as "Prepare the scalpel for the surgeon" or "Hand me the forceps" and executes the corresponding robotic actions. With the latest LeRobot release (v0.4.0) you will be able to post-train GR00T N1.5 natively in LeRobot! </p>
<h2> <a href="#end-to-end-sim-collecttraineval-pipelines"> <span></span> </a> <span> End-to-End Sim Collect–Train–Eval Pipelines </span>
</h2>
<p>Simulation is most powerful when it's part of a loop: collect data → train → evaluate → deploy. Isaac Lab supports this full pipeline: </p>
<h3> <a href="#generate-synthetic-data-in-simulation"> <span></span> </a> <span> Generate Synthetic Data in Simulation </span>
</h3>
<ul>
<li>Teleoperate robots using keyboard or hardware controllers </li>
<li>Capture multi-camera observations, robot states, and actions </li>
<li>Create diverse datasets with edge cases impossible to collect safely in real environments</li>
</ul>
<h3> <a href="#train-and-evaluate-policies"> <span></span> </a> <span> Train and Evaluate Policies </span>
</h3>
<ul>
<li>Deep integration with Isaac Lab's RL framework for PPO training </li>
<li>Parallel environments (thousands of simulations simultaneously) </li>
<li>Built-in trajectory analysis and success metrics </li>
<li>Statistical validation across varied scenarios</li>
</ul>
<h3> <a href="#convert-models-to-tensorrt"> <span></span> </a> <span> Convert Models to TensorRT </span>
</h3>
<ul>
<li>Automatic optimization for production deployment </li>
<li>Support for dynamic shapes and multi-camera inference </li>
<li>Benchmarking tools to verify real-time performance</li>
</ul>
<p>This reduces time from experiment to deployment and makes sim-to-real a practical part of daily development. </p>
<h2> <a href="#getting-started"> <span></span> </a> <span> Getting Started </span>
</h2>
<p>Isaac for Healthcare SO-ARM Starter Workflow is available now. To get started: </p>
<ul>
<li><strong>Clone the repository</strong>: <code>git clone https://github.com/isaac-for-healthcare/i4h-workflows.git</code> </li>
<li><strong>Choose a workflow</strong>: Start with the SO-ARM Starter Workflow for surgical assistance or explore other workflows </li>
<li><strong>Run the setup</strong>: Each workflow includes an automated setup script (for example, <code>tools/env_setup_so_arm_starter.sh</code>)</li>
</ul>
<h2> <a href="#resources"> <span></span> </a> <span> Resources </span>
</h2>
<ul>
<li><a href="https://github.com/isaac-for-healthcare/i4h-workflows">GitHub Repository</a>: Complete workflow implementations </li>
<li><a href="https://isaac-for-healthcare.github.io/i4h-docs/">Documentation</a>: Setup and usage guides </li>
<li><a href="https://huggingface.co/nvidia/GR00T-N1.5-3B">GR00T Models</a>: Pre-trained foundation models </li>
<li><a href="https://huggingface.co/docs/lerobot/so101">Hardware Guides</a>: SO-ARM101 setup instructions </li>
<li><a href="https://github.com/huggingface/lerobot">LeRobot Repository</a>: End-to-end robotics learning</li>
</ul>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>