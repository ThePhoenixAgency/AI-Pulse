<!DOCTYPE html>
<html lang="pt">
<head>
<meta charset="UTF-8">
<title>Instant LLM Updates with Doc-to-LoRA and Text-to-LoRA</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>Instant LLM Updates with Doc-to-LoRA and Text-to-LoRA</h1>
  <div class="metadata">
    Source: Hacker News (nouveautés) | Date: 2/27/2026 8:00:42 PM | <a href="https://pub.sakana.ai/doc-to-lora/" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: PT
  </div>
  <div class="content">
    <div><p> </p><h3>This page requires JavaScript. Please enable it to view the website.</h3> <p></p><div> <p> </p><figcaption> TL;DR </figcaption> <figcaption> Long-term memory and continual adaptation of Large Language Models (LLMs) are two key challenges of current agentic systems. Here, we propose the usage of auxiliary modulator networks (so-called <i>"hypernetworks"</i>) that modify LLM weights on the fly to compress document information and master new skills. <strong>Doc-to-LoRA</strong> enables knowledge updates by turning documents into LoRA adapters, allowing a model to internalize new factual content without retraining. <strong>Text-to-LoRA</strong> creates LoRA adapters for task-specific fine-tuning, using only a short task description. <br> </figcaption> <p></p> <figure> <figcaption><b><span>Doc-to-LoRA Chat Interface Video Demonstration.</span></b> The user provides a context document to internalize (left panel). The base model then answers queries (right panel) using the generated internalized adapter, without being shown the raw context at query time. This makes the user experience feel like a persistent memory toggle. We can internalize once, then ask multiple questions in a standard chat box.</figcaption>
</figure>
<hr> <p>Recent LLM agents have shown impressive capabilities on complex computer use and long-horizon tasks. Yet, they still struggle with <em><strong>long-term memory</strong></em> and <em><strong>adaptation</strong></em>--two of the most important cognitive capabilities that still limit LLMs today.
Without long-term memory, users have to provide LLMs with relevant content at the start of every new session, creating friction, discontinuity, and longer time-to-response.
Additionally, due to the lack of adaptation, they do not learn from mistakes or user preferences from previous sessions, making each interaction as cumbersome as the first.
Traditionally, these two problems are tackled by "updating" the model.</p>
<br>
<figure> <div> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/doc-to-lora-static-top.png"></p><figcaption><b><span>Traditional LLM knowledge update.</span></b> Users can provide new knowledge to LLMs by placing it in the context window, albeit with slower generation and higher memory (VRAM) requirements. Context distillation lets LLMs access new factual knowledge from their parameters. However, the knowledge-update process can be very expensive and slow.</figcaption> </div>
</figure>
<p><strong>1. LLM knowledge update (memory).</strong> When a user provides a long document, e.g., a policy, a report, or a private PDF, the standard solution is to put it in the context window. This works, but it means every new query re-reads the same document, paying the full latency and VRAM cost each time. Practical workarounds like KV-cache pre-filling help, but they do not eliminate the per-query overhead, and they break down entirely once the document exceeds the model's context window.
An alternative is <em>context distillation</em> , which encodes new information directly into the model's parameters, allowing it to access that knowledge without re-reading the source. However, this update process tends to be slow and computationally expensive.</p>
<br>
<figure> <div> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/text-to-lora-static-top.png"></p><figcaption><b><span>Traditional LLM fine-tuning.</span></b> LLM fine-tuning enables practitioners to adapt LLMs for many downstream tasks but typically requires expensive data collection steps, careful curation of datasets, and tedious engineering.</figcaption> </div>
</figure>
<p><strong>2. LLM fine-tuning (adaptation).</strong> When practitioners want a model to consistently follow a new format, handle a specialized task, or adopt a particular style, the standard solution is fine-tuning. This also works, but it requires collecting or generating data, curating it carefully, and running an expensive training pipeline. Furthermore, iterating on the traditional fine-tuning pipeline requires repetitive data collection and running fine-tuning jobs, slowing down experimentation speed.</p> <figure> <div> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/doc-to-lora-static-bottom.png"> </p> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/text-to-lora-static-bottom.png"> </p> <figcaption><b><span>Instant Update Interfaces for LLMs.</span></b> Two complementary “instant update” interfaces for LLMs. <b>Doc-to-LoRA</b> turns a document into a LoRA adapter that <i>internalizes</i> the document’s information, so later questions can be answered without repeatedly including the entire document in the context window. <b>Text-to-LoRA</b> turns a natural-language task description into a LoRA adapter in a single forward pass--an alternative to running supervised fine-tuning every time we need a new skill. </figcaption> </div>
</figure>
<p>Both kinds of model updates, fine-tuning and context distillation, share a common bottleneck. Specifically, we want to move information inside the model, but the path to get it there is slow and expensive.
In this post, we introduce two complementary research papers that take a different approach to model updates. Instead of naively updating the model during deployment, our methods pay the update costs up front by learning an <em>"update generator"</em> that can be reused cheaply at deployment time.
The key procedure is to train a <em>hypernetwork</em>--a network whose output is the parameters of another network--to instantly and cheaply generate compact LoRA adapters.
Once trained, this hypernetwork acts as our update generator that produces task-specific updates for the target LLM on the fly.</p>
<p>In our papers, this concept is implemented as a two-phase <em>update cost amortization</em> workflow, with a clean separation of costs. First, at <em>meta-training</em> time (expensive, done once), we train a hypernetwork that learns how to generate effective LoRA updates from a given input. Then, at deployment time (cheap, done often), that input (a document or a task description) is fed to the hypernetwork, which returns a LoRA adapter in a single sub-second forward pass. Therefore, we can avoid the expensive per-task optimization pipeline entirely.
While <em><strong>Doc-to-LoRA</strong></em> and <em><strong>Text-to-LoRA</strong></em> share this update cost amortization framework, they differ in what they learn and how they are used:</p>
<ul>
<li><em><strong>Doc-to-LoRA</strong></em> addresses the long-term memory problem. Given a document, it generates a LoRA adapter that internalizes the document's content into the base model's weights. Subsequent questions can then be answered without the document ever appearing in the context window--cutting both latency and VRAM, and adding long-term memory well beyond the model's native context limit.</li>
<li><em><strong>Text-to-LoRA</strong></em> addresses the adaptation problem. Given a natural-language task description, it generates a LoRA adapter that steers the frozen base model toward the described behavior. As a result, writing a description replaces running a fine-tuning pipeline, allowing adaptation to happen on the fly.</li>
</ul>
<figure>
<figcaption><b><span>Doc-to-LoRA vs. Text-to-LoRA at a glance.</span></b> The two systems are based on the update amortization concept but target different kinds of updates.</figcaption>
<table> <thead> <tr> <th></th> <th>Doc-to-LoRA</th> <th>Text-to-LoRA</th> </tr> </thead> <tbody> <tr> <td>Problem solved</td> <td>Expensive context distillation for knowledge updates</td> <td>Expensive fine-tuning pipeline for model adaptation</td> </tr> <tr> <td>What the hypernetwork learns</td> <td>Instantly adding new factual knowledge to an LLM</td> <td>Instantly adapting an LLM for downstream tasks</td> </tr> <tr> <td>Input to hypernetwork</td> <td>Document (text or visual tokens via a VLM)</td> <td>Natural-language task description</td> </tr> <tr> <td>What LoRA stores</td> <td>Factual knowledge from the document</td> <td>Task-specific behavior or skill</td> </tr> </tbody>
</table>
</figure> <p><strong><a href="https://openreview.net/forum?id=nZeVKeeFYf9">LoRA (Low-Rank Adaptation)</a></strong> is a parameter-efficient way to modify a large model without updating all of its weights, by learning a <em>low-rank</em> update instead.
In practice, that means the model update is stored in a compact adapter that is much smaller than the model itself, fast to apply at inference time without changing the base weights, and modular enough that we can store many adapters (skills, customers, domains), swap them, and sometimes even compose them.</p>
<p><strong><a href="https://openreview.net/forum?id=rkpACe1lx">Hypernetworks</a></strong> are neural networks that output parameters for another neural network. Here, the hypernetwork outputs LoRA parameters. Once trained, generating an “update” becomes a single forward pass, replacing the traditional optimization loop, and the base model can remain frozen.
Conceptually, the hypernetwork is learning an “update rule”--a function that maps from an input (description or document) to a structured weight change (i.e., a LoRA adapter) that specializes the model for a specific task or internalizes a specific document.</p> <hr> <figure> </figure> <p>The lack of long-term memory in LLMs has real-world implications. For instance, every time a user asks about a document, the model re-reads it in full, paying the full latency and VRAM overhead. Alternatively, instead of keeping the document in the context window, we can distill it directly into the base model's weights as a LoRA adapter, acting as a long-term memory.
The standard route for this is context distillation, but as we noted above, it requires per-document optimization with large memory requirements and takes a substantial amount of wall-clock time, which is not suitable for low-latency applications. Doc-to-LoRA asks whether a hypernetwork can meta-learn to perform that distillation step cheaply by mapping a document to a LoRA adapter in a single forward pass, with no per-document gradient updates. Unlike task adapters that usually encode task-specific behavior, LoRAs generated by Doc-to-LoRA act as factual storage. Once a document is internalized, the LLM can answer any number of questions without the document ever appearing in the context window again.</p> <h4>Training Overview</h4>
<figure> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/doc-to-lora-training-overview.png"> </p> <figcaption><b><span>Doc-to-LoRA Training Overview.</span></b> A frozen LLM encodes context activations and a shared hypernetwork predicts LoRA updates so the adapted model can answer queries without re-reading the original document.</figcaption>
</figure>
<p>Doc-to-LoRA uses the teacher-student objective from context distillation, but amortizes the per-document training cost through meta-training. Instead of per-document optimization at deployment time, the hypernetwork learns to <i>predict</i> document-specific LoRA updates instantly.
The training is conceptually very simple. We encode the document through a frozen LLM to get per-layer token activations. A <i>Perceiver</i>-based hypernetwork maps those activations to rank-8 LoRA matrices, trained to minimize the gap between teacher (full document context) and student (LoRA-adapted, no context) responses.</p>
<p>
Implementation-wise, we use 8 cross-attention blocks, ~309M parameters, targeting MLP layers in <code>Gemma-2-2b-it</code>. At inference, it can be run in <i>batched</i> mode (all layers at once, faster) or <i>iterative</i> mode (one layer at a time, lower memory). Both finish in under a second.
One key design choice involves using a chunking mechanism to handle long documents. A single fixed-rank adapter can become a bottleneck for very long contexts, so we partition documents into contiguous chunks and process each independently with the same hypernetwork. Each chunk produces a rank-<i>r</i> LoRA, which we compose by concatenating along the rank dimension, yielding an effective rank of <i>r × K</i>. This design scales with document length without changing the hypernetwork architecture.
This matters especially for documents longer than the model's context window. On a Needle-in-a-Haystack task, Doc-to-LoRA achieves near-perfect accuracy on contexts up to 32K tokens--despite training only on sequences up to 256 tokens. The chunking mechanism generalizes well beyond training length, handling long sequences effectively.
</p>
<h4>Doc-to-LoRA Internalizes Needles Beyond the Base Context Window</h4>
<figure> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/doc-to-lora-niah.png"></p> <figcaption><b><span>NIAH Retrieval Beyond the Context Window.</span></b> Once the haystack exceeds the base model's native context window, direct in-context retrieval collapses, while Doc-to-LoRA maintains strong retrieval with nearly constant additional inference memory.</figcaption>
</figure>
<p>In this experiment, we aim to show that D2L (i) successfully induces knowledge internalization, enabling the base model to recall the implanted information without reading the raw context, (ii) effectively bypasses the inherent context-length limitations of the base language model, and (iii) reduces the computational requirements for inference, especially when the inputs are long. For illustrative purposes, we evaluate D2L on a synthetic needle-in-a-haystack (NIAH) information retrieval task. During D2L’s meta-training, we use input contexts ranging from 32 to 256 tokens in length. The training inputs are randomly chunked from 1 to 8 chunks with a minimum chunk size of 25 tokens.</p>
<p>During evaluation, the baseline has direct access to both the haystack and the query. For D2L, the base LLM does not have direct access to any part of the original context but is simply given the query prompt.
Doc-to-LoRA segments the haystack into 1,024-token chunks and composes them into a single adapter.
Doc-to-LoRA's accuracy stays near-perfect up to ~40K tokens--despite training only up to 8 chunks. The base model's 8K context window fails beyond 8K tokens, while Doc-to-LoRA accesses information that is completely out of reach for direct in-context retrieval. There is also a huge efficiency gain. The base model needs 12+ GB of extra memory for a 128K-token haystack. With Doc-to-LoRA, internalized knowledge uses under 50 MB--constant regardless of document length. For real-world use, this is powerful: users can internalize private documents once, then chat without the memory overhead of keeping everything in context.</p>
<h4>Efficient and Effective Internalization on Reading Comprehension Tasks</h4>
<figure> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/doc-to-lora-short-qa.png"></p> <figcaption><b><span>Short-Context QA Trade-Offs.</span></b> SQuAD results comparing in-context and in-parameter methods. Doc-to-LoRA targets a practical deployment requirement: strong relative quality with sub-second model updates.</figcaption>
</figure>
<p>Next, we evaluate Doc-to-LoRA on real-world reading comprehension benchmarks. In this blog post, we show SQuAD performance relative to the base model with full in-context access and other relevant baselines.
Doc-to-LoRA reaches 83.5% of the full-context upper bound--without any document information in the context window--using less than one second of update time. For comparison, oracle context distillationOracle CD uses the test query directly for distillation, unlike generated-query CD which requires expensive query generation and backprop. needs 40 seconds for the model update. Traditional CD exceeds 100 seconds due to a high-latency query-generation process.
Memory efficiency is equally impressive. Doc-to-LoRA and oracle CD both use ~1 GB, while generated-query CD exceeds 40 GB. This represents a major practical advantage for Doc-to-LoRA, achieving competitive quality at sub-second speed with minimal memory. The gain only grows with longer documents.</p>
<h4>Instant Zero-Shot Internalization of Long-Context Information</h4>
<figure> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/doc-to-lora-long-qa-table.png"></p> <figcaption><b><span>Long-Context QA Trade-Offs.</span></b> Compared with context distillation, Doc-to-LoRA reduces internalization to sub-second latency while keeping competitive performance.</figcaption>
</figure>
<p>Long-context QA tasks pose a real challenge for standard distillation due to memory and compute constraints. We note that test samples can go up to 32K tokens, far beyond Doc-to-LoRA's longest training example (2,344 tokens). Doc-to-LoRA can generalize beyond its training length thanks to the chunking mechanism.
Doc-to-LoRA achieves 85% relative accuracy, again with sub-second update latency. Oracle CD scores higher at 90% but takes 40 seconds and requires more than 7 GB VRAM. The longer the document, the bigger this advantage. For full technical details, see <a href="https://arxiv.org/abs/xxx" target="_blank" rel="noopener noreferrer">our paper</a>.</p> <figure> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/doc-to-lora-visual-encoder-diagram.png"> </p> <figcaption><b><span>Zero-Shot Internalization of Visual Information via Doc-to-LoRA.</span></b> Using a vision-language model (VLM, Gemma-3-4b-it) as the visual context encoder for Doc-to-LoRA allows us to transmit visual information to a text-only model. The VLM processes an image and produces activations, which the hypernetwork maps into LoRA updates for a <b>pure text</b> model (Gemma-2-2b-it). The text model then answers questions about the image using only the internalized update <em>without having ever looked at any images</em>.</figcaption>
</figure>
<div> <div> <h4>Zero-Shot Generalization to Encoding Visual Information as LoRA</h4> <p>The main motivation for Doc-to-LoRA is adding new knowledge to LLMs cheaply. Naturally, factual information usually comes in textual form, e.g., static manuals or textbooks. However, we are not restricted to only textual information. Thus, in this experiment, we test an extreme form of “internalization”: <i>can a text-only model answer questions about an image processed by a VLM without ever giving the text model anything beyond the internalized information?</i> Specifically, we train another hypernetwork instance using a VLM (Gemma-3-4b-it) as the document encoder and never include any images during the training phase of the hypernetwork. Thus, this experiment tests Doc-to-LoRA's <b>zero-shot</b> capability to encode visual information into LoRA.But how can Doc-to-LoRA generalize if the hypernetwork and the target model have never seen any images? We hypothesize that, since modern VLM architectures map visual tokens into the same latent space as textual tokens, reading information from visual tokens is similar to reading information from non-English textual tokens (assuming that English provides a global anchor for other languages in VLMs and LLMs).</p> <p> We find that the answer can be “yes” to a surprising degree. On the 10‑class Imagenette subset of ImageNet, the target text-only model reaches <b>75.03%</b> accuracy purely through information stored in the generated LoRA adapter. This result is remarkable as neither the hypernetwork nor the base model have seen any visual tokens during training. More broadly, this result suggests that Doc-to-LoRA could be used as a general <i>"Context-to-LoRA"</i> mapping. An interesting research idea is to explore how hypernetworks can become a modality bridge that moves information extracted by one model into another model’s parameters, à la <a href="https://sakana.ai/evolutionary-model-merge/" target="_blank" rel="noopener noreferrer"><i>model merging</i></a>.</p> </div> <figure> <figcaption><b><span>(V)QA Benchmark Accuracy.</span></b> Doc-to-LoRA successfully learns to map the VLM’s activations into LoRA matrices of the target LLM. Although using a VLM as the context encoder negatively impacts text-based QA performance, it enables us to directly communicate visual information extracted by the VLM.</figcaption> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/doc-to-lora-visual-transfer.png"></p> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/doc-to-lora-imagenette-confusion-matrix.png"> </p> <figcaption><b><span>Imagenette Confusion Matrix.</span></b> Imagenette confusion matrix for the VLM→LLM internalization setting (overall accuracy: 75.03%).</figcaption> </figure>
</div>
<p>To make things more concrete, we include an interactive demo below that shows transcripts from real Doc-to-LoRA sessions. In each session, a document is internalized once into a generated adapter, and the model then answers questions about it with no raw context in the prompt. Toggle between No Context (the base model) and Internalized (the adapter active) to see the difference. Hover over highlighted spans to trace exactly which parts of the document each answer draws from.</p> <div> <p>Document Source</p> </div> <hr> <figure> </figure>
<p>While Doc-to-LoRA targets the long-term memory problem, Text-to-LoRA targets adaptation. Adapting LLMs via fine-tuning requires collecting or synthesizing data, curating it carefully, and running a training job. Each iteration through that loop is slow, manual, and expensive. Furthermore, the result is a single adapter tightly coupled to one specific dataset.
Text-to-LoRA asks whether a hypernetwork can meta-learn the entire fine-tuning pipeline. Given only a natural-language description of a task, can it generate a useful LoRA adapter in a single forward pass? If so, then per-task adaptation cost disappears, replaced by a one-time upfront meta-training investment in the hypernetwork itself. During deployment, adaptation and task specialization become as simple as writing a description.</p> <h4>Training Overview</h4>
<div> <div> <p>This training setup captures the same paradigm shift presented in the introduction. Instead of optimizing a new adapter for every task at deployment time, we train one generator that can <i>predict</i> adapters on demand. Concretely, a task description is first encoded into a task embedding and the hypernetwork outputs all target LoRA weights in one forward pass. In our paper, we explore two objectives. In <b>reconstruction training</b>, Text-to-LoRA learns to match existing task-specific LoRA adapters. In <b>SFT training</b>, Text-to-LoRA is trained end-to-end through downstream task loss. That is, generated adapters are applied to a frozen base model, and gradients update the hypernetwork directly without requiring intermediate target adapters.</p> <p>In the main setup, the base model is <code>Mistral-7B-Instruct</code>, LoRA targets <code>q_proj</code> and <code>v_proj</code> at rank 8 across all layers (about 3.4M adapter parameters), and training data comes from the SNI-derived Lots-of-LoRAs collection (479 training tasks after filtering). This constrained output space keeps updates compact and modular while still enabling single-forward-pass generation at deployment.</p> </div> <figure> <div> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/text-to-lora-training-overview.png"></p><figcaption><b><span>Text-to-LoRA Training Overview.</span></b><br>The hypernetwork generates LoRA weights (∆W) from a task embedding. Training can be done either via reconstruction loss (distill existing LoRAs) or via supervised fine-tuning loss.</figcaption> </div> </figure>
</div>
<h4>Reconstruction-trained Text-to-LoRA Recovers Target Adapters</h4>
<figure> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/text-to-lora-recon-table.png"></p> <figcaption><b><span>Reconstruction-Trained Text-to-LoRA Results.</span></b> Reconstruction training can recover much of the performance of task-specific LoRAs on benchmark tasks. Many task adapters can be represented implicitly through one hypernetwork.</figcaption>
</figure>
<p>Reconstruction training is essentially <em>adapter-library compression</em>. Instead of storing many task-specific adapters, we store one hypernetwork that can regenerate them from descriptions. It's a sanity check to confirm that Text-to-LoRA can learn to reproduce known weights.
On seen tasks, reconstruction-trained Text-to-LoRA recovers most of the task-specific performance--sometimes even matching the original adapter. The issue is that reproducing known weights isn't the same as learning a robust <em>description-to-LoRA</em> mapping that generalizes to new tasks.</p>
<p>To test generalization, we move to end-to-end SFT training on a different set of tasks. The question becomes whether it can generate useful adapters for tasks it has never seen.</p>
<h4>SFT-trained Text-to-LoRA Zero-Shot Generates Useful Adapters for Unseen Tasks</h4>
<figure> <div> <div> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/text-to-lora-sft-table.png"></p> </div> <div> <p><img src="https://pub.sakana.ai/doc-to-lora/figs/text-to-lora-scaling-plot.png"></p> </div> </div> <figcaption><b><span>SFT Text-to-LoRA Zero-Shot Performance and Scaling.</span></b> Left: SFT-trained Text-to-LoRA can <b>zero-shot</b> generate adapters for benchmark tasks, performing competitively against various baselines (e.g., multi-task LoRA and prior hypernetwork approaches). Right: performance improves as the number of training datasets increases, especially for larger Text-to-LoRA variants.</figcaption>
</figure>
<p>SFT training addresses the core question of whether we can generate useful adapters from unseen task descriptions. We train on 479 diverse tasks from the Lots-of-LoRAs dataset. This diversity teaches the hypernetwork a general mapping from natural-language task descriptions to corresponding model updates. Even without oracle adapters to guide training, SFT-trained Text-to-LoRA generates adapters that beat the base model and outperform baselines on held-out tasks.</p>
<p>We also observe a clear scaling trend where larger hypernetworks and more training data produce better, more generalizable instances. This confirms that the hypernetwork "adapter generator" scales well--larger models and more data lead to better text-to-adapter mappings. See <a href="https://arxiv.org/abs/2506.06105" target="_blank" rel="noopener noreferrer">our paper</a> for full details on architecture, experiments, ablations, and analyses.</p> <figure> <figcaption><b><span>Text-to-LoRA Video Demonstration.</span></b> A task description is used to generate a LoRA adapter, which is then applied to the base model to improve task performance (here illustrated on a math-style query).</figcaption>
</figure>
<hr>
<h2>Conclusion and outlook</h2>
<p><strong>Doc-to-LoRA</strong> and <strong>Text-to-LoRA</strong> are built around a simple idea. Instead of treating model updates as slow and expensive training jobs, we can <em>learn</em> them as hypernetworks. The theme of our research in this direction is cost amortization, where we pay the meta-training cost upfront for one update generator that can produce many task- or document-specific LoRAs on demand, turning what used to be an engineering pipeline into a single forward pass.</p> <p>Framing hypernetworks as update generators is powerful, and at first glance almost too good to be true for improving downstream performance on arbitrary tasks.
Indeed, it is not a free lunch. Meta-training can be very expensive (days to weeks on multiple GPUs), and there are important trade-offs and limitations to consider.
However, we are very excited about this research direction because it opens a new design space of instant, modular updates that can be generated and applied cheaply on demand.</p>
<p>Looking forward, we see update generators potentially being a foundational interface. By scaling both compute and data, one could train a <em>foundation hypernetwork</em> that unifies Doc-to-LoRA and Text-to-LoRA (and future modalities) so that the same system can ingest task descriptions, documents, or experiences and generate useful adapters for future interactions.
We envision a shared “update API” for LLMs, where different sources of supervision are simply different inputs to the same generator, yielding modular, composable adapters.</p> <p>Furthermore, instant update interfaces allow many new kinds of LLM memory architectures. For instance, instead of dumping all memory as external files, models can <em>“nap”</em>We take the analogy from <i>Lin, Kevin, et al. "Sleep-time compute: Beyond inference scaling at test-time."</i> between interactions, distill new information into adapters, and wake up with updated behavior. This memory architecture allows users to start new sessions to avoid high latency in long conversations without losing relevant information, as past conversations have already been distilled between sessions. In practice, the model update could happen overnight to refresh the model in a personalized way and be ready for the next day's user interactions. We believe that this approach can enable rapid personalization, mass model customization, and continual learning without paying the full cost of fine‑tuning every time.</p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>