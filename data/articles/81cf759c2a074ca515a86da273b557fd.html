<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>NVIDIA Releases 6 Million Multi-Lingual Reasoning Dataset</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>NVIDIA Releases 6 Million Multi-Lingual Reasoning Dataset</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 8/21/2025 12:13:18 AM | Lang: EN |
    <a href="https://huggingface.co/blog/nvidia/multilingual-reasoning-v1" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/jscowcroft"><img alt="Jane Polak Scowcroft's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/67d8b4a36aa351375456049e/hWuvpThKX6vVy5QcrDe6d.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/dnathawani"><img alt="Dhruv Nathawani's avatar" src="https://huggingface.co/avatars/3a6742d8a37d936d1410a88fb259660c.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/sding"><img alt="Shuoyang Ding's avatar" src="https://huggingface.co/avatars/3b1f3b54f11d7bd8bc25977df7aec538.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/okuchaiev"><img alt="Oleksii Kuchaiev's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1649475932211-622937a4acd5bef90e55c49d.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/vlavrukhin"><img alt="Vitaly Lavrukhin's avatar" src="https://huggingface.co/avatars/13e0128db035a8b647ce7479d5f4768c.svg"></a> </span> </span></p> </div></div> <p>Authors: Dhruv Nathawani, Shuoyang Ding US, Vitaly Lavrukhin US, Jane Polak Scowcroft US, Oleksii Kuchaiev US </p>
<p> NVIDIA continues releasing permissive datasets in support of the open ecosystem with <a href="https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v2">6 Million Multilingual Reasoning Dataset</a>.</p>
<p>Continuing the success of the recent <a href="https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1">Nemotron Post-Training Dataset v1</a> release used in <a href="https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/">Llama Nemotron Super model</a>, and our <a href="https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset">Llama Nemotron Post-Training Dataset</a> release earlier this year, we’re excited to release the reasoning dataset translated into five target languages: French, Spanish, German, Italian, and Japanese.</p>
<p>The newly released <a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2">NVIDIA Nemotron Nano 2 9B</a> brings these capabilities to the edge with leading accuracy and efficiency with a hybrid Transformer–Mamba architecture and a configurable thinking budget—so you can dial accuracy, throughput, and cost to match your real‑world needs.</p>
<h2> <a href="#model-highlights-tldr"> </a> <span> Model Highlights (TL;DR) </span>
</h2>
<ul>
<li><strong>Model size</strong>: 9B parameters</li>
<li><strong>Architecture</strong>: Hybrid Transformer–Mamba (Mamba‑2 + a small number of attention layers) for higher throughput at similar accuracy to Transformer‑only peers</li>
<li><strong>Throughput</strong>: Up to 6× higher token generation than other leading models in its size class</li>
<li><strong>Cost</strong>: Thinking budget lets you control how many “thinking” tokens are used—saving up to 60% lower reasoning costs</li>
<li><strong>Target</strong>: Agents for customer service, support chatbots, analytics copilots, and edge/RTX deployments</li>
<li><strong>Availability</strong>: The model weights are available on Hugging Face, you can try the endpoint on build.nvidia.com, and the model will be available as NVIDIA NIM for high throughput and low latency</li>
<li><strong>License</strong>: nvidia-open-model-license</li>
</ul>
<p>The release represents a significant step forward in our continued commitment to openness and transparency in model development and improvement. By releasing training data, in addition to the training tools and final model weights, NVIDIA supports continued improvement of open‑weight models.</p>
<h2> <a href="#whats-in-the-dataset-and-how-we-built-it"> </a> <span> What’s in the dataset and how we built it </span>
</h2>
<p>At a high level, the Nemotron Post-Training Dataset V2 takes our previously released English reasoning data and translates them into five target languages (French, German, Italian, Japanese, Spanish). To best take advantage of English knowledge instilled during pre‑training, we translate the user prompt and model response while preserving the original English reasoning chain.</p>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/67ee17721b341f4d5608e585/Tov1aIsRIX16n12P22NTE.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/67ee17721b341f4d5608e585/Tov1aIsRIX16n12P22NTE.png"></a></p>
<p>According to results from the WMT 2024 general translation shared task, LLMs are achieving state‑of‑the‑art results for machine translation tasks. However, for synthetic generation of post‑training data, our preliminary studies have shown that:</p>
<ul>
<li>LLMs are more prone to hallucinations when translating SFT datasets compared to translating common machine translation test sets (e.g., FLORES).</li>
<li>The translation quality and hallucination rate of open‑source LLMs deteriorate significantly as input length increases.</li>
</ul>
<p>Hence, we incorporate several mechanisms to maintain high translation quality and easy hallucination detection. To summarize:</p>
<ul>
<li>We break down sentences by newline and translate line‑by‑line. If a line is non‑translatable (e.g., only tabs) or is part of a code block, it won’t be translated.</li>
<li>We enforce a specific format (“Wrap the translated text in brackets 〘〙”) and use this special matching bracket to extract translations. Other examples are discarded (see Table 1).</li>
<li>We run fastText language ID on the translation of prompt inputs to filter out off‑target data points. We discarded another 55,567 examples (another 1.1% of all multilingual examples).</li>
</ul>
<p><em>Table 1: Ratio of discarded data (measured by bytes) by enforcing output format</em></p>
<div> <table> <thead><tr>
<th>Language</th>
<th>code</th>
<th>qa</th>
<th>math</th>
</tr> </thead><tbody><tr>
<td>de</td>
<td>2.28%</td>
<td>1.11%</td>
<td>2.47%</td>
</tr>
<tr>
<td>es</td>
<td>26.14%</td>
<td>5.15%</td>
<td>6.38%</td>
</tr>
<tr>
<td>fr</td>
<td>11.01%</td>
<td>1.37%</td>
<td>1.96%</td>
</tr>
<tr>
<td>it</td>
<td>4.94%</td>
<td>1.36%</td>
<td>0.75%</td>
</tr>
<tr>
<td>ja</td>
<td>7.68%</td>
<td>2.51%</td>
<td>3.86%</td>
</tr>
</tbody> </table>
</div>
<p>After benchmarking, we selected <code>Qwen2.5-32B-Instruct-AWQ</code> (for German) and <code>Qwen2.5-14B-Instruct</code> (for others) to conduct the translation. The considerations for selecting these models include:</p>
<ul>
<li>Robust translation quality</li>
<li>Can fit onto a single A100 GPU for inference</li>
<li>Wide domain coverage in training data</li>
<li>Open license (Apache 2.0)</li>
</ul>
<h2> <a href="#how-to-use-it"> </a> <span> How to use it </span>
</h2>
<pre><code><span>from</span> datasets <span>import</span> load_dataset
ds = load_dataset(<span>"nvidia/Nemotron-Post-Training-Dataset-v2"</span>)
</code></pre>
<p> Explore the dataset here: <a href="https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v2">Hugging Face dataset page</a></p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'top') scrollToTop();
      if (data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>