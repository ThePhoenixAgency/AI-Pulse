<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="UTF-8">
<title>Ettin Suite: SoTA Paired Encoders and Decoders</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Ettin Suite: SoTA Paired Encoders and Decoders</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 7/16/2025 2:00:00 AM | Lang: DE |
    <a href="https://huggingface.co/blog/ettin" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/orionweller"><img alt="Orion Weller's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6362d9712691058b19de1ba4/Hdqj5aGrFJJbF7oUSzoIh.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/kdricci"><img alt="K Ricci's avatar" src="https://huggingface.co/avatars/253dcce19a22cb745038a879f480baaf.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/mmarone"><img alt="Marc Marone's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/63e410e88083f19a218be964/8gqsYilSTSCt8E3JmajP-.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/NohTow"><img alt="Antoine Chaffin's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1620819560688-609bbe2f4932693ca2009d6a.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/dlawrie"><img alt="Dawn Lawrie's avatar" src="https://huggingface.co/avatars/b2495c05e0838ad117bf607ccddbb3e4.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/vandurme"><img alt="Ben Van Durme's avatar" src="https://huggingface.co/avatars/5723dcb12e63d6b30cb83cd189f96c46.svg"></a> </span> </span></p> </div></div> <h2> <div><nav><ul><li><a href="#tldr">TL;DR</a> <ul></ul> </li><li><a href="#encoders-vs-decoders-the-architecture-divide">Encoders vs Decoders: The Architecture Divide</a> <ul></ul> </li><li><a href="#training-recipe-modern-techniques-for-both-architectures">Training Recipe: Modern Techniques for Both Architectures</a> <ul><li><a href="#sizes">Sizes</a> <ul></ul> </li><li><a href="#three-phase-training-process">Three-Phase Training Process</a> <ul></ul> </li><li><a href="#modern-architecture-components">Modern Architecture Components</a> <ul></ul> </li><li><a href="#data-sources-and-quality">Data Sources and Quality</a> <ul></ul> </li></ul> </li><li><a href="#encoder-results-beating-modernbert">Encoder Results: Beating ModernBERT</a> <ul></ul> </li><li><a href="#decoder-results-beating-llama-32-and-smollm2">Decoder Results: Beating Llama 3.2 and SmolLM2</a> <ul></ul> </li><li><a href="#fair-fight-encoders-vs-decoders-on-even-ground">Fair Fight: Encoders vs Decoders on Even Ground</a> <ul><li><a href="#architecture-specific-advantages-persist">Architecture-Specific Advantages Persist</a> <ul></ul> </li><li><a href="#cross-objective-training-falls-short">Cross-Objective Training Falls Short</a> <ul></ul> </li></ul> </li><li><a href="#beyond-performance-understanding-model-behavior">Beyond Performance: Understanding Model Behavior</a> <ul></ul> </li><li><a href="#usage-examples">Usage Examples</a> <ul><li><a href="#encoders">Encoders</a> <ul></ul> </li><li><a href="#decoders">Decoders</a> <ul></ul> </li></ul> </li><li><a href="#fine-tuning-examples">Fine-tuning Examples</a> <ul><li><a href="#encoders-1">Encoders</a> <ul></ul> </li><li><a href="#decoders-1">Decoders</a> <ul></ul> </li></ul> </li><li><a href="#model-family-and-links">Model Family and Links</a> <ul></ul> </li></ul></nav></div> <a href="#tldr"> <span></span> </a> <span> TL;DR </span>
</h2>
<p>What would happen if you took the ModernBERT recipe and applied it to a decoder-only model? Turns out, a state-of-the-art decoder language model that beats Llama 3.2 1B and SmolLM2! </p>
<p>We introduce a new open-data training recipe to reproduce the encoder-only ModernBERT model (and actually beat it!). We then apply the exact same recipe to decoder-only models. For the first time, we have two state-of-the-art models trained in the same setup but with two different training objectives: masked language modeling (MLM), and causal language modeling (CLM).</p>
<p>This blog post introduces <a href="https://huggingface.co/collections/jhu-clsp/encoders-vs-decoders-the-ettin-suite-686303e16142257eed8e6aeb">Ettin</a>, the first suite of SoTA <strong>paired encoder-only and decoder-only models</strong> (17M-1B params) trained with identical data (2T tokens), architecture, and training recipes. Ettin enables true apples-to-apples comparisons between architectures and delivers <strong>state-of-the-art performance for open-data models</strong> in both categories. We then further explore whether it is possible to get a competitive encoder starting from the decoder and vice-versa.</p>
<p>If you are interested in trying out the models, some boilerplates are available <a href="#usage-examples">at the end of this blogpost!</a></p>
<p><a href="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/attention_masks.png?raw=true"><img alt="Attention patterns comparison between encoder and decoder models" src="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/attention_masks.png?raw=true"></a></p>
<h2> <a href="#encoders-vs-decoders-the-architecture-divide"> <span></span> </a> <span> Encoders vs Decoders: The Architecture Divide </span>
</h2>
<p>The LLM community has largely converged on decoder-only models like GPT, Llama, and Qwen. Their generative capabilities are impressive, but this focus is detracting attention from other categories, such as encoder-only models like BERT.</p>
<p>However, encoder BERT-like models remain the workhorses of production systems for classification, retrieval, and embedding tasks. They're faster, more memory-efficient, and often more accurate for discriminative tasks. The key difference lies in their attention patterns:</p>
<ul>
<li><strong>Encoder models</strong> use bidirectional attention, allowing each token to "see" all other tokens in the sequence (fully visible)</li>
<li><strong>Decoder models</strong> use causal attention, where tokens can only "see" previous tokens to enable autoregressive generation</li>
</ul>
<p>While decoder models have seen rapid innovation, encoder model development had stagnated – until recently, with efforts like <a href="https://huggingface.co/blog/modernbert">ModernBERT</a> modernizing them. But which architecture is better? Previous comparisons between encoders and decoders used different datasets, architectures, and training recipes, so it was hard to tell.</p>
<p>Named after the two-headed Norse giant, Ettin provides a <strong>controlled comparison</strong> by training with both architectures on identical data, identical model shapes, and identical training recipes. They only differ in attention patterns and training objectives!</p>
<h2> <a href="#training-recipe-modern-techniques-for-both-architectures"> <span></span> </a> <span> Training Recipe: Modern Techniques for Both Architectures </span>
</h2>
<p>We build on the ModernBERT recipe, which borrowed modern techniques from decoder-only models and brought them to encoder training. This provides a strong base for training both architectures.</p>
<h3> <a href="#sizes"> <span></span> </a> <span> Sizes </span>
</h3>
<p>We train six different sizes, ranging from 17M to 1B parameters. This allows us to test the effects of scale, and provides a wide variety of models for you to use!
No matter if you need a blazing fast on-device model or a powerful but slower model, we got you covered!</p>
<p><a href="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/sizes_small.png?raw=true"><img alt="Sizes of Ettin models" src="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/sizes_small.png?raw=true"></a></p>
<h3> <a href="#three-phase-training-process"> <span></span> </a> <span> Three-Phase Training Process </span>
</h3>
<p>We use a comprehensive three-phase training approach to maximize performance:</p>
<p><strong>Phase 1 - Pre-training (1.7T tokens)</strong>: We start with a diverse mixture of high-quality data sources, training on shorter contexts (1024 tokens) to establish strong foundational knowledge.</p>
<p><strong>Phase 2 - Context Extension (250B tokens)</strong>: We increase context length to 8K tokens using higher-quality filtered data, allowing models to understand longer documents and more complex relationships.</p>
<p><strong>Phase 3 - Decay (100B tokens)</strong>: We finish with premium data sources including scientific papers, textbooks, and curated content while gradually reducing the learning rate.</p>
<h3> <a href="#modern-architecture-components"> <span></span> </a> <span> Modern Architecture Components </span>
</h3>
<p>Our encoder models gain all the benefits of ModernBERT's speed, allowing them to be significantly faster than the previous generations of encoders.</p>
<h3> <a href="#data-sources-and-quality"> <span></span> </a> <span> Data Sources and Quality </span>
</h3>
<p>Unlike ModernBERT, <strong>all our training data is public and reproducible</strong>:</p>
<p><a href="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/training_data.jpg?raw=true"><img alt="Data used to train Ettin models" src="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/training_data.jpg?raw=true"></a></p>
<p>You can continue to train these models on new data or propose a new recipe to further improve results!</p>
<h2> <a href="#encoder-results-beating-modernbert"> <span></span> </a> <span> Encoder Results: Beating ModernBERT </span>
</h2>
<p>Our encoder models <strong>outperform ModernBERT</strong> across all tasks and model sizes, while using completely open training data. Since we provide a large range of sizes, you can now use ModernBERT-style models in smaller sizes (great for on-device or for fast-inference), or power up with a 1B-sized encoder that crushes the competition.</p>
<p><a href="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/encoder_results.jpg?raw=true"><img alt="Encoder performance comparison showing Ettin models beating ModernBERT" src="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/encoder_results.jpg?raw=true"></a></p>
<h2> <a href="#decoder-results-beating-llama-32-and-smollm2"> <span></span> </a> <span> Decoder Results: Beating Llama 3.2 and SmolLM2 </span>
</h2>
<p>Applying the same recipe to decoder models yields equally impressive results, with our models <strong>outperforming or matching established baselines</strong> such as Llama 3.2 and SmolLM2:</p>
<p><a href="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/decoder_results.jpg?raw=true"><img alt="Decoder performance comparison showing Ettin models beating Llama 3.2 and SmolLM2" src="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/decoder_results.jpg?raw=true"></a></p>
<p>The gains are particularly strong on knowledge-intensive tasks like SciQ, reflecting the benefits of our high-quality training data mixture. These results demonstrate that our training recipe creates genuinely strong models in both architectural paradigms.</p>
<h2> <a href="#fair-fight-encoders-vs-decoders-on-even-ground"> <span></span> </a> <span> Fair Fight: Encoders vs Decoders on Even Ground </span>
</h2>
<p>For the first time, we can fairly compare encoder and decoder architectures trained with identical data and recipes. The results reveal fundamental architectural advantages that persist even when all other factors are controlled:</p>
<p><a href="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/enc_vs_dec.jpg?raw=true"><img alt="Encoder vs decoder comparison across model sizes and tasks" src="https://github.com/JHU-CLSP/ettin-encoder-vs-decoder/blob/main/assets/enc_vs_dec.jpg?raw=true"></a></p>
<h3> <a href="#architecture-specific-advantages-persist"> <span></span> </a> <span> Architecture-Specific Advantages Persist </span>
</h3>
<p>The results show clear patterns:</p>
<p><strong>Encoders dominate classification and retrieval</strong>: On MNLI classification, even a 150M encoder (89.2) outperforms a 400M decoder (88.2). For retrieval tasks, the gap is smaller but still noticeable - especially when decoders are not trained with MNTP.</p>
<p><strong>Decoders excel at generation</strong>: On generative tasks, decoders maintain consistent advantages, with the performance gap actually widening at larger model sizes.</p>
<p><strong>Size doesn't always matter</strong>: A 400M encoder beats a 1B decoder on classification tasks, while a 400M decoder beats a 1B encoder on generation tasks.</p>
<h3> <a href="#cross-objective-training-falls-short"> <span></span> </a> <span> Cross-Objective Training Falls Short </span>
</h3>
<p>Due to the lack of new encoder models, works like <a href="https://arxiv.org/abs/2404.05961">LLM2Vec</a> have proposed to continue pre-training decoders with MLM. We can now test the effectiveness of this strategy!</p>
<p>We switched the objective and continued to train our models with the opposite objective for 50B additional tokens. This is what we found:</p>
<ul>
<li><strong>Encoder-from-decoder</strong>: Still generally trails native encoders on classification/retrieval</li>
<li><strong>Decoder-from-encoder</strong>: Are significantly worse than native decoders, especially at larger scales. This may be because the encoders were trained with MLM instead of MNTP (masked next token prediction) as proposed by LLM2Vec (and used in our encoder from decoder recipe).</li>
</ul>
<p>This suggests the architecture choice matters fundamentally, not just the training objective.</p>
<h2> <a href="#beyond-performance-understanding-model-behavior"> <span></span> </a> <span> Beyond Performance: Understanding Model Behavior </span>
</h2>
<p>With identical training data, we can study how different objectives affect learning. For example, analyzing gender bias using the WinoGender benchmark reveals:</p>
<ul>
<li><strong>Encoder models</strong> prefer gender-neutral pronouns more often (60%+ neutral vs 30%+ for decoders)</li>
<li><strong>Both architectures</strong> show male bias, but decoders slightly more so</li>
<li><strong>Cross-objective training</strong> affects bias patterns in measurable ways</li>
</ul>
<p>This opens doors for systematic studies of how training objectives influence model behavior beyond just accuracy metrics.</p>
<h2> <a href="#usage-examples"> <span></span> </a> <span> Usage Examples </span>
</h2>
<p>You can use these models with just a few lines of code!</p>
<h3> <a href="#encoders"> <span></span> </a> <span> Encoders </span>
</h3>
<pre><code><span>from</span> transformers <span>import</span> AutoTokenizer, AutoModel <span># Load encoder for classification/embeddings</span>
tokenizer = AutoTokenizer.from_pretrained(<span>"jhu-clsp/ettin-encoder-150m"</span>)
model = AutoModel.from_pretrained(<span>"jhu-clsp/ettin-encoder-150m"</span>) <span>def</span> <span>predict_masked_token</span>(<span>text</span>): inputs = tokenizer(text, return_tensors=<span>"pt"</span>) <span>with</span> torch.no_grad(): outputs = model(**inputs) <span># Get predictions for [MASK] tokens</span> mask_indices = torch.where(inputs[<span>"input_ids"</span>] == tokenizer.mask_token_id) predictions = outputs.logits[mask_indices] <span># Get top 5 predictions</span> top_tokens = torch.topk(predictions, <span>5</span>, dim=-<span>1</span>) <span>return</span> [tokenizer.decode(token) <span>for</span> token <span>in</span> top_tokens.indices[<span>0</span>]] <span># Example</span>
masked_text = <span>"The capital of France is [MASK]."</span>
predictions = predict_masked_token(masked_text)
<span>print</span>(<span>f"Predictions: <span>{predictions}</span>"</span>)
</code></pre>
<p><strong>For classification and retrieval tasks, use encoder models:</strong> You may want to use a fine-tuned version for these tasks as well.</p>
<h3> <a href="#decoders"> <span></span> </a> <span> Decoders </span>
</h3>
<p><strong>For text generation tasks, use decoder models:</strong></p>
<pre><code><span>from</span> transformers <span>import</span> AutoTokenizer, AutoModelForCausalLM <span># Load decoder for generation</span>
tokenizer = AutoTokenizer.from_pretrained(<span>"jhu-clsp/ettin-decoder-150m"</span>)
model = AutoModelForCausalLM.from_pretrained(<span>"jhu-clsp/ettin-decoder-150m"</span>) <span># Generate text</span>
prompt = <span>"The future of artificial intelligence is"</span>
inputs = tokenizer(prompt, return_tensors=<span>"pt"</span>)
outputs = model.generate(inputs.input_ids, max_length=<span>50</span>, temperature=<span>0.7</span>)
generated_text = tokenizer.decode(outputs[<span>0</span>], skip_special_tokens=<span>True</span>)
</code></pre>
<h2> <a href="#fine-tuning-examples"> <span></span> </a> <span> Fine-tuning Examples </span>
</h2>
<h3> <a href="#encoders-1"> <span></span> </a> <span> Encoders </span>
</h3>
Click to see how to finetune this into a dense embedding model using Sentence Transformers <pre><code><span>import</span> argparse <span>from</span> datasets <span>import</span> load_dataset
<span>from</span> sentence_transformers <span>import</span> ( SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments,
)
<span>from</span> sentence_transformers.evaluation <span>import</span> TripletEvaluator
<span>from</span> sentence_transformers.losses <span>import</span> CachedMultipleNegativesRankingLoss
<span>from</span> sentence_transformers.training_args <span>import</span> BatchSamplers <span>def</span> <span>main</span>(): <span># parse the lr &amp; model name</span> parser = argparse.ArgumentParser() parser.add_argument(<span>"--lr"</span>, <span>type</span>=<span>float</span>, default=<span>8e-5</span>) parser.add_argument(<span>"--model_name"</span>, <span>type</span>=<span>str</span>, default=<span>"jhu-clsp/ettin-encoder-150m"</span>) args = parser.parse_args() lr = args.lr model_name = args.model_name model_shortname = model_name.split(<span>"/"</span>)[-<span>1</span>] <span># 1. Load a model to finetune</span> model = SentenceTransformer(model_name) <span># 2. Load a dataset to finetune on</span> dataset = load_dataset( <span>"sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1"</span>, <span>"triplet-hard"</span>, split=<span>"train"</span>, ) dataset_dict = dataset.train_test_split(test_size=<span>1_000</span>, seed=<span>12</span>) train_dataset = dataset_dict[<span>"train"</span>].select(<span>range</span>(<span>1_250_000</span>)) eval_dataset = dataset_dict[<span>"test"</span>] <span># 3. Define a loss function</span> loss = CachedMultipleNegativesRankingLoss(model, mini_batch_size=<span>16</span>) <span># Increase mini_batch_size if you have enough VRAM</span> run_name = <span>f"<span>{model_shortname}</span>-DPR-<span>{lr}</span>"</span> <span># 4. (Optional) Specify training arguments</span> args = SentenceTransformerTrainingArguments( <span># Required parameter:</span> output_dir=<span>f"output/<span>{model_shortname}</span>/<span>{run_name}</span>"</span>, <span># Optional training parameters:</span> num_train_epochs=<span>1</span>, per_device_train_batch_size=<span>512</span>, per_device_eval_batch_size=<span>512</span>, warmup_ratio=<span>0.05</span>, fp16=<span>False</span>, <span># Set to False if GPU can't handle FP16</span> bf16=<span>True</span>, <span># Set to True if GPU supports BF16</span> batch_sampler=BatchSamplers.NO_DUPLICATES, <span># (Cached)MultipleNegativesRankingLoss benefits from no duplicates</span> learning_rate=lr, <span># Optional tracking/debugging parameters:</span> save_strategy=<span>"steps"</span>, save_steps=<span>500</span>, save_total_limit=<span>2</span>, logging_steps=<span>500</span>, run_name=run_name, <span># Used in `wandb`, `tensorboard`, `neptune`, etc. if installed</span> ) <span># 5. (Optional) Create an evaluator &amp; evaluate the base model</span> dev_evaluator = TripletEvaluator( anchors=eval_dataset[<span>"query"</span>], positives=eval_dataset[<span>"positive"</span>], negatives=eval_dataset[<span>"negative"</span>], name=<span>"msmarco-co-condenser-dev"</span>, ) dev_evaluator(model) <span># 6. Create a trainer &amp; train</span> trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator, ) trainer.train() <span># 7. (Optional) Evaluate the trained model on the evaluator after training</span> dev_evaluator(model) <span># 8. Save the model</span> model.save_pretrained(<span>f"output/<span>{model_shortname}</span>/<span>{run_name}</span>/final"</span>) <span># 9. (Optional) Push it to the Hugging Face Hub</span> model.push_to_hub(run_name, private=<span>False</span>) <span>if</span> __name__ == <span>"__main__"</span>: main()
</code></pre> Click to see how to finetune this into a multi-vector embedding model with PyLate <pre><code><span>from</span> datasets <span>import</span> load_dataset
<span>from</span> pylate <span>import</span> losses, models, utils
<span>from</span> sentence_transformers <span>import</span> ( SentenceTransformerTrainer, SentenceTransformerTrainingArguments,
) <span>def</span> <span>main</span>(): <span># Load the datasets required for knowledge distillation (train, queries, documents)</span> train = load_dataset( path=<span>"lightonai/ms-marco-en-bge"</span>, name=<span>"train"</span>, ) queries = load_dataset( path=<span>"lightonai/ms-marco-en-bge"</span>, name=<span>"queries"</span>, ) documents = load_dataset( path=<span>"lightonai/ms-marco-en-bge"</span>, name=<span>"documents"</span>, ) <span># Set the transformation to load the documents/queries texts using the corresponding ids on the fly</span> train.set_transform( utils.KDProcessing(queries=queries, documents=documents).transform, ) <span># Define the base model, training parameters, and output directory</span> num_train_epochs = <span>1</span> lr = <span>8e-5</span> batch_size = <span>16</span> accum_steps = <span>1</span> model_name = <span>"jhu-clsp/ettin-encoder-150m"</span> model_shortname = model_name.split(<span>"/"</span>)[-<span>1</span>] <span># Set the run name for logging and output directory</span> run_name = <span>f"<span>{model_shortname}</span>-colbert-KD-<span>{lr}</span>"</span> output_dir = <span>f"output/<span>{model_shortname}</span>/<span>{run_name}</span>"</span> <span># Initialize the ColBERT model from the base model</span> model = models.ColBERT(model_name_or_path=model_name) <span># Configure the training arguments (e.g., epochs, batch size, learning rate)</span> args = SentenceTransformerTrainingArguments( output_dir=output_dir, num_train_epochs=num_train_epochs, per_device_train_batch_size=batch_size, fp16=<span>False</span>, <span># Set to False if you get an error that your GPU can't run on FP16</span> bf16=<span>True</span>, <span># Set to True if you have a GPU that supports BF16</span> run_name=run_name, logging_steps=<span>10</span>, learning_rate=lr, gradient_accumulation_steps=accum_steps, warmup_ratio=<span>0.05</span>, ) <span># Use the Distillation loss function for training</span> train_loss = losses.Distillation(model=model) <span># Initialize the trainer</span> trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train, loss=train_loss, data_collator=utils.ColBERTCollator(tokenize_fn=model.tokenize), ) <span># Start the training process</span> trainer.train() model.save_pretrained(<span>f"<span>{output_dir}</span>/final"</span>) <span>if</span> __name__ == <span>"__main__"</span>: main()
</code></pre> Click to see how to finetune this into a sparse retrieval model using Sentence Transformers <pre><code><span>import</span> logging <span>from</span> datasets <span>import</span> load_dataset <span>from</span> sentence_transformers <span>import</span> ( SparseEncoder, SparseEncoderModelCardData, SparseEncoderTrainer, SparseEncoderTrainingArguments,
)
<span>from</span> sentence_transformers.sparse_encoder.evaluation <span>import</span> SparseNanoBEIREvaluator
<span>from</span> sentence_transformers.sparse_encoder.losses <span>import</span> SparseMultipleNegativesRankingLoss, SpladeLoss
<span>from</span> sentence_transformers.training_args <span>import</span> BatchSamplers logging.basicConfig(<span>format</span>=<span>"%(asctime)s - %(message)s"</span>, datefmt=<span>"%Y-%m-%d %H:%M:%S"</span>, level=logging.INFO) <span># 1. Load a model to finetune with 2. (Optional) model card data</span>
model = SparseEncoder( <span>"jhu-clsp/ettin-encoder-150m"</span>, model_card_data=SparseEncoderModelCardData( language=<span>"en"</span>, license=<span>"apache-2.0"</span>, )
) <span># 3. Load a dataset to finetune on</span>
full_dataset = load_dataset(<span>"sentence-transformers/natural-questions"</span>, split=<span>"train"</span>).select(<span>range</span>(<span>100_000</span>))
dataset_dict = full_dataset.train_test_split(test_size=<span>1_000</span>, seed=<span>12</span>)
train_dataset = dataset_dict[<span>"train"</span>]
eval_dataset = dataset_dict[<span>"test"</span>] <span># 4. Define a loss function</span>
loss = SpladeLoss( model=model, loss=SparseMultipleNegativesRankingLoss(model=model), query_regularizer_weight=<span>5e-5</span>, document_regularizer_weight=<span>3e-5</span>,
) <span># 5. (Optional) Specify training arguments</span>
run_name = <span>"splade-distilbert-base-uncased-nq"</span>
args = SparseEncoderTrainingArguments( <span># Required parameter:</span> output_dir=<span>f"models/<span>{run_name}</span>"</span>, <span># Optional training parameters:</span> num_train_epochs=<span>1</span>, per_device_train_batch_size=<span>16</span>, per_device_eval_batch_size=<span>16</span>, learning_rate=<span>2e-5</span>, warmup_ratio=<span>0.1</span>, fp16=<span>True</span>, <span># Set to False if you get an error that your GPU can't run on FP16</span> bf16=<span>False</span>, <span># Set to True if you have a GPU that supports BF16</span> batch_sampler=BatchSamplers.NO_DUPLICATES, <span># MultipleNegativesRankingLoss benefits from no duplicate samples in a batch</span> <span># Optional tracking/debugging parameters:</span> eval_strategy=<span>"steps"</span>, eval_steps=<span>1000</span>, save_strategy=<span>"steps"</span>, save_steps=<span>1000</span>, save_total_limit=<span>2</span>, logging_steps=<span>200</span>, run_name=run_name, <span># Will be used in W&amp;B if `wandb` is installed</span>
) <span># 6. (Optional) Create an evaluator &amp; evaluate the base model</span>
dev_evaluator = SparseNanoBEIREvaluator(dataset_names=[<span>"msmarco"</span>, <span>"nfcorpus"</span>, <span>"nq"</span>], batch_size=<span>16</span>) <span># 7. Create a trainer &amp; train</span>
trainer = SparseEncoderTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator,
)
trainer.train() <span># 8. Evaluate the model performance again after training</span>
dev_evaluator(model) <span># 9. Save the trained model</span>
model.save_pretrained(<span>f"models/<span>{run_name}</span>/final"</span>) <span># 10. (Optional) Push it to the Hugging Face Hub</span>
model.push_to_hub(run_name)
</code></pre> Click to see how to finetune this into a reranker model using Sentence Transformers <pre><code><span>import</span> logging
<span>import</span> traceback <span>import</span> torch
<span>from</span> datasets <span>import</span> load_dataset <span>from</span> sentence_transformers <span>import</span> SentenceTransformer
<span>from</span> sentence_transformers.cross_encoder <span>import</span> ( CrossEncoder, CrossEncoderModelCardData, CrossEncoderTrainer, CrossEncoderTrainingArguments,
)
<span>from</span> sentence_transformers.cross_encoder.evaluation <span>import</span> ( CrossEncoderNanoBEIREvaluator, CrossEncoderRerankingEvaluator,
)
<span>from</span> sentence_transformers.cross_encoder.losses <span>import</span> BinaryCrossEntropyLoss
<span>from</span> sentence_transformers.evaluation <span>import</span> SequentialEvaluator
<span>from</span> sentence_transformers.util <span>import</span> mine_hard_negatives <span># Set the log level to INFO to get more information</span>
logging.basicConfig(<span>format</span>=<span>"%(asctime)s - %(message)s"</span>, datefmt=<span>"%Y-%m-%d %H:%M:%S"</span>, level=logging.INFO) <span>def</span> <span>main</span>(): model_name = <span>"jhu-clsp/ettin-encoder-150m"</span> train_batch_size = <span>64</span> num_epochs = <span>1</span> num_hard_negatives = <span>5</span> <span># How many hard negatives should be mined for each question-answer pair</span> <span># 1a. Load a model to finetune with 1b. (Optional) model card data</span> model = CrossEncoder( model_name, model_card_data=CrossEncoderModelCardData( language=<span>"en"</span>, license=<span>"apache-2.0"</span>, ), ) <span>print</span>(<span>"Model max length:"</span>, model.max_length) <span>print</span>(<span>"Model num labels:"</span>, model.num_labels) <span># 2a. Load the GooAQ dataset: https://huggingface.co/datasets/sentence-transformers/gooaq</span> logging.info(<span>"Read the gooaq training dataset"</span>) full_dataset = load_dataset(<span>"sentence-transformers/gooaq"</span>, split=<span>"train"</span>).select(<span>range</span>(<span>100_000</span>)) dataset_dict = full_dataset.train_test_split(test_size=<span>1_000</span>, seed=<span>12</span>) train_dataset = dataset_dict[<span>"train"</span>] eval_dataset = dataset_dict[<span>"test"</span>] logging.info(train_dataset) logging.info(eval_dataset) <span># 2b. Modify our training dataset to include hard negatives using a very efficient embedding model</span> embedding_model = SentenceTransformer(<span>"sentence-transformers/static-retrieval-mrl-en-v1"</span>, device=<span>"cpu"</span>) hard_train_dataset = mine_hard_negatives( train_dataset, embedding_model, num_negatives=num_hard_negatives, <span># How many negatives per question-answer pair</span> margin=<span>0</span>, <span># Similarity between query and negative samples should be x lower than query-positive similarity</span> range_min=<span>0</span>, <span># Skip the x most similar samples</span> range_max=<span>100</span>, <span># Consider only the x most similar samples</span> sampling_strategy=<span>"top"</span>, <span># Sample the top negatives from the range</span> batch_size=<span>4096</span>, <span># Use a batch size of 4096 for the embedding model</span> output_format=<span>"labeled-pair"</span>, <span># The output format is (query, passage, label), as required by BinaryCrossEntropyLoss</span> use_faiss=<span>True</span>, ) logging.info(hard_train_dataset) <span># 2c. (Optionally) Save the hard training dataset to disk</span> <span># hard_train_dataset.save_to_disk("gooaq-hard-train")</span> <span># Load again with:</span> <span># hard_train_dataset = load_from_disk("gooaq-hard-train")</span> <span># 3. Define our training loss.</span> <span># pos_weight is recommended to be set as the ratio between positives to negatives, a.k.a. `num_hard_negatives`</span> loss = BinaryCrossEntropyLoss(model=model, pos_weight=torch.tensor(num_hard_negatives)) <span># 4a. Define evaluators. We use the CrossEncoderNanoBEIREvaluator, which is a light-weight evaluator for English reranking</span> nano_beir_evaluator = CrossEncoderNanoBEIREvaluator( dataset_names=[<span>"msmarco"</span>, <span>"nfcorpus"</span>, <span>"nq"</span>], batch_size=train_batch_size, ) <span># 4b. Define a reranking evaluator by mining hard negatives given query-answer pairs</span> <span># We include the positive answer in the list of negatives, so the evaluator can use the performance of the</span> <span># embedding model as a baseline.</span> hard_eval_dataset = mine_hard_negatives( eval_dataset, embedding_model, corpus=full_dataset[<span>"answer"</span>], <span># Use the full dataset as the corpus</span> num_negatives=<span>30</span>, <span># How many documents to rerank</span> batch_size=<span>4096</span>, include_positives=<span>True</span>, output_format=<span>"n-tuple"</span>, use_faiss=<span>True</span>, ) logging.info(hard_eval_dataset) reranking_evaluator = CrossEncoderRerankingEvaluator( samples=[ { <span>"query"</span>: sample[<span>"question"</span>], <span>"positive"</span>: [sample[<span>"answer"</span>]], <span>"documents"</span>: [sample[column_name] <span>for</span> column_name <span>in</span> hard_eval_dataset.column_names[<span>2</span>:]], } <span>for</span> sample <span>in</span> hard_eval_dataset ], batch_size=train_batch_size, name=<span>"gooaq-dev"</span>, <span># Realistic setting: only rerank the positives that the retriever found</span> <span># Set to True to rerank *all* positives</span> always_rerank_positives=<span>False</span>, ) <span># 4c. Combine the evaluators &amp; run the base model on them</span> evaluator = SequentialEvaluator([reranking_evaluator, nano_beir_evaluator]) evaluator(model) <span># 5. Define the training arguments</span> short_model_name = model_name <span>if</span> <span>"/"</span> <span>not</span> <span>in</span> model_name <span>else</span> model_name.split(<span>"/"</span>)[-<span>1</span>] run_name = <span>f"reranker-<span>{short_model_name}</span>-gooaq-bce"</span> args = CrossEncoderTrainingArguments( <span># Required parameter:</span> output_dir=<span>f"models/<span>{run_name}</span>"</span>, <span># Optional training parameters:</span> num_train_epochs=num_epochs, per_device_train_batch_size=train_batch_size, per_device_eval_batch_size=train_batch_size, learning_rate=<span>2e-5</span>, warmup_ratio=<span>0.1</span>, fp16=<span>False</span>, <span># Set to False if you get an error that your GPU can't run on FP16</span> bf16=<span>True</span>, <span># Set to True if you have a GPU that supports BF16</span> dataloader_num_workers=<span>4</span>, load_best_model_at_end=<span>True</span>, metric_for_best_model=<span>"eval_gooaq-dev_ndcg@10"</span>, <span># Optional tracking/debugging parameters:</span> eval_strategy=<span>"steps"</span>, eval_steps=<span>1000</span>, save_strategy=<span>"steps"</span>, save_steps=<span>1000</span>, save_total_limit=<span>2</span>, logging_steps=<span>200</span>, logging_first_step=<span>True</span>, run_name=run_name, <span># Will be used in W&amp;B if `wandb` is installed</span> seed=<span>12</span>, ) <span># 6. Create the trainer &amp; start training</span> trainer = CrossEncoderTrainer( model=model, args=args, train_dataset=hard_train_dataset, loss=loss, evaluator=evaluator, ) trainer.train() <span># 7. Evaluate the final model, useful to include these in the model card</span> evaluator(model) <span># 8. Save the final model</span> final_output_dir = <span>f"models/<span>{run_name}</span>/final"</span> model.save_pretrained(final_output_dir) <span># 9. (Optional) save the model to the Hugging Face Hub!</span> <span># It is recommended to run `huggingface-cli login` to log into your Hugging Face account first</span> <span>try</span>: model.push_to_hub(run_name) <span>except</span> Exception: logging.error( <span>f"Error uploading model to the Hugging Face Hub:\n<span>{traceback.format_exc()}</span>To upload it manually, you can run "</span> <span>f"`huggingface-cli login`, followed by loading the model using `model = CrossEncoder(<span>{final_output_dir!r}</span>)` "</span> <span>f"and saving it using `model.push_to_hub('<span>{run_name}</span>')`."</span> ) <span>if</span> __name__ == <span>"__main__"</span>: main()
</code></pre> <h3> <a href="#decoders-1"> <span></span> </a> <span> Decoders </span>
</h3> Click to expand decoder training code <h2> <a href="#full-training"> <span></span> </a> <span> Full training </span>
</h2>
<pre><code>python trl/scripts/sft.py \ --model_name_or_path jhu-clsp/ettin-decoder-17m \ --dataset_name trl-lib/Capybara \ --learning_rate 2.0e-5 \ --num_train_epochs 1 \ --packing \ --per_device_train_batch_size 2 \ --gradient_accumulation_steps 8 \ --gradient_checkpointing \ --eos_token <span>'&lt;|im_end|&gt;'</span> \ --eval_strategy steps \ --eval_steps 100 \ --output_dir ettin-decoder-17m \ --push_to_hub
</code></pre>
<h2> <a href="#lora"> <span></span> </a> <span> LoRA </span>
</h2>
<pre><code>python trl/scripts/sft.py \ --model_name_or_path jhu-clsp/ettin-decoder-17m \ --dataset_name trl-lib/Capybara \ --learning_rate 2.0e-4 \ --num_train_epochs 1 \ --packing \ --per_device_train_batch_size 2 \ --gradient_accumulation_steps 8 \ --gradient_checkpointing \ --eos_token <span>'&lt;|im_end|&gt;'</span> \ --eval_strategy steps \ --eval_steps 100 \ --use_peft \ --lora_r 32 \ --lora_alpha 16 \ --output_dir ettin-decoder-17m \ --push_to_hub
</code></pre>
<p>with <code>sft.py</code>:</p>
<pre><code><span>import</span> argparse <span>from</span> datasets <span>import</span> load_dataset
<span>from</span> transformers <span>import</span> AutoConfig, AutoModelForCausalLM, AutoTokenizer
<span>from</span> transformers.models.auto.modeling_auto <span>import</span> MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES <span>from</span> trl <span>import</span> ( ModelConfig, ScriptArguments, SFTConfig, SFTTrainer, TrlParser, clone_chat_template, get_kbit_device_map, get_peft_config, get_quantization_config,
) <span>def</span> <span>main</span>(<span>script_args, training_args, model_args</span>): <span>################</span> <span># Model init kwargs &amp; Tokenizer</span> <span>################</span> quantization_config = get_quantization_config(model_args) model_kwargs = <span>dict</span>( revision=model_args.model_revision, trust_remote_code=model_args.trust_remote_code, attn_implementation=model_args.attn_implementation, torch_dtype=model_args.torch_dtype, use_cache=<span>False</span> <span>if</span> training_args.gradient_checkpointing <span>else</span> <span>True</span>, device_map=get_kbit_device_map() <span>if</span> quantization_config <span>is</span> <span>not</span> <span>None</span> <span>else</span> <span>None</span>, quantization_config=quantization_config, ) <span># Create model</span> config = AutoConfig.from_pretrained(model_args.model_name_or_path) valid_image_text_architectures = MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES.values() <span>if</span> config.architectures <span>and</span> <span>any</span>(arch <span>in</span> valid_image_text_architectures <span>for</span> arch <span>in</span> config.architectures): <span>from</span> transformers <span>import</span> AutoModelForImageTextToText model_kwargs.pop(<span>"use_cache"</span>, <span>None</span>) <span># Image models do not support cache</span> model = AutoModelForImageTextToText.from_pretrained(model_args.model_name_or_path, **model_kwargs) <span>else</span>: model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs) <span># Create tokenizer</span> tokenizer = AutoTokenizer.from_pretrained( model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, use_fast=<span>True</span> ) <span># Set default chat template if needed</span> <span>if</span> tokenizer.chat_template <span>is</span> <span>None</span>: <span># <span>TODO:</span> source should be passed as an argument</span> model, tokenizer = clone_chat_template(model, tokenizer, <span>"Qwen/Qwen3-0.6B"</span>) <span>################</span> <span># Dataset</span> <span>################</span> dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config) <span>################</span> <span># Training</span> <span>################</span> trainer = SFTTrainer( model=model, args=training_args, train_dataset=dataset[script_args.dataset_train_split], eval_dataset=dataset[script_args.dataset_test_split] <span>if</span> training_args.eval_strategy != <span>"no"</span> <span>else</span> <span>None</span>, processing_class=tokenizer, peft_config=get_peft_config(model_args), ) trainer.train() <span># Save and push to hub</span> trainer.save_model(training_args.output_dir) <span>if</span> training_args.push_to_hub: trainer.push_to_hub(dataset_name=script_args.dataset_name) <span>def</span> <span>make_parser</span>(<span>subparsers: argparse._SubParsersAction = <span>None</span></span>): dataclass_types = (ScriptArguments, SFTConfig, ModelConfig) <span>if</span> subparsers <span>is</span> <span>not</span> <span>None</span>: parser = subparsers.add_parser(<span>"sft"</span>, <span>help</span>=<span>"Run the SFT training script"</span>, dataclass_types=dataclass_types) <span>else</span>: parser = TrlParser(dataclass_types) <span>return</span> parser <span>if</span> __name__ == <span>"__main__"</span>: parser = make_parser() <span># When using the trl cli, this script may be run with additional arguments, corresponding accelerate arguments.</span> <span># To ensure that their parsing does not interfere with the script arguments, parse the arguments with</span> <span># `return_remaining_strings=True`, then ignore the remaining strings.</span> script_args, training_args, model_args, _ = parser.parse_args_and_config(return_remaining_strings=<span>True</span>) main(script_args, training_args, model_args)
</code></pre> <h2> <a href="#model-family-and-links"> <span></span> </a> <span> Model Family and Links </span>
</h2>
<p>The complete Ettin suite includes models at six different scales (for both encoders and decoders):</p>
<p><strong>Standard Models:</strong></p>
<ul>
<li><a href="https://huggingface.co/jhu-clsp/ettin-encoder-17m">ettin-encoder-17m</a> / <a href="https://huggingface.co/jhu-clsp/ettin-decoder-17m">ettin-decoder-17m</a> (17M params)</li>
<li><a href="https://huggingface.co/jhu-clsp/ettin-encoder-32m">ettin-encoder-32m</a> / <a href="https://huggingface.co/jhu-clsp/ettin-decoder-32m">ettin-decoder-32m</a> (32M params)</li>
<li><a href="https://huggingface.co/jhu-clsp/ettin-encoder-68m">ettin-encoder-68m</a> / <a href="https://huggingface.co/jhu-clsp/ettin-decoder-68m">ettin-decoder-68m</a> (68M params)</li>
<li><a href="https://huggingface.co/jhu-clsp/ettin-encoder-150m">ettin-encoder-150m</a> / <a href="https://huggingface.co/jhu-clsp/ettin-decoder-150m">ettin-decoder-150m</a> (150M params) </li>
<li><a href="https://huggingface.co/jhu-clsp/ettin-encoder-400m">ettin-encoder-400m</a> / <a href="https://huggingface.co/jhu-clsp/ettin-decoder-400m">ettin-decoder-400m</a> (400M params)</li>
<li><a href="https://huggingface.co/jhu-clsp/ettin-encoder-1b">ettin-encoder-1b</a> / <a href="https://huggingface.co/jhu-clsp/ettin-decoder-1b">ettin-decoder-1b</a> (1B params)</li>
</ul>
<p><strong>Research Resources:</strong></p>
<ul>
<li><a href="https://huggingface.co/collections/jhu-clsp/encoders-vs-decoders-the-ettin-suite-686303e16142257eed8e6aeb"> Ettin Model Collection</a></li>
<li><a href="https://github.com/jhu-clsp/ettin-encoder-vs-decoder"> Paper</a> </li>
<li><a href="https://huggingface.co/datasets/jhu-clsp/ettin-pretraining-data"> Training Data</a> (2T+ tokens, fully open)</li>
<li><a href="https://github.com/jhu-clsp/ettin-encoder-vs-decoder"> GitHub Repository</a></li>
<li><a href="https://huggingface.co/jhu-clsp/ettin-checkpoints"> 250+ Training Checkpoints</a> for studying training dynamics or knowledge learning</li>
</ul>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'top') scrollToTop();
      if (data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>