<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>20x Faster TRL Fine-tuning with RapidFire AI</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>20x Faster TRL Fine-tuning with RapidFire AI</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 11/21/2025 1:00:00 AM | Lang: EN |
    <a href="https://huggingface.co/blog/rapidfireai" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/kbigdelysh"><img alt="Kamran Bigdely's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_iT4VSLJ5t4wsAJAp5WJ0.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/arunkk09"><img alt="Arun Kumar's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/66dca5d6f7d6d9129031431a/1igNGyjLni1PuZnPT4Bnf.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/qgallouedec"><img alt="Quentin Gallouédec's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1677431596830-631ce4b244503b72277fc89f.jpeg"></a> </span> </span></p> </div></div> <p>Hugging Face TRL now officially integrates with RapidFire AI to accelerate your fine-tuning and post-training experiments. TRL users can now discover, install, and run RapidFire AI as the fastest way to compare multiple fine-tuning/post-training configurations to customize LLMs without major code changes and without bloating GPU requirements.</p>
<h2> <a href="#why-this-matters"> </a> <span> Why this matters </span>
</h2>
<p>When fine-tuning or post-training LLMs, teams often do not have the time and/or budget to compare multiple configs even though that can significantly boost eval metrics. RapidFire AI lets you launch multiple TRL configs concurrently--even on a single GPU--and compare them in near real time via a new adaptive, chunk-based scheduling and execution scheme. In internal benchmarks referenced in the TRL page, this delivers ~16–24× higher experimentation throughput than sequentially comparing configs one after another, enabling you to reach much better metrics much faster.</p>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rapidfireai_intro/rf-usage.png"><img alt="RapidFire AI Architecture" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rapidfireai_intro/rf-usage.png"></a>
<em>RapidFire AI establishes live three-way communication between your IDE, a metrics dashboard, and a multi-GPU execution backend</em></p>
<h2> <a href="#what-you-get-out-of-the-box"> </a> <span> What you get, out of the box </span>
</h2>
<ul>
<li><p><strong>Drop-in TRL wrappers</strong> — Use <code>RFSFTConfig</code>, <code>RFDPOConfig</code>, and <code>RFGRPOConfig</code> as near-zero-code replacements for TRL's SFT/DPO/GRPO configs.</p>
</li>
<li><p><strong>Adaptive chunk-based concurrent training</strong> — RapidFire AI shards the dataset into a given number of chunks and cycles configs at chunk boundaries to enable earlier apples-to-apples comparisons and also maximize GPU utilization.</p>
</li>
<li><p><strong>Interactive Control Ops (IC Ops)</strong> — From the dashboard itself, you can Stop, Resume, Delete, and Clone-Modify, possibly with Warm-Start, any runs in flight to avoid wasting resources on underperforming configs and double-down on better performing configs--no job restarts, no juggling separate GPUs or clusters, no resource bloat.</p>
</li>
</ul>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rapidfireai_intro/icop-clone.png"><img alt="Interactive Control Operations" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rapidfireai_intro/icop-clone.png"></a>
<em>Clone promising configurations with modified hyperparameters, optionally warm-starting from the parent's weights, all from the live dashboard</em></p>
<ul>
<li><p><strong>Multi-GPU orchestration</strong> — The RapidFire AI scheduler automatically places and orchestrates configs across available GPUs on chunks of data via effcient shared-memory mechanisms. You focus on your models and eval metrics, not plumbing.</p>
</li>
<li><p><strong>MLflow-based dashboard</strong> — Real-time metrics, logs, and IC Ops in one place as soon as you start your experiment. Support for more dashboards such as Trackio, W&amp;B, and TensorBoard coming soon.</p>
</li>
</ul>
<h2> <a href="#how-it-works"> </a> <span> How it works </span>
</h2>
<p>RapidFire AI splits your dataset randomly into "chunks" and cycles LLM configurations through the GPUs at chunk boundaries. You get incremental signal on eval metrics across all configs much more quickly. The automatic checkpointing via an efficient shared-memory-based adapter/model spilling/loading mechanism keeps training smooth, stable, and consistent. Use IC Ops to adapt mid-flight to stop low-performers earlier and clone promising ones with tweaked config knobs, optionally warm-starting from the parent's weights.</p>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rapidfireai_intro/gantt-2gpu.png"><img alt="GPU Scheduling Comparison" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rapidfireai_intro/gantt-2gpu.png"></a>
<em>Sequential vs. Task Parallel vs. RapidFire AI: The adaptive scheduler maximizes GPU utilization across multiple configs and GPUs. The bottom row shows IC Ops in action—stopping, cloning, and modifying runs mid-flight.</em></p>
<h2> <a href="#getting-started"> </a> <span> Getting Started </span>
</h2>
<p>Install RapidFire AI and get running in under a minute:</p>
<pre><code>pip install rapidfireai <span># Authenticate with Hugging Face</span>
huggingface-cli login --token YOUR_TOKEN <span># Workaround for current issue</span>
pip uninstall -y hf-xet <span># Initialize and start RapidFire AI</span>
rapidfireai init
rapidfireai start
</code></pre>
<p>The dashboard launches at <code>http://localhost:3000</code> where you can monitor and control all your experiments.</p>
<h2> <a href="#supported-trl-trainers"> </a> <span> Supported TRL trainers </span>
</h2>
<ul>
<li>SFT with <code>RFSFTConfig</code></li>
<li>DPO with <code>RFDPOConfig</code></li>
<li>GRPO with <code>RFGRPOConfig</code></li>
</ul>
<p>These are designed as drop-in replacements so that you can keep your TRL mental model while gaining far more concurrency and control for your fine-tuning/post-training applications. </p>
<h2> <a href="#minimal-trl-sft-example"> </a> <span> Minimal TRL SFT example </span>
</h2>
<p>Here's what it looks like to train <strong>multiple configurations concurrently</strong> even on a single GPU:</p>
<pre><code><span>from</span> rapidfireai <span>import</span> Experiment
<span>from</span> rapidfireai.automl <span>import</span> <span>List</span>, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig
<span>from</span> datasets <span>import</span> load_dataset
<span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer <span># Setup: load your dataset and define formatting</span>
dataset = load_dataset(<span>"bitext/Bitext-customer-support-llm-chatbot-training-dataset"</span>)
train_dataset = dataset[<span>"train"</span>].select(<span>range</span>(<span>128</span>)).shuffle(seed=<span>42</span>) <span>def</span> <span>formatting_function</span>(<span>row</span>): <span>return</span> { <span>"prompt"</span>: [ {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful customer support assistant."</span>}, {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: row[<span>"instruction"</span>]}, ], <span>"completion"</span>: [{<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: row[<span>"response"</span>]}] } dataset = dataset.<span>map</span>(formatting_function) <span># Define multiple configs to compare</span>
config_set = <span>List</span>([ RFModelConfig( model_name=<span>"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span>, peft_config=RFLoraConfig(r=<span>8</span>, lora_alpha=<span>16</span>, target_modules=[<span>"q_proj"</span>, <span>"v_proj"</span>]), training_args=RFSFTConfig(learning_rate=<span>1e-3</span>, max_steps=<span>128</span>, fp16=<span>True</span>), ), RFModelConfig( model_name=<span>"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span>, peft_config=RFLoraConfig(r=<span>32</span>, lora_alpha=<span>64</span>, target_modules=[<span>"q_proj"</span>, <span>"v_proj"</span>]), training_args=RFSFTConfig(learning_rate=<span>1e-4</span>, max_steps=<span>128</span>, fp16=<span>True</span>), formatting_func=formatting_function, )
]) <span># Run all configs concurrently with chunk-based scheduling</span>
experiment = Experiment(experiment_name=<span>"sft-comparison"</span>)
config_group = RFGridSearch(configs=config_set, trainer_type=<span>"SFT"</span>) <span>def</span> <span>create_model</span>(<span>model_config</span>): model = AutoModelForCausalLM.from_pretrained( model_config[<span>"model_name"</span>], device_map=<span>"auto"</span>, torch_dtype=<span>"auto"</span> ) tokenizer = AutoTokenizer.from_pretrained(model_config[<span>"model_name"</span>]) <span>return</span> (model, tokenizer) experiment.run_fit(config_group, create_model, train_dataset, num_chunks=<span>4</span>, seed=<span>42</span>)
experiment.end()
</code></pre>
<p><strong>What happens when you run this?</strong></p>
<p>Suppose you run the above on a 2-GPU machine. Instead of training sequentially (Config 1 → wait → Config 2 → wait), both configs train concurrently:</p>
<div> <table> <thead><tr>
<th>Approach</th>
<th>Time till Comparative Decision</th>
<th>GPU utilization</th>
</tr> </thead><tbody><tr>
<td>Sequential (traditional)</td>
<td>~15 minutes</td>
<td>60% utilization</td>
</tr>
<tr>
<td>RapidFire AI (concurrent)</td>
<td>~5 minutes</td>
<td>95%+ utilization</td>
</tr>
</tbody> </table>
</div>
<p>You can get to a comparative decision <strong>3× sooner</strong> on the same resources after both configs finish processing the first data chunk instead of waiting for them to see the whole dataset one after another. Open <code>http://localhost:3000</code> to watch live metrics and use IC Ops to stop, clone, or tweak runs in real-time based on what you're seeing.</p>
<h2> <a href="#benchmarks-real-world-speedups"> </a> <span> Benchmarks: Real-World Speedups </span>
</h2>
<p>Here is what teams see on time to reach a comparable overall best training loss (across all tried configs) when switching from sequential comparisons to RapidFire AI-enabled hyperparallel experimentation:</p>
<div> <table> <thead><tr>
<th>Scenario</th>
<th>Sequential Time</th>
<th>RapidFire AI Time</th>
<th>Speedup</th>
</tr> </thead><tbody><tr>
<td>4 configs, 1 GPU</td>
<td>120 min</td>
<td>7.5 min</td>
<td><strong>16×</strong></td>
</tr>
<tr>
<td>8 configs, 1 GPU</td>
<td>240 min</td>
<td>12 min</td>
<td><strong>20×</strong></td>
</tr>
<tr>
<td>4 configs, 2 GPUs</td>
<td>60 min</td>
<td>4 min</td>
<td><strong>15×</strong></td>
</tr>
</tbody> </table>
</div>
<p><em>Benchmarks on NVIDIA A100 40GB with TinyLlama-1.1B and Llama-3.2-1B models</em></p>
<h2> <a href="#get-started-today"> </a> <span> Get Started Today </span>
</h2>
<p><strong> Try it hands-on</strong>: <a href="http://tinyurl.com/rapidfireai-colab">Interactive Colab Notebook</a> — Zero setup, runs in your browser</p>
<p><strong> Full Documentation</strong>: <a href="https://oss-docs.rapidfire.ai/">oss-docs.rapidfire.ai</a> — Complete guides, examples, and API reference</p>
<p><strong> GitHub</strong>: <a href="https://github.com/RapidFireAI/rapidfireai">RapidFireAI/rapidfireai</a> — Open source, production-ready</p>
<p><strong> Install via PyPI</strong>: <a href="https://pypi.org/project/rapidfireai">pypi.org/project/rapidfireai</a> — <code>pip install rapidfireai</code></p>
<p><strong> Join the Community</strong>: <a href="https://discord.gg/6vSTtncKNN">Discord</a> — Get help, share results, request features</p>
<hr>
<p>RapidFire AI was built&nbsp;because the common status quo of trying one config at a time wastes both time and GPU cycles. With this official integration, every TRL user can fine-tune/post-train smarter, iterate faster, and ship better models.</p>
<p><strong>Try the integration and let us know</strong>: How much faster is your experimentation loop? What should we build next? We're just getting started, and your feedback shapes where we go from here.</p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollStep(-1)">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollStep(1)">▼</button>
  </div>
  <script>
    function scrollStep(direction) {
      var step = Math.max(220, Math.round(window.innerHeight * 0.72));
      window.scrollBy({ top: direction * step, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up') scrollStep(-1);
      if (data.direction === 'down') scrollStep(1);
      if (data.direction === 'top') window.scrollTo({ top: 0, behavior: 'smooth' });
      if (data.direction === 'bottom') window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    });
  </script>
</body>
</html>