<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Welcome GPT OSS, the new open-source model family from OpenAI!</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Welcome GPT OSS, the new open-source model family from OpenAI!</h1>
  <div class="metadata">
<<<<<<< HEAD
    Source: Hugging Face Blog | Date: 8/5/2025 2:00:00 AM | Lang: EN |
=======
    Source: Hugging Face Blog | Date: 8/5/2025 12:00:00 AM | Lang: EN |
>>>>>>> 48d6193da6f49976a64b6a30483399bfb54b1b8d
    <a href="https://huggingface.co/blog/welcome-openai-gpt-oss" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/reach-vb"><img alt="Vaibhav Srivastav's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1655385361868-61b85ce86eb1f2c5e6233736.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/pcuenq"><img alt="Pedro Cuenca's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/lewtun"><img alt="Lewis Tunstall's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1594651707950-noauth.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/clem"><img alt="Clem 's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/Rocketknight1"><img alt="Matthew Carrigan's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1660312628256-60ba519750effef3a58beac3.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/clefourrier"><img alt="Clémentine Fourrier's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1644340617257-noauth.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/celinah"><img alt="Célina Hanouti's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6192895f3b8aa351a46fadfd/2VifD-AAKYk24AUmfSr_X.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/Wauplin"><img alt="Lucain Pouget's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1659336880158-6273f303f6d63a28483fde12.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/marcsun13"><img alt="Marc Sun's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/63ce875d199b36f7552d4f07/bpUrvhXDagzRqZ3vxTcSF.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/pagezyhf"><img alt="Simon Pagezy's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/w3Z6xyKVBA6np65Tb16dP.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/ahadnagy"><img alt="Ákos Hadnagy's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6750d772d11bc4e8b7352e83/TRsjaWjkuomCcXvhJSYNd.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/joaogante"><img alt="Joao Gante's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1641203017724-noauth.png"></a> </span> </span></p> </div></div> <p>GPT OSS is a hugely anticipated open-weights release by OpenAI, designed for powerful reasoning, agentic tasks, and versatile developer use cases. It comprises two models: a big one with 117B parameters (<a href="https://hf.co/openai/gpt-oss-120b">gpt-oss-120b</a>), and a smaller one with 21B parameters (<a href="https://hf.co/openai/gpt-oss-20b">gpt-oss-20b</a>). Both are mixture-of-experts (MoEs) and use a 4-bit quantization scheme (MXFP4), enabling fast inference (thanks to fewer active parameters, see details below) while keeping resource usage low. The large model fits on a single H100 GPU, while the small one runs within 16GB of memory and is perfect for consumer hardware and on-device applications.</p>
<p>To make it even better and more impactful for the community, the models are licensed under the <strong>Apache 2.0 license</strong>, along with a minimal usage policy: </p>
<blockquote>
<p>We aim for our tools to be used safely, responsibly, and democratically, while maximizing your control over how you use them. By using gpt-oss, you agree to comply with all applicable law.</p>
</blockquote>
<p>According to OpenAI, this release is a meaningful step in their commitment to the open-source ecosystem, in line with their stated mission to make the benefits of AI broadly accessible. Many use cases rely on private and/or local deployments, and we at Hugging Face are super excited to welcome <a href="https://huggingface.co/openai">OpenAI</a> to the community. We believe these will be long-lived, inspiring and impactful models.</p>
<h2> <a href="#contents"> </a> <span> Contents </span>
</h2>
<ul>
<li><a href="#welcome-gpt-oss-the-new-open-source-model-family-from-openai">Introduction</a></li>
<li><a href="#overview-of-capabilities-and-architecture">Overview</a></li>
<li><a href="#api-access-through-inference-providers">API access through Inference Providers</a></li>
<li><a href="#local-inference">Local Inference</a><ul>
<li><a href="#using-transformers">Using transformers</a><ul>
<li><a href="#flash-attention-3">Flash Attention 3</a></li>
<li><a href="#other-optimizations">Other optimizations</a></li>
<li><a href="#amd-rocm-support">AMD ROCm support</a></li>
<li><a href="#summary-of-available-optimizations">Summary of Optimizations</a></li>
</ul>
</li>
<li><a href="#llamacpp">llama.cpp</a></li>
<li><a href="#vllm">vLLM</a></li>
<li><a href="#transformers-serve">transformers serve</a></li>
</ul>
</li>
<li><a href="#fine-tuning">Fine Tuning</a></li>
<li><a href="#deploy-on-hugging-face-partners">Deploy on Hugging Face Partners</a><ul>
<li><a href="#azure">Azure</a></li>
<li><a href="#dell">Dell</a></li>
</ul>
</li>
<li><a href="#evaluating-the-model">Evaluating the Model</a></li>
<li><a href="#chats-and-chat-templates">Chats and Chat Templates</a><ul>
<li><a href="#system-and-developer-messages">System and Developer Messages</a></li>
<li><a href="#tool-use-with-transformers">Tool use with transformers</a></li>
</ul>
</li>
</ul>
<h2> <a href="#overview-of-capabilities-and-architecture"> </a> <span> Overview of Capabilities and Architecture </span>
</h2>
<ul>
<li>21B and 117B total parameters, with 3.6B and 5.1B <em>active</em> parameters, respectively. </li>
<li>4-bit quantization scheme using <a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">mxfp4</a> format. Only applied on the MoE weights. As stated, the 120B fits in a single 80 GB GPU and the 20B fits in a single 16GB GPU. </li>
<li>Reasoning, text-only models; with chain-of-thought and adjustable reasoning effort levels. </li>
<li>Instruction following and tool use support. </li>
<li>Inference implementations using transformers, vLLM, llama.cpp, and ollama. </li>
<li><a href="https://platform.openai.com/docs/api-reference/responses">Responses API</a> is recommended for inference. </li>
<li>License: Apache 2.0, with a small complementary use policy.</li>
</ul>
<p><strong>Architecture</strong></p>
<ul>
<li>Token-choice MoE with SwiGLU activations. </li>
<li>When calculating the MoE weights, a softmax is taken over selected experts (softmax-after-topk). </li>
<li>Each attention layer uses RoPE with 128K context. </li>
<li>Alternate attention layers: full-context, and sliding 128-token window. </li>
<li>Attention layers use a <em>learned attention sink</em> per-head, where the denominator of the softmax has an additional additive value. </li>
<li>It uses the same tokenizer as GPT-4o and other OpenAI API models. <ul>
<li>Some new tokens have been incorporated to enable compatibility with the Responses API.</li>
</ul>
</li>
</ul>
<figure> <img alt="Some benchmark results of OpenAI GPT OSS models" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/openai/gpt-oss-evals.png"> <figcaption> Benchmark results from OpenAI GPT OSS models, compared with <code>o3</code> and <code>o4-mini</code> (<b><i>Source: <a href="https://openai.com/open-models/">OpenAI</a></i></b>). </figcaption>
</figure> <h2> <a href="#api-access-through-inference-providers"> </a> <span> API access through Inference Providers </span>
</h2>
<p>OpenAI GPT OSS models are accessible through Hugging Face’s <a href="https://huggingface.co/docs/inference-providers/en/index">Inference Providers</a> service, allowing you to send requests to any supported provider using the same JavaScript or Python code. This is the same infrastructure that powers OpenAI’s official demo on <a href="http://gpt-oss.com/">gpt-oss.com</a>, and you can use it for your own projects.</p>
<p>Below is an example that uses Python and the super-fast Cerebras provider. For more info and additional snippets, check the <a href="https://huggingface.co/openai/gpt-oss-120b?inference_api=true&amp;inference_provider=auto&amp;language=python&amp;client=openai">inference providers section in the model cards</a> and the <a href="https://huggingface.co/docs/inference-providers/guides/gpt-oss">dedicated guide we crafted for these models</a>. </p>
<pre><code><span>import</span> os
<span>from</span> openai <span>import</span> OpenAI client = OpenAI( base_url=<span>"https://router.huggingface.co/v1"</span>, api_key=os.environ[<span>"HF_TOKEN"</span>],
) completion = client.chat.completions.create( model=<span>"openai/gpt-oss-120b:cerebras"</span>, messages=[ { <span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"How many rs are in the word 'strawberry'?"</span>, } ],
) <span>print</span>(completion.choices[<span>0</span>].message)
</code></pre>
<p>Inference Providers also implements an OpenAI-compatible Responses API, the most advanced OpenAI interface for chat models, designed for more flexible and intuitive interactions.<br>Below is an example using the Responses API with the Fireworks AI provider. For more details, check out the open-source <a href="https://github.com/huggingface/responses.js">responses.js</a> project.</p>
<pre><code><span>import</span> os
<span>from</span> openai <span>import</span> OpenAI client = OpenAI( base_url=<span>"https://router.huggingface.co/v1"</span>, api_key=os.getenv(<span>"HF_TOKEN"</span>),
) response = client.responses.create( model=<span>"openai/gpt-oss-20b:fireworks-ai"</span>, <span>input</span>=<span>"How many rs are in the word 'strawberry'?"</span>,
) <span>print</span>(response)
</code></pre>
<h2> <a href="#local-inference"> </a> <span> Local Inference </span>
</h2>
<h3> <a href="#using-transformers"> </a> <span> Using Transformers </span>
</h3>
<p>You need to install the latest <code>transformers</code> release (v4.55.1 or later), as well as <code>accelerate</code> and <code>kernels</code>. We also recommend installing triton 3.4 or better, as it unblocks support for <code>mxfp4</code> quantization on CUDA hardware:</p>
<pre><code>pip install --upgrade transformers kernels accelerate "triton&gt;=3.4"
</code></pre>
<p>The model weights are quantized in <code>mxfp4</code> format, which was originally available on GPUs of the Hopper or Blackwell families, but now works on previous CUDA architectures (including Ada, Ampere, and Tesla). Installing triton 3.4, together with the <code>kernels</code> library, makes it possible to download optimized <code>mxfp4</code> kernels on first use, achieving large memory savings. With these components in place, you can run the 20B model on GPUs with 16 GB of RAM. This includes many consumer cards (3090, 4090, 5080) as well as Colab and Kaggle!</p>
<p>If the previous libraries are not installed (or you don’t have a compatible GPU), loading the model will fall back to <code>bfloat16</code>, unpacked from the quantized weights.</p>
<p>The following snippet shows simple inference with the 20B model. As explained, it runs on 16 GB GPUs when using <code>mxfp4</code>, or ~48 GB in <code>bfloat16</code>.</p>
<pre><code><span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer model_id = <span>"openai/gpt-oss-20b"</span> tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained( model_id, device_map=<span>"auto"</span>, torch_dtype=<span>"auto"</span>,
) messages = [ {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"How many rs are in the word 'strawberry'?"</span>},
] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt=<span>True</span>, return_tensors=<span>"pt"</span>, return_dict=<span>True</span>,
).to(model.device) generated = model.generate(**inputs, max_new_tokens=<span>100</span>)
<span>print</span>(tokenizer.decode(generated[<span>0</span>][inputs[<span>"input_ids"</span>].shape[-<span>1</span>]:]))
</code></pre>
<h4> <a href="#flash-attention-3"> </a> <span> Flash Attention 3 </span>
</h4>
<p>The models use <em>attention sinks</em>, a technique the vLLM team made compatible with Flash Attention 3. We have packaged and integrated their optimized kernel in <a href="https://huggingface.co/kernels-community/vllm-flash-attn3"><code>kernels-community/vllm-flash-attn3</code></a>. At the time of writing, this super-fast kernel has been tested on Hopper cards with PyTorch 2.7 and 2.8. We expect increased coverage in the coming days. If you run the models on Hopper cards (for example, H100 or H200), you need to <code>pip install --upgrade kernels</code> and add the following line to your snippet:</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer model_id = "openai/gpt-oss-20b" tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained( model_id, device_map="auto", torch_dtype="auto",
<span>+ # Flash Attention with Sinks</span>
<span>+ attn_implementation="kernels-community/vllm-flash-attn3",</span>
) messages = [ {"role": "user", "content": "How many rs are in the word 'strawberry'?"},
] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt=True, return_tensors="pt", return_dict=True,
).to(model.device) generated = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(generated[0][inputs["input_ids"].shape[-1]:]))
</code></pre>
<p>This snippet will download the optimized, pre-compiled kernel code from <code>kernels-community</code>, as explained in our <a href="https://huggingface.co/blog/hello-hf-kernels">previous blog post</a>. The transformers team has built, packaged, and tested the code, so it’s totally safe for you to use.</p>
<h4> <a href="#other-optimizations"> </a> <span> Other optimizations </span>
</h4>
<p>We recommend you use <code>mxfp4</code> if your GPU supports it. If you can additionally use Flash Attention 3, then by all means do enable it! </p>
<blockquote>
<p>If your GPU is not compatible with <code>mxfp4</code>, then we recommend you use MegaBlocks MoE kernels for a nice speed bump. To do so, you just need to adjust your inference code like this:</p>
</blockquote>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer model_id = "openai/gpt-oss-20b" tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained( model_id, device_map="auto", torch_dtype="auto",
<span>+ # Optimize MoE layers with downloadable` MegaBlocksMoeMLP</span>
<span>+ use_kernels=True,</span>
) messages = [ {"role": "user", "content": "How many rs are in the word 'strawberry'?"},
] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=True, return_tensors="pt", return_dict=True,
).to(model.device) generated = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(generated[0][inputs["input_ids"].shape[-1]:]))
</code></pre>
<blockquote>
<p>MegaBlocks optimized MoE kernels require the model to run on <code>bfloat16</code>, so memory consumption will be higher than running on <code>mxfp4</code>. We recommend you use <code>mxfp4</code> if you can, otherwise opt in to MegaBlocks via <code>use_kernels=True</code>.</p>
</blockquote>
<h4> <a href="#amd-rocm-support"> </a> <span> AMD ROCm support </span>
</h4>
<p>OpenAI GPT OSS has been verified on AMD Instinct hardware, and we’re happy to announce initial support for AMD’s ROCm platform in our kernels library, setting the stage for upcoming optimized ROCm kernels in Transformers. MegaBlocks MoE kernel acceleration is already available for OpenAI GPT OSS on AMD Instinct (e.g., MI300-series), enabling better training and inference performance. You can test it with the same inference code shown above.</p>
<p>AMD also prepared a Hugging Face <a href="https://huggingface.co/spaces/amd/gpt-oss-120b-chatbot">Space</a> for users to try the model on AMD hardware.</p>
<h4> <a href="#summary-of-available-optimizations"> </a> <span> Summary of Available Optimizations </span>
</h4>
<p>At the time of writing, this table summarizes our <em>recommendations</em> based on GPU compatibility and our tests. We expect Flash Attention 3 (with sink attention) to become compatible with additional GPUs.</p>
<div> <table> <thead><tr>
<th></th>
<th>mxfp4</th>
<th>Flash Attention 3 (w/ sink attention)</th>
<th>MegaBlocks MoE kernels</th>
</tr> </thead><tbody><tr>
<td>Hopper GPUs (H100, H200)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CUDA GPUS with 16+ GB of RAM</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Other CUDA GPUs</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AMD Instinct (MI3XX)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><em>How to enable</em></td>
<td>triton 3.4 + kernels library</td>
<td>Use vllm-flash-attn3 from kernels-community</td>
<td><code>use_kernels</code></td>
</tr>
</tbody> </table>
</div>
<p>Even though the 120B model fits on a single H100 GPU (using <code>mxfp4</code>), you can also run it easily on multiple GPUs using <code>accelerate</code> or <code>torchrun</code>. Transformers provides a default parallelization plan, and you can leverage optimized attention kernels as well. The following snippet can be run with <code>torchrun --nproc_per_node=4 generate.py</code> on a system with 4 GPUs:</p>
<pre><code><span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer
<span>from</span> transformers.distributed <span>import</span> DistributedConfig
<span>import</span> torch model_path = <span>"openai/gpt-oss-120b"</span>
tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=<span>"left"</span>) device_map = { <span>"tp_plan"</span>: <span>"auto"</span>, <span># Enable Tensor Parallelism</span>
} model = AutoModelForCausalLM.from_pretrained( model_path, torch_dtype=<span>"auto"</span>, attn_implementation=<span>"kernels-community/vllm-flash-attn3"</span>, **device_map,
) messages = [ {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"Explain how expert parallelism works in large language models."</span>}
] inputs = tokenizer.apply_chat_template( messages, add_generation_prompt=<span>True</span>, return_tensors=<span>"pt"</span>, return_dict=<span>True</span>,
).to(model.device) outputs = model.generate(**inputs, max_new_tokens=<span>1000</span>) <span># Decode and print</span>
response = tokenizer.decode(outputs[<span>0</span>])
<span>print</span>(<span>"Model response:"</span>, response.split(<span>"&lt;|channel|&gt;final&lt;|message|&gt;"</span>)[-<span>1</span>].strip())
</code></pre>
<p>The OpenAI GPT OSS models have been trained extensively to leverage tool use as part of their reasoning efforts. The chat template we crafted for transformers provides a lot of flexibility, please check our <a href="#tool-use-with-transformers">dedicated section later in this post</a>.</p>
<h3> <a href="#llamacpp"> </a> <span> Llama.cpp </span>
</h3>
<p>Llama.cpp offers native MXFP4 support with Flash Attention, delivering optimal performance across various backends such as Metal, CUDA, and Vulkan, right from the day-0 release.</p>
<p>To install it, follow the guide in <a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md">llama.cpp Github’s repository</a>.</p>
<pre><code># MacOS
brew install llama.cpp # Windows
winget install llama.cpp
</code></pre>
<p>The recommended way is to use it via llama-server:</p>
<pre><code>llama-server -hf ggml-org/gpt-oss-120b-GGUF -c 0 -fa --jinja --reasoning-format none # Then, access http://localhost:8080
</code></pre>
<p>We support both the 120B and 20B models. For more detailed information, visit <a href="https://github.com/ggml-org/llama.cpp/pull/15091">this PR</a> or the <a href="https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf">GGUF model collection</a>.</p>
<h3> <a href="#vllm"> </a> <span> vLLM </span>
</h3>
<p>As mentioned, vLLM developed optimized Flash Attention 3 kernels that support sink attention, so you’ll get best results on Hopper cards. Both the Chat Completion and the Responses APIs are supported. You can install and start a server with the following snippet, which assumes 2 H100 GPUs are used:</p>
<pre><code>vllm serve openai/gpt-oss-120b --tensor-parallel-size 2
</code></pre>
<p>Or, use it in Python directly like:</p>
<pre><code><span>from</span> vllm <span>import</span> LLM
llm = LLM(<span>"openai/gpt-oss-120b"</span>, tensor_parallel_size=<span>2</span>)
output = llm.generate(<span>"San Francisco is a"</span>)
</code></pre>
<h3> <a href="#transformers-serve"> </a> <span> <code>transformers serve</code> </span>
</h3>
<p>You can use <a href="https://huggingface.co/docs/transformers/main/serving"><code>transformers serve</code></a> to experiment locally with the models, without any other dependencies. You can launch the server with just:</p>
<pre><code>transformers serve
</code></pre>
<p>To which you can send requests using the <a href="https://platform.openai.com/docs/api-reference/responses">Responses API</a>. </p>
<pre><code><span># </span><span>responses API</span>
curl -X POST http://localhost:8000/v1/responses \
-H "Content-Type: application/json" \
-d '{"input": [{"role": "system", "content": "hello"}], "temperature": 1.0, "stream": true, "model": "openai/gpt-oss-120b"}'
</code></pre>
<p>You can also send requests using the standard Completions API:</p>
<pre><code><span># </span><span>completions API</span>
curl -X POST http://localhost:8000/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{"messages": [{"role": "system", "content": "hello"}], "temperature": 1.0, "max_tokens": 1000, "stream": true, "model": "openai/gpt-oss-120b"}'
</code></pre>
<h2> <a href="#fine-tuning"> </a> <span> Fine-Tuning </span>
</h2>
<p>GPT OSS models are fully integrated with <code>trl</code>. We have developed a couple of fine-tuning examples using <code>SFTTrainer</code> to get you started:</p>
<ul>
<li>A LoRA example in the <a href="https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers">OpenAI cookbook</a>, which shows how the model can be fine-tuned to reason in multiple languages. </li>
<li><a href="https://github.com/huggingface/gpt-oss-recipes/blob/main/sft.py">A basic fine-tuning script</a> that you can adapt to your needs.</li>
</ul>
<h2> <a href="#deploy-on-hugging-face-partners"> </a> <span> Deploy on Hugging Face Partners </span>
</h2>
<h3> <a href="#azure"> </a> <span> Azure </span>
</h3>
<p>Hugging Face collaborates with Azure on their Azure AI Model Catalog to bring the most popular open-source models —spanning text, vision, speech, and multimodal tasks— directly into customers environments for secured deployments to managed online endpoints, leveraging Azure’s enterprise-grade infrastructure, autoscaling, and monitoring.</p>
<p>The GPT OSS models are now available on the Azure AI Model Catalog (<a href="https://ai.azure.com/explore/models/openai-gpt-oss-20b/version/1/registry/HuggingFace">GPT OSS 20B</a>, <a href="https://ai.azure.com/explore/models/openai-gpt-oss-120b/version/1/registry/HuggingFace">GPT OSS 120B</a>), ready to be deployed to an online endpoints for real time inference.</p>
<p><img alt="model card in azure ai model catalog" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/gpt-oss/partners/azure-model-card.png"></p><h3> <a href="#dell"> </a> <span> Dell </span>
</h3>
<p>The Dell Enterprise Hub is a secure online portal that simplifies training and deploying the latest open AI models on-premise using Dell platforms. Developed in collaboration with Dell, it offers optimized containers, native support for Dell hardware, and enterprise-grade security features. </p>
<p>The GPT OSS models are now available on <a href="https://dell.huggingface.co/">Dell Enterprise Hub</a>, ready to be deployed on-prem using Dell platforms.</p>
<p><img alt="model card in dell enterprise hub" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/gpt-oss/partners/deh-model-card.png"></p><h2> <a href="#evaluating-the-model"> </a> <span> Evaluating the Model </span>
</h2>
<p>GPT OSS models are reasoning models: they therefore require a very large generation size (maximum number of new tokens) for evaluations, as their generation will first contain reasoning, then the actual answer. Using too small a generation size risks interrupting the prediction in the middle of reasoning, which will cause false negatives. The reasoning trace should then be removed from the model answer before computing metrics, to avoid parsing errors, especially with math or instruct evaluations.</p>
<p>Here’s an example on how to evaluate the models with lighteval (you need to install from source).</p>
<pre><code>git clone https://github.com/huggingface/lighteval
pip install -e .[dev] # make sure you have the correct transformers version installed!
lighteval accelerate \ "model_name=openai/gpt-oss-20b,max_length=16384,skip_special_tokens=False,generation_parameters={temperature:1,top_p:1,top_k:40,min_p:0,max_new_tokens:16384}" \ "extended|ifeval|0|0,lighteval|aime25|0|0" \ --save-details --output-dir "openai_scores" \ --remove-reasoning-tags --reasoning-tags="[('&lt;|channel|&gt;analysis&lt;|message|&gt;','&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;')]" </code></pre>
<p>For the 20B model, this should give you 69.5 (+/-1.9) for IFEval (strict prompt), and 63.3 (+/-8.9) for AIME25 (in pass@1), scores within expected range for a reasoning model of this size.</p>
<p>If you want to do your custom evaluation script, note that to filter out the reasoning tags properly, you will need to use <code>skip_special_tokens=False</code> in the tokenizer, in order to get the full trace in the model output (to filter reasoning using the same string pairs as in the example above) - you can discover why below.</p>
<h2> <a href="#chats-and-chat-templates"> </a> <span> Chats and Chat Templates </span>
</h2>
<p>OpenAI GPT OSS uses the concept of “channels” in its outputs. Most of the time, you will see an “analysis” channel that contains things that are not intended to be sent to the end-user, like chains of thought, and a “final” channel containing messages that are actually intended to be displayed to the user. </p>
<p>Assuming no tools are being used, the structure of the model output looks like this:</p>
<pre><code>&lt;|start|&gt;assistant&lt;|channel|&gt;analysis&lt;|message|&gt;CHAIN_OF_THOUGHT&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;ACTUAL_MESSAGE
</code></pre>
<p>Most of the time, you should ignore everything except the text after <strong>&lt;|channel|&gt;final&lt;|message|&gt;.</strong> Only this text should be appended to the chat as the assistant message, or displayed to the user. There are two exceptions to this rule, though: You may need to include <strong>analysis</strong> messages in the history during <strong>training</strong> or if the model is <strong>calling external tools.</strong></p>
<p><strong>When training:</strong>
If you’re formatting examples for training, you generally want to include the chain of thought in the final message. The right place to do this is in the <strong>thinking</strong> key.</p>
<pre><code>chat = [ {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"Hi there!"</span>}, {<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: <span>"Hello!"</span>}, {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"Can you think about this one?"</span>}, {<span>"role"</span>: <span>"assistant"</span>, <span>"thinking"</span>: <span>"Thinking real hard..."</span>, <span>"content"</span>: <span>"Okay!"</span>}
] <span># add_generation_prompt=False is generally only used in training, not inference</span>
inputs = tokenizer.apply_chat_template(chat, add_generation_prompt=<span>False</span>)
</code></pre>
<p>You can feel free to include <strong>thinking</strong> keys in previous turns, or when you’re doing inference rather than training, but they will generally be ignored. The chat template will only ever include the most recent chain of thought, and only in training (when <code>add_generation_prompt=False</code> and the final turn is an assistant turn).</p>
<p>The reason why we do it this way is subtle: The OpenAI gpt-oss models were trained on multi-turn data where all but the final chain of thought was dropped. This means that when you want to fine-tune an OpenAI <code>gpt-oss</code> model, you should do the same.</p>
<ul>
<li>Let the chat template drop all chains of thought except the final one </li>
<li>Mask the labels on all turns except the final assistant turn, or else you will be training it on the previous turns without chains of thought, which will teach it to emit responses without CoTs. This means that you cannot train on an entire multi-turn conversation as a single sample; instead, you must break it into one sample per assistant turn with only the final assistant turn unmasked each time, so that the model can learn from each turn while still correctly only seeing a chain of thought on the final message each time.</li>
</ul>
<h3> <a href="#system-and-developer-messages"> </a> <span> System and Developer Messages </span>
</h3>
<p>OpenAI GPT OSS is unusual because it distinguishes between a “system” message and a “developer” message at the start of the chat, but most other models only use “system”. In GPT OSS, the system message follows a strict format and contains information like the current date, the model identity and the level of reasoning effort to use, and the “developer” message is more freeform, which makes it (very confusingly) similar to the “system” messages of most other models.</p>
<p>To make GPT OSS easier to use with the standard API, the chat template will treat a message with “system” or “developer” role as the <strong>developer</strong> message. If you want to modify the actual system message, you can pass the specific arguments <strong>model_identity</strong> or <strong>reasoning_effort</strong> to the chat template:</p>
<pre><code>chat = [ {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"This will actually become a developer message!"</span>}
] tokenizer.apply_chat_template( chat, model_identity=<span>"You are OpenAI GPT OSS."</span>, reasoning_effort=<span>"high"</span> <span># Defaults to "medium", but also accepts "high" and "low"</span>
)
</code></pre>
<h3> <a href="#tool-use-with-transformers"> </a> <span> Tool Use With transformers </span>
</h3>
<p>GPT OSS supports two kinds of tools: The “builtin” tools <strong>browser</strong> and <strong>python</strong>, and custom tools supplied by the user. To enable builtin tools, pass their names in a list to the <strong>builtin_tools</strong> argument of the chat template, as shown below. To pass custom tools, you can pass them either as JSON schema or as Python functions with type hints and docstrings using the tools argument. See the <a href="https://huggingface.co/docs/transformers/en/chat_extras">chat template tools documentation</a> for more details, or you can just modify the example below:</p>
<pre><code><span>def</span> <span>get_current_weather</span>(<span>location: <span>str</span></span>): <span>"""</span>
<span> Returns the current weather status at a given location as a string.</span>
<span></span>
<span> Args:</span>
<span> location: The location to get the weather for.</span>
<span> """</span> <span>return</span> <span>"Terrestrial."</span> <span># We never said this was a good weather tool</span> chat = [ {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"What's the weather in Paris right now?"</span>}
] inputs = tokenizer.apply_chat_template( chat, tools=[weather_tool], builtin_tools=[<span>"browser"</span>, <span>"python"</span>], add_generation_prompt=<span>True</span>, return_tensors=<span>"pt"</span>
)
</code></pre>
<p>If the model chooses to call a tool (indicated by a message ending in <code>&lt;|call|&gt;</code>), then you should add the tool call to the chat, call the tool, then add the tool result to the chat and generate again:</p>
<pre><code>tool_call_message = { <span>"role"</span>: <span>"assistant"</span>, <span>"tool_calls"</span>: [ { <span>"type"</span>: <span>"function"</span>, <span>"function"</span>: { <span>"name"</span>: <span>"get_current_temperature"</span>, <span>"arguments"</span>: {<span>"location"</span>: <span>"Paris, France"</span>} } } ]
}
chat.append(tool_call_message) tool_output = get_current_weather(<span>"Paris, France"</span>) tool_result_message = { <span># Because GPT OSS only calls one tool at a time, we don't</span> <span># need any extra metadata in the tool message! The template can</span> <span># figure out that this result is from the most recent tool call.</span> <span>"role"</span>: <span>"tool"</span>, <span>"content"</span>: tool_output
}
chat.append(tool_result_message) <span># You can now apply_chat_template() and generate() again, and the model can use</span>
<span># the tool result in conversation.</span>
</code></pre>
<h2> <a href="#acknowledgements"> </a> <span> Acknowledgements </span>
</h2>
<p>This is an important release for the community and it took a momentous effort across teams and companies to comprehensively support the new models in the ecosystem.</p>
<p>The authors of this blog post were selected among the ones who contributed content to the post itself, and does not represent dedication to the project. In addition to the author list, others contributed significant content reviews, including Merve and Sergio. Thank you!</p>
<p>The integration and enablement work involved dozens of people. In no particular order, we'd like to highlight Cyril, Lysandre, Arthur, Marc, Mohammed, Nouamane, Harry, Benjamin, Matt from the open source team. From the TRL team, Ed, Lewis, and Quentin were all involved. We'd also like to thank Clémentine from Evaluations, and David and Daniel from the Kernels team. On the commercial partnerships side we got significant contributions from Simon, Alvaro, Jeff, Akos, Alvaro, and Ivar. The Hub and Product teams contributed Inference Providers support, llama.cpp support, and many other improvements, all thanks to Simon, Célina, Pierric, Lucain, Xuan-Son, Chunte, and Julien. Magda and Anna were involved from the legal team.</p>
<p>Hugging Face's role is to enable the community to use these models effectively. We are indebted to companies such as vLLM for advancing the field, and cherish our continued collaboration with inference providers to provide ever simpler ways to build on top of them.</p>
<p>And of course, we deeply appreciate OpenAI's decision to release these models for the community at large. Here's to many more!</p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>