<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="UTF-8">
<title>Le Pentagone menace Anthropic de la consid�rer comme � risque pour la cha�ne d'approvisionnement � s'il n'a pas un acc�s total � son IA Claude pour le ciblage d'armes autonomes et la surveillance de masse</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>Le Pentagone menace Anthropic de la consid�rer comme � risque pour la cha�ne d'approvisionnement � s'il n'a pas un acc�s total � son IA Claude pour le ciblage d'armes autonomes et la surveillance de masse</h1>
  <div class="metadata">
    Source: Developpez.com | Date: 2/17/2026 3:54:00 PM | <a href="https://intelligence-artificielle.developpez.com/actu/380318/Le-Pentagone-menace-Anthropic-de-la-considerer-comme-risque-pour-la-chaine-d-approvisionnement-s-il-n-a-pas-un-acces-total-a-son-IA-Claude-pour-le-ciblage-d-armes-autonomes-et-la-surveillance-de-masse/" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: FR
  </div>
  <div class="content">
    <div><div> <p><img src="https://www.developpez.com/images/logos/intelligence-artificielle2.png"> <b>Les militaires am�ricains veulent un <a href="https://intelligence-artificielle.developpez.com/actu/379751/" target="_blank">acc�s illimit� � la technologie IA Claude pour le ciblage d'armes autonomes et la surveillance domestique</a>, mais Anthropic dit non. En <a href="https://intelligence-artificielle.developpez.com/actu/379751/" target="_blank">refusant de lever ses garde-fous �thiques sur l'usage militaire de Claude</a>, Anthropic se retrouve dans la ligne de mire de Pete Hegseth, secr�taire � la D�fense des �tats-Unis. Le Pentagone menace de classer l'entreprise en � risque pour la cha�ne d'approvisionnement � � une d�signation d'ordinaire r�serv�e aux adversaires �trangers. Derri�re ce conflit embl�matique, c'est toute la question de la gouvernance de l'IA de combat qui est pos�e : qui contr�le, in fine, les syst�mes d'intelligence artificielle d�ploy�s sur les champs de bataille ?</b></p><p>Il y a moins d'un an, la relation entre Anthropic et le Pentagone semblait exemplaire. � l'�t� 2025, les deux parties signaient un contrat pouvant atteindre 200 millions de dollars, et Claude devenait le premier mod�le d'IA frontier � �tre int�gr� dans les r�seaux classifi�s du D�partement de la D�fense am�ricain. Un exploit technique et un signal politique fort : l'administration Biden, puis la transition vers Trump, avaient toutes deux reconnu la sup�riorit� de Claude pour les applications gouvernementales sensibles.</p><p>Selon Axios, Claude est aujourd'hui le seul mod�le d'IA disponible dans les syst�mes classifi�s de l'arm�e am�ricaine, et il se positionne en t�te sur de nombreuses applications professionnelles. Huit des dix plus grandes entreprises am�ricaines utilisent d�j� Claude dans leurs workflows. Une p�n�tration commerciale et institutionnelle qui conf�rait � Anthropic un avantage consid�rable � et qui rend aujourd'hui la rupture d'autant plus compliqu�e pour les deux camps.</p><p>Mais les fissures sont apparues rapidement. Des mois de n�gociations tendues se sont engag�s autour des conditions d'utilisation de Claude : le Pentagone souhaite pouvoir <a href="https://intelligence-artificielle.developpez.com/actu/379751/" target="_blank">d�ployer les mod�les d'Anthropic pour � toutes les fins l�gales �</a> (all lawful purposes), une formulation qui recouvre le d�veloppement d'armements, la collecte de renseignements et les op�rations sur le champ de bataille. Anthropic, pour sa part, <a href="https://intelligence-artificielle.developpez.com/actu/379751/" target="_blank">maintient deux lignes rouges absolues</a> : l'interdiction d'utiliser ses mod�les pour la surveillance de masse des citoyens am�ricains, et l'interdiction de contr�ler des armes enti�rement autonomes, c'est-�-dire des syst�mes capables de tirer sans intervention humaine.</p><p><img src="https://www.developpez.net/forums/attachments/p674309d1/a/a/a"></p>
<p><b><span>L'op�ration Maduro : l'�tincelle qui a mis le feu aux poudres</span></b></p><p>La crise a atteint son paroxysme avec une r�v�lation g�nante : lors du raid spectaculaire ayant conduit � la capture du pr�sident v�n�zu�lien Nicol�s Maduro le 3 janvier 2026 � l'op�ration baptis�e � Absolute Resolve � � le Pentagone a utilis� l'IA Claude d'Anthropic, notamment pour la planification et la conduite de l'op�ration. L'IA a �t� d�ploy�e via le partenariat d'Anthropic avec la soci�t� de logiciels Palantir, elle-m�me sous-traitante historique du D�partement de la D�fense. L'op�ration a caus� 83 morts et mobilis� des centaines d'a�ronefs ainsi que des tirs de missiles.</p><p>Selon les sources cit�es par Axios, un cadre d'Anthropic a pris contact avec un cadre de Palantir pour s'enqu�rir de l'usage qui avait �t� fait de Claude lors du raid. Cette d�marche a �t� interpr�t�e par les responsables du Pentagone comme une d�sapprobation implicite : � Cela a �t� soulev� d'une mani�re � sugg�rer qu'ils pourraient ne pas approuver l'utilisation de leur logiciel, car il y a eu des tirs r�els lors de ce raid, des gens ont �t� bless�s �, a d�clar� le responsable am�ricain.</p><p>Anthropic a formellement d�menti ce r�cit. Un porte-parole de la soci�t� a indiqu� que la compagnie n'avait � pas discut� de l'utilisation de Claude pour des op�rations sp�cifiques avec le D�partement de la Guerre � et que ses conversations avec le Pentagone se concentraient sur � des questions pr�cises de politique d'utilisation � notamment ses lignes rouges concernant les armes enti�rement autonomes et la surveillance domestique de masse � aucune d'elles n'ayant de lien avec les op�rations en cours �. </p><p>Qu'importe : le mal �tait fait. Des sources proches du dossier ont indiqu� � Axios que les hauts responsables de la d�fense �taient frustr�s par Anthropic depuis un certain temps et ont � saisi l'opportunit� de d�clencher un affrontement public �. Autrement dit, la crise n'est pas totalement spontan�e : elle a �t�, au moins en partie, d�lib�r�ment orchestr�e par le Pentagone.</p><div>
<p></p>
</div>
<p><b><span>La menace nucl�aire : le statut de � risque pour la cha�ne d'approvisionnement �</span></b></p><p>Le 16 f�vrier 2026, Axios r�v�le la nature exacte de la menace brandie par Pete Hegseth, et elle est d'une gravit� exceptionnelle. Le secr�taire � la D�fense serait � proche � de couper les liens commerciaux avec Anthropic et de d�signer l'entreprise comme un � risque pour la cha�ne d'approvisionnement � � ce qui signifierait que tout sous-traitant souhaitant conserver ses contrats avec le Pentagone devrait couper toute relation avec Anthropic. Un responsable anonyme du D�partement de la D�fense a d�clar� sans ambages : � Ce sera un �norme casse-t�te � d�manteler, et nous allons nous assurer qu'ils paient le prix de nous avoir forc� la main comme �a. �</p><p>Cette d�signation de � risque pour la cha�ne d'approvisionnement � est normalement utilis�e pour �tiqueter des adversaires �trangers et d'autres acteurs hostiles, plut�t que des entreprises am�ricaines � ce qui rend la menace inhabituellement s�v�re pour une soci�t� consid�r�e comme l'une des �toiles montantes de la technologie nationale.</p><p>Le Pentagone a �galement envisag� d'exiger que tous ses fournisseurs et sous-traitants certifient qu'ils n'utilisent aucun mod�le Anthropic. Une telle mesure, appliqu�e dans un secteur o� Claude est devenu omnipr�sent, aurait des r�percussions consid�rables bien au-del� du simple contrat de 200 millions de dollars. Amazon, par exemple � l'un des plus grands investisseurs d'Anthropic avec plus de 8 milliards de dollars engag�s � a vu son cours boursier souffrir de ces r�v�lations.</p><p><b><span>La valeur strat�gique du conflit : Anthropic, seul ma�tre � bord des syst�mes classifi�s</span></b></p><p>L'ironie de cette situation tient � un paradoxe central : bien que le Pentagone soit en n�gociations avec OpenAI, Google et xAI, un responsable de l'administration a conc�d� que les mod�les concurrents � sont simplement en retard � pour les applications gouvernementales sp�cialis�es, ce qui complique s�rieusement une transition abrupte. </p><p>OpenAI, Google et xAI ont tous accept� ce standard dans les syst�mes non classifi�s des militaires, et des n�gociations sont en cours pour les syst�mes classifi�s. Le Pentagone est confiant qu'ils accepteront le principe de � tout usage l�gal �. En d'autres termes, Anthropic est le seul r�calcitrant � mais aussi, pour l'instant, le seul � op�rer dans les environnements les plus sensibles.</p><p>Le contrat menac� d'annulation ne repr�sente que 200 millions de dollars, une fraction infime du chiffre d'affaires annuel de 14 milliards de dollars d'Anthropic. Ce qui est en jeu n'est donc pas d'abord financier � c'est symbolique, politique, et surtout syst�mique. Si Anthropic c�de, elle valide l'id�e que n'importe quel acteur de l'IA peut se voir contraint d'abandonner ses garde-fous au nom de la � l�galit� � telle qu'interpr�t�e unilat�ralement par un client gouvernemental. Si elle r�siste, elle envoie un signal fort � toute l'industrie, au risque d'�tre exclue d'un march� militaire en pleine expansion.</p><div>
<p></p>
</div>
<p><b><span>Le choc des cultures : l'IA constitutionnelle face � la doctrine militaire</span></b></p><p>Fond�e par les fr�res Dario et Daniela Amodei � anciens directeurs de la recherche chez OpenAI � Anthropic a �t� cr��e comme une soci�t� d'int�r�t public et se pr�sente depuis ses d�buts comme un laboratoire � la pointe de l'IA � responsable �. Ses mod�les Claude sont form�s selon une approche dite � d'IA constitutionnelle �, avec des <a href="https://intelligence-artificielle.developpez.com/actu/379543/" target="_blank">r�gles internes con�ues pour rejeter les demandes impliquant des dommages physiques, des violations des droits humains ou une surveillance invasive</a>.</p><p>Ces principes ne sont pas de simples slogans marketing. Pour justifier sa prudence, un responsable d'Anthropic a expliqu� � Axios que les lois existantes � n'ont en aucune fa�on rattrap� ce que l'IA peut faire �. Le risque est concret : avec Claude, le Pentagone pourrait analyser en continu l'ensemble des publications sur les r�seaux sociaux de chaque Am�ricain, crois�es avec des donn�es publiques comme les listes �lectorales, les permis de port d'arme ou les autorisations de manifestation, pour �tablir automatiquement des profils de surveillance civile.</p><p>Du c�t� du Pentagone, on ne nie pas les capacit�s de surveillance de l'IA � on argue simplement qu'elles sont � l�gales �. Le d�partement de la D�fense peut d�j� collecter d'�normes quantit�s d'informations sur les individus, des publications sur les r�seaux sociaux aux permis de port d'arme, et des experts en vie priv�e soulignent que l'IA peut d�multiplier cette capacit� pour cibler des civils. C'est pr�cis�ment cette zone grise � ce que la loi permet aujourd'hui, et ce que l'IA rend techniquement possible demain � qu'Anthropic refuse de laisser sans garde-fous.</p><p>La politique d'utilisation acceptable d'Anthropic interdit explicitement l'utilisation de Claude pour faciliter des activit�s violentes, pour le d�veloppement d'armes ou pour des activit�s de surveillance. Ces restrictions ne sont pas lev�es pour les utilisateurs militaires ou gouvernementaux, sauf si le contrat inclut des garanties sp�cifiques qu'Anthropic juge ad�quates.</p><p><img src="https://www.developpez.net/forums/attachments/p674310d1/a/a/a"></p>
<p><b><span>Le message envoy� � toute l'industrie</span></b></p><p>La d�claration du porte-parole en chef du...
</p><p>La fin de cet article est r�serv�e aux abonn�s. Soutenez le Club Developpez.com en <a href="https://premium.developpez.com/abonnement">prenant un abonnement</a> pour que nous puissions continuer � vous proposer des publications.</p> </div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>