<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Accelerating Qwen3-8B Agent on Intel Core Ultra with Depth-Pruned Draft Models</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Accelerating Qwen3-8B Agent on Intel Core Ultra with Depth-Pruned Draft Models</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 9/29/2025 2:00:00 AM | Lang: EN |
    <a href="https://huggingface.co/blog/intel-qwen3-agent" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/imargulis"><img alt="Igor Margulis's avatar" src="https://huggingface.co/avatars/a09cbec3bcd29b5093ce30b3c47e27f6.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/ofirzaf"><img alt="Ofir Zafrir's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1616423186722-5f8907c65d083370c711f284.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/sguskin"><img alt="Shira Guskin's avatar" src="https://huggingface.co/avatars/bd13e3c006573db51fcdf74b016d1aa3.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/guybd"><img alt="Guy Boudoukh's avatar" src="https://huggingface.co/avatars/06df4ead5a2014480c128103b9862f98.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/pcuenq"><img alt="Pedro Cuenca's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg"></a> </span> </span></p> </div></div> <div><nav><ul><li><a href="#qwen3">Qwen3</a> <ul></ul> </li><li><a href="#accelerating-qwen3-8b-on-intel-core-ultra-with-speculative-decoding">Accelerating Qwen3-8B on Intel Core Ultra with Speculative Decoding</a> <ul></ul> </li><li><a href="#pushing-performance-further">Pushing Performance Further</a> <ul></ul> </li><li><a href="#integration-with-smolagents">Integration with smolagents</a> <ul><li><a href="#references">References</a> <ul></ul> </li></ul> </li></ul></nav></div><p><a href="https://huggingface.co/blog/assets/intel-qwen3-agent/smolagents-1300-650.png"><img alt="banner image" src="https://huggingface.co/blog/assets/intel-qwen3-agent/smolagents-1300-650.png"></a></p> <p>TL;DR:</p>
<ul>
<li><p><a href="https://huggingface.co/Qwen/Qwen3-8B">Qwen3-8B</a> is one of the most exciting recent releases—a model with native agentic capabilities, making it a natural fit for the AIPC.</p>
</li>
<li><p>With <a href="https://github.com/openvinotoolkit/openvino.genai">OpenVINO.GenAI</a>, we’ve been able to accelerate generation by ~1.3× using speculative decoding with a lightweight Qwen3-0.6B draft.</p>
</li>
<li><p>By using speculative decoding and applying a simple pruning process to the draft, we pushed the speedup even further to ~1.4×</p>
</li>
<li><p>We wrapped this up by showing how these improvements can be used to run a fast, local AI Agent with <a href="https://github.com/huggingface/smolagents">smolagents</a></p>
</li>
</ul>
<h2> <a href="#qwen3"> <span></span> </a> <span> Qwen3 </span>
</h2>
<p>Qwen3-8B is part of the latest Qwen family, trained with explicit agentic behaviors. It supports tool invocation, multi-step reasoning, and long-context handling capabilities, that make it well-suited for complex agent workflows. When integrated with frameworks like Hugging Face smolagents, QwenAgent, or AutoGen, it enables a wide range of agentic applications built around tool use and reasoning. Unlike single-turn chatbots, agentic applications rely on reasoning models that produce “thinking aloud” traces, intermediate steps that expand token usage, making inference speed critical to responsiveness.
The combination of optimized inference and built-in agentic intelligence makes Qwen3-8B a compelling foundation for next-gen AI agents.</p>
<h2> <a href="#accelerating-qwen3-8b-on-intel-core-ultra-with-speculative-decoding"> <span></span> </a> <span> Accelerating Qwen3-8B on Intel Core Ultra with Speculative Decoding </span>
</h2>
<p>We started by benchmarking the 4-bit optimized OpenVINO version of Qwen3-8B on an Intel Lunar Lake integrated GPU, establishing this as our baseline for further acceleration</p>
<p><a href="https://arxiv.org/abs/2211.17192">Speculative decoding</a> is a method to speed up auto-regressive generation. It works by using a smaller, faster model as a draft to propose multiple tokens in a single forward pass, which are then validated by the larger target model in one forward pass. In our setup, <a href="https://huggingface.co/OpenVINO/Qwen3-8B-int4-ov">Qwen3-8B</a> served as the target model while <a href="https://huggingface.co/OpenVINO/Qwen3-0.6B-int8-ov">Qwen3-0.6B</a> was used as the draft. This approach delivered an average of 1.3× speedup over the baseline.</p>
<pre><code><span>from</span> openvino_genai <span>import</span> LLMPipeline, draft_model target_path = <span>"/path/to/target/Qwen3-8B-int4-ov"</span>
draft_path = <span>"/path/to/draft/Qwen3-0.6B-int8-ov"</span>
device = <span>"GPU"</span> model = LLMPipeline(target_path, device, draft_model=draft_model(draft_path, device)) streamer = <span>lambda</span> x: <span>print</span>(x, end=<span>""</span>, flush=<span>True</span>)
model.generate(<span>"What is speculative decoding and how does it improve inference speed?"</span>, max_new_tokens=<span>100</span>, streamer=streamer)
</code></pre>
<blockquote>
<p>Before initializing the <code>LLMPipeline</code>, make sure both the target and draft models are converted to OpenVINO. You can either download pre-converted models from the provided links or follow these <a href="https://huggingface.co/docs/optimum-intel/en/openvino/export">instructions</a> to convert your own.</p>
</blockquote>
<h2> <a href="#pushing-performance-further"> <span></span> </a> <span> Pushing Performance Further </span>
</h2>
<p> The speculative decoding speedup depends on the average number of generated tokens per forward step of the target, <span><span>γ \gamma </span></span>, the speculation window size, and the ratio between the target and draft models' latency <span><span>c c </span></span>. A smaller, faster (though less accurate) draft can often deliver greater acceleration. This inspired us to shrink the draft model while still preserving its quality, i.e. <span><span>E(#generated_tokens) E(\# generated\_tokens) </span></span>.</p>
<p><span><span><span>Speedup=E(#generated_tokens)γc+1
Speedup = \frac{E(\# generated\_tokens)}{\gamma c + 1}
</span></span></span></p>
<p>Our <a href="https://huggingface.co/papers/2411.11055">recent work</a> shows that model depth (number of layers) is a major contributor to inference latency.
We drew inspiration from recent work on layer-wise compression[1]. In our approach, we identify blocks of layers that contribute little, measured using angular distance, and remove them. After pruning, we apply fine-tuning to recover accuracy. Using this method, we pruned 6 out of 28 layers from the Qwen3-0.6B draft model.
To recover the quality of the pruned draft model, we further finetuned it using synthetic data generated by Qwen3-8B.
The data was produced by generating responses to 500k prompts from <a href="https://huggingface.co/datasets/BAAI/Infinity-Instruct">BAAI/Infinity-Instruct dataset</a>.</p>
<p>The resulting pruned draft model delivered ~1.4x speedup compared to the baseline, an improvement over the ~1.3× gain achieved with the original draft. This outcome aligns with theoretical expectations - reducing draft latency improves the over-all speedup, enabling faster and more efficient inference.</p>
<p>This demonstrates how pruning + speculative decoding can unlock faster and more efficient inference—making local AI agents even more practical.</p>
<p>Check out the <a href="https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/supplementary_materials/notebooks/qwen-3/qwen3.ipynb">notebook</a> and the Qwen3-0.6B depth-pruned <a href="https://huggingface.co/OpenVINO/Qwen3-pruned-6L-from-0.6B-int8-ov">draft model</a> to reproduce our results step by step</p>
<h2> <a href="#integration-with-smolagents"> <span></span> </a> <span> Integration with smolagents </span>
</h2>
<p>To showcase the real-world potential, we deployed our optimized setup with the smolagents library. With this integration, developers can plug in Qwen3-8B (paired with our pruned draft) to build agents that call APIs and external tools, write and execute code, handle long-context reasoning and run efficiently on Intel Core Ultra.
The benefits aren’t limited to Hugging Face, this model pairing can also be used seamlessly with frameworks like AutoGen or QwenAgent, further strengthening the agentic ecosystem.</p>
<p>In our demo, we assigned the accelerated Qwen3-based agent a task: Summarize the key features of the Qwen3 model series and present them in a slide deck.</p>
<p>Here’s how it worked:
1.	The agent used a web search tool to gather up-to-date information.
2.	It then switched to the Python interpreter to generate slides with the <code>python-pptx</code> library.
This simple workflow highlights just a fraction of the possibilities unlocked when accelerated Qwen3 models meet frameworks like smolagents, bringing practical, efficient AI agents to life on AI PC. Try it <a href="https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/supplementary_materials/notebooks/qwen-3/smolagents/qwen3_agent.ipynb">here</a> </p> <h3> <a href="#references"> <span></span> </a> <span> References </span>
</h3>
<p>[1] Gromov, A., Tirumala, K., Shapourian, H., Glorioso, P., &amp; Roberts, D. A. (2025, January 22). The unreasonable ineffectiveness of the deeper layers. Poster presented at ICLR 2025. <a href="https://arxiv.org/abs/2403.17887">https://arxiv.org/abs/2403.17887</a></p>
<blockquote>
<p><strong>Performance and legal notices</strong></p>
<ul>
<li>Performance results are based on internal benchmarking with OpenVINO 2025.2 as of September 2025, using a configuration with an Intel Core Ultra 7 268V 2.20 GHz processor with an integrated Intel Arc 140V GPU, paired with 32 GB of DDR5 memory.</li>
<li>Performance varies by use, configuration and other factors. Learn more at <a href="http://www.intel.com/PerformanceIndex">www.Intel.com/PerformanceIndex</a>.</li>
<li>No product or component can be absolutely secure.</li>
<li>Your costs and results may vary.</li>
<li>Intel technologies may require enabled hardware, software, or service activation.</li>
<li> Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.</li>
<li>Other names and brands may be claimed as the property of others.</li>
</ul>
</blockquote>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>