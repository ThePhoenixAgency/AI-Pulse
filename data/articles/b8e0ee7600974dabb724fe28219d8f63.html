<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>'The world is in peril' — 5 reasons why the AI apocalypse might be closer than you think</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>'The world is in peril' — 5 reasons why the AI apocalypse might be closer than you think</h1>
  <div class="metadata">
<<<<<<< HEAD
    Source: TechRadar | Date: 2/16/2026 6:00:00 PM | Lang: EN |
=======
    Source: TechRadar | Date: 2/16/2026 5:00:00 PM | Lang: EN |
>>>>>>> 48d6193da6f49976a64b6a30483399bfb54b1b8d
    <a href="https://www.techradar.com/ai-platforms-assistants/the-world-is-in-peril-5-reasons-why-the-ai-apocalypse-might-be-closer-than-you-think" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div>
<section>
<div>
<figure> <img src="https://cdn.mos.cms.futurecdn.net/KrLJtRfrQNCBaifpgDWmhV.jpg" alt="Terminator"> <figcaption> <span>(Image credit: Getty Images)</span>
</figcaption>
</figure></div>
<div> <p>There's been an endless parade of proclamations over the last few years about an AI golden age. Developers proclaim a new industrial revolution and executives promise frictionless productivity and amazing breakthroughs accelerated by machine intelligence. Every new product seems to boast of its AI capability, no matter how unnecessary.</p><p>But that golden sheen has a darker edge. There are more indications that the issues around AI technology are not a small matter to be fixed in the next update, but a persistent, unavoidable element of the technology and its deployment. Some of the concerns are born out of <a href="https://www.techradar.com/ai-platforms-assistants/5-common-myths-about-ai-tools-debunked">myths about AI</a>, but that doesn't mean that there's nothing to worry about. Even if the technology isn't scary, how people use it can be <a href="https://www.techradar.com/computing/artificial-intelligence/metas-ai-chief-is-right-to-call-ai-fearmongering-b-s-but-not-for-the-reason-he-thinks">plenty frightening</a>. And solutions offered by the biggest proponents of AI solutions often seem likely to <a href="https://www.techradar.com/ai-platforms-assistants/an-ai-executives-dire-warnings-about-the-future-are-chilling-but-his-solution-is-worse-than-the-problem">make things worse</a>.</p><h2>1. AI safety experts flee</h2><figure><div><p> <img src="https://cdn.mos.cms.futurecdn.net/scxFkyfYSQbrtGvqrmFqgU.jpg" alt="Claude by Anthropic">
</p></div><figcaption><span>(Image credit: Shutterstock)</span></figcaption></figure><p>This month, the head of AI safety research at Anthropic <a href="https://www.bbc.com/news/articles/c62dlvdq3e3o" target="_blank">resigned</a> and did so loudly. In a public statement, he warned that “the world is in peril” and questioned whether core values were still steering the company’s decisions. A senior figure whose job was to think about the long term and how increasingly capable systems might go wrong decided it was impossible to keep going. His departure followed a string of other exits across the industry, including <a href="https://www.nbcnews.com/tech/elon-musk/xai-musk-addresses-wave-departures-xai-founder-rcna258651" target="_blank">founders and senior staff at xAI</a> and other high-profile labs. The pattern has been difficult to ignore.</p><p>Resignations happen in tech all the time, of course, but these departures have come wrapped in moral concern. They have been accompanied by essays and interviews that describe internal debates about safety standards, competitive pressure, and whether the race to build more powerful models is outpacing the ability to control them. When the people tasked with installing the brakes begin stepping away from the vehicle, it suggests that the car may be accelerating in ways even insiders find troubling.</p><p>AI companies are building systems that will shape economies, education, media, and possibly warfare. If their own safety leaders feel compelled to warn that the world is veering into dangerous territory, that warning deserves more than a shrug.</p><h2>2. Deepfake dangers</h2><p>It's hard to argue there's an issue in AI safety when regulators in the United Kingdom and elsewhere find credible evidence of horrific misuse of AI like reports that <a href="https://www.techradar.com/ai-platforms-assistants/elon-musk-and-grok-face-deeply-troubling-questions-from-uk-regulators-over-data-use-and-consent">Grok on X had generated sexually explicit and abusive imagery</a>, including deepfake content involving minors.</p><div><section><p>Sign up for breaking news, reviews, opinion, top tech deals, and more.</p></section></div><p>Not that deepfakes are new, but now, with the right prompts and a few minutes of patience, users can produce fabricated images that would have required significant technical expertise just a few years ago. Victims have little recourse once manipulated images spread across the internet’s memory.</p><p>From an apocalyptic perspective, this is not about a single scandal. It is about erosion. Trust in visual evidence was already fragile. Now it is cracking. If any image can be plausibly dismissed as synthetic and any person can be digitally placed into compromising situations, the shared factual ground beneath public discourse begins to dissolve. The hopeful counterpoint is that regulators are paying attention and that platform operators are being forced to confront misuse. Stronger safeguards, better detection tools, and clearer legal standards could blunt the worst outcomes, but that won't undo the damage already done.</p><h2>3. Real world hallucinations</h2><p>For years, most AI failures lived on screens, but that boundary is fading as AI systems are starting to help steer cars, coordinate warehouse robots, and guide drones. They interpret sensor data and make split-second decisions in the physical world. And security researchers have been warning that these systems are <a href="https://www.machine.news/ai-driven-robot-cars-can-be-hacked-and-hijacked-by-street-signs" target="_blank">surprisingly easy</a> to trick.</p><p>Studies have demonstrated that subtle environmental changes, altered road signs, strategically placed stickers, or misleading text can cause AI vision systems to misclassify objects. In a lab, that might mean a stop sign interpreted as a speed limit sign. On a busy road, it could mean something far worse. Malicious actors could be an emerging threat. The more infrastructure depends on AI decision-making, the more attractive it becomes as a target.</p><p>The apocalyptic imagination leaps quickly from there. Ideally, awareness of these weaknesses will drive investment in robust security practices before autonomous systems become ubiquitous. But, if deployment outruns safety, we could all learn some painful lessons in real time</p><h2>4. Chatbots get a sales team</h2><figure><div><p> <img src="https://cdn.mos.cms.futurecdn.net/yhXcPLB4Y32pbRZBaNCTo7.jpg" alt="Getting image with Future Edit">
</p></div><figcaption><span>(Image credit: Getty Images / Future)</span></figcaption></figure><p>OpenAI began rolling out advertising within ChatGPT recently, and the response has been mixed, to say the least. A senior OpenAI researcher <a href="https://arstechnica.com/information-technology/2026/02/openai-researcher-quits-over-fears-that-chatgpt-ads-could-manipulate-users" target="_blank">resigning</a> publicly and arguing that ad-driven AI products risk drifting toward user manipulation probably didn't help calm matters. When a system that knows your fears, ambitions, and habits also carries commercial incentives, the lines between assistance and persuasion blur.</p><p>Social media platforms once promised simple connections and information sharing, but their ad-based business models shaped design choices that maximized engagement, sometimes at the cost of user well-being. An AI assistant embedded with advertising could face similar pressures. Subtle prioritization of certain answers. Nudges that align with sponsor interests. Recommendations calibrated not only for relevance but for revenue.</p><p>To be fair, advertising does not automatically corrupt a product. Clear labeling, strict firewalls between monetization and core model behavior, and regulatory oversight could prevent worst-case scenarios. Still, that resignation means there's real, unresolved tension in that regard.</p><h2>5. A growing catalogue of mishaps</h2><p>There is a simple accumulation of evidence of problems with AI. Between November 2025 and January 2026, the AI Incident Database logged 108 new <a href="https://incidentdatabase.ai/blog/incident-report-2025-november-december-2026-january/" target="_blank">incidents</a>. Each entry documents a failure, misuse, or unintended consequence tied to AI systems. Many cases may be minor, but every report of AI used fraud or dispensing dangerous advice adds up.</p><p>Acceleration is what matters here. AI tools are popping up everywhere, and so the number of problems multiplies. Perhaps the uptick in reported incidents is simply a sign of better tracking rather than worsening performance, but it's doubtful that accounts for everything. A still relatively new technology has a lot of harm to its name.</p><p>Apocalypse is a loaded word evoking images of collapse and finality. AI may not be at that point and might never reach it, but it's undeniably causing a lot of turbulence. Catastrophe is not inevitable, but complacency would be a mistake. It might not be the end of the world, but AI can certainly make it feel that way.</p><hr><p><a href="https://news.google.com/publications/CAAqKAgKIiJDQklTRXdnTWFnOEtEWFJsWTJoeVlXUmhjaTVqYjIwb0FBUAE?hl=en-GB&amp;gl=GB&amp;ceid=GB%3Aen" target="_blank"><em><strong>Follow TechRadar on Google News</strong></em></a> and<em> </em><a href="https://www.google.com/preferences/source?q=techradar.com" target="_blank"><em><strong>add us as a preferred source</strong></em></a><em> to get our expert news, reviews, and opinion in your feeds. Make sure to click the Follow button!</em></p><p><em>And of course you can also </em><a href="https://www.tiktok.com/@techradar" target="_blank"><em><strong>follow TechRadar on TikTok</strong></em></a><em> for news, reviews, unboxings in video form, and get regular updates from us on </em><a href="https://whatsapp.com/channel/0029Va6HybZ9RZAY7pIUK12h" target="_blank"><em><strong>WhatsApp</strong></em></a><em> too.</em></p><hr><div><figure><div><img src="https://cdn.mos.cms.futurecdn.net/hhZUEBD3xpcxphhUVzPa5G.png" alt="Purple circle with the words Best business laptops in white"></div></figure></div><hr>
</div> <div><p>Eric Hal Schwartz is a freelance writer for TechRadar with more than 15 years of experience covering the intersection of the world and technology. For the last five years, he served as head writer for Voicebot.ai and was on the leading edge of reporting on generative AI and large language models. He's since become an expert on the products of generative AI models, such as OpenAI’s ChatGPT, Anthropic’s Claude, Google Gemini, and every other synthetic media tool. His experience runs the gamut of media, including print, digital, broadcast, and live events. Now, he's continuing to tell the stories people want and need to hear about the rapidly evolving AI space and its impact on their lives. Eric is based in New York City.</p></div>
</section> <div>
<p>You must confirm your public display name before commenting</p>
<p>Please logout and then login again, you will then be prompted to enter your display name.</p>
</div> </div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollStep(-1)">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollStep(1)">▼</button>
  </div>
  <script>
    function scrollStep(direction) {
      var step = Math.max(220, Math.round(window.innerHeight * 0.72));
      window.scrollBy({ top: direction * step, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up') scrollStep(-1);
      if (data.direction === 'down') scrollStep(1);
      if (data.direction === 'top') window.scrollTo({ top: 0, behavior: 'smooth' });
      if (data.direction === 'bottom') window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    });
  </script>
</body>
</html>