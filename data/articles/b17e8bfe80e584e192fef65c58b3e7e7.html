<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Introducing swift-huggingface: The Complete Swift Client for Hugging Face</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Introducing swift-huggingface: The Complete Swift Client for Hugging Face</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 12/5/2025 1:00:00 AM | <a href="https://huggingface.co/blog/swift-huggingface" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div> <p><span><span><a href="https://huggingface.co/mattt"><img alt="Mattt's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1662384077127-6315f7823edbcfbc5d43306b.jpeg"></a> </span> </span></p> </div> <div><nav><ul><li><a href="#the-problem">The Problem</a> <ul></ul> </li><li><a href="#introducing-swift-huggingface">Introducing swift-huggingface</a> <ul></ul> </li><li><a href="#flexible-authentication-with-tokenprovider">Flexible Authentication with TokenProvider</a> <ul></ul> </li><li><a href="#oauth-for-user-facing-apps">OAuth for User-Facing Apps</a> <ul></ul> </li><li><a href="#reliable-downloads">Reliable Downloads</a> <ul></ul> </li><li><a href="#shared-cache-with-python">Shared Cache with Python</a> <ul></ul> </li><li><a href="#before-and-after">Before and After</a> <ul></ul> </li><li><a href="#beyond-downloads">Beyond Downloads</a> <ul></ul> </li><li><a href="#whats-next">What's Next</a> <ul></ul> </li><li><a href="#try-it-out">Try It Out</a> <ul></ul> </li><li><a href="#resources">Resources</a> <ul></ul> </li></ul></nav></div><p>Today, we're announcing <a href="https://github.com/huggingface/swift-huggingface">swift-huggingface</a>,
a new Swift package that provides a complete client for the Hugging Face Hub.</p>
<p>You can start using it today as a standalone package,
and it will soon integrate into swift-transformers as a replacement for its current <code>HubApi</code> implementation.</p>
<h2> <a href="#the-problem"> <span></span> </a> <span> The Problem </span>
</h2>
<p>When we released <a href="https://huggingface.co/blog/swift-transformers">swift-transformers 1.0</a> earlier this year,
we heard loud and clear from the community:</p>
<ul>
<li><strong>Downloads were slow and unreliable.</strong>
Large model files (often several gigabytes)
would fail partway through with no way to resume.
Developers resorted to manually downloading models and bundling them with their apps —
defeating the purpose of dynamic model loading.</li>
<li><strong>No shared cache with the Python ecosystem.</strong>
The Python <code>transformers</code> library stores models in <code>~/.cache/huggingface/hub</code>.
Swift apps downloaded to a different location with a different structure.
If you'd already downloaded a model using the Python CLI,
you'd download it again for your Swift app.</li>
<li><strong>Authentication is confusing.</strong>
Where should tokens come from?
Environment variables? Files? Keychain?
The answer is, <em>"It depends"</em>,
and the existing implementation didn't make the options clear.</li>
</ul>
<h2> <a href="#introducing-swift-huggingface"> <span></span> </a> <span> Introducing swift-huggingface </span>
</h2>
<p>swift-huggingface is a ground-up rewrite focused on reliability and developer experience.
It provides:</p>
<ul>
<li><strong>Complete Hub API coverage</strong> — models, datasets, spaces, collections, discussions, and more</li>
<li><strong>Robust file operations</strong> — progress tracking, resume support, and proper error handling</li>
<li><strong>Python-compatible cache</strong> — share downloaded models between Swift and Python clients</li>
<li><strong>Flexible authentication</strong> — a <code>TokenProvider</code> pattern that makes credential sources explicit</li>
<li><strong>OAuth support</strong> — first-class support for user-facing apps that need to authenticate users</li>
<li><strong>Xet storage backend support</strong> <em>(Coming soon!)</em> — chunk-based deduplication for significantly faster downloads</li>
</ul>
<p>Let's look at some examples.</p>
<hr>
<h2> <a href="#flexible-authentication-with-tokenprovider"> <span></span> </a> <span> Flexible Authentication with TokenProvider </span>
</h2>
<p>One of the biggest improvements is how authentication works. The <code>TokenProvider</code> pattern makes it explicit where credentials come from:</p>
<pre><code><span>import</span> HuggingFace <span>// For development: auto-detect from environment and standard locations</span>
<span>// Checks HF_TOKEN, HUGGING_FACE_HUB_TOKEN, ~/.cache/huggingface/token, etc.</span>
<span>let</span> client <span>=</span> <span>HubClient</span>.default <span>// For CI/CD: explicit token</span>
<span>let</span> client <span>=</span> <span>HubClient</span>(tokenProvider: .static(<span>"hf_xxx"</span>)) <span>// For production apps: read from Keychain</span>
<span>let</span> client <span>=</span> <span>HubClient</span>(tokenProvider: .keychain(service: <span>"com.myapp"</span>, account: <span>"hf_token"</span>))
</code></pre>
<p>The auto-detection follows the same conventions as the Python <code>huggingface_hub</code> library:</p>
<ol>
<li><code>HF_TOKEN</code> environment variable</li>
<li><code>HUGGING_FACE_HUB_TOKEN</code> environment variable</li>
<li><code>HF_TOKEN_PATH</code> environment variable (path to token file)</li>
<li><code>$HF_HOME/token</code> file</li>
<li><code>~/.cache/huggingface/token</code> (standard HF CLI location)</li>
<li><code>~/.huggingface/token</code> (fallback location)</li>
</ol>
<p>This means if you've already logged in with <code>hf auth login</code>,
swift-huggingface will automatically find and use that token.</p>
<h2> <a href="#oauth-for-user-facing-apps"> <span></span> </a> <span> OAuth for User-Facing Apps </span>
</h2>
<p>Building an app where users sign in with their Hugging Face account?
swift-huggingface includes a complete OAuth 2.0 implementation:</p>
<pre><code><span>import</span> HuggingFace <span>// Create authentication manager</span>
<span>let</span> authManager <span>=</span> <span>try</span> <span>HuggingFaceAuthenticationManager</span>( clientID: <span>"your_client_id"</span>, redirectURL: <span>URL</span>(string: <span>"yourapp://oauth/callback"</span>)<span>!</span>, scope: [.openid, .profile, .email], keychainService: <span>"com.yourapp.huggingface"</span>, keychainAccount: <span>"user_token"</span>
) <span>// Sign in user (presents system browser)</span>
<span>try</span> <span>await</span> authManager.signIn() <span>// Use with Hub client</span>
<span>let</span> client <span>=</span> <span>HubClient</span>(tokenProvider: .oauth(manager: authManager)) <span>// Tokens are automatically refreshed when needed</span>
<span>let</span> userInfo <span>=</span> <span>try</span> <span>await</span> client.whoami()
<span>print</span>(<span>"Signed in as: <span>\(userInfo.name)</span>"</span>)
</code></pre>
<p>The OAuth manager handles token storage in Keychain,
automatic refresh, and secure sign-out.
No more manual token management.</p>
<h2> <a href="#reliable-downloads"> <span></span> </a> <span> Reliable Downloads </span>
</h2>
<p>Downloading large models is now straightforward with proper progress tracking and resume support:</p>
<pre><code><span>// Download with progress tracking</span>
<span>let</span> progress <span>=</span> <span>Progress</span>(totalUnitCount: <span>0</span>) <span>Task</span> { <span>for</span> <span>await</span> <span>_</span> <span>in</span> progress.publisher(for: \.fractionCompleted).values { <span>print</span>(<span>"Download: <span>\(Int(progress.fractionCompleted <span>*</span> <span>100</span>))</span>%"</span>) }
} <span>let</span> fileURL <span>=</span> <span>try</span> <span>await</span> client.downloadFile( at: <span>"model.safetensors"</span>, from: <span>"microsoft/phi-2"</span>, to: destinationURL, progress: progress
)
</code></pre>
<p>If a download is interrupted,
you can resume it:</p>
<pre><code><span>// Resume from where you left off</span>
<span>let</span> fileURL <span>=</span> <span>try</span> <span>await</span> client.resumeDownloadFile( resumeData: savedResumeData, to: destinationURL, progress: progress
)
</code></pre>
<p>For downloading entire model repositories,
<code>downloadSnapshot</code> handles everything:</p>
<pre><code><span>let</span> modelDir <span>=</span> <span>try</span> <span>await</span> client.downloadSnapshot( of: <span>"mlx-community/Llama-3.2-1B-Instruct-4bit"</span>, to: cacheDirectory, matching: [<span>"*.safetensors"</span>, <span>"*.json"</span>], <span>// Only download what you need</span> progressHandler: { progress <span>in</span> <span>print</span>(<span>"Downloaded <span>\(progress.completedUnitCount)</span> of <span>\(progress.totalUnitCount)</span> files"</span>) }
)
</code></pre>
<p>The snapshot function tracks metadata for each file,
so subsequent calls only download files that have changed.</p>
<h2> <a href="#shared-cache-with-python"> <span></span> </a> <span> Shared Cache with Python </span>
</h2>
<p>Remember the second problem we mentioned?
<em>"No shared cache with the Python ecosystem."</em>
That's now solved.</p>
<p>swift-huggingface implements a Python-compatible cache structure
that allows seamless sharing between Swift and Python clients:</p>
<pre><code>~/.cache/huggingface/hub/
├── models--deepseek-ai--DeepSeek-V3.2/
│ ├── blobs/
│ │ └── &lt;etag&gt; # actual file content
│ ├── refs/
│ │ └── main # contains commit hash
│ └── snapshots/
│ └── &lt;commit_hash&gt;/
│ └── config.json # symlink → ../../blobs/&lt;etag&gt;
</code></pre>
<p>This means:</p>
<ul>
<li><strong>Download once, use everywhere.</strong>
If you've already downloaded a model with the <code>hf</code> CLI or the Python library,
swift-huggingface will find it automatically.</li>
<li><strong>Content-addressed storage.</strong>
Files are stored by their ETag in the <code>blobs/</code> directory.
If two revisions share the same file, it's only stored once.</li>
<li><strong>Symlinks for efficiency.</strong>
Snapshot directories contain symlinks to blobs,
minimizing disk usage while maintaining a clean file structure.</li>
</ul>
<p>The cache location follows the same environment variable conventions as Python:</p>
<ol>
<li><code>HF_HUB_CACHE</code> environment variable</li>
<li><code>HF_HOME</code> environment variable + <code>/hub</code></li>
<li><code>~/.cache/huggingface/hub</code> (default)</li>
</ol>
<p>You can also use the cache directly:</p>
<pre><code><span>let</span> cache <span>=</span> <span>HubCache</span>.default <span>// Check if a file is already cached</span>
<span>if</span> <span>let</span> cachedPath <span>=</span> cache.cachedFilePath( repo: <span>"deepseek-ai/DeepSeek-V3.2"</span>, kind: .model, revision: <span>"main"</span>, filename: <span>"config.json"</span>
) { <span>let</span> data <span>=</span> <span>try</span> <span>Data</span>(contentsOf: cachedPath) <span>// Use cached file without any network request</span>
}
</code></pre>
<p>To prevent race conditions when multiple processes access the same cache,
swift-huggingface uses file locking
(<a href="https://man7.org/linux/man-pages/man2/flock.2.html"><code>flock(2)</code></a>).</p>
<h2> <a href="#before-and-after"> <span></span> </a> <span> Before and After </span>
</h2>
<p>Here's what downloading a model snapshot looked like with the old <code>HubApi</code>:</p>
<pre><code><span>// Before: HubApi in swift-transformers</span>
<span>let</span> hub <span>=</span> <span>HubApi</span>()
<span>let</span> repo <span>=</span> <span>Hub</span>.<span>Repo</span>(id: <span>"mlx-community/Llama-3.2-1B-Instruct-4bit"</span>) <span>// No progress tracking, no resume, errors swallowed</span>
<span>let</span> modelDir <span>=</span> <span>try</span> <span>await</span> hub.snapshot( from: repo, matching: [<span>"*.safetensors"</span>, <span>"*.json"</span>]
) { progress <span>in</span> <span>// Progress object exists but wasn't always accurate</span> <span>print</span>(progress.fractionCompleted)
}
</code></pre>
<p>And here's the same operation with swift-huggingface:</p>
<pre><code><span>// After: swift-huggingface</span>
<span>let</span> client <span>=</span> <span>HubClient</span>.default <span>let</span> modelDir <span>=</span> <span>try</span> <span>await</span> client.downloadSnapshot( of: <span>"mlx-community/Llama-3.2-1B-Instruct-4bit"</span>, to: cacheDirectory, matching: [<span>"*.safetensors"</span>, <span>"*.json"</span>], progressHandler: { progress <span>in</span> <span>// Accurate progress per file</span> <span>print</span>(<span>"<span>\(progress.completedUnitCount)</span>/<span>\(progress.totalUnitCount)</span> files"</span>) }
)
</code></pre>
<p>The API is similar, but the implementation is completely different —
built on <code>URLSession</code> download tasks with proper
delegate handling, resume data support, and metadata tracking.</p>
<h2> <a href="#beyond-downloads"> <span></span> </a> <span> Beyond Downloads </span>
</h2>
<p>But wait, there's more!
swift-huggingface contains a complete Hub client:</p>
<pre><code><span>// List trending models</span>
<span>let</span> models <span>=</span> <span>try</span> <span>await</span> client.listModels( filter: <span>"library:mlx"</span>, sort: <span>"trending"</span>, limit: <span>10</span>
) <span>// Get model details</span>
<span>let</span> model <span>=</span> <span>try</span> <span>await</span> client.getModel(<span>"mlx-community/Llama-3.2-1B-Instruct-4bit"</span>)
<span>print</span>(<span>"Downloads: <span>\(model.downloads <span>??</span> <span>0</span>)</span>"</span>)
<span>print</span>(<span>"Likes: <span>\(model.likes <span>??</span> <span>0</span>)</span>"</span>) <span>// Work with collections</span>
<span>let</span> collections <span>=</span> <span>try</span> <span>await</span> client.listCollections(owner: <span>"huggingface"</span>, sort: <span>"trending"</span>) <span>// Manage discussions</span>
<span>let</span> discussions <span>=</span> <span>try</span> <span>await</span> client.listDiscussions(kind: .model, <span>"username/my-model"</span>)
</code></pre>
<p>And that's not all!
swift-huggingface has everything you need to interact with
<a href="https://huggingface.co/docs/inference-providers/index">Hugging Face Inference Providers</a>,
giving your app instant access to hundreds of machine learning models,
powered by world-class inference providers:</p>
<pre><code><span>import</span> HuggingFace <span>// Create a client (uses auto-detected credentials from environment)</span>
<span>let</span> client <span>=</span> <span>InferenceClient</span>.default <span>// Generate images from a text prompt</span>
<span>let</span> response <span>=</span> <span>try</span> <span>await</span> client.textToImage( model: <span>"black-forest-labs/FLUX.1-schnell"</span>, prompt: <span>"A serene Japanese garden with cherry blossoms"</span>, provider: .hfInference, width: <span>1024</span>, height: <span>1024</span>, numImages: <span>1</span>, guidanceScale: <span>7.5</span>, numInferenceSteps: <span>50</span>, seed: <span>42</span>
) <span>// Save the generated image</span>
<span>try</span> response.image.write(to: <span>URL</span>(fileURLWithPath: <span>"generated.png"</span>))
</code></pre>
<p>Check the <a href="https://github.com/huggingface/swift-huggingface">README</a> for a full list of everything that's supported.</p>
<h2> <a href="#whats-next"> <span></span> </a> <span> What's Next </span>
</h2>
<p>We're actively working on two fronts:</p>
<p><strong>Integration with swift-transformers.</strong>
We have a <a href="https://github.com/huggingface/swift-transformers/pull/297">pull request in progress</a> to replace <code>HubApi</code> with swift-huggingface.
This will bring reliable downloads to everyone using swift-transformers,
<a href="https://github.com/ml-explore/mlx-swift-lm">mlx-swift-lm</a>,
and the broader ecosystem.
If you maintain a Swift-based library or app and want help adopting swift-huggingface, reach out — we're happy to help.</p>
<p><strong>Faster downloads with Xet.</strong>
We're adding support for the <a href="https://huggingface.co/docs/hub/storage-backends">Xet storage backend</a>,
which enables chunk-based deduplication and significantly faster downloads for large models.
More on this soon.</p>
<h2> <a href="#try-it-out"> <span></span> </a> <span> Try It Out </span>
</h2>
<p>Add swift-huggingface to your project:</p>
<pre><code>dependencies: [ .package(url: <span>"https://github.com/huggingface/swift-huggingface.git"</span>, from: <span>"0.4.0"</span>)
]
</code></pre>
<p>We'd love your feedback.
If you've been frustrated with model downloads in Swift, give this a try and
<a href="https://github.com/huggingface/swift-huggingface/issues">let us know how it goes</a>.
Your experience reports will help us prioritize what to improve next.</p>
<h2> <a href="#resources"> <span></span> </a> <span> Resources </span>
</h2>
<ul>
<li><a href="https://github.com/huggingface/swift-huggingface">swift-huggingface on GitHub</a></li>
<li><a href="https://github.com/huggingface/swift-transformers">swift-transformers</a></li>
<li><a href="https://github.com/ml-explore/mlx-swift-examples">mlx-swift-examples</a></li>
<li><a href="https://github.com/mattt/AnyLanguageModel">AnyLanguageModel</a></li>
</ul>
<hr>
<p><em>Thanks to the swift-transformers community for the feedback that shaped this project, and to everyone who filed issues and shared their experiences. This is for you.</em> </p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>