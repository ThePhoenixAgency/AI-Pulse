<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Smol2Operator: Post-Training GUI Agents for Computer Use</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Smol2Operator: Post-Training GUI Agents for Computer Use</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 9/23/2025 12:00:00 AM | <a href="https://huggingface.co/blog/smol2operator" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/A-Mahla"><img alt="Amir Mahla's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/67f2f500e329a81a62a05d44/DOlzc8GFQzrnfVrsOdtbN.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/merve"><img alt="merve's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6141a88b3a0ec78603c9e784/DJsxSmWV39M33JFheLobC.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/sergiopaniego"><img alt="Sergio Paniego's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/61929226ded356549e20c5da/ONUjP2S5fUWd07BiFXm0i.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/reach-vb"><img alt="Vaibhav Srivastav's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1655385361868-61b85ce86eb1f2c5e6233736.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/lewtun"><img alt="Lewis Tunstall's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1594651707950-noauth.jpeg"></a> </span> </span></p> </div></div> <p><strong>TL;DR:</strong> This work shows how a lightweight vision–language model can acquire GUI-grounded skills and evolve into an agentic GUI coder. We release all training recipes, data-processing tools, resulting model, demo and datasets to enable full reproducibility and foster further research . Find the collection <a href="https://huggingface.co/collections/smolagents/smol2operator-release-68d288e87d3fa8f551d2ce2e">here</a>.</p>
<hr>
<figure> <figcaption> This video demonstrates the model obtained through the recipe described below, executing a task end-to-end. </figcaption>
</figure> <hr>
<h2> <a href="#table-of-contents"> </a> <span> Table of Contents </span>
</h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#1-data-transformation-and-unified-action-space">1. Data Transformation and Unified Action Space</a><ul>
<li><a href="#the-challenge-of-inconsistent-action-spaces">The Challenge of Inconsistent Action Spaces</a></li>
<li><a href="#our-unified-approach">Our Unified Approach</a></li>
<li><a href="#example-data-transformation">Example Data Transformation</a></li>
<li><a href="#bonus-custom-action-space-adaptation-with-action-space-converter">Custom Action Space Adaptation with Action Space Converter</a></li>
<li><a href="#key-features">Key Features</a></li>
<li><a href="#usage-example">Usage Example</a></li>
<li><a href="#transformed-and-released-datasets">Transformed and Released Datasets</a></li>
</ul>
</li>
<li><a href="#2-phase-1-from-zero-to-perception">2. Phase 1: From Zero to Perception</a><ul>
<li><a href="#training-data">Training Data</a></li>
<li><a href="#optimization-experiments">Optimization Experiments</a></li>
<li><a href="#image-resolution-and-coordinate-system-analysis">Image Resolution and Coordinate System Analysis</a></li>
<li><a href="#key-findings">Key Findings</a></li>
<li><a href="#phase-1-results">Phase 1 Results</a></li>
</ul>
</li>
<li><a href="#3-phase-2-from-perception-to-cognition">3. Phase 2: From Perception to Cognition</a><ul>
<li><a href="#training-data-1">Training Data</a></li>
<li><a href="#phase-2-results">Phase 2 Results</a></li>
</ul>
</li>
<li><a href="#4-all-you-need-is-open-source">4. All you need is Open Source</a></li>
<li><a href="#5-conclusion">5. Conclusion</a></li>
<li><a href="#whats-next">What's Next?</a></li>
</ul>
<br> <h2> <a href="#introduction"> </a> <span> Introduction </span>
</h2>
<p>Graphical User Interface (GUI) automation is one of the most challenging frontiers in computer vision. Developing models that see and interact with user interfaces enables AI agents to navigate mobile, desktop, and web platforms. This will reshape the future of digital interaction.</p>
<p>In this blog post, we present a comprehensive approach to training vision-language models for GUI automation through a multi-phase training strategy. We demonstrate how to transform a model with zero grounding capabilities into an agentic coder capable of understanding and interacting with graphical interfaces.</p>
<p>Rather than aiming for a SOTA model, our goal is to demonstrate the entire process, from data processing to model training, and, in doing so, show how to unlock GUI-grounding capabilities in VLMs.</p>
<div> <p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smol2operator/google.png"><img alt="GUI capabilities combine understanding of the interface and precise element localization. These abilities enable the model to translate high-level tasks into low-level GUI actions such as clicking, typing, …" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smol2operator/google.png"></a></p>
<p><em>GUI capabilities combine understanding of the interface and precise element localization. These abilities enable the model to translate high-level tasks into low-level GUI actions such as clicking, typing, …</em></p> </div> <p>Our approach leverages <a href="https://huggingface.co/HuggingFaceTB/SmolVLM2-2.2B-Instruct"><strong>SmolVLM2-2.2B-Instruct</strong></a> as the baseline model, a small powerful vision-language model that initially has no grounding capabilities for GUI tasks. This makes it an ideal candidate to demonstrate the effectiveness of our training methodology. Through our two-phase training process, we first instill grounding capabilities in the model, then enhance it with agentic reasoning abilities using Supervised Fine-Tuning (SFT).</p>
<p>We evaluate our approach on an established perception benchmark: <a href="https://huggingface.co/datasets/HongxinLi/ScreenSpot_v2"><strong>ScreenSpot-v2</strong></a>, which tests the model’s ability to understand and locate elements within screenshots. Our process is inspired by the <a href="https://huggingface.co/papers/2412.04454">AGUVIS</a> paper, and we leverage their carefully curated datasets to build upon their foundational work.</p>
<div> <p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smol2operator/screenspot-v2.png"><img alt="Evolution of ScreenSpot-v2 performance during the training phase of the base model SmolVLM2-2.2B-Instruct." src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smol2operator/screenspot-v2.png"></a></p>
<p><em>Evolution of ScreenSpot-v2 performance during the training phase of the base model <strong>SmolVLM2-2.2B-Instruct</strong>.</em></p> </div> <h2> <a href="#1-data-transformation-and-unified-action-space"> </a> <span> 1. Data Transformation and Unified Action Space </span>
</h2>
<p><em>This section explains how we <strong>convert heterogeneous GUI actions format from multiple datasets into a single unified format</strong>. By standardizing function names, signatures, and parameters, we create consistent, high-quality data that forms the foundation for effective model training.</em></p>
<h3> <a href="#the-challenge-of-inconsistent-action-spaces"> </a> <span> The Challenge of Inconsistent Action Spaces </span>
</h3>
<p>One of the primary challenges when working with multiple GUI automation datasets is the lack of standardization in action representations. Different datasets use varying function signatures, parameter naming conventions, and action taxonomies, making it difficult to train a unified model across diverse data sources.</p>
<h3> <a href="#our-unified-approach"> </a> <span> Our Unified Approach </span>
</h3>
<p>We took the open-source datasets (<a href="https://huggingface.co/datasets/xlangai/aguvis-stage1">xlangai/aguvis-stage1</a>, <a href="https://huggingface.co/datasets/xlangai/aguvis-stage2">xlangai/aguvis-stage2</a>), originally used by <a href="https://huggingface.co/papers/2412.04454">AGUVIS</a>, and implemented a comprehensive data transformation pipeline to create a unified action space. Our approach involved:</p>
<ol>
<li><strong>Function Parsing and Normalization</strong>: We developed a function parser (see <code>utils/function_parser.py</code>) that can extract and parse function calls from various formats across all datasets. This parser supports any function signature format, handles complex parameter structures, and can reconstruct function calls with proper parameter ordering.</li>
<li><strong>Action Space Unification</strong>: We implemented a comprehensive action conversion system (see <code>preprocessing/action_conversion.py</code>) that transforms all original action representations into a standardized function naming and argument structure. This process highlighted the significant inconsistencies in function signatures across different datasets and allowed us to:<ul>
<li>Remove undesired or redundant actions</li>
<li>Standardize parameter naming conventions</li>
<li>Create a cohesive action vocabulary</li>
</ul>
</li>
<li><strong>(Bonus) Flexible Adaptation Framework</strong>: Our transformation pipeline includes utilities that allow users to:<ul>
<li>Adapt the entire dataset to their own action space naming conventions using the <code>utils/action_space_converter.py</code> tool</li>
<li>Extract and analyze the current action space structure</li>
</ul>
</li>
</ol>
<h3> <a href="#example-data-transformation"> </a> <span> Example Data Transformation </span>
</h3>
<p>Here are real examples from our action conversion system (<code>preprocessing/action_conversion.py</code>) showing how we transform heterogeneous action representations into our unified format (grounding coordinates normalized to [0,1]):</p>
<p><strong>Before (Original Action Dataset Formats):</strong></p>
<pre><code><span># Mobile Actions</span>
mobile.home()
mobile.open_app(app_name=<span>'drupe'</span>)
mobile.swipe(from_coord=[<span>0.581</span>, <span>0.898</span>], to_coord=[<span>0.601</span>, <span>0.518</span>])
mobile.long_press(x=<span>0.799</span>, y=<span>0.911</span>)
mobile.terminate(status=<span>'success'</span>)
<span># Desktop Actions</span>
pyautogui.click(x=<span>0.8102</span>, y=<span>0.9463</span>)
pyautogui.doubleClick(x=<span>0.8102</span>, y=<span>0.9463</span>)
pyautogui.hotkey(keys=[<span>'ctrl'</span>, <span>'c'</span>])
pyautogui.scroll(page=-<span>0.1</span>)
pyautogui.write(message=<span>'bread buns'</span>)
pyautogui.dragTo(from_coord=[<span>0.87</span>, <span>0.423</span>], to_coord=[<span>0.8102</span>, <span>0.9463</span>])
</code></pre>
<p><strong>After (Unified Action Dataset Formats):</strong></p>
<pre><code><span># Unified Mobile Actions</span>
navigate_home()
open_app(app_name=<span>'drupe'</span>)
swipe(from_coord=[<span>0.581</span>, <span>0.898</span>], to_coord=[<span>0.601</span>, <span>0.518</span>])
long_press(x=<span>0.799</span>, y=<span>0.911</span>)
final_answer(<span>'success'</span>)
<span># Unified Desktop Actions</span>
click(x=<span>0.8102</span>, y=<span>0.9463</span>)
double_click(x=<span>0.8102</span>, y=<span>0.9463</span>)
press(keys=[<span>'ctrl'</span>, <span>'c'</span>])
scroll(direction=<span>'up'</span>, amount=<span>10</span>) <span># Smart direction detection</span>
<span>type</span>(text=<span>'bread buns'</span>)
drag(from_coord=[<span>0.87</span>, <span>0.423</span>], to_coord=[<span>0.8102</span>, <span>0.9463</span>])
</code></pre>
<p>This unification process was essential for creating coherent training data that allows the model to learn consistent action patterns across diverse GUI environments.</p> <p><strong> Why Normalized Coordinates?</strong>
<br>
Using raw pixel coordinates in text-action datapoint (e.g. <code>click(x=302, y=63)</code>) ties them to a single image size. Vision Language Models (VLMs) often resize images, causing pixel coordinates to break and require adjustment. Normalized coordinates (relative to image size) remain valid at any resolution and keep the dataset consistent.
</p> <h3> <a href="#bonus-custom-action-space-adaptation-with-action-space-converter"> </a> <span> (Bonus) Custom Action Space Adaptation with Action Space Converter </span>
</h3>
<p>To maximize flexibility for different use cases, we developed the <strong>Action Space Converter</strong> (<code>utils/action_space_converter.py</code>), a tool that allows users to easily adapt from an action space to their own custom action vocabularies and naming conventions. </p>
<p>You can use this tool to transform one action signature (function names, parameter names, and parameter value changes, ...) into another:</p>
<p><strong>Before</strong></p>
<pre><code>assistant_message: <span>"Action: click(x=0.5, y=0.3)"</span>
</code></pre>
<p><strong>After</strong></p>
<pre><code>assistant_message: <span>"Action: touch(x_coord=200, y_coord=300)"</span>
</code></pre>
<h3> <a href="#key-features"> </a> <span> Key Features </span>
</h3>
<p>The Action Space Converter provides:</p>
<ol>
<li><strong>Configurable Mappings</strong>: Define custom mappings between unified actions and your preferred action names</li>
<li><strong>Parameter Transformation</strong>: Rename parameters, apply value transformations, and set default values</li>
<li><strong>Flexible Architecture</strong>: Support for both simple parameter mappings and complex custom transformation functions</li>
<li><strong>Validation</strong>: Built-in validation to ensure mapping configurations are valid</li>
</ol>
<h3> <a href="#usage-example"> </a> <span> Usage Example </span>
</h3>
<pre><code><span>from</span> utils.action_space_converter <span>import</span> ActionSpaceConverter, ActionMapping, ParameterMapping
<span>from</span> utils.function_parser <span>import</span> parse_function_call <span># Create custom mappings</span>
mappings = [ ActionMapping( source_function=<span>"click"</span>, target_function=<span>"touch"</span>, parameter_mappings=[ ParameterMapping(source_name=<span>"x"</span>, target_name=<span>"x_coord"</span>), ParameterMapping(source_name=<span>"y"</span>, target_name=<span>"y_coord"</span>) ], description=<span>"Touch screen at coordinates"</span> ), ActionMapping( source_function=<span>"type"</span>, <span># source_function is the name of the function in the original function call</span> target_function=<span>"write"</span>, <span># target_function is the name of the function in the target function call </span> parameter_mappings=[ ParameterMapping(source_name=<span>"text"</span>, target_name=<span>"content"</span>) <span># source_name is the name of the parameter in the original function call </span> <span># target_name is the name of the parameter in the target function call </span> ], description=<span>"Input text"</span> )
] assistant_message = <span>"I'll interact at those coordinates for you. click(x=0.5, y=0.3) Now I'll input the text. type(text='hello world')"</span> <span># Parse function calls</span>
parsed_function_calls = parse_function_call(text) <span># Initialize converter</span>
converter = ActionSpaceConverter(mappings) <span># Convert actions</span>
converted_actions = converter.convert_actions(parsed_function_calls)
<span>for</span> new_function_call, old_function_call <span>in</span> <span>zip</span>(converted_actions, parsed_function_calls): text = text.replace(old_function_call.to_string(), new_function_call.to_string()) <span>print</span>(text)
<span># Output: I'll interact at those coordinates for you. touch(x_coord=0.5, y_coord=0.3) Now I'll input the text. write(content='hello world')</span>
</code></pre>
<p>This tool enables researchers and practitioners to:</p>
<ul>
<li><strong>Customize Training Data</strong>: Adapt the dataset to match their specific action vocabulary requirements</li>
<li><strong>Domain Adaptation</strong>: Transform actions for different platforms (mobile vs.&nbsp;desktop vs.&nbsp;web)</li>
<li><strong>Framework Integration</strong>: Easily align training data with existing automation frameworks</li>
<li><strong>Rapid Experimentation</strong>: Quickly test different action space configurations</li>
<li><strong>Release Preparation</strong>: Standardize action spaces for production deployment with consistent naming conventions</li>
</ul>
<p>The Action Space Converter is particularly valuable for preparing datasets for training, as it ensures consistent action vocabularies across different deployment environments while maintaining compatibility with existing automation frameworks.</p>
<h3> <a href="#transformed-and-released-datasets"> </a> <span> Transformed and Released Datasets </span>
</h3>
<p>Through this pipeline, we transform the open-source datasets <a href="https://huggingface.co/datasets/xlangai/aguvis-stage1">xlangai/aguvis-stage1</a>, <a href="https://huggingface.co/datasets/xlangai/aguvis-stage2">xlangai/aguvis-stage2</a> into our unified action space (see <a href="https://www.notion.so/Smol2Operator-Post-Training-GUI-Agents-for-Computer-Use-Draft-Blog-Post-2701384ebcac8035bbaad69b5b32ed99?pvs=21">here</a>). The output of this process is released as two new fully formatted datasets: <a href="https://huggingface.co/datasets/smolagents/aguvis-stage-1">smolagents/aguvis-stage-1</a> and <a href="https://huggingface.co/datasets/smolagents/aguvis-stage-2">smolagents/aguvis-stage-2</a>.</p>
<h2> <a href="#2-phase-1-from-zero-to-perception"> </a> <span> 2. Phase 1: From Zero to Perception </span>
</h2>
<h3> <a href="#training-data"> </a> <span> Training Data </span>
</h3>
<p>Phase 1 leverages the <a href="https://huggingface.co/datasets/smolagents/aguvis-stage-1">smolagents/aguvis-stage-1</a> dataset, which introduces <strong>GUI grounding</strong> by pairing low-level instructions with diverse executable actions (expressed in code form). For example, a user/assistant turn in <a href="https://huggingface.co/datasets/smolagents/aguvis-stage-1">smolagents/aguvis-stage-1</a> follows the structure:</p>
<pre><code><span>{</span> <span>"user"</span><span>:</span> <span>"click on more button"</span><span>,</span> <span>"assistant"</span><span>:</span> <span>"click(x=0.8875, y=0.2281)"</span><span>,</span>
<span>}</span>
</code></pre>
<p>Each sample links a screenshot with multi-turn user/assistant interactions, enabling the model to learn fine-grained action grounding across dialogue turns. During fine-tuning, the data collator masks everything except the assistant’s answers when computing the loss. </p> <h3> <a href="#optimization-experiments"> </a> <span> Optimization Experiments </span>
</h3>
<p>Before proceeding with full-scale Phase 1 training, we conducted comprehensive ablation studies to determine optimal training configurations</p>
<h3> <a href="#image-resolution-and-coordinate-system-analysis"> </a> <span> Image Resolution and Coordinate System Analysis </span>
</h3>
<p>We experimented with different image sizes and coordinate representation systems to identify the optimal configuration for SmolVLM2:</p>
<ul>
<li><strong>Image Sizes Tested</strong>: 384px, 768px, 1152px</li>
<li><strong>Coordinate Systems</strong>: Pixel coordinates vs.&nbsp;normalized coordinates (0-1 range)</li>
<li><strong>Training Data</strong>: 400K samples from Aguvis datasets</li>
</ul>
<blockquote>
<p>Some SOTA GUI VLMs (e.g., Qwen-VL) appear also to use a different normalized range (0–1000), which was not tested in this experiment.</p>
</blockquote>
<div>
<table>
<thead>
<tr>
<th>Configuration (coords / image size)</th>
<th>Screenspot-v2 (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Normalized coordinates</em></td>
<td></td>
</tr>
<tr>
<td>Base / –</td>
<td>0.47</td>
</tr>
<tr>
<td>384</td>
<td>31.28</td>
</tr>
<tr>
<td>764</td>
<td>32.32</td>
</tr>
<tr>
<td>1152</td>
<td><strong>33.72</strong></td>
</tr>
<tr>
<td><em>Pixel coordinates</em></td>
<td></td>
</tr>
<tr>
<td>Base / –</td>
<td>0.55</td>
</tr>
<tr>
<td>384</td>
<td>1.17</td>
</tr>
<tr>
<td>764</td>
<td>2.67</td>
</tr>
<tr>
<td>1152</td>
<td>4.32</td>
</tr>
</tbody>
</table>
<p><strong>Table 1:</strong> <em>Baseline on HuggingFaceTB/SmolVLM2-2.2B-Instruct (400k samples, aguvis-stage-1). Higher is better.</em></p>
</div> <p><em>As demonstrated in our benchmark results, SmolVLM2-2.2B-Instruct base initially achieved 0% performance on perception benchmarks like ScreenSpot-v2. This complete lack of grounding capability provided us with a clean slate to evaluate the effectiveness of our training methodology.</em></p>
<h3> <a href="#key-findings"> </a> <span> Key Findings </span>
</h3>
<p>From our experiments, we determined that:</p>
<ul>
<li><strong>Image Size</strong>: 1152px</li>
<li><strong>Coordinate System</strong>: Normalized coordinates (0-1 range) proved most effective for SmolVLM2</li>
<li>Note: The optimal choice between pixel and normalized coordinates may vary depending on the base model’s pre-training approach</li>
</ul>
<h3> <a href="#phase-1-results"> </a> <span> Phase 1 Results </span>
</h3>
<p>Using the optimal configuration (1152px resolution with normalized coordinates), we trained for 2 epochs on the smolagents/aguvis-stage-1 dataset. The results were remarkable, <strong>+41% improvement over baseline on ScreenSpot-v2</strong></p>
<p>This dramatic improvement demonstrates that our Phase 1 training successfully instilled fundamental grounding capabilities in the model, enabling it to understand and locate visual elements within screenshots.</p>
<div>
<table>
<thead>
<tr>
<th>Configuration (coords / image size)</th>
<th>Screenspot-v2 (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Normalized coordinates / 1152</td>
<td>41.27</td>
</tr>
</tbody>
</table>
<p><strong>Table 2:</strong> <em>Baseline on HuggingFaceTB/SmolVLM2-2.2B-Instruct (2 epochs, aguvis-stage-1).</em></p>
</div> <h2> <a href="#3-phase-2-from-perception-to-cognition"> </a> <span> 3. Phase 2: From Perception to Cognition </span>
</h2>
<p>Whereas Phase 1 provided grounding capabilities, Phase 2 targets <strong>agentic reasoning,</strong> the ability to deliberate and plan before acting. This stage transforms the model from a reactive system identifying GUI elements into a proactive agent capable of executing complex, multi-step interactions.</p>
<h3> <a href="#training-data-1"> </a> <span> Training Data </span>
</h3>
<p>Phase 2 uses the <a href="https://huggingface.co/datasets/smolagents/aguvis-stage-2">smolagents/aguvis-stage-2</a> dataset, which introduces agentic scenarios:</p>
<ul>
<li><p><strong>Explicit reasoning</strong> about upcoming actions</p>
</li>
<li><p><strong>Context consistency</strong> across multiple interaction steps</p>
</li>
<li><p><strong>High-level instructions</strong> require multi-step, low-level actions.</p>
</li>
</ul>
<p>For example, the <a href="https://huggingface.co/datasets/smolagents/aguvis-stage-2">smolagents/aguvis-stage-2</a> chat message is like this:</p>
<pre><code><span>{</span> <span>"system"</span><span>:</span> <span>"You are a helpful GUI agent. ..."</span><span>,</span> <span>"user"</span><span>:</span> <span>"Please generate the next move according to the UI screenshot, instruction and previous actions.\n\nInstruction: What information does the site provide about Judith Lauand's career, works and exhibitions?\n\nPrevious actions:\nNone"</span><span>,</span> <span>"assistant"</span><span>:</span> <span>"&lt;think&gt;\nClick on the link labeled 'Judith Lauand: Brazilian 1922-2022' to explore more about her career and exhibitions.\n&lt;/think&gt;\n&lt;code&gt;\nclick(x=0.41, y=0.178)\n&lt;/code&gt;"</span><span>,</span>
<span>}</span>
</code></pre>
<p>Each sample links a screenshot with a system/user/assistant turn. During fine-tuning, the data collator masks everything except the assistant’s answers when computing the loss. </p> <h3> <a href="#phase-2-results"> </a> <span> Phase 2 Results </span>
</h3>
<p>Starting from the Phase 1 checkpoint (1152 px resolution, normalized coordinates), we fine-tuned the model for two epochs on <a href="https://huggingface.co/datasets/smolagents/aguvis-stage-2">smolagents/aguvis-stage-2</a>. The accuracy on <strong>ScreenSpot-v2 increased from 41% to 61%</strong>, indicating that explicit reasoning improves GUI grounding performance.</p>
<div>
<table>
<thead>
<tr>
<th>Configuration (coords / image size)</th>
<th>Screenspot-v2 (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Normalized coordinates / 1152</td>
<td>61.71</td>
</tr>
</tbody>
</table>
<p><strong>Table 2:</strong> <em>Baseline on HuggingFaceTB/SmolVLM2-2.2B-Instruct after Phase 1 finetuning (2 epochs, aguvis-stage-1).</em></p>
</div> <p> <em>We also reproduced the two-phase training on a much smaller VLM (nanoVLM-460M). Despite its reduced capacity, the model achieved <strong>~58% on ScreenSpot-v2</strong>, demonstrating that the training strategy scales down effectively, <strong>making it SOTA on ScreenSpot-v2 for this model size (460M parameters)</strong>. In addition, aguvis-stage-1 is already included in <a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision">FineVision Dataset</a>!</em>
</p> <h2> <a href="#4-all-you-need-is-open-source"> </a> <span> 4. All you need is Open Source </span>
</h2>
<p>All training code, data processing pipelines, datasets and model are open-source!</p>
<ol>
<li><strong>Training Recipe</strong> (<a href="https://github.com/huggingface/smol2operator/blob/main/recipe.ipynb"><code>recipe.ipynb</code></a>): Complete training pipeline for both Phase 1 and Phase 2, including dataset mixture configurations and training orchestration. We leverage the <a href="https://huggingface.co/docs/trl/en/index">TRL</a> library to train our models.</li>
<li><strong>Datasets</strong> (<a href="https://huggingface.co/datasets/smolagents/aguvis-stage-1"><code>smolagents/aguvis-stage-1</code></a>, <a href="https://huggingface.co/datasets/smolagents/aguvis-stage-2"><code>smolagents/aguvis-stage-2</code></a>): all datasets used are open-source.</li>
<li><strong>Model</strong> (<a href="https://huggingface.co/smolagents/SmolVLM2-2.2B-Instruct-Agentic-GUI"><code>smolagents/SmolVLM2-2.2B-Instruct-Agentic-GUI</code></a>): the model produced by applying the training recipe described above.</li>
<li><strong>Preprocessing Tools:</strong><ul>
<li><strong>Function Parser</strong> (<a href="https://github.com/huggingface/smol2operator/blob/main/utils/function_parser.py"><code>utils/function_parser.py</code></a>): Utilities for parsing, normalizing, and reconstructing function calls from diverse dataset formats. Supports complex parameter structures, positional arguments, and multiple function call extraction.</li>
<li><strong>Action Conversion System</strong> (<a href="https://github.com/huggingface/smol2operator/blob/main/preprocessing/action_conversion.py"><code>preprocessing/action_conversion.py</code></a>): Core unification engine transforming mobile and PyAutoGUI desktop actions into a standardized API format. Features smart coordinate handling, direction detection for scroll actions, and comprehensive parameter normalization.</li>
<li><strong>Action Space Converter</strong> (<a href="https://github.com/huggingface/smol2operator/blob/main/utils/action_space_converter.py"><code>utils/action_space_converter.py</code></a>): Flexible tool for adapting the unified action space to custom vocabularies and naming conventions. Enables domain-specific customization through configurable parameter mappings.</li>
</ul>
</li>
</ol>
<p> We’ve also released a Space to try the model’s agentic grounding capabilities: <a href="https://huggingface.co/spaces/A-Mahla/Smol2Operator">A-Mahla/Smol2Operator</a>
</p> <h2> <a href="#5-conclusion"> </a> <span> 5. Conclusion </span>
</h2>
<p>Our experiments demonstrate that high-quality, reasoning-oriented data can substantially improve GUI grounding, even for small VLMs, using only supervised fine-tuning (SFT). Beyond raw performance gains, these results show that the GUI grounding capabilities are largely determined by the quality of the data. Carefully curated datasets teach models the structure and semantics of user interfaces, providing the grounding needed for accurate action prediction.</p>
<p>To support the development of GUI agents, we’re open-sourcing everything: our complete pipeline, datasets, and trained model. You can reproduce our results, experiment with different models and architectures, or adapt our approach to new domains. The future of agentic AI depends on researchers like you pushing these boundaries further!</p>
<h2> <a href="#whats-next"> </a> <span> What's Next? </span>
</h2>
<p>While SFT excels at supervised tasks, emerging methods such as Reinforcement Learning (RL) or Direct Preference Optimization (DPO) help develop stronger reasoning capabilities and enable real-time adaptation. These advances point toward a new generation of GUI agents that learn and improve through interaction rather than relying solely on static datasets.</p>
<p>Let’s build the future of GUI agents together </p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>