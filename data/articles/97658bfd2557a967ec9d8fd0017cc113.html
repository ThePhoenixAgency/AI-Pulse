<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="UTF-8">
<title>Anthropic abandonne sa promesse de s�curit� au milieu d'une bataille sur la ligne rouge de l'IA avec le Pentagone, le nouveau cadre de s�curit� non contraignant n'entravera pas sa capacit� � �tre comp�titif</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>Anthropic abandonne sa promesse de s�curit� au milieu d'une bataille sur la ligne rouge de l'IA avec le Pentagone, le nouveau cadre de s�curit� non contraignant n'entravera pas sa capacit� � �tre comp�titif</h1>
  <div class="metadata">
    Source: Developpez.com | Date: 2/26/2026 3:41:00 PM | <a href="https://intelligence-artificielle.developpez.com/actu/380626/Anthropic-abandonne-sa-promesse-de-securite-au-milieu-d-une-bataille-sur-la-ligne-rouge-de-l-IA-avec-le-Pentagone-le-nouveau-cadre-de-securite-non-contraignant-n-entravera-pas-sa-capacite-a-etre-competitif/" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: FR
  </div>
  <div class="content">
    <div><div> <p><img src="https://www.developpez.com/images/logos/antropic.png"> <b>Anthropic a d�cider d'assouplir son principe de s�curit� fondamental pour faire face � la concurrence. Au lieu d'imposer des garde-fous � son d�veloppement de mod�les d'IA, Anthropic adopte un cadre de s�curit� non contraignant qui, selon elle, peut �voluer et �voluera. Cette annonce est surprenante, car Anthropic s'est d�crite comme l'entreprise d'IA avec une � �me �. Elle intervient �galement la semaine m�me o� Anthropic m�ne une bataille importante avec le Pentagone au sujet des lignes rouges en mati�re d'IA. Le changement de politique est distinct et sans rapport avec les discussions d'Anthropic avec le Pentagone, selon une source famili�re de l'affaire.</b></p><p>Anthropic est une entreprise am�ricaine de recherche et de d�veloppement en intelligence artificielle fond�e en 2021 par d�anciens membres d�OpenAI, dont Dario Amodei et Daniela Amodei. La soci�t� est sp�cialis�e dans les travaux li�s � la s�ret�, � l�alignement et � la gouvernance des syst�mes d�IA avanc�s. Son objectif affich� est de d�velopper des mod�les puissants tout en r�duisant les risques associ�s � leur d�ploiement � grande �chelle, notamment en mati�re de s�curit�, de biais, de manipulation et d�impacts soci�taux.</p><p>En fait, depuis sa cr�ation, Anthropic s�est positionn�e comme l�anti-th�se d�une IA purement utilitariste, optimis�e uniquement pour la performance. Avec Claude, l�entreprise revendique une approche dite de � constitutional AI �, o� le mod�le apprend non seulement � r�pondre, mais aussi � se corriger lui-m�me en se r�f�rant � un corpus de principes explicites. <a href="https://intelligence-artificielle.developpez.com/actu/379543/La-nouvelle-constitution-de-Claude-d-Anthropic-sois-serviable-honnete-et-ne-detruis-pas-l-humanite-Le-modele-est-entraine-a-raisonner-sur-les-motifs-pour-lesquels-une-reponse-pourrait-etre-problematique/" target="_blank" rel="noopener noreferrer">La nouvelle constitution publi�e marque une �volution notable</a> : elle n�est plus un simple outil interne d�entra�nement, mais un document revendiqu� comme central dans l�identit� m�me du mod�le. </p><p>Ce texte agit comme une sorte de charte fondamentale. Il ne d�crit pas des comportements pr�cis � adopter, mais des valeurs, des priorit�s et des hi�rarchies de principes. Claude est entra�n� � �valuer ses propres r�ponses � l�aune de ces r�gles, � d�tecter ses d�rives potentielles et � reformuler de lui-m�me ses sorties lorsqu�elles entrent en tension avec la constitution.</p><p>Puis fin janvier 2026, un conflit discret mais f�roce a �clat� entre l'arm�e am�ricaine et Anthropic. Le conflit porte sur une question d'une simplicit� trompeuse : qui d�cide de l'utilisation d'une IA puissante dans la guerre ? Anthropic, l'entreprise � l'origine de l'assistant d'IA Claude, a trac� une ligne de d�marcation. <a href="https://intelligence-artificielle.developpez.com/actu/379751/Les-militaires-americains-veulent-un-acces-illimite-a-la-technologie-IA-Claude-pour-le-ciblage-d-armes-autonomes-et-la-surveillance-domestique-mais-Anthropic-refuse-de-lever-ses-garde-fous/" target="_blank" rel="noopener noreferrer">Les n�gociateurs du Pentagone veulent que l'entreprise l�ve les restrictions</a> qui emp�chent actuellement sa technologie d'�tre d�ploy�e pour des op�rations autonomes de ciblage et de surveillance d'armes � l'int�rieur des fronti�res am�ricaines. Anthropic a refus�.</p><p>R�cemment, <a href="https://intelligence-artificielle.developpez.com/actu/380551/Le-Pentagone-fixe-a-vendredi-la-date-limite-pour-qu-Anthropic-delaisse-ses-regles-ethiques-en-matiere-d-IA-sous-peine-de-perdre-son-contrat-federal-et-d-etre-qualifie-de-menace-pour-la-chaine-d-approvisionnem/" target="_blank" rel="noopener noreferrer">le secr�taire am�ricain � la D�fense Pete Hegseth a donn� au PDG d'Anthropic jusqu'au 27 f�vrier 2026</a> � 17h pour ouvrir la technologie d'IA de l'entreprise � une utilisation militaire sans restriction, sous peine de perdre son contrat avec le gouvernement f�d�ral. Il aurait �galement menac� de d�signer Anthropic comme un risque pour la cha�ne d'approvisionnement. Le Pentagone acc�l�re l'int�gration de l'IA pour maintenir sa comp�titivit� face � la Chine.</p><p>En r�ponse, Anthropic a d�cider d'assouplir son principe de s�curit� fondamental pour faire face � la concurrence. Au lieu d'imposer des garde-fous � son d�veloppement de mod�les d'IA, Anthropic adopte un cadre de s�curit� non contraignant qui, selon elle, peut �voluer et �voluera. Dans un billet de blog d�crivant sa nouvelle politique, Anthropic a d�clar� que les lacunes de sa politique de mise � l'�chelle responsable, vieille de deux ans, pourraient entraver sa capacit� � �tre comp�titive sur un march� de l'IA en pleine croissance.</p><div>
<p></p>
</div><p>
Cette annonce est surprenante, car Anthropic s'est d�crite comme l'entreprise d'IA avec une � �me �. Elle intervient �galement la semaine m�me o� Anthropic m�ne une bataille importante avec le Pentagone au sujet des lignes rouges en mati�re d'IA. Le changement de politique est distinct et sans rapport avec les discussions d'Anthropic avec le Pentagone, selon une source famili�re de l'affaire. Le secr�taire � la d�fense Pete Hegseth a lanc� un ultimatum au PDG d'Anthropic, Dario Amodei, pour qu'il revienne sur les mesures de protection de l'entreprise en mati�re d'IA, sous peine de perdre un contrat du Pentagone d'une valeur de 200 millions de dollars. Le Pentagone a menac� d'inscrire Anthropic sur une liste noire du gouvernement.</p><p>Dans son billet de blog, l'entreprise explique que sa politique de s�curit� pr�c�dente visait � �tablir un consensus au sein de l'industrie sur l'att�nuation des risques li�s � l'IA - des garde-fous que l'industrie n'a pas r�ussi � franchir. Anthropic a �galement soulign� que sa politique de s�curit� �tait en d�calage avec le climat politique anti-r�glementaire qui r�gne actuellement � Washington.</p><p>La politique pr�c�dente d'Anthropic stipulait qu'elle devait interrompre la formation de mod�les plus puissants si leurs capacit�s d�passaient la capacit� de l'entreprise � les contr�ler et � assurer leur s�curit� - une mesure qui a �t� supprim�e dans la nouvelle politique. Anthropic a fait valoir que les d�veloppeurs d'IA responsables qui interrompent leur croissance alors que des acteurs moins prudents vont de l'avant risquent � d'aboutir � un monde moins s�r �. Dans le cadre de la nouvelle politique, Anthropic a d�clar� qu'elle s�parerait ses propres plans de s�curit� de ses recommandations pour l'industrie de l'IA.</p><p>Anthropic a �crit qu'elle avait esp�r� que ses principes de s�curit� initiaux � encourageraient d'autres entreprises d'IA � introduire des politiques similaires �. C'est l'id�e d'une � course vers le haut � (l'inverse d'une � course vers le bas �), dans laquelle les diff�rents acteurs de l'industrie sont incit�s � am�liorer, plut�t qu'� affaiblir, les mesures de protection de leurs mod�les et leur position globale en mati�re de s�curit�". L'entreprise sugg�re aujourd'hui que cela n'a pas �t� le cas.</p><p>Un porte-parole d'Anthropic a d�crit la politique mise � jour comme � <i>la plus solide � ce jour en mati�re de responsabilit� publique et de transparence </i>�. � <i>Nous avons franchi une �tape importante par rapport � nos politiques pr�c�dentes en nous engageant � publier � intervalles r�guliers des rapports d�taill�s sur nos plans visant � renforcer nos mesures d'att�nuation des risques, ainsi que sur les mod�les de menace et les capacit�s de tous nos mod�les</i> �, a d�clar� le porte-parole. � <i>Depuis le d�but, nous avons dit que le rythme de l'IA et les incertitudes dans le domaine nous obligeraient � it�rer et � am�liorer rapidement la politique.</i> �</p><p>En juin 2025, <a href="https://intelligence-artificielle.developpez.com/actu/372839/Anthropic-lance-un-nouveau-service-Claude-destine-a-l-armee-et-aux-services-de-renseignement-Claude-Gov-pourrait-mieux-analyser-les-informations-classifiees-et-concurrence-directement-ChatGPT-Gov-d-OpenAI/" target="_blank" rel="noopener noreferrer">Anthropic avait lanc� Claude Gov pour les clients am�ricains du secteur de la s�curit� nationale</a>, un ensemble exclusif de mod�les d'intelligence artificielle (IA) qui est d�j� entre les mains de certaines agences gouvernementales. Anthropic a d�clar� que les mod�les disponibles sous Claude Gov sont con�us pour traiter des documents classifi�s, fonctionner mieux dans les langues et dialectes "essentiels � la s�curit� nationale" et avoir une meilleure compr�hension des contextes du renseignement et de la d�fense. Le produit d'IA destin� au gouvernement d'Anthropic �tait annonc� alors que plusieurs entreprises se font concurrence pour vendre des outils technologiques �mergents aux agences f�d�rales. L'enjeu est de remporter de nouveaux contrats et d'acqu�rir le prestige de travailler sur des missions gouvernementales importantes.</p><p><img src="https://www.developpez.net/forums/attachments/p674569d1/a/a/a"></p><p>
Voici l'annonce d'Anthropic :</p><p><b><span>Politique de mise � l'�chelle responsable d'Anthropic : Version 3.0</span></b></p><p>Nous publions la troisi�me version de notre Politique de Changement d'�chelle responsable (Responsible Scaling Policy - RSP), le cadre volontaire que nous utilisons pour att�nuer les risques catastrophiques des syst�mes d'IA.</p><p>Anthropic dispose d'une RSP depuis plus de deux ans, et nous avons beaucoup appris sur ses avantages et ses d�fauts. Nous mettons donc � jour la politique afin de renforcer ce qui a bien fonctionn� jusqu'� pr�sent, d'am�liorer la politique si n�cessaire et de mettre en �uvre de nouvelles mesures pour accro�tre la transparence et la responsabilit� de notre prise de d�cision.</p><p>Dans ce billet, nous examinerons certaines des id�es qui sous-tendent les changements.</p><p><b>Le RSP original et notre th�orie du changement</b></p><p>Le RSP est notre tentative de r�soudre le probl�me de la prise en compte des risques li�s � l'IA qui ne sont pas pr�sents au moment o� la politique est r�dig�e, mais qui pourraient appara�tre rapidement en raison de l'�volution exponentielle de la technologie. Lorsque nous avons r�dig� le RSP original en septembre 2023, les grands mod�les de langage �taient essentiellement des interfaces de chat. Aujourd'hui, ils peuvent naviguer sur le web, �crire et ex�cuter du code, utiliser des ordinateurs et entreprendre des actions autonomes en plusieurs �tapes. L'apparition de ces nouvelles capacit�s s'est accompagn�e de l'apparition de nouveaux risques. Nous nous attendons � ce que cette tendance se poursuive.</p><p>Nous avons ax� le RSP sur le principe des engagements conditionnels, ou � si-alors �. Si un mod�le d�passe certains niveaux de capacit� (par exemple, les capacit�s en sciences biologiques qui pourraient aider � la cr�ation d'armes dangereuses), la politique stipule que nous devons introduire un nouvel ensemble de garanties plus strictes (par exemple, contre l'utilisation abusive des mod�les et le vol des poids des mod�les).</p><p>Chaque ensemble de garanties correspond � un � niveau de s�curit� de l'IA � (AI Safety Level - ASL) : par exemple, l'ASL-2 correspond � un ensemble de garanties requises, tandis que l'ASL-3 correspond � un ensemble de garanties plus strictes n�cessaires pour des mod�les d'IA plus performants.</p><p>Les premi�res ASL (ASL-2 et ASL-3) ont �t� d�finies de mani�re tr�s d�taill�e, mais il �tait plus difficile de sp�cifier les garanties correctes pour des mod�les qui n'�taient pas encore au point depuis plusieurs g�n�rations. Nous avons donc intentionnellement laiss� les ASL ult�rieures (ASL-4 et suivantes) largement ind�finies et esp�rions les d�velopper plus en d�tail une fois que nous aurions une meilleure id�e de ce qu'impliqueraient des niveaux de capacit� d'IA plus �lev�s.</p><p>Voici une description approximative de notre � th�orie du changement �, c'est-�-dire des m�canismes par lesquels nous esp�rions influer sur l'�cosyst�me avec le RSP :</p><p>- <i>Une fonction de for�age interne.</i> Au sein d'Anthropic, nous esp�rions que le RSP nous obligerait � consid�rer d'importantes mesures de protection comme des conditions n�cessaires au lancement (et � la formation) de nouveaux mod�les. L'importance de ces garde-fous serait ainsi clairement per�ue par la grande organisation en pleine croissance, ce qui nous inciterait � progresser plus rapidement.</p><p>- <i>Une course au sommet.</i> Nous esp�rions que l'annonce de notre RSP encouragerait d'autres entreprises d'IA � mettre en place des politiques similaires. C'est l'id�e d'une � course vers le haut � (l'inverse d'une � course vers le bas �), dans laquelle les diff�rents acteurs de l'industrie sont incit�s � am�liorer, plut�t qu'� affaiblir, les mesures de protection de leurs mod�les et leur position globale en mati�re de s�curit�. Au fil du temps, nous esp�rions que les RSP, ou des politiques similaires, deviendraient des normes industrielles volontaires ou serviraient de base � des lois sur l'IA visant � encourager la s�curit� et la transparence dans le d�veloppement de mod�les d'IA.</p><p>- <i>Cr�er un plus grand consensus sur les risques.</i> Nous avons consid�r� les seuils de capacit� comme des moments potentiellement importants pour l'industrie. Si nous atteignions un seuil de capacit� important (tel que la capacit� des mod�les d'IA � soutenir la production de bout en bout d'armes biologiques), nous mettrions nous-m�mes en place les mesures de protection appropri�es et utiliserions les preuves que nous aurions obtenues sur les capacit�s de l'IA pour recommander � d'autres entreprises et gouvernements de prendre �galement des mesures en ce sens. En d'autres termes, nous pensions que les seuils de capacit� pourraient �tre des points int�ressants pour aller au-del� d'une action unilat�rale (Anthropic exigeant des mesures de protection pour ses propres mod�les) et encourager une action multilat�rale (d'autres entreprises d'IA et/ou des gouvernements exigeant �galement de telles mesures de protection).</p><p>- <i>Regarder vers l'avenir.</i> Nous avons reconnu qu'� certains des derniers seuils de capacit�, l'intensit� des contre-mesures que nous envisagions (par exemple, l'obtention d'une robustesse �lev�e contre l'utilisation abusive des mod�les d'IA par des acteurs �tatiques) serait probablement difficile, voire impossible, pour Anthropic d'agir de mani�re unilat�rale. Nous esp�rions qu'au moment o� nous atteindrions ces capacit�s sup�rieures, le monde aurait clairement per�u les dangers et que nous serions en mesure de coordonner avec les gouvernements du monde entier la mise en �uvre de mesures de protection qu'il est difficile pour une entreprise de mettre en �uvre seule.</p><p><b>�valuation de notre th�orie du changement</b></p><p>Deux ans et demi plus tard, nous estimons honn�tement que certaines parties de cette th�orie du changement ont fonctionn� comme nous l'esp�rions, mais que d'autres n'ont pas fonctionn�. Voici les domaines dans lesquels le RSP a �t� couronn� de succ�s :</p><p>- Notre RSP nous a incit�s � mettre en place des garanties plus solides. Par exemple, pour nous conformer � notre norme de d�ploiement ASL-3 (qui concerne principalement les risques li�s aux armes chimiques et biologiques provenant d'acteurs de la menace disposant de ressources et de comp�tences relativement modestes), nous avons mis au point des m�thodes de plus en plus sophistiqu�es et pr�cises (en particulier des classificateurs d'entr�e et de sortie) pour bloquer les contenus pr�occupants.</p><p>- D'une mani�re plus g�n�rale, la mise en �uvre globale de la norme ASL-3 s'est av�r�e r�alisable. Nous avons activ� les mesures de protection ASL-3 pour les mod�les concern�s en mai 2025 et nous nous effor�ons de les am�liorer depuis lors.</p><p>- Notre RSP a encourag� d'autres entreprises d'IA � adopter des normes quelque peu similaires : quelques mois apr�s l'annonce de notre RSP, OpenAI et Google DeepMind ont tous deux adopt� des cadres largement similaires. Certaines entreprises ont �galement mis en �uvre des classificateurs li�s aux armes biologiques dans une veine similaire � nos d�fenses ASL-3. Les principes qui sous-tendent ces normes volontaires, y compris celles du RSP, ont contribu� � l'�laboration des premi�res politiques en mati�re d'IA. Nous avons vu des gouvernements du monde entier (par exemple en Californie avec la loi SB 53, � New York avec la loi RAISE et avec les codes de pratique de la loi europ�enne sur l'IA) commencer � exiger des d�veloppeurs d'IA d'avant-garde qu'ils cr�ent et publient des cadres d'�valuation et de gestion des risques catastrophiques - exigences auxquelles Anthropic r�pond par le biais de documents publics, notamment son cadre de conformit� aux fronti�res (Frontier Compliance Framework). Encourager ce type de cadres de transparence rigoureux pour l'industrie �tait exactement ce que notre RSP avait pr�vu de faire.</p><p>N�anmoins, d'autres �l�ments de notre th�orie du changement n'ont pas donn� les r�sultats escompt�s :</p><p>- L'id�e d'utiliser les seuils du RSP pour cr�er un plus grand consensus sur les risques li�s � l'IA ne s'est pas concr�tis�e dans la pratique, m�me si cet effet s'est partiellement manifest�. Nous avons constat� que les niveaux de capacit� pr�d�finis �taient beaucoup plus ambigus que nous l'avions pr�vu : dans certains cas, les capacit�s des mod�les se sont clairement rapproch�es des seuils du PSR, mais nous n'avons pas pu d�terminer avec certitude s'ils avaient d�finitivement franchi ces seuils. La science de l'�valuation des mod�les n'est pas suffisamment d�velopp�e pour fournir des r�ponses d�finitives. Dans de tels cas, nous avons adopt� une approche de pr�caution et mis en �uvre les mesures de sauvegarde appropri�es, mais notre incertitude interne se traduit par un faible argumentaire externe en faveur d'une action multilat�rale dans l'ensemble de l'industrie de l'IA.</p><p>Les risques biologiques sont un exemple de cette � zone d'ambigu�t� �. Nos mod�les pr�sentent d�sormais suffisamment de connaissances biologiques pour r�ussir la plupart des tests que nous pouvons effectuer rapidement et facilement, de sorte que nous ne pouvons plus avancer d'arguments solides pour d�montrer que les risques d'un mod�le donn� sont faibles. Mais ces tests ne suffisent pas non plus � d�montrer que les risques sont �lev�s. Nous avons cherch� � obtenir des preuves suppl�mentaires, par exemple en soutenant un essai approfondi en laboratoire humide, mais les r�sultats restent ambigus, en particulier parce que les �tudes prennent suffisamment de temps pour que des mod�les plus puissants soient disponibles au moment o� elles sont achev�es.</p><p>- Malgr� les progr�s rapides des capacit�s de l'IA au cours des trois derni�res ann�es, l'action gouvernementale en mati�re de s�curit� de l'IA a progress� lentement. L'environnement politique a �volu� pour donner la priorit� � la comp�titivit� de l'IA et � la croissance �conomique, alors que les discussions sur la s�curit� n'ont pas encore gagn� en importance au niveau f�d�ral. Nous restons convaincus qu'un engagement gouvernemental efficace sur la s�curit� de l'IA est � la fois n�cessaire et r�alisable, et nous souhaitons continuer � faire avancer une conversation fond�e sur des preuves, des int�r�ts de s�curit� nationale, la comp�titivit� �conomique et la confiance du public. Mais il s'agit d'un projet � long terme, qui ne se met pas en place de mani�re organique � mesure que l'IA devient plus performante ou franchit certains seuils.</p><p>Comme indiqu� plus haut, nous avons �t� en mesure de mettre en �uvre les garanties de l'ASL-3 de mani�re unilat�rale et � des co�ts raisonnables pour le fonctionnement de l'entreprise. Toutefois, cela pourrait ne pas �tre le cas pour des niveaux de capacit� et des ASL plus �lev�s. Alors que nos ASL sup�rieures sont largement ind�finies, les mesures d'att�nuation robustes que nous avons d�finies dans le RSP pr�c�dent pourraient s'av�rer carr�ment impossibles � mettre en �uvre sans une action collective. Pour illustrer l'ampleur du d�fi, un rapport de la RAND sur la s�curit� des poids mod�les indique que sa norme de s�curit� � SL5 �,...
</p><p>La fin de cet article est r�serv�e aux abonn�s. Soutenez le Club Developpez.com en <a href="https://premium.developpez.com/abonnement">prenant un abonnement</a> pour que nous puissions continuer � vous proposer des publications.</p> </div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>