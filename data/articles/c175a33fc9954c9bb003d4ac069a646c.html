<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>GitHub - shiyu-coder/Kronos: Kronos: A Foundation Model for the Language of Financial Markets</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.8; color: #e2e8f0; max-width: 800px; margin: 40px auto; padding: 0 20px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.5em; }
  .metadata { color: #94a3b8; font-size: 0.9em; margin-bottom: 2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 1em; }
  img { max-width: 100%; height: auto; border-radius: 8px; }
  a { color: #00d9ff; }
  p { margin-bottom: 1em; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 15px; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 15px; border-radius: 6px; overflow-x: auto; }

  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }

</style>
</head>
<body>
  <h1>GitHub - shiyu-coder/Kronos: Kronos: A Foundation Model for the Language of Financial Markets</h1>
  <div class="metadata">
    Source: GitHub Trending Python | Date: Invalid Date | Lang: EN |
    <a href="https://github.com/shiyu-coder/Kronos" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div><article><div>
  <p></p><h2><b>Kronos: A Foundation Model for the Language of Financial Markets </b></h2><a href="#kronos-a-foundation-model-for-the-language-of-financial-markets-"></a><p></p>
</div>
<p><a href="https://huggingface.co/NeoQuasar"> 
<img src="https://camo.githubusercontent.com/0be781a4e24a8195532785d11548bf3331b8f25e9b5d6ecc7130deb4002eb03f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa4972d48756767696e675f466163652d79656c6c6f77" alt="Hugging Face" /> 
</a> 
<a href="https://shiyu-coder.github.io/Kronos-demo/"> <img src="https://camo.githubusercontent.com/3cec5ca305d9b76e8e13eea39292c8c4ffdaa9c30bdf6f7a39573ba77ece7e9f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f9a802d4c6976655f44656d6f2d627269676874677265656e" alt="Live Demo" /> </a>
<a href="https://github.com/shiyu-coder/Kronos/graphs/commit-activity"> 
<img src="https://camo.githubusercontent.com/f82b9951e5f5b0c57b405be2fa9792688abf3615a1b195f729d9381e7b1cb95f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f73686979752d636f6465722f4b726f6e6f733f636f6c6f723d626c7565" alt="Last Commit" /> 
</a> 
<a href="https://github.com/shiyu-coder/Kronos/stargazers"> 
<img src="https://camo.githubusercontent.com/0a45762c8f44ee3fabbfe05393fe66baf7cfc7a87028b9f030221d40537ff8e9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73686979752d636f6465722f4b726f6e6f733f636f6c6f723d6c69676874626c7565" alt="GitHub Stars" /> 
</a> 
<a href="https://github.com/shiyu-coder/Kronos/network/members"> 
<img src="https://camo.githubusercontent.com/61587d14ff21c47928e143ff230096108955264120898e19ad1fd8d85fa1ce15/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f73686979752d636f6465722f4b726f6e6f733f636f6c6f723d79656c6c6f77" alt="GitHub Forks" /> 
</a> 
<a href="https://github.com/shiyu-coder/Kronos/blob/master/LICENSE"> 
<img src="https://camo.githubusercontent.com/c3be246ea79c5acb596a2b78d4b832c2248cd5e3a096b3699d446639a447e561/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f73686979752d636f6465722f4b726f6e6f733f636f6c6f723d677265656e" alt="License" /> 
</a>
</p>

<p>
<a target="_blank" href="https://github.com/shiyu-coder/Kronos/blob/master/figures/logo.png"><img src="https://github.com/shiyu-coder/Kronos/raw/master/figures/logo.png" /></a>
</p>
<blockquote>
<p>Kronos is the <strong>first open-source foundation model</strong> for financial candlesticks (K-lines),
trained on data from over <strong>45 global exchanges</strong>.</p>
</blockquote>

<p></p><h2> News</h2><a href="#-news"></a><p></p>
<ul>
<li> <strong>[2025.11.10]</strong> Kronos has been accpeted by AAAI 2026.</li>
<li> <strong>[2025.08.17]</strong> We have released the scripts for fine-tuning! Check them out to adapt Kronos to your own tasks.</li>
<li> <strong>[2025.08.02]</strong> Our paper is now available on <a href="https://arxiv.org/abs/2508.02739">arXiv</a>!</li>
</ul>
<p></p><h2> Introduction</h2><a href="#-introduction"></a><p></p>
<p><strong>Kronos</strong> is a family of decoder-only foundation models, pre-trained specifically for the "language" of financial marketsâ€”K-line sequences. Unlike general-purpose TSFMs, Kronos is designed to handle the unique, high-noise characteristics of financial data. It leverages a novel two-stage framework:</p>
<ol>
<li>A specialized tokenizer first quantizes continuous, multi-dimensional K-line data (OHLCV) into <strong>hierarchical discrete tokens</strong>.</li>
<li>A large, autoregressive Transformer is then pre-trained on these tokens, enabling it to serve as a unified model for diverse quantitative tasks.</li>
</ol>
<p>
    <a target="_blank" href="https://github.com/shiyu-coder/Kronos/blob/master/figures/overview.png"><img src="https://github.com/shiyu-coder/Kronos/raw/master/figures/overview.png" alt="" /></a>
</p>
<p></p><h2> Live Demo</h2><a href="#-live-demo"></a><p></p>
<p>We have set up a live demo to visualize Kronos's forecasting results. The webpage showcases a forecast for the <strong>BTC/USDT</strong> trading pair over the next 24 hours.</p>
<p><strong> <a href="https://shiyu-coder.github.io/Kronos-demo/">Access the Live Demo Here</a></strong></p>
<p></p><h2> Model Zoo</h2><a href="#-model-zoo"></a><p></p>
<p>We release a family of pre-trained models with varying capacities to suit different computational and application needs. All models are readily accessible from the Hugging Face Hub.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Tokenizer</th>
<th>Context length</th>
<th>Params</th>
<th>Open-source</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kronos-mini</td>
<td><a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-2k">Kronos-Tokenizer-2k</a></td>
<td>2048</td>
<td>4.1M</td>
<td> <a href="https://huggingface.co/NeoQuasar/Kronos-mini">NeoQuasar/Kronos-mini</a></td>
</tr>
<tr>
<td>Kronos-small</td>
<td><a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base">Kronos-Tokenizer-base</a></td>
<td>512</td>
<td>24.7M</td>
<td> <a href="https://huggingface.co/NeoQuasar/Kronos-small">NeoQuasar/Kronos-small</a></td>
</tr>
<tr>
<td>Kronos-base</td>
<td><a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base">Kronos-Tokenizer-base</a></td>
<td>512</td>
<td>102.3M</td>
<td> <a href="https://huggingface.co/NeoQuasar/Kronos-base">NeoQuasar/Kronos-base</a></td>
</tr>
<tr>
<td>Kronos-large</td>
<td><a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base">Kronos-Tokenizer-base</a></td>
<td>512</td>
<td>499.2M</td>
<td></td>
</tr>
</tbody>
</table>
<p></p><h2> Getting Started</h2><a href="#-getting-started"></a><p></p>
<p></p><h3>Installation</h3><a href="#installation"></a><p></p>
<ol>
<li>Install Python 3.10+, and then install the dependencies:</li>
</ol>
<div><pre>pip install -r requirements.txt</pre></div>
<p></p><h3> Making Forecasts</h3><a href="#-making-forecasts"></a><p></p>
<p>Forecasting with Kronos is straightforward using the <code>KronosPredictor</code> class. It handles data preprocessing, normalization, prediction, and inverse normalization, allowing you to get from raw data to forecasts in just a few lines of code.</p>
<p><strong>Important Note</strong>: The <code>max_context</code> for <code>Kronos-small</code> and <code>Kronos-base</code> is <strong>512</strong>. This is the maximum sequence length the model can process. For optimal performance, it is recommended that your input data length (i.e., <code>lookback</code>) does not exceed this limit. The <code>KronosPredictor</code> will automatically handle truncation for longer contexts.</p>
<p>Here is a step-by-step guide to making your first forecast.</p>
<p></p><h4>1. Load the Tokenizer and Model</h4><a href="#1-load-the-tokenizer-and-model"></a><p></p>
<p>First, load a pre-trained Kronos model and its corresponding tokenizer from the Hugging Face Hub.</p>
<div><pre><span>from</span> <span>model</span> <span>import</span> <span>Kronos</span>, <span>KronosTokenizer</span>, <span>KronosPredictor</span>

<span># Load from Hugging Face Hub</span>
<span>tokenizer</span> <span>=</span> <span>KronosTokenizer</span>.<span>from_pretrained</span>(<span>"NeoQuasar/Kronos-Tokenizer-base"</span>)
<span>model</span> <span>=</span> <span>Kronos</span>.<span>from_pretrained</span>(<span>"NeoQuasar/Kronos-small"</span>)</pre></div>
<p></p><h4>2. Instantiate the Predictor</h4><a href="#2-instantiate-the-predictor"></a><p></p>
<p>Create an instance of <code>KronosPredictor</code>, passing the model, tokenizer, and desired device.</p>
<div><pre><span># Initialize the predictor</span>
<span>predictor</span> <span>=</span> <span>KronosPredictor</span>(<span>model</span>, <span>tokenizer</span>, <span>max_context</span><span>=</span><span>512</span>)</pre></div>
<p></p><h4>3. Prepare Input Data</h4><a href="#3-prepare-input-data"></a><p></p>
<p>The <code>predict</code> method requires three main inputs:</p>
<ul>
<li><code>df</code>: A pandas DataFrame containing the historical K-line data. It must include columns <code>['open', 'high', 'low', 'close']</code>. <code>volume</code> and <code>amount</code> are optional.</li>
<li><code>x_timestamp</code>: A pandas Series of timestamps corresponding to the historical data in <code>df</code>.</li>
<li><code>y_timestamp</code>: A pandas Series of timestamps for the future periods you want to predict.</li>
</ul>
<div><pre><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>

<span># Load your data</span>
<span>df</span> <span>=</span> <span>pd</span>.<span>read_csv</span>(<span>"./data/XSHG_5min_600977.csv"</span>)
<span>df</span>[<span>'timestamps'</span>] <span>=</span> <span>pd</span>.<span>to_datetime</span>(<span>df</span>[<span>'timestamps'</span>])

<span># Define context window and prediction length</span>
<span>lookback</span> <span>=</span> <span>400</span>
<span>pred_len</span> <span>=</span> <span>120</span>

<span># Prepare inputs for the predictor</span>
<span>x_df</span> <span>=</span> <span>df</span>.<span>loc</span>[:<span>lookback</span><span>-</span><span>1</span>, [<span>'open'</span>, <span>'high'</span>, <span>'low'</span>, <span>'close'</span>, <span>'volume'</span>, <span>'amount'</span>]]
<span>x_timestamp</span> <span>=</span> <span>df</span>.<span>loc</span>[:<span>lookback</span><span>-</span><span>1</span>, <span>'timestamps'</span>]
<span>y_timestamp</span> <span>=</span> <span>df</span>.<span>loc</span>[<span>lookback</span>:<span>lookback</span><span>+</span><span>pred_len</span><span>-</span><span>1</span>, <span>'timestamps'</span>]</pre></div>
<p></p><h4>4. Generate Forecasts</h4><a href="#4-generate-forecasts"></a><p></p>
<p>Call the <code>predict</code> method to generate forecasts. You can control the sampling process with parameters like <code>T</code>, <code>top_p</code>, and <code>sample_count</code> for probabilistic forecasting.</p>
<div><pre><span># Generate predictions</span>
<span>pred_df</span> <span>=</span> <span>predictor</span>.<span>predict</span>(
    <span>df</span><span>=</span><span>x_df</span>,
    <span>x_timestamp</span><span>=</span><span>x_timestamp</span>,
    <span>y_timestamp</span><span>=</span><span>y_timestamp</span>,
    <span>pred_len</span><span>=</span><span>pred_len</span>,
    <span>T</span><span>=</span><span>1.0</span>,          <span># Temperature for sampling</span>
    <span>top_p</span><span>=</span><span>0.9</span>,      <span># Nucleus sampling probability</span>
    <span>sample_count</span><span>=</span><span>1</span>  <span># Number of forecast paths to generate and average</span>
)

<span>print</span>(<span>"Forecasted Data Head:"</span>)
<span>print</span>(<span>pred_df</span>.<span>head</span>())</pre></div>
<p>The <code>predict</code> method returns a pandas DataFrame containing the forecasted values for <code>open</code>, <code>high</code>, <code>low</code>, <code>close</code>, <code>volume</code>, and <code>amount</code>, indexed by the <code>y_timestamp</code> you provided.</p>
<p>For efficient processing of multiple time series, Kronos provides a <code>predict_batch</code> method that enables parallel prediction on multiple datasets simultaneously. This is particularly useful when you need to forecast multiple assets or time periods at once.</p>
<div><pre><span># Prepare multiple datasets for batch prediction</span>
<span>df_list</span> <span>=</span> [<span>df1</span>, <span>df2</span>, <span>df3</span>]  <span># List of DataFrames</span>
<span>x_timestamp_list</span> <span>=</span> [<span>x_ts1</span>, <span>x_ts2</span>, <span>x_ts3</span>]  <span># List of historical timestamps</span>
<span>y_timestamp_list</span> <span>=</span> [<span>y_ts1</span>, <span>y_ts2</span>, <span>y_ts3</span>]  <span># List of future timestamps</span>

<span># Generate batch predictions</span>
<span>pred_df_list</span> <span>=</span> <span>predictor</span>.<span>predict_batch</span>(
    <span>df_list</span><span>=</span><span>df_list</span>,
    <span>x_timestamp_list</span><span>=</span><span>x_timestamp_list</span>,
    <span>y_timestamp_list</span><span>=</span><span>y_timestamp_list</span>,
    <span>pred_len</span><span>=</span><span>pred_len</span>,
    <span>T</span><span>=</span><span>1.0</span>,
    <span>top_p</span><span>=</span><span>0.9</span>,
    <span>sample_count</span><span>=</span><span>1</span>,
    <span>verbose</span><span>=</span><span>True</span>
)

<span># pred_df_list contains prediction results in the same order as input</span>
<span>for</span> <span>i</span>, <span>pred_df</span> <span>in</span> <span>enumerate</span>(<span>pred_df_list</span>):
    <span>print</span>(<span>f"Predictions for series <span><span>{</span><span>i</span><span>}</span></span>:"</span>)
    <span>print</span>(<span>pred_df</span>.<span>head</span>())</pre></div>
<p><strong>Important Requirements for Batch Prediction:</strong></p>
<ul>
<li>All series must have the same historical length (lookback window)</li>
<li>All series must have the same prediction length (<code>pred_len</code>)</li>
<li>Each DataFrame must contain the required columns: <code>['open', 'high', 'low', 'close']</code></li>
<li><code>volume</code> and <code>amount</code> columns are optional and will be filled with zeros if missing</li>
</ul>
<p>The <code>predict_batch</code> method leverages GPU parallelism for efficient processing and automatically handles normalization and denormalization for each series independently.</p>
<p></p><h4>5. Example and Visualization</h4><a href="#5-example-and-visualization"></a><p></p>
<p>For a complete, runnable script that includes data loading, prediction, and plotting, please see <a href="https://github.com/shiyu-coder/Kronos/blob/master/examples/prediction_example.py"><code>examples/prediction_example.py</code></a>.</p>
<p>Running this script will generate a plot comparing the ground truth data against the model's forecast, similar to the one shown below:</p>
<p>
    <a target="_blank" href="https://github.com/shiyu-coder/Kronos/blob/master/figures/prediction_example.png"><img src="https://github.com/shiyu-coder/Kronos/raw/master/figures/prediction_example.png" alt="Forecast Example" /></a>
</p>
<p>Additionally, we provide a script that makes predictions without Volume and Amount data, which can be found in <a href="https://github.com/shiyu-coder/Kronos/blob/master/examples/prediction_wo_vol_example.py"><code>examples/prediction_wo_vol_example.py</code></a>.</p>
<p></p><h2> Finetuning on Your Own Data (A-Share Market Example)</h2><p></p>
<p>We provide a complete pipeline for finetuning Kronos on your own datasets. As an example, we demonstrate how to use <a href="https://github.com/microsoft/qlib">Qlib</a> to prepare data from the Chinese A-share market and conduct a simple backtest.</p>
<blockquote>
<p><strong>Disclaimer:</strong> This pipeline is intended as a demonstration to illustrate the finetuning process. It is a simplified example and not a production-ready quantitative trading system. A robust quantitative strategy requires more sophisticated techniques, such as portfolio optimization and risk factor neutralization, to achieve stable alpha.</p>
</blockquote>
<p>The finetuning process is divided into four main steps:</p>
<ol>
<li><strong>Configuration</strong>: Set up paths and hyperparameters.</li>
<li><strong>Data Preparation</strong>: Process and split your data using Qlib.</li>
<li><strong>Model Finetuning</strong>: Finetune the Tokenizer and the Predictor models.</li>
<li><strong>Backtesting</strong>: Evaluate the finetuned model's performance.</li>
</ol>
<p></p><h3>Prerequisites</h3><a href="#prerequisites"></a><p></p>
<ol>
<li>First, ensure you have all dependencies from <code>requirements.txt</code> installed.</li>
<li>This pipeline relies on <code>qlib</code>. Please install it:
<div><pre>  pip install pyqlib</pre></div>
</li>
<li>You will need to prepare your Qlib data. Follow the <a href="https://github.com/microsoft/qlib">official Qlib guide</a> to download and set up your data locally. The example scripts assume you are using daily frequency data.</li>
</ol>
<p></p><h3>Step 1: Configure Your Experiment</h3><a href="#step-1-configure-your-experiment"></a><p></p>
<p>All settings for data, training, and model paths are centralized in <code>finetune/config.py</code>. Before running any scripts, please <strong>modify the following paths</strong> according to your environment:</p>
<ul>
<li><code>qlib_data_path</code>: Path to your local Qlib data directory.</li>
<li><code>dataset_path</code>: Directory where the processed train/validation/test pickle files will be saved.</li>
<li><code>save_path</code>: Base directory for saving model checkpoints.</li>
<li><code>backtest_result_path</code>: Directory for saving backtesting results.</li>
<li><code>pretrained_tokenizer_path</code> and <code>pretrained_predictor_path</code>: Paths to the pre-trained models you want to start from (can be local paths or Hugging Face model names).</li>
</ul>
<p>You can also adjust other parameters like <code>instrument</code>, <code>train_time_range</code>, <code>epochs</code>, and <code>batch_size</code> to fit your specific task. If you don't use <a href="https://www.comet.com/">Comet.ml</a>, set <code>use_comet = False</code>.</p>
<p></p><h3>Step 2: Prepare the Dataset</h3><a href="#step-2-prepare-the-dataset"></a><p></p>
<p>Run the data preprocessing script. This script will load raw market data from your Qlib directory, process it, split it into training, validation, and test sets, and save them as pickle files.</p>
<div><pre>python finetune/qlib_data_preprocess.py</pre></div>
<p>After running, you will find <code>train_data.pkl</code>, <code>val_data.pkl</code>, and <code>test_data.pkl</code> in the directory specified by <code>dataset_path</code> in your config.</p>
<p></p><h3>Step 3: Run the Finetuning</h3><a href="#step-3-run-the-finetuning"></a><p></p>
<p>The finetuning process consists of two stages: finetuning the tokenizer and then the predictor. Both training scripts are designed for multi-GPU training using <code>torchrun</code>.</p>
<p></p><h4>3.1 Finetune the Tokenizer</h4><a href="#31-finetune-the-tokenizer"></a><p></p>
<p>This step adjusts the tokenizer to the data distribution of your specific domain.</p>
<div><pre><span><span>#</span> Replace NUM_GPUS with the number of GPUs you want to use (e.g., 2)</span>
torchrun --standalone --nproc_per_node=NUM_GPUS finetune/train_tokenizer.py</pre></div>
<p>The best tokenizer checkpoint will be saved to the path configured in <code>config.py</code> (derived from <code>save_path</code> and <code>tokenizer_save_folder_name</code>).</p>
<p></p><h4>3.2 Finetune the Predictor</h4><a href="#32-finetune-the-predictor"></a><p></p>
<p>This step finetunes the main Kronos model for the forecasting task.</p>
<div><pre><span><span>#</span> Replace NUM_GPUS with the number of GPUs you want to use (e.g., 2)</span>
torchrun --standalone --nproc_per_node=NUM_GPUS finetune/train_predictor.py</pre></div>
<p>The best predictor checkpoint will be saved to the path configured in <code>config.py</code>.</p>
<p></p><h3>Step 4: Evaluate with Backtesting</h3><a href="#step-4-evaluate-with-backtesting"></a><p></p>
<p>Finally, run the backtesting script to evaluate your finetuned model. This script loads the models, performs inference on the test set, generates prediction signals (e.g., forecasted price change), and runs a simple top-K strategy backtest.</p>
<div><pre><span><span>#</span> Specify the GPU for inference</span>
python finetune/qlib_test.py --device cuda:0</pre></div>
<p>The script will output a detailed performance analysis in your console and generate a plot showing the cumulative return curves of your strategy against the benchmark, similar to the one below:</p>
<p>
    <a target="_blank" href="https://github.com/shiyu-coder/Kronos/blob/master/figures/backtest_result_example.png"><img src="https://github.com/shiyu-coder/Kronos/raw/master/figures/backtest_result_example.png" alt="Backtest Example" /></a>
</p>
<p></p><h3> From Demo to Production: Important Considerations</h3><a href="#-from-demo-to-production-important-considerations"></a><p></p>
<ul>
<li><strong>Raw Signals vs. Pure Alpha</strong>: The signals generated by the model in this demo are raw predictions. In a real-world quantitative workflow, these signals would typically be fed into a portfolio optimization model. This model would apply constraints to neutralize exposure to common risk factors (e.g., market beta, style factors like size and value), thereby isolating the <strong>"pure alpha"</strong> and improving the strategy's robustness.</li>
<li><strong>Data Handling</strong>: The provided <code>QlibDataset</code> is an example. For different data sources or formats, you will need to adapt the data loading and preprocessing logic.</li>
<li><strong>Strategy and Backtesting Complexity</strong>: The simple top-K strategy used here is a basic starting point. Production-level strategies often incorporate more complex logic for portfolio construction, dynamic position sizing, and risk management (e.g., stop-loss/take-profit rules). Furthermore, a high-fidelity backtest should meticulously model transaction costs, slippage, and market impact to provide a more accurate estimate of real-world performance.</li>
</ul>
<blockquote>
<p><strong> AI-Generated Comments</strong>: Please note that many of the code comments within the <code>finetune/</code> directory were generated by an AI assistant (Gemini 2.5 Pro) for explanatory purposes. While they aim to be helpful, they may contain inaccuracies. We recommend treating the code itself as the definitive source of logic.</p>
</blockquote>
<p></p><h2> Citation</h2><a href="#-citation"></a><p></p>
<p>If you use Kronos in your research, we would appreciate a citation to our <a href="https://arxiv.org/abs/2508.02739">paper</a>:</p>
<div><pre><code>@misc{shi2025kronos,
      title={Kronos: A Foundation Model for the Language of Financial Markets}, 
      author={Yu Shi and Zongliang Fu and Shuo Chen and Bohan Zhao and Wei Xu and Changshui Zhang and Jian Li},
      year={2025},
      eprint={2508.02739},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST},
      url={https://arxiv.org/abs/2508.02739}, 
}
</code></pre></div>
<p></p><h2> License</h2><a href="#-license"></a><p></p>
<p>This project is licensed under the <a href="https://github.com/shiyu-coder/Kronos/blob/master/LICENSE">MIT License</a>.</p>
</article></div></div>
  </div>
</body>
</html>