<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="UTF-8">
<title>OpenAI lance GPT-5.3-Codex-Spark, optimis� par la puce Wafer Scale Engine 3 de Cerebras, pour un codage en temps r�el ultra-rapide, 15 fois plus rapide que son pr�d�cesseur</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>OpenAI lance GPT-5.3-Codex-Spark, optimis� par la puce Wafer Scale Engine 3 de Cerebras, pour un codage en temps r�el ultra-rapide, 15 fois plus rapide que son pr�d�cesseur</h1>
  <div class="metadata">
    Source: Developpez.com | Date: 2/13/2026 3:34:00 PM | Lang: FR |
    <a href="https://intelligence-artificielle.developpez.com/actu/380250/OpenAI-lance-GPT-5-3-Codex-Spark-optimise-par-la-puce-Wafer-Scale-Engine-3-de-Cerebras-pour-un-codage-en-temps-reel-ultra-rapide-15-fois-plus-rapide-que-son-predecesseur/" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><img src="https://www.developpez.com/images/logos/openai.png"> <b>OpenAI lance GPT-5.3-Codex-Spark pour un codage en temps r�el ultra-rapide. Aliment� par la puce Wafer Scale Engine 3 de Cerebras, Spark permettrait une inf�rence plus rapide et constitue la premi�re �tape importante du partenariat pluriannuel entre OpenAI et Cerebras. Le mod�le GPT-5.3-Codex original sert � des t�ches plus longues et plus lourdes qui n�cessitent un raisonnement et une ex�cution plus approfondis. En revanche, GPT-5.3-Codex-Spark se concentre sur des op�rations rapides. OpenAI le d�crit comme une version plus petite con�ue sp�cifiquement pour r�duire la latence pendant les processus d'inf�rence. Codex-Spark est lanc� en tant qu'aper�u de recherche pour les utilisateurs de ChatGPT Pro dans les derni�res versions de l'application Codex, de l'interface CLI et de l'extension VS Code.</b></p><p>Les outils de codage IA ont connu un essor fulgurant au cours de l'ann�e derni�re, poussant OpenAI a fourni plus d'efforts pour gagner des parts de march� sur ses concurrents tels qu'Anthropic et Cursor. R�cemment, <a href="https://intelligence-artificielle.developpez.com/actu/379998/OpenAI-lance-GPT-5-3-Codex-qui-est-25-pourcent-plus-rapide-que-GPT-5-2-et-qui-a-ete-le-premier-modele-d-OpenAI-a-aider-a-son-propre-debogage-pendant-son-developpement/" target="_blank">OpenAI a lanc� un nouveau mod�le de codage, GPT-5.3-Codex</a>. OpenAI est une entreprise am�ricaine d'intelligence artificielle (IA) qui s'est donn� pour mission de d�velopper et de promouvoir une intelligence artificielle g�n�rale � s�re et b�n�fique � toute l'humanit� �. L'entreprise est connue pour ses grands mod�les de langage tels que GPT-4o, la s�rie de mod�les de g�n�ration d'images DALL-E et le mod�le de g�n�ration de vid�os Sora.</p><p>Lors de l'annonce de GPT-5.3-Codex, la soci�t� a d�clar� que ce nouveau mod�le avait am�lior� ses � capacit�s de raisonnement et de connaissances professionnelles � et qu'il fonctionnerait 25 % plus rapidement que son pr�d�cesseur. OpenAI pr�cise que le nouveau mod�le GPT-5.3-Codex est son � premier mod�le qui a contribu� � sa propre cr�ation �. OpenAI affirme notamment : � <i>L'�quipe Codex a utilis� les premi�res versions pour d�boguer sa propre formation, g�rer son propre d�ploiement et diagnostiquer les r�sultats des tests et des �valuations. Notre �quipe a �t� impressionn�e par la capacit� de Codex � acc�l�rer son propre d�veloppement.</i> �</p><p>Plus r�cemment, OpenAI lance GPT-5.3-Codex-Spark pour un codage en temps r�el ultra-rapide. Ce mod�le est la premi�re �tape importante de la production issue du partenariat de plus de 10 milliards de dollars entre OpenAI et Cerebras, annonc� en janvier 2026. Cerebras Systems Inc. est une soci�t� am�ricaine sp�cialis�e dans l'intelligence artificielle (IA) qui construit des syst�mes informatiques pour des applications complexes d'apprentissage profond en IA. Sa technologie, le Cerebras Wafer Scale Engine (WSE), est un processeur int�gr� unique � l'�chelle d'une plaquette qui comprend des capacit�s de calcul, de m�moire et d'interconnexion.</p><div>
<p></p>
</div>
<p><b><span>GPT-5.3-Codex-Spark optimis� par Cerebras</span></b></p><p>En septembre 2025, Nvidia et OpenAI ont annonc� en grande pompe un protocole d�accord selon lequel Nvidia investirait jusqu�� 100 milliards de dollars dans une infrastructure d�OpenAI pour soutenir l�entra�nement et l�exploitation de ses mod�les d'IA. Cet investissement devait �galement s�accompagner de la fourniture de syst�mes GPU de tr�s grande capacit� destin�s � OpenAI. <a href="https://intelligence-artificielle.developpez.com/actu/379946/L-accord-de-100-milliards-de-dollars-entre-Nvidia-et-OpenAI-semble-avoir-disparu-Deux-geants-de-l-IA-ebranlent-la-confiance-du-marche-apres-l-echec-de-leur-investissement/" target="_blank">Mais le projet bat d�sormais de l�aile.</a> Aucune transaction n�a �t� finalis�e et le PDG de Nvidia, Jensen Huang, a clarifi� que le montant de 100 milliards de dollars n'�tait pas un engagement juridiquement contraignant.</p><p>De son c�t�, OpenAI rechercherait discr�tement des alternatives aux puces Nvidia depuis l'ann�e derni�re. La nouvelle annonce d'OpenAI semble confimer cela. Ainsi, OpenAI a annonc� GPT-5.3-Codex-Spark, une version all�g�e de son outil de codage agentique Codex. Aliment� par la puce Wafer Scale Engine 3 de Cerebras, Spark permettrait une inf�rence plus rapide et constitue la premi�re �tape importante du partenariat pluriannuel entre OpenAI et Cerebras.</p><p>Le mod�le GPT-5.3-Codex original sert � des t�ches plus longues et plus lourdes qui n�cessitent un raisonnement et une ex�cution plus approfondis. En revanche, GPT-5.3-Codex-Spark se concentre sur des op�rations rapides. OpenAI le d�crit comme une version plus petite con�ue sp�cifiquement pour r�duire la latence pendant les processus d'inf�rence. Ce nouvel outil int�gre le mat�riel de Cerebras directement dans l'infrastructure physique d'OpenAI, ce qui repr�sente une collaboration plus approfondie entre les deux soci�t�s.</p><p>OpenAI et Cerebras ont r�v�l� leur partenariat le mois dernier � travers un accord pluriannuel �valu� � plus de 10 milliards de dollars. � cette occasion, OpenAI a d�clar� : � L'int�gration de Cerebras dans notre gamme de solutions informatiques vise � rendre notre IA beaucoup plus r�active. � La soci�t� positionne d�sormais Spark comme la premi�re r�alisation de cette alliance, soulignant son r�le dans l'acc�l�ration des r�ponses de l'IA.</p><div>
<p></p>
</div><p>
Le Wafer Scale Engine 3 de Cerebras alimente les capacit�s d'inf�rence de Spark. Cette m�gapuce de troisi�me g�n�ration � l'�chelle d'une plaquette contient 4 000 milliards de transistors, permettant un calcul haute performance adapt� aux charges de travail de l'IA. OpenAI souligne l'ad�quation de Spark pour la collaboration en temps r�el et l'it�ration rapide. L'outil fonctionne comme un moteur de productivit� quotidien, aidant les utilisateurs � r�aliser rapidement des prototypes plut�t que des calculs prolong�s g�r�s par le mod�le de base GPT-5.3-Codex.</p><p>Spark fonctionne avec la latence la plus faible possible sur Codex. OpenAI explique son objectif dans une d�claration officielle : � <i>Codex-Spark est la premi�re �tape vers un Codex qui fonctionne selon deux modes compl�mentaires : la collaboration en temps r�el lorsque vous souhaitez une it�ration rapide, et les t�ches de longue dur�e lorsque vous avez besoin d'un raisonnement et d'une ex�cution plus approfondis.</i> � Les puces de Cerebras prennent en charge les flux de travail qui exigent une latence extr�mement faible.</p><p>Actuellement, Spark appara�t comme un aper�u de recherche r�serv� aux utilisateurs de ChatGPT Pro dans l'application Codex. Ce d�ploiement limit� permet de proc�der � des tests initiaux aupr�s des abonn�s au plan Pro. Avant l'annonce, le PDG d'OpenAI, Sam Altman, avait laiss� entendre la sortie sur X/Twitter. Il a publi� : � <i>Nous avons une nouveaut� sp�ciale � lancer pour les utilisateurs de Codex au plan Pro plus tard dans la journ�e.</i> � Altman a ajout� : � <i>Cela me r�jouit.</i> �</p><p>Fond�e il y a plus de dix ans, Cerebras s'est impos�e dans le secteur de l'IA. R�cemment, la soci�t� a lev� 1 milliard de dollars de capitaux frais, atteignant une valorisation de 23 milliards de dollars. Cerebras a fait part de son intention de proc�der � une introduction en bourse. Sean Lie, directeur technique et cofondateur de Cerebras, a comment� cette �volution : � <i>Ce qui nous enthousiasme le plus � propos de GPT-5.3-Codex-Spark, c'est le partenariat avec OpenAI et la communaut� des d�veloppeurs pour d�couvrir ce que l'inf�rence rapide rend possible : de nouveaux mod�les d'interaction, de nouveaux cas d'utilisation et une exp�rience de mod�le fondamentalement diff�rente.</i> � Lie a d�crit cet aper�u comme � un simple d�but �.</p><div>
<p></p>
</div><p>
Voici un extrait de l'annonce :</p><p><b><span>Pr�sentation de GPT‑5.3‑Codex‑Spark</span></b></p><p><b>Rapidit� et intelligence</b></p><p>Codex-Spark est optimis� pour les travaux interactifs o� la latence est aussi importante que l'intelligence. Vous pouvez collaborer avec le mod�le en temps r�el, l'interrompre ou le rediriger pendant qu'il fonctionne, et it�rer rapidement avec des r�ponses quasi instantan�es. Parce qu'il est con�u pour la rapidit�, Codex-Spark conserve un mode de fonctionnement par d�faut l�ger : il effectue des modifications minimales et cibl�es et n'ex�cute pas automatiquement de tests, sauf si vous le lui demandez.</p><p><b>Codage</b></p><p>Codex-Spark est un petit mod�le tr�s performant optimis� pour une inf�rence rapide. Sur SWE-Bench Pro et Terminal-Bench 2.0, deux benchmarks �valuant les capacit�s d'ing�nierie logicielle agentique, GPT‑5.3‑Codex‑Spark affiche des performances solides tout en accomplissant les t�ches en un temps record par rapport � GPT‑5.3‑Codex.</p><p><img src="https://www.developpez.net/forums/attachments/p674205d1/a/a/a"><br>
<img src="https://www.developpez.net/forums/attachments/p674206d1/a/a/a"></p>
<p><b>Am�lioration de la latence pour tous les mod�les</b></p><p>Au fur et � mesure que nous avons form� Codex-Spark, il est devenu �vident que la vitesse du mod�le n'�tait qu'une partie de l'�quation pour la collaboration en temps r�el : nous devions �galement r�duire la latence sur l'ensemble du pipeline de requ�tes-r�ponses. Nous avons mis en �uvre des am�liorations de la latence de bout en bout dans notre harnais, qui profiteront � tous les mod�les. En coulisses, nous avons rationalis� la mani�re dont les r�ponses circulent du client vers le serveur et vice versa, r��crit des �l�ments cl�s de notre pile d'inf�rence et retravaill� la mani�re dont les sessions sont initialis�es afin que le premier jeton visible apparaisse plus t�t et que Codex reste r�actif � mesure que vous it�rez. Gr�ce � l'introduction d'une connexion WebSocket persistante et � des optimisations cibl�es au sein de l'API Responses, nous avons r�duit la surcharge par aller-retour client...
</p><p>La fin de cet article est r�serv�e aux abonn�s. Soutenez le Club Developpez.com en <a href="https://premium.developpez.com/abonnement">prenant un abonnement</a> pour que nous puissions continuer � vous proposer des publications.</p> </div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'smooth' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>