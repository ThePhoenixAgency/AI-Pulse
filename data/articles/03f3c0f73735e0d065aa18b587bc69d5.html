<!DOCTYPE html>
<html lang="es">
<head>
<meta charset="UTF-8">
<title>mmBERT: ModernBERT goes Multilingual</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>mmBERT: ModernBERT goes Multilingual</h1>
  <div class="metadata">
<<<<<<< HEAD
    Source: Hugging Face Blog | Date: 9/9/2025 2:00:00 AM | Lang: ES |
=======
    Source: Hugging Face Blog | Date: 9/9/2025 12:00:00 AM | Lang: ES |
>>>>>>> 48d6193da6f49976a64b6a30483399bfb54b1b8d
    <a href="https://huggingface.co/blog/mmbert" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/mmarone"><img alt="Marc Marone's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/63e410e88083f19a218be964/8gqsYilSTSCt8E3JmajP-.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/orionweller"><img alt="Orion Weller's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6362d9712691058b19de1ba4/Hdqj5aGrFJJbF7oUSzoIh.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/will-fleshman"><img alt="William Fleshman's avatar" src="https://huggingface.co/avatars/f73424b8fc72073cb0916e2d2cf9ca56.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/eugene-yang"><img alt="Eugene Yang's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1671479459688-63a0c07a3c8841cfe2cd1e70.jpeg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/dlawrie"><img alt="Dawn Lawrie's avatar" src="https://huggingface.co/avatars/b2495c05e0838ad117bf607ccddbb3e4.svg"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/vandurme"><img alt="Ben Van Durme's avatar" src="https://huggingface.co/avatars/5723dcb12e63d6b30cb83cd189f96c46.svg"></a> </span> </span></p> </div></div> <h2> <div><nav><ul><li><a href="#tldr">TL;DR</a> <ul></ul> </li><li><a href="#training-data">Training Data</a> <ul></ul> </li><li><a href="#training-recipe-and-novel-components">Training Recipe and Novel Components</a> <ul><li><a href="#architecture">Architecture</a> <ul></ul> </li><li><a href="#three-phase-training-approach">Three-Phase Training Approach</a> <ul></ul> </li><li><a href="#novel-training-techniques">Novel Training Techniques</a> <ul></ul> </li></ul> </li><li><a href="#results">Results</a> <ul><li><a href="#natural-language-understanding-nlu">Natural Language Understanding (NLU)</a> <ul></ul> </li><li><a href="#retrieval-performance">Retrieval Performance</a> <ul></ul> </li></ul> </li><li><a href="#learning-languages-in-the-decay-phase">Learning Languages in the Decay Phase</a> <ul></ul> </li><li><a href="#efficiency-improvements">Efficiency Improvements</a> <ul></ul> </li><li><a href="#usage-examples">Usage Examples</a> <ul></ul> </li><li><a href="#fine-tuning-examples">Fine-tuning Examples</a> <ul><li><a href="#encoders">Encoders</a> <ul></ul> </li></ul> </li><li><a href="#model-family-and-links">Model Family and Links</a> <ul></ul> </li></ul></nav></div> <a href="#tldr"> <span></span> </a> <span> TL;DR </span>
</h2>
<p>This blog post introduces <a href="https://huggingface.co/collections/jhu-clsp/mmbert-a-modern-multilingual-encoder-68b725831d7c6e3acc435ed4">mmBERT</a>, a state-of-the-art massively multilingual encoder model trained on 3T+ tokens of text in over 1800 languages. It shows significant performance and speed improvements over previous multilingual models, being the first to improve upon XLM-R, while also developing new strategies for effectively learning low-resource languages. mmBERT builds upon ModernBERT for a blazingly fast architecture, and adds novel components to enable efficient multilingual learning.</p>
<p>If you are interested in trying out the models yourself, some example boilerplate is available <a href="#usage-examples">at the end of this blogpost!</a></p>
<h2> <a href="#training-data"> <span></span> </a> <span> Training Data </span>
</h2>
<figure> <img alt="Distribution of each phase of pre-training" src="https://github.com/JHU-CLSP/mmBERT/blob/main/assets/data_dist.jpg?raw=true"> <figcaption> Figure 1: the training data is progressively annealed to include more languages and more uniform sampling throughout training.</figcaption>
</figure> <p>mmBERT was trained on a carefully curated multilingual dataset totaling over 3T tokens across three distinct training phases. The foundation of our training data consists of three primary open-source and high-quality web crawls that enable both multilingual coverage and data quality:</p>
<p><strong>DCLM and Filtered DCLM</strong> provides the highest quality English content available, serving as the backbone for strong English performance (with the filtered data coming from <a href="https://huggingface.co/datasets/allenai/dolmino-mix-1124">Dolmino</a>). This dataset represents state-of-the-art web filtering techniques and forms a crucial component. Due to the high quality of this data, we use a signficantly higher proportion of English than previous generation multilingual encoder models (up to 18%).</p>
<p><strong>FineWeb2</strong> delivers broad <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-2">multilingual web content</a> covering over 1,800 languages. This dataset enables our extensive multilingual coverage while maintaining reasonable quality standards across diverse language families and scripts.</p>
<p><strong>FineWeb2-HQ</strong> consists of a <a href="https://huggingface.co/datasets/epfml/FineWeb2-HQ">filtered subset of FineWeb2</a> focusing on 20 high-resource languages. This filtered version provides higher-quality multilingual content that bridges the gap between English-only filtered data and broad multilingual coverage.</p>
<p>The training data also incorporates specialized corpora from <a href="https://arxiv.org/abs/2402.00159">Dolma</a>, <a href="https://arxiv.org/abs/2508.03828">MegaWika v2</a>, <a href="https://arxiv.org/abs/2410.02660">ProLong</a> and more: code repositories (StarCoder, ProLong), academic content (ArXiv, PeS2o), reference materials (Wikipedia, textbooks), and community discussions (StackExchange), along with instruction and mathematical datasets.</p>
<p>The key innovation in our data approach is the <strong>progressive language inclusion strategy</strong> shown in <a href="#figure1">Figure 1</a>. At each phase we progressively sample from a <em>flatter</em> distribution (i.e. closer to uniform), while also adding new languages. This means that high resource languages like Russian start off with a high percentage of the data (i.e. 9%) and then in the last phase of training end around half of that. We start with 60 high-resource languages during pre-training, expand to 110 languages during mid-training, and finally include all 1,833 languages from FineWeb2 during the decay phase. This allows us to maximize the impact of limited low-resource language data without excess reptitions and while maintaining high overall data quality.</p>
<h2> <a href="#training-recipe-and-novel-components"> <span></span> </a> <span> Training Recipe and Novel Components </span>
</h2>
<p>mmBERT builds upon the <a href="https://huggingface.co/blog/modernbert">ModernBERT</a> architecture but introduces several key innovations for multilingual learning:</p>
<h3> <a href="#architecture"> <span></span> </a> <span> Architecture </span>
</h3>
<p>We use the same core architecture as ModernBERT-base with 22 layers and 1152 intermediate dimensions, but switch to the Gemma 2 tokenizer to better handle multilingual text. The base model has 110M non-embedding parameters (307M total due to the larger vocabulary), while the small variant has 42M non-embedding parameters (140M total).</p>
<h3> <a href="#three-phase-training-approach"> <span></span> </a> <span> Three-Phase Training Approach </span>
</h3>
<p>Our training follows a carefully designed three-phase schedule:</p>
<ol>
<li><strong>Pre-training (2.3T tokens)</strong>: Warmup and stable learning rate phase using 60 languages with 30% mask rate</li>
<li><strong>Mid-training (600B tokens)</strong>: Context extension to 8192 tokens, higher-quality data, expanded to 110 languages with 15% mask rate </li>
<li><strong>Decay phase (100B tokens)</strong>: Inverse square root learning rate decay, all 1,833 languages included with 5% mask rate</li>
</ol>
<h3> <a href="#novel-training-techniques"> <span></span> </a> <span> Novel Training Techniques </span>
</h3>
<p><strong>Inverse Mask Ratio Schedule</strong>: Instead of using a fixed masking rate, we progressively reduce the mask ratio from 30% → 15% → 5% across training phases. This allows the model to learn basic representations with higher masking early on, then focus on more nuanced understanding with lower masking rates.</p>
<p><strong>Annealed Language Learning</strong>: We dynamically adjust the temperature for multilingual data sampling from τ=0.7 → 0.5 → 0.3. This creates a progression from high-resource language bias toward more uniform sampling, enabling the model to build a strong multilingual foundation before learning low-resource languages. </p>
<p><strong>Progressive Language Addition</strong>: Rather than training on all languages simultaneously, we strategically add languages at each phase (60 → 110 → 1,833). This maximizes learning efficiency by avoiding excessive epochs on limited low-resource data while still achieving strong performance.</p>
<p><strong>Model Merging</strong>: We train three different variants during the decay phase (English-focused, 110-language, and all-language) and use TIES merging to combine their strengths into the final model.</p>
<h2> <a href="#results"> <span></span> </a> <span> Results </span>
</h2>
<h3> <a href="#natural-language-understanding-nlu"> <span></span> </a> <span> Natural Language Understanding (NLU) </span>
</h3>
<figure> <img alt="GLUE performance" src="https://github.com/JHU-CLSP/mmBERT/blob/main/assets/glue.jpeg?raw=true"> <figcaption>Table 1: Performance on GLUE (English)</figcaption>
</figure> <p><strong>English Performance</strong>: On the English GLUE benchmark (<a href="#table1">Table 1</a>), mmBERT base achieves strong performance, substantially outperforming other multilingual models like XLM-R (multilingual RoBERTa) base and mGTE base, while remaining competitive to English-only models despite less than 25% of the mmBERT training data being English.</p>
<figure> <img alt="XTREME performance" src="https://github.com/JHU-CLSP/mmBERT/blob/main/assets/xtreme.jpeg?raw=true"> <figcaption>Table 2: Performance on XTREME (Multilingual)</figcaption>
</figure> <p><strong>Multilingual Performance</strong>: mmBERT shows significant improvements on XTREME benchmark compared to XLM-R as demonstrated in <a href="#table2">Table 2</a>. Notable gains include strong performance on XNLI classification, substantial improvements in question answering tasks like TyDiQA, and competitive results across PAWS-X and XCOPA for cross-lingual understanding.</p>
<p>The model performs well across most categories, with the exception of some structured prediction tasks like NER and POS tagging, likely due to tokenizer differences that affect word boundary detection. On these categories, it performs about the same as the previous generation, but can be applied to more languages.</p>
<h3> <a href="#retrieval-performance"> <span></span> </a> <span> Retrieval Performance </span>
</h3>
<figure> <img alt="MTEB v2 Eng performance" src="https://github.com/JHU-CLSP/mmBERT/blob/main/assets/mteb-english.jpeg?raw=true"> <figcaption>Table 3: Performance on MTEB v2 English</figcaption>
</figure> <p><strong>English Retrieval</strong>: Even though mmBERT is designed for massively multilingual settings, in the MTEB v2 English benchmarks (<a href="#table3">Table 3</a>), mmBERT shows significant gains over previous multilingual models and even ties the capabilities of English-only models like ModernBERT!</p>
<figure> <img alt="MTEB v2 Multilingual performance" src="https://github.com/JHU-CLSP/mmBERT/blob/main/assets/mteb-multilingual.jpeg?raw=true"> <figcaption>Table 4: Performance on MTEB v2 Multilingual</figcaption>
</figure> <p><strong>Multilingual Retrieval</strong>: mmBERT shows consistent improvements on MTEB v2 multilingual benchmarks compared to other models (<a href="#table4">Table 4</a>).</p>
<figure> <img alt="CoIR performance" src="https://github.com/JHU-CLSP/mmBERT/blob/main/assets/coir.jpeg?raw=true"> <figcaption>Table 5: Performance on CoIR code benchmark</figcaption>
</figure> <p><strong>Code Retrieval</strong>: Due to the modern tokenizer (based on Gemma 2) mmBERT also shows strong coding performance (<a href="#table5">Table 5</a>), making mmBERT suitable for any type of textual data. The only model that outperforms it is EuroBERT, which was able to use the non-publicly accessible Stack v2 dataset.</p>
<h2> <a href="#learning-languages-in-the-decay-phase"> <span></span> </a> <span> Learning Languages in the Decay Phase </span>
</h2>
<p>One of mmBERT's most significant novel features is demonstrating that low-resource languages can be effectively learned during the short decay phase of training. We validated this approach by testing on languages only introduced during the final 100B token decay phase.</p>
<figure> <img alt="Improvements from adding new languages in the decay phase" src="https://github.com/JHU-CLSP/mmBERT/blob/main/assets/low_resource_merge.jpg?raw=true"> <figcaption> Figure 2: adding more than 1700 languages in the decay phase allows for rapid learning, which we keep through model merging.</figcaption>
</figure> <p><strong>Dramatic Performance Gains</strong>: Testing on TiQuaD (Tigrinya) and FoQA (Faroese), we observed substantial improvements when these languages were included in the decay phase, as shown in <a href="#figure2">Figure 2</a>. The results demonstrate the effectiveness of our progressive language learning approach.</p>
<p><strong>Competitive with Large Models</strong>: Despite only seeing these languages in the final training phase, mmBERT achieves performance levels that exceed much larger models. On Faroese question answering where LLMs have been benchmarked, mmBERT outperforms Google Gemini 2.5 Pro and OpenAI o3.</p>
<p><strong>Rapid Learning Mechanism</strong>: The success of decay-phase language learning stems from the model's ability to leverage its strong multilingual foundation built during earlier phases. When exposed to new languages, the model can quickly adapt existing cross-lingual representations rather than learning from scratch.</p>
<p><strong>Model Merging Benefits</strong>: The final mmBERT models successfully retain most of the decay-phase improvements while benefiting from the English-focused and high-resource variants through TIES merging.</p>
<h2> <a href="#efficiency-improvements"> <span></span> </a> <span> Efficiency Improvements </span>
</h2>
<p>mmBERT delivers substantial efficiency gains over previous multilingual encoder models through architectural improvements inherited from ModernBERT:</p>
<figure> <img alt="mmBERT is much more efficient than previous multilingual models" src="https://github.com/JHU-CLSP/mmBERT/blob/main/assets/inference_times.jpg?raw=true"> <figcaption> Figure 3: mmBERT is significantly more efficient than previous multilingual models, up to 2-4x as much!</figcaption>
</figure> <p><strong>Throughput Performance</strong>: mmBERT processes text significantly faster than existing multilingual models across various sequence lengths, as demonstrated in <a href="#figure3">Figure 3</a>. Both the small and base models show substantial speed improvements over previous multilingual encoders.</p>
<p><strong>Modern Architecture Benefits</strong>: The efficiency gains come from two main technical improvements:</p>
<ul>
<li><strong>Flash Attention 2</strong>: Optimized attention computation for better memory usage and speed</li>
<li><strong>Unpadding techniques</strong>: Elimination of unnecessary padding tokens during processing</li>
</ul>
<p><strong>Sequence Length Scaling</strong>: Unlike older models limited to 512 tokens, mmBERT handles up to 8,192 tokens efficiently while maintaining high throughput. This makes it suitable for longer document processing tasks that are increasingly common in multilingual applications.</p>
<p><strong>Energy Efficiency</strong>: The combination of better throughput and modern architecture results in lower computational costs for inference, making mmBERT more practical for production deployments where multilingual support is needed at scale.</p>
<p>These efficiency improvements make mmBERT not just more accurate than previous multilingual encoders, but also significantly more practical for real usage.</p>
<h2> <a href="#usage-examples"> <span></span> </a> <span> Usage Examples </span>
</h2>
<p>You can use these models with just a few lines of code!</p>
<pre><code><span>from</span> transformers <span>import</span> AutoTokenizer, AutoModelForMaskedLM
<span>import</span> torch tokenizer = AutoTokenizer.from_pretrained(<span>"jhu-clsp/mmBERT-base"</span>)
model = AutoModelForMaskedLM.from_pretrained(<span>"jhu-clsp/mmBERT-base"</span>) <span>def</span> <span>predict_masked_token</span>(<span>text</span>): inputs = tokenizer(text, return_tensors=<span>"pt"</span>) <span>with</span> torch.no_grad(): outputs = model(**inputs) mask_indices = torch.where(inputs[<span>"input_ids"</span>] == tokenizer.mask_token_id) predictions = outputs.logits[mask_indices] top_tokens, top_indices = torch.topk(predictions, <span>5</span>, dim=-<span>1</span>) <span>return</span> [tokenizer.decode(token) <span>for</span> token <span>in</span> top_indices[<span>0</span>]] <span># Works across languages</span>
texts = [ <span>"The capital of France is &lt;mask&gt;."</span>, <span>"La capital de España es &lt;mask&gt;."</span>, <span>"Die Hauptstadt von Deutschland ist &lt;mask&gt;."</span>,
] <span>for</span> text <span>in</span> texts: predictions = predict_masked_token(text) <span>print</span>(<span>f"Text: <span>{text}</span>"</span>) <span>print</span>(<span>f"Predictions: <span>{predictions}</span>\n"</span>)
</code></pre>
<h2> <a href="#fine-tuning-examples"> <span></span> </a> <span> Fine-tuning Examples </span>
</h2>
<h3> <a href="#encoders"> <span></span> </a> <span> Encoders </span>
</h3>
Click to see how to finetune this into a dense embedding model using Sentence Transformers <pre><code><span>import</span> argparse <span>from</span> datasets <span>import</span> load_dataset
<span>from</span> sentence_transformers <span>import</span> ( SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments,
)
<span>from</span> sentence_transformers.evaluation <span>import</span> TripletEvaluator
<span>from</span> sentence_transformers.losses <span>import</span> CachedMultipleNegativesRankingLoss
<span>from</span> sentence_transformers.training_args <span>import</span> BatchSamplers <span>def</span> <span>main</span>(): <span># parse the lr &amp; model name</span> parser = argparse.ArgumentParser() parser.add_argument(<span>"--lr"</span>, <span>type</span>=<span>float</span>, default=<span>8e-5</span>) parser.add_argument(<span>"--model_name"</span>, <span>type</span>=<span>str</span>, default=<span>"jhu-clsp/mmBERT-small"</span>) args = parser.parse_args() lr = args.lr model_name = args.model_name model_shortname = model_name.split(<span>"/"</span>)[-<span>1</span>] <span># 1. Load a model to finetune</span> model = SentenceTransformer(model_name) <span># 2. Load a dataset to finetune on</span> dataset = load_dataset( <span>"sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1"</span>, <span>"triplet-hard"</span>, split=<span>"train"</span>, ) dataset_dict = dataset.train_test_split(test_size=<span>1_000</span>, seed=<span>12</span>) train_dataset = dataset_dict[<span>"train"</span>].select(<span>range</span>(<span>1_250_000</span>)) eval_dataset = dataset_dict[<span>"test"</span>] <span># 3. Define a loss function</span> loss = CachedMultipleNegativesRankingLoss(model, mini_batch_size=<span>16</span>) <span># Increase mini_batch_size if you have enough VRAM</span> run_name = <span>f"<span>{model_shortname}</span>-DPR-<span>{lr}</span>"</span> <span># 4. (Optional) Specify training arguments</span> args = SentenceTransformerTrainingArguments( <span># Required parameter:</span> output_dir=<span>f"output/<span>{model_shortname}</span>/<span>{run_name}</span>"</span>, <span># Optional training parameters:</span> num_train_epochs=<span>1</span>, per_device_train_batch_size=<span>512</span>, per_device_eval_batch_size=<span>512</span>, warmup_ratio=<span>0.05</span>, fp16=<span>False</span>, <span># Set to False if GPU can't handle FP16</span> bf16=<span>True</span>, <span># Set to True if GPU supports BF16</span> batch_sampler=BatchSamplers.NO_DUPLICATES, <span># (Cached)MultipleNegativesRankingLoss benefits from no duplicates</span> learning_rate=lr, <span># Optional tracking/debugging parameters:</span> save_strategy=<span>"steps"</span>, save_steps=<span>500</span>, save_total_limit=<span>2</span>, logging_steps=<span>500</span>, run_name=run_name, <span># Used in `wandb`, `tensorboard`, `neptune`, etc. if installed</span> ) <span># 5. (Optional) Create an evaluator &amp; evaluate the base model</span> dev_evaluator = TripletEvaluator( anchors=eval_dataset[<span>"query"</span>], positives=eval_dataset[<span>"positive"</span>], negatives=eval_dataset[<span>"negative"</span>], name=<span>"msmarco-co-condenser-dev"</span>, ) dev_evaluator(model) <span># 6. Create a trainer &amp; train</span> trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator, ) trainer.train() <span># 7. (Optional) Evaluate the trained model on the evaluator after training</span> dev_evaluator(model) <span># 8. Save the model</span> model.save_pretrained(<span>f"output/<span>{model_shortname}</span>/<span>{run_name}</span>/final"</span>) <span># 9. (Optional) Push it to the Hugging Face Hub</span> model.push_to_hub(run_name, private=<span>False</span>) <span>if</span> __name__ == <span>"__main__"</span>: main()
</code></pre> Click to see how to finetune this into a multi-vector embedding model with PyLate <pre><code><span>from</span> datasets <span>import</span> load_dataset
<span>from</span> pylate <span>import</span> losses, models, utils
<span>from</span> sentence_transformers <span>import</span> ( SentenceTransformerTrainer, SentenceTransformerTrainingArguments,
) <span>def</span> <span>main</span>(): <span># Load the datasets required for knowledge distillation (train, queries, documents)</span> train = load_dataset( path=<span>"lightonai/ms-marco-en-bge"</span>, name=<span>"train"</span>, ) queries = load_dataset( path=<span>"lightonai/ms-marco-en-bge"</span>, name=<span>"queries"</span>, ) documents = load_dataset( path=<span>"lightonai/ms-marco-en-bge"</span>, name=<span>"documents"</span>, ) <span># Set the transformation to load the documents/queries texts using the corresponding ids on the fly</span> train.set_transform( utils.KDProcessing(queries=queries, documents=documents).transform, ) <span># Define the base model, training parameters, and output directory</span> num_train_epochs = <span>1</span> lr = <span>8e-5</span> batch_size = <span>16</span> accum_steps = <span>1</span> model_name = <span>"jhu-clsp/mmBERT-small"</span> model_shortname = model_name.split(<span>"/"</span>)[-<span>1</span>] <span># Set the run name for logging and output directory</span> run_name = <span>f"<span>{model_shortname}</span>-colbert-KD-<span>{lr}</span>"</span> output_dir = <span>f"output/<span>{model_shortname}</span>/<span>{run_name}</span>"</span> <span># Initialize the ColBERT model from the base model</span> model = models.ColBERT(model_name_or_path=model_name) <span># Configure the training arguments (e.g., epochs, batch size, learning rate)</span> args = SentenceTransformerTrainingArguments( output_dir=output_dir, num_train_epochs=num_train_epochs, per_device_train_batch_size=batch_size, fp16=<span>False</span>, <span># Set to False if you get an error that your GPU can't run on FP16</span> bf16=<span>True</span>, <span># Set to True if you have a GPU that supports BF16</span> run_name=run_name, logging_steps=<span>10</span>, learning_rate=lr, gradient_accumulation_steps=accum_steps, warmup_ratio=<span>0.05</span>, ) <span># Use the Distillation loss function for training</span> train_loss = losses.Distillation(model=model) <span># Initialize the trainer</span> trainer = SentenceTransformerTrainer( model=model, args=args, train_dataset=train, loss=train_loss, data_collator=utils.ColBERTCollator(tokenize_fn=model.tokenize), ) <span># Start the training process</span> trainer.train() model.save_pretrained(<span>f"<span>{output_dir}</span>/final"</span>) <span>if</span> __name__ == <span>"__main__"</span>: main()
</code></pre> Click to see how to finetune this into a sparse retrieval model using Sentence Transformers <pre><code><span>import</span> logging <span>from</span> datasets <span>import</span> load_dataset <span>from</span> sentence_transformers <span>import</span> ( SparseEncoder, SparseEncoderModelCardData, SparseEncoderTrainer, SparseEncoderTrainingArguments,
)
<span>from</span> sentence_transformers.sparse_encoder.evaluation <span>import</span> SparseNanoBEIREvaluator
<span>from</span> sentence_transformers.sparse_encoder.losses <span>import</span> SparseMultipleNegativesRankingLoss, SpladeLoss
<span>from</span> sentence_transformers.training_args <span>import</span> BatchSamplers logging.basicConfig(<span>format</span>=<span>"%(asctime)s - %(message)s"</span>, datefmt=<span>"%Y-%m-%d %H:%M:%S"</span>, level=logging.INFO) <span># 1. Load a model to finetune with 2. (Optional) model card data</span>
model = SparseEncoder( <span>"jhu-clsp/mmBERT-small"</span>, model_card_data=SparseEncoderModelCardData( language=<span>"en"</span>, license=<span>"apache-2.0"</span>, )
) <span># 3. Load a dataset to finetune on</span>
full_dataset = load_dataset(<span>"sentence-transformers/natural-questions"</span>, split=<span>"train"</span>).select(<span>range</span>(<span>100_000</span>))
dataset_dict = full_dataset.train_test_split(test_size=<span>1_000</span>, seed=<span>12</span>)
train_dataset = dataset_dict[<span>"train"</span>]
eval_dataset = dataset_dict[<span>"test"</span>] <span># 4. Define a loss function</span>
loss = SpladeLoss( model=model, loss=SparseMultipleNegativesRankingLoss(model=model), query_regularizer_weight=<span>5e-5</span>, document_regularizer_weight=<span>3e-5</span>,
) <span># 5. (Optional) Specify training arguments</span>
run_name = <span>"splade-distilbert-base-uncased-nq"</span>
args = SparseEncoderTrainingArguments( <span># Required parameter:</span> output_dir=<span>f"models/<span>{run_name}</span>"</span>, <span># Optional training parameters:</span> num_train_epochs=<span>1</span>, per_device_train_batch_size=<span>16</span>, per_device_eval_batch_size=<span>16</span>, learning_rate=<span>2e-5</span>, warmup_ratio=<span>0.1</span>, fp16=<span>True</span>, <span># Set to False if you get an error that your GPU can't run on FP16</span> bf16=<span>False</span>, <span># Set to True if you have a GPU that supports BF16</span> batch_sampler=BatchSamplers.NO_DUPLICATES, <span># MultipleNegativesRankingLoss benefits from no duplicate samples in a batch</span> <span># Optional tracking/debugging parameters:</span> eval_strategy=<span>"steps"</span>, eval_steps=<span>1000</span>, save_strategy=<span>"steps"</span>, save_steps=<span>1000</span>, save_total_limit=<span>2</span>, logging_steps=<span>200</span>, run_name=run_name, <span># Will be used in W&amp;B if `wandb` is installed</span>
) <span># 6. (Optional) Create an evaluator &amp; evaluate the base model</span>
dev_evaluator = SparseNanoBEIREvaluator(dataset_names=[<span>"msmarco"</span>, <span>"nfcorpus"</span>, <span>"nq"</span>], batch_size=<span>16</span>) <span># 7. Create a trainer &amp; train</span>
trainer = SparseEncoderTrainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, loss=loss, evaluator=dev_evaluator,
)
trainer.train() <span># 8. Evaluate the model performance again after training</span>
dev_evaluator(model) <span># 9. Save the trained model</span>
model.save_pretrained(<span>f"models/<span>{run_name}</span>/final"</span>) <span># 10. (Optional) Push it to the Hugging Face Hub</span>
model.push_to_hub(run_name)
</code></pre> Click to see how to finetune this into a reranker model using Sentence Transformers <pre><code><span>import</span> logging
<span>import</span> traceback <span>import</span> torch
<span>from</span> datasets <span>import</span> load_dataset <span>from</span> sentence_transformers <span>import</span> SentenceTransformer
<span>from</span> sentence_transformers.cross_encoder <span>import</span> ( CrossEncoder, CrossEncoderModelCardData, CrossEncoderTrainer, CrossEncoderTrainingArguments,
)
<span>from</span> sentence_transformers.cross_encoder.evaluation <span>import</span> ( CrossEncoderNanoBEIREvaluator, CrossEncoderRerankingEvaluator,
)
<span>from</span> sentence_transformers.cross_encoder.losses <span>import</span> BinaryCrossEntropyLoss
<span>from</span> sentence_transformers.evaluation <span>import</span> SequentialEvaluator
<span>from</span> sentence_transformers.util <span>import</span> mine_hard_negatives <span># Set the log level to INFO to get more information</span>
logging.basicConfig(<span>format</span>=<span>"%(asctime)s - %(message)s"</span>, datefmt=<span>"%Y-%m-%d %H:%M:%S"</span>, level=logging.INFO) <span>def</span> <span>main</span>(): model_name = <span>"jhu-clsp/mmBERT-small"</span> train_batch_size = <span>64</span> num_epochs = <span>1</span> num_hard_negatives = <span>5</span> <span># How many hard negatives should be mined for each question-answer pair</span> <span># 1a. Load a model to finetune with 1b. (Optional) model card data</span> model = CrossEncoder( model_name, model_card_data=CrossEncoderModelCardData( language=<span>"en"</span>, license=<span>"apache-2.0"</span>, ), ) <span>print</span>(<span>"Model max length:"</span>, model.max_length) <span>print</span>(<span>"Model num labels:"</span>, model.num_labels) <span># 2a. Load the GooAQ dataset: https://huggingface.co/datasets/sentence-transformers/gooaq</span> logging.info(<span>"Read the gooaq training dataset"</span>) full_dataset = load_dataset(<span>"sentence-transformers/gooaq"</span>, split=<span>"train"</span>).select(<span>range</span>(<span>100_000</span>)) dataset_dict = full_dataset.train_test_split(test_size=<span>1_000</span>, seed=<span>12</span>) train_dataset = dataset_dict[<span>"train"</span>] eval_dataset = dataset_dict[<span>"test"</span>] logging.info(train_dataset) logging.info(eval_dataset) <span># 2b. Modify our training dataset to include hard negatives using a very efficient embedding model</span> embedding_model = SentenceTransformer(<span>"sentence-transformers/static-retrieval-mrl-en-v1"</span>, device=<span>"cpu"</span>) hard_train_dataset = mine_hard_negatives( train_dataset, embedding_model, num_negatives=num_hard_negatives, <span># How many negatives per question-answer pair</span> margin=<span>0</span>, <span># Similarity between query and negative samples should be x lower than query-positive similarity</span> range_min=<span>0</span>, <span># Skip the x most similar samples</span> range_max=<span>100</span>, <span># Consider only the x most similar samples</span> sampling_strategy=<span>"top"</span>, <span># Sample the top negatives from the range</span> batch_size=<span>4096</span>, <span># Use a batch size of 4096 for the embedding model</span> output_format=<span>"labeled-pair"</span>, <span># The output format is (query, passage, label), as required by BinaryCrossEntropyLoss</span> use_faiss=<span>True</span>, ) logging.info(hard_train_dataset) <span># 2c. (Optionally) Save the hard training dataset to disk</span> <span># hard_train_dataset.save_to_disk("gooaq-hard-train")</span> <span># Load again with:</span> <span># hard_train_dataset = load_from_disk("gooaq-hard-train")</span> <span># 3. Define our training loss.</span> <span># pos_weight is recommended to be set as the ratio between positives to negatives, a.k.a. `num_hard_negatives`</span> loss = BinaryCrossEntropyLoss(model=model, pos_weight=torch.tensor(num_hard_negatives)) <span># 4a. Define evaluators. We use the CrossEncoderNanoBEIREvaluator, which is a light-weight evaluator for English reranking</span> nano_beir_evaluator = CrossEncoderNanoBEIREvaluator( dataset_names=[<span>"msmarco"</span>, <span>"nfcorpus"</span>, <span>"nq"</span>], batch_size=train_batch_size, ) <span># 4b. Define a reranking evaluator by mining hard negatives given query-answer pairs</span> <span># We include the positive answer in the list of negatives, so the evaluator can use the performance of the</span> <span># embedding model as a baseline.</span> hard_eval_dataset = mine_hard_negatives( eval_dataset, embedding_model, corpus=full_dataset[<span>"answer"</span>], <span># Use the full dataset as the corpus</span> num_negatives=<span>30</span>, <span># How many documents to rerank</span> batch_size=<span>4096</span>, include_positives=<span>True</span>, output_format=<span>"n-tuple"</span>, use_faiss=<span>True</span>, ) logging.info(hard_eval_dataset) reranking_evaluator = CrossEncoderRerankingEvaluator( samples=[ { <span>"query"</span>: sample[<span>"question"</span>], <span>"positive"</span>: [sample[<span>"answer"</span>]], <span>"documents"</span>: [sample[column_name] <span>for</span> column_name <span>in</span> hard_eval_dataset.column_names[<span>2</span>:]], } <span>for</span> sample <span>in</span> hard_eval_dataset ], batch_size=train_batch_size, name=<span>"gooaq-dev"</span>, <span># Realistic setting: only rerank the positives that the retriever found</span> <span># Set to True to rerank *all* positives</span> always_rerank_positives=<span>False</span>, ) <span># 4c. Combine the evaluators &amp; run the base model on them</span> evaluator = SequentialEvaluator([reranking_evaluator, nano_beir_evaluator]) evaluator(model) <span># 5. Define the training arguments</span> short_model_name = model_name <span>if</span> <span>"/"</span> <span>not</span> <span>in</span> model_name <span>else</span> model_name.split(<span>"/"</span>)[-<span>1</span>] run_name = <span>f"reranker-<span>{short_model_name}</span>-gooaq-bce"</span> args = CrossEncoderTrainingArguments( <span># Required parameter:</span> output_dir=<span>f"models/<span>{run_name}</span>"</span>, <span># Optional training parameters:</span> num_train_epochs=num_epochs, per_device_train_batch_size=train_batch_size, per_device_eval_batch_size=train_batch_size, learning_rate=<span>2e-5</span>, warmup_ratio=<span>0.1</span>, fp16=<span>False</span>, <span># Set to False if you get an error that your GPU can't run on FP16</span> bf16=<span>True</span>, <span># Set to True if you have a GPU that supports BF16</span> dataloader_num_workers=<span>4</span>, load_best_model_at_end=<span>True</span>, metric_for_best_model=<span>"eval_gooaq-dev_ndcg@10"</span>, <span># Optional tracking/debugging parameters:</span> eval_strategy=<span>"steps"</span>, eval_steps=<span>1000</span>, save_strategy=<span>"steps"</span>, save_steps=<span>1000</span>, save_total_limit=<span>2</span>, logging_steps=<span>200</span>, logging_first_step=<span>True</span>, run_name=run_name, <span># Will be used in W&amp;B if `wandb` is installed</span> seed=<span>12</span>, ) <span># 6. Create the trainer &amp; start training</span> trainer = CrossEncoderTrainer( model=model, args=args, train_dataset=hard_train_dataset, loss=loss, evaluator=evaluator, ) trainer.train() <span># 7. Evaluate the final model, useful to include these in the model card</span> evaluator(model) <span># 8. Save the final model</span> final_output_dir = <span>f"models/<span>{run_name}</span>/final"</span> model.save_pretrained(final_output_dir) <span># 9. (Optional) save the model to the Hugging Face Hub!</span> <span># It is recommended to run `huggingface-cli login` to log into your Hugging Face account first</span> <span>try</span>: model.push_to_hub(run_name) <span>except</span> Exception: logging.error( <span>f"Error uploading model to the Hugging Face Hub:\n<span>{traceback.format_exc()}</span>To upload it manually, you can run "</span> <span>f"`huggingface-cli login`, followed by loading the model using `model = CrossEncoder(<span>{final_output_dir!r}</span>)` "</span> <span>f"and saving it using `model.push_to_hub('<span>{run_name}</span>')`."</span> ) <span>if</span> __name__ == <span>"__main__"</span>: main()
</code></pre> <h2> <a href="#model-family-and-links"> <span></span> </a> <span> Model Family and Links </span>
</h2>
<p><strong>Standard Models:</strong></p>
<ul>
<li><a href="https://huggingface.co/jhu-clsp/mmBERT-small">mmBERT-small</a> (140M params total, 42M non-embed)</li>
<li><a href="https://huggingface.co/jhu-clsp/mmBERT-base">mmBERT-base</a> (307M params total, 110M non-embed)</li>
</ul>
<p><strong>Research Resources:</strong></p>
<ul>
<li><a href="https://huggingface.co/collections/jhu-clsp/mmbert-a-modern-multilingual-encoder-68b725831d7c6e3acc435ed4"> mmBERT Model Collection</a></li>
<li><a href="https://arxiv.org/abs/2509.06888"> Paper</a> </li>
<li><a href="https://huggingface.co/datasets/jhu-clsp/mmbert-pretrain-p1-fineweb2-langs"> Training Data</a> (3T+ tokens, fully open)</li>
<li><a href="https://github.com/jhu-clsp/mmBERT"> GitHub Repository</a></li>
<li><a href="https://huggingface.co/jhu-clsp/mmBERT-checkpoints"> Training Checkpoints</a> for studying training or continued pre-training</li>
</ul>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>