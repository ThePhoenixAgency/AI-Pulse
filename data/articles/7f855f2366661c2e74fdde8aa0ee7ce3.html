<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Building a LangGraph Agent from Scratch</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>Building a LangGraph Agent from Scratch</h1>
  <div class="metadata">
    Source: Towards Data Science | Date: 2/17/2026 8:22:34 PM | <a href="https://towardsdatascience.com/building-a-langgraph-agent-from-scratch/" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <div><div>
<h2></h2> <p>The term ‚Äú<strong>AI agent</strong>‚Äù is one of the most popular right now. They emerged after the LLM hype, when people realized that the latest LLM capabilities are impressive but that they can only perform tasks on which they have been explicitly trained. In that sense, normal LLMs do not have tools that would allow them to do anything outside their scope of knowledge.</p> <h3>RAG</h3> <p>To address this, <strong>Retrieval-Augmented Generation (RAG)</strong> was later introduced to retrieve additional context from external data sources and inject it into the prompt, so the LLM becomes aware of more context. We can roughly say that RAG made the LLM more knowledgeable, but for more complex problems, the LLM + RAG approach still failed when the solution path was not known in advance.</p> <figure><img src="https://contributor.insightmediagroup.io/wp-content/uploads/2026/02/Open-Generative-QA-1024x218.png" alt=""><figcaption>RAG pipeline</figcaption></figure> <h3>Agents</h3> <p>Agents are a remarkable concept built around LLMs that introduce <strong>state</strong>, <strong>decision-making</strong>, and <strong>memory</strong>. Agents can be thought of as a set of predefined tools for analyzing results and storing them in memory for later use before producing the final answer.</p> <h2>LangGraph</h2> <p>LangGraph is a popular framework used for creating agents. As the name suggests, agents are constructed using graphs with nodes and edges.</p> <div><p>Nodes represent the agent‚Äôs state, which evolves over time. Edges define the control flow by specifying transition rules and conditions between nodes.</p><p>To better understand LangGraph in practice, we will go through a detailed example. While LangGraph might seem too verbose for the problem below, it usually has a much larger impact on complex problems with large graphs.</p><p>First, we need to install the necessary libraries.</p></div> <pre><code>langgraph==1.0.5
langchain-community==0.4.1
jupyter==1.1.1
notebook==7.5.1
langchain[openai]</code></pre> <p>Then we import the necessary modules.</p> <pre><code>import os
from dotenv import load_dotenv</code></pre> <pre><code>import json
import random
from pydantic import BaseModel
from typing import Optional, List, Dict, Any</code></pre> <pre><code>from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain.chat_models import init_chat_model
from langchain.tools import tool</code></pre> <pre><code>from IPython.display import Image, display</code></pre> <p>We would also need to create an <em>.env</em> file and add an <em>OPENAI_API_KEY</em> there:</p> <pre><code>OPENAI_API_KEY=...</code></pre> <p>Then, with <strong>load_dotenv()</strong>, we can load the environment variables into the system.</p> <pre><code>load_dotenv()</code></pre> <h3>Extra functionalities</h3> <p>The function below will be useful for us to visually display constructed graphs.</p> <pre><code>def display_graph(graph): return display(Image(graph.get_graph().draw_mermaid_png()))</code></pre> <h3>Agent</h3> <p>Let us initialize an agent based on GPT-5-nano using a simple command:</p> <pre><code>llm = init_chat_model("openai:gpt-5-nano")</code></pre> <h3>State</h3> <p>In our example, we will construct an agent capable of answering questions about soccer. Its thought process will be based on retrieved statistics about players.</p> <p>To do that, we need to define a state. In our case, it will be an entity containing all the information an LLM needs about a player. To define a state, we need to write a class that inherits from <em>pydantic.BaseModel</em>:</p> <pre><code>class PlayerState(BaseModel): question: str selected_tools: Optional[List[str]] = None name: Optional[str] = None club: Optional[str] = None country: Optional[str] = None number: Optional[int] = None rating: Optional[int] = None goals: Optional[List[int]] = None minutes_played: Optional[List[int]] = None summary: Optional[str] = None</code></pre> <p>When moving between LangGraph nodes, each node takes as input an instance of <strong>PlayerState</strong> that specifies how to process the state. Our task will be to define how exactly that state is processed.</p> <h3>Tools</h3> <p>First, we will define some of the tools an agent can use. A <strong>tool </strong>can be roughly thought of as an additional function that an agent can call to retrieve the information needed to answer a user‚Äôs question.</p> <p>To define a tool, we need to write a function with a <strong>@tool</strong> decorator. <em>It is important to use clear parameter names and function docstrings, as the agent will consider them when deciding whether to call the tool based on the input context.</em></p> <p>To make our examples simpler, we are going to use mock data instead of real data retrieved from external sources, which is usually the case for production applications.</p> <p>In the first tool, we will return information about a player‚Äôs club and country by name.</p> <pre><code>@tool
def fetch_player_information_tool(name: str): """Contains information about the football club of a player and its country""" data = { 'Haaland': { 'club': 'Manchester City', 'country': 'Norway' }, 'Kane': { 'club': 'Bayern', 'country': 'England' }, 'Lautaro': { 'club': 'Inter', 'country': 'Argentina' }, 'Ronaldo': { 'club': 'Al-Nassr', 'country': 'Portugal' } } if name in data: print(f"Returning player information: {data[name]}") return data[name] else: return { 'club': 'unknown', 'country': 'unknown' } def fetch_player_information(state: PlayerState): return fetch_player_information_tool.invoke({'name': state.name})</code></pre> <p>You might be asking why we place a tool inside another function, which seems like over-engineering. In fact, these two functions have different responsibilities.</p> <p>The function <em>fetch_player_information()</em> takes a state as a parameter and is compatible with the LangGraph framework. It extracts the name field and calls a tool that operates on the parameter level.</p> <p>It provides a clear separation of concerns and allows easy reuse of the same tool across multiple graph nodes.</p> <p>Then we have an analogous function that retrieves a player‚Äôs jersey number:</p> <pre><code>@tool
def fetch_player_jersey_number_tool(name: str): "Returns player jersey number" data = { 'Haaland': 9, 'Kane': 9, 'Lautaro': 10, 'Ronaldo': 7 } if name in data: print(f"Returning player number: {data[name]}") return {'number': data[name]} else: return {'number': 0} def fetch_player_jersey_number(state: PlayerState): return fetch_player_jersey_tool.invoke({'name': state.name})</code></pre> <p>For the third tool, we will be fetching the player‚Äôs FIFA rating:</p> <pre><code>@tool
def fetch_player_rating_tool(name: str): "Returns player rating in the FIFA" data = { 'Haaland': 92, 'Kane': 89, 'Lautaro': 88, 'Ronaldo': 90 } if name in data: print(f"Returning rating data: {data[name]}") return {'rating': data[name]} else: return {'rating': 0} def fetch_player_rating(state: PlayerState): return fetch_player_rating_tool.invoke({'name': state.name})</code></pre> <p>Now, let us write several more graph node functions that will retrieve external data. We are not going to label them as tools as before, which means they won‚Äôt be something the agent decides to call or not.</p> <pre><code>def retrieve_goals(state: PlayerState): name = state.name data = { 'Haaland': [25, 40, 28, 33, 36], 'Kane': [33, 37, 41, 38, 29], 'Lautaro': [19, 25, 27, 24, 25], 'Ronaldo': [27, 32, 28, 30, 36] } if name in data: return {'goals': data[name]} else: return {'goals': [0]}</code></pre> <p>Here is a graph node that retrieves the number of minutes played over the last several seasons.</p> <pre><code>def retrieve_minutes_played(state: PlayerState): name = state.name data = { 'Haaland': [2108, 3102, 3156, 2617, 2758], 'Kane': [2924, 2850, 3133, 2784, 2680], 'Lautaro': [2445, 2498, 2519, 2773], 'Ronaldo': [3001, 2560, 2804, 2487, 2771] } if name in data: return {'minutes_played': data[name]} else: return {'minutes_played': [0]}</code></pre> <p>Below is a node that extracts a player‚Äôs name from a user question.</p> <pre><code>def extract_name(state: PlayerState): question = state.question prompt = f"""
You are a football name extractor assistant.
Your goal is to just extract a surname of a footballer in the following question.
User question: {question}
You have to just output a string containing one word - footballer surname. """ response = llm.invoke([HumanMessage(content=prompt)]).content print(f"Player name: ", response) return {'name': response}</code></pre> <p>Now is the time when things get interesting. Do you remember the three tools we defined above? Thanks to them, we can now create a planner that will ask the agent to choose a specific tool to call based on the context of the situation:</p> <pre><code>def planner(state: PlayerState): question = state.question prompt = f"""
You are a football player summary assistant.
You have the following tools available: ['fetch_player_jersey_number', 'fetch_player_information', 'fetch_player_rating']
User question: {question}
Decide which tools are required to answer.
Return a JSON list of tool names, e.g. ['fetch_player_jersey_number', 'fetch_rating'] """ response = llm.invoke([HumanMessage(content=prompt)]).content try: selected_tools = json.loads(response) except: selected_tools = [] return {'selected_tools': selected_tools}</code></pre> <p>In our case, we will ask the agent to create a summary of a soccer player. It will decide on its own which tool to call to retrieve additional data. Docstrings under tools play an important role: they provide the agent with additional context about the tools. </p> <p>Below is our final graph node, which will take multiple fields retrieved from previous steps and call the LLM to generate final summary.</p> <pre><code>def write_summary(state: PlayerState): question = state.question data = { 'name': state.name, 'country': state.country, 'number': state.number, 'rating': state.rating, 'goals': state.goals, 'minutes_played': state.minutes_played, } prompt = f"""
You are a football reporter assistant.
Given the following data and statistics of the football player, you will have to create a markdown summary of that player.
Player data:
{json.dumps(data, indent=4)}
The markdown summary has to include the following information: - Player full name (if only first name or last name is provided, try to guess the full name)
- Player country (also add flag emoji)
- Player number (also add the number in the emoji(-s) form)
- FIFA rating
- Total number of goals in last 3 seasons
- Average number of minutes required to score one goal
- Response to the user question: {question} """ response = llm.invoke([HumanMessage(content=prompt)]).content return {"summary": response}</code></pre> <h3>Graph construction</h3> <p>We now have all the elements to build a graph. Firstly, we initialize the graph using the <strong>StateGraph</strong> constructor. Then, we add nodes to that graph one by one using the<strong> add_node()</strong> method. It takes two parameters: a string used to assign a name to the node, and a callable function associated with the node that takes a graph state as its only parameter.</p> <pre><code>graph_builder = StateGraph(PlayerState)
graph_builder.add_node('extract_name', extract_name)
graph_builder.add_node('planner', planner)
graph_builder.add_node('fetch_player_jersey_number', fetch_player_jersey_number)
graph_builder.add_node('fetch_player_information', fetch_player_information)
graph_builder.add_node('fetch_player_rating', fetch_player_rating)
graph_builder.add_node('retrieve_goals', retrieve_goals)
graph_builder.add_node('retrieve_minutes_played', retrieve_minutes_played)
graph_builder.add_node('write_summary', write_summary)</code></pre> <p>Right now, our graph consists only of nodes. We need to add edges to it. The edges in LangGraph are oriented and added via the <strong>add_edge()</strong> method, specifying the names of the start and end nodes.</p> <p>The only thing we need to take into account is the planner, which behaves slightly differently from other nodes. As shown above, it can return the <strong>selected_tools</strong> field, which contains 0 to 3 output nodes. </p> <p>For that, we need to use the <strong>add_conditional_edges()</strong> method taking three parameters:</p> <ul>
<li>The planner node name;</li> <li>A callable function taking a LangGraph node and returning a list of strings indicating the list of node names should be called;</li> <li>A dictionary mapping strings from the second parameter to node names.</li>
</ul> <p>In our case, we will define the <strong>route_tools()</strong> node to simply return the <strong>state.selected_tools</strong> field as a result of a planner function.</p> <pre><code>def route_tools(state: PlayerState): return state.selected_tools or []</code></pre> <p>Then we can construct nodes:</p> <pre><code>graph_builder.add_edge(START, 'extract_name')
graph_builder.add_edge('extract_name', 'planner')
graph_builder.add_conditional_edges( 'planner', route_tools, { 'fetch_player_jersey_number': 'fetch_player_jersey_number', 'fetch_player_information': 'fetch_player_information', 'fetch_player_rating': 'fetch_player_rating' }
)
graph_builder.add_edge('fetch_player_jersey_number', 'retrieve_goals')
graph_builder.add_edge('fetch_player_information', 'retrieve_goals')
graph_builder.add_edge('fetch_player_rating', 'retrieve_goals')
graph_builder.add_edge('retrieve_goals', 'retrieve_minutes_played')
graph_builder.add_edge('retrieve_minutes_played', 'write_summary')
graph_builder.add_edge('write_summary', END)</code></pre> <p>START and END are LangGraph constants used to define the graph‚Äôs start and end points.</p> <p>The last step is to compile the graph. We can optionally visualize it using the helper function defined above.</p> <pre><code>graph = graph_builder.compile()
display_graph(graph)</code></pre> <figure><img src="https://contributor.insightmediagroup.io/wp-content/uploads/2026/02/Graph-1-1024x287.png" alt="Getting first experience with LangGraph"><figcaption>Graph diagram</figcaption></figure> <h3>Example</h3> <p>We are now finally able to use our graph! To do so, we can use the invoke method and pass a dictionary containing the question field with a custom user question:</p> <pre><code>result = graph.invoke({ 'question': 'Will Haaland be able to win the FIFA World Cup for Norway in 2026 based on his recent performance and stats?'
})</code></pre> <p>And here is an example result we can obtain!</p> <pre><code>{'question': 'Will Haaland be able to win the FIFA World Cup for Norway in 2026 based on his recent performance and stats?', 'selected_tools': ['fetch_player_information', 'fetch_player_rating'], 'name': 'Haaland', 'club': 'Manchester City', 'country': 'Norway', 'rating': 92, 'goals': [25, 40, 28, 33, 36], 'minutes_played': [2108, 3102, 3156, 2617, 2758], 'summary': '- Full name: Erling Haaland\n- Country: Norway üá≥üá¥\n- Number: N/A
- FIFA rating: 92\n- Total goals in last 3 seasons: 97 (28 + 33 + 36)\n- Average minutes per goal (last 3 seasons): 87.95 minutes per goal\n- Will Haaland win the FIFA World Cup for Norway in 2026 based on recent performance and stats?\n - Short answer: Not guaranteed. Haaland remains among the world‚Äôs top forwards (92 rating, elite goal output), and he could be a key factor for Norway. However, World Cup success is a team achievement dependent on Norway‚Äôs overall squad quality, depth, tactics, injuries, and tournament context. Based on statistics alone, he strengthens Norway‚Äôs chances, but a World Cup title in 2026 cannot be predicted with certainty.'}</code></pre> <p>A cool thing is that we can observe the entire state of the graph and analyze the tools the agent has chosen to generate the final answer. The final summary looks great!</p> <h2>Conclusion</h2> <p>In this article, we have examined AI agents that have opened a new chapter for LLMs. Equipped with state-of-the-art tools and decision-making, we now have much greater potential to solve complex tasks.</p> <p>An example we saw in this article introduced us to LangGraph ‚Äî one of the most popular frameworks for building agents. Its simplicity and elegance allow to construct complex decision chains. While, for our simple example, LangGraph might seem like overkill, it becomes extremely useful for larger projects where state and graph structures are much more complex.</p> <h2>Resources</h2> <ul>
<li><a href="https://www.langchain.com/langgraph">LangGraph | Documentation</a></li>
</ul>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">‚ñ≤</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">‚ñº</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>