<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Nemotron ColEmbed V2: Raising the Bar for Multimodal Retrieval with ViDoRe V3‚Äôs Top Model</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.8; color: #e2e8f0; max-width: 800px; margin: 40px auto; padding: 0 20px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.5em; }
  .metadata { color: #94a3b8; font-size: 0.9em; margin-bottom: 2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 1em; }
  img { max-width: 100%; height: auto; border-radius: 8px; }
  a { color: #00d9ff; }
  p { margin-bottom: 1em; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 15px; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 15px; border-radius: 6px; overflow-x: auto; }
</style>
</head>
<body>
  <h1>Nemotron ColEmbed V2: Raising the Bar for Multimodal Retrieval with ViDoRe V3‚Äôs Top Model</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 2/4/2026 | Lang: EN |
    <a href="https://huggingface.co/blog/nvidia/nemotron-colembed-v2" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div>
					<p><a href="https://huggingface.co/blog">
							Back to Articles</a></p>

					
					
					
					<div><div>

<p><span><span><a href="https://huggingface.co/ronay-nv"><img alt="Ronay Ak's avatar" src="https://huggingface.co/avatars/d9fa8404f258c96df1c500cffd10752f.svg" /></a>
				</span>
	</span></p>
			</div><div>

<p><span><span><a href="https://huggingface.co/gmoreira-nv"><img alt="Gabriel de Souza Pereira Moreira's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/668c4e78976e4e1f330104c2/XGC_sq6HIh2zPyPQFsrNW.jpeg" /></a>
				</span>
	</span></p>
			</div></div>
					

					
<p>Modern search systems are increasingly designed to process heterogeneous document images that may contain text, tables, charts, figures, and other visual components. In this context, accurately retrieving relevant information across these diverse modalities is a central challenge. Multimodal embedding models built on top of foundational vision‚Äìlanguage models (VLMs) map diverse content types into a shared representation space, enabling unified retrieval over text, images, and structured visual elements. Although encoding an entire query and candidate document into a single vector is a common practice‚Äîexemplified by our recently released commercial-ready <a href="https://huggingface.co/blog/nvidia/llama-nemotron-vl-1b">Llama-Nemotron-Embed-VL-1B</a> which prioritizes efficiency and low storage‚Äîthere is an increasing research direction on multi-vector, late-interaction style embedding architectures which provide fine-grained multi-vector interaction between queries and documents. By enabling richer token representations, these models better capture more detailed semantic relationships, and they have shown higher accuracy performance on various (multimodal) benchmarks.</p>
<p>NVIDIA introduces the Nemotron ColEmbed V2 family, a set of late-interaction embedding models available in three sizes‚Äî3B, 4B, and 8B‚Äîdesigned for highly accurate multimodal retrieval. These models adopt a unified approach to text‚Äìimage retrieval and achieve state-of-the-art performance on the ViDoRe V1, V2, and V3 <a href="https://huggingface.co/vidore">benchmarks</a>.</p>
<h2>
	<a href="#nemotron-colembed-v2-highlights-tldr">
		
	</a>
	<span>
		Nemotron ColEmbed V2 Highlights (TL;DR)
	</span>
</h2>
<p>The <a href="https://huggingface.co/nvidia/nemotron-colembed-vl-8b-v2">nemotron-colembed-vl-8b-v2</a>, <a href="https://huggingface.co/nvidia/nemotron-colembed-vl-4b-v2">nemotron-colembed-vl-4b-v2</a> and <a href="https://huggingface.co/nvidia/llama-nemotron-colembed-vl-3b-v2">llama-nemotron-colembed-vl-3b-v2</a> are state-of-the-art late interaction embedding models that rank 1st, 3rd and 6th‚Äîthe highest ranked models in each weight class, as of Feb 3, 2026, on the ViDoRe V3 benchmark: a comprehensive evaluation of visual document retrieval for enterprise use-case benchmark. </p>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/A4KY3NSZrrxGEcO6mihU8.png"><img alt="late_interaction" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/A4KY3NSZrrxGEcO6mihU8.png" /></a></p>
<p>The late interaction mechanism introduced by <a href="https://arxiv.org/pdf/2004.12832">ColBERT</a> for multi-vector embedding matching has been extended in our work to a multimodal setting, enabling fine-grained interactions between query and document tokens, whether textual or visual. As illustrated in the figure, each query token embedding interacts with all document token embeddings via the <code>MaxSim</code> operator, which selects the maximum similarity for each query token and then sums these maxima to produce the final relevance score. This approach requires storing the token embeddings for the entire document corpus, whether textual or visual, thereby increasing storage requirements. During inference, query token embeddings are computed and matched against the stored document embeddings using the same MaxSim operation. </p>
<p>Nemotron ColEmbed V2 family of models is intended for researchers exploring visual document retrieval applications where accuracy is paramount. This distinguishes it from our <a href="https://huggingface.co/blog/nvidia/llama-nemotron-vl-1b">1B single-vector model</a> released last month, which was designed for commercial environments requiring minimal storage and high throughput. It is instrumental in multimodal RAG systems, where textual queries can be used to retrieve document images, such as pages, text, charts, tables, or infographics. The models output multi-vector embeddings for input queries and documents. Potential applications include multimedia search engines, cross-modal retrieval systems, and conversational AI with rich input understanding.</p>
<p>As a new benchmark, <a href="https://huggingface.co/blog/QuentinJG/introducing-vidore-v3">ViDoRe V3</a> is designed to set an industry standard for multi-modal enterprise document retrieval. It tackles a key challenge in production RAG systems: accurately extracting information from complex, visually-rich documents.  With its strong multi-modal document retrieval capability, the <a href="https://huggingface.co/nvidia/nemotron-colembed-vl-8b-v2">nemotron-colembed-vl-8b-v2</a> model ranks <strong>#1</strong> on the ViDoRe V3 leaderboard, setting a new standard for accuracy.</p>
<p><strong>Visual Document Retrieval benchmark (page retrieval) ‚Äì Avg NDCG@10 on ViDoRe V3 public and private tasks.</strong></p>
<div>
	<table>
		<thead><tr>
<th>Model</th>
<th>Emb_dim</th>
<th># of parameters</th>
<th>ViDoRe V3 Accuracy (NDCG@10)</th>
</tr>

		</thead><tbody><tr>
<td><a href="https://huggingface.co/nvidia/nemotron-colembed-vl-8b-v2">nemotron-colembed-vl-8b-v2</a></td>
<td>4096</td>
<td>8.8B</td>
<td>63.42</td>
</tr>
<tr>
<td><a href="https://huggingface.co/nvidia/nemotron-colembed-vl-4b-v2">nemotron-colembed-vl-4b-v2</a></td>
<td>2560</td>
<td>4.8B</td>
<td>61.54</td>
</tr>
<tr>
<td><a href="https://huggingface.co/nvidia/llama-nemotron-colembed-vl-3b-v2">llama-nemotron-colembed-vl-3b-v2</a></td>
<td>3072</td>
<td>4.4B</td>
<td>59.79</td>
</tr>
<tr>
<td><a href="https://huggingface.co/nvidia/llama-nemoretriever-colembed-3b-v1">lama-nemoretriever-colembed-3b-v1</a></td>
<td>3072</td>
<td>4.4B</td>
<td>57.26</td>
</tr>
</tbody>
	</table>
</div>
<h2>
	<a href="#models-architecture">
		
	</a>
	<span>
		Models‚Äô Architecture
	</span>
</h2>
<p>The llama-nemotron-colembed-vl-3b-v2 is a transformer-based multimodal embedding model built on top of a VLM based on google/siglip2-giant-opt-patch16-384 and meta-llama/Llama-3.2-3B.  The nemotron-colembed-vl-8b-v2 and nemotron-colembed-vl-4b-v2 multimodal encoder models were built from <a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct">Qwen3-VL-8B-Instruct</a> and <a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct">Qwen3-VL-4B-Instruct</a>, respectively. </p>
<h3>
	<a href="#architecture-modifications">
		
	</a>
	<span>
		Architecture modifications:
	</span>
</h3>
<ul>
<li>Our models use bi-directional self-attention instead of the original uni-directional causal self-attention from the LLM decoder models. This allows the model to learn rich representations from the whole input sequence.</li>
<li>ColBERT-style late interaction mechanism- for each input token, each model outputs an n-dimensional embedding vector of floating-point values, where n is determined by the model‚Äôs hidden size.</li>
</ul>
<h3>
	<a href="#training-methodology">
		
	</a>
	<span>
		Training Methodology
	</span>
</h3>
<p>The <a href="https://huggingface.co/nvidia/nemotron-colembed-vl-8b-v2">nemotron-colembed-vl-8b-v2</a>, <a href="https://huggingface.co/nvidia/nemotron-colembed-vl-4b-v2">nemotron-colembed-vl-4b-v2</a> and <a href="https://huggingface.co/nvidia/llama-nemotron-colembed-vl-3b-v2">llama-nemotron-colembed-vl-3b-v2</a> models were trained using a bi-encoder architecture, independently. This involves encoding a pair of sentences (for example, a query and a document) independently using the embedding model. Using contrastive learning, it is used to maximize the late interaction similarity between the query and the document that contains the answer, while minimizing the similarity between the query and sampled negative documents not useful to answer the question.</p>
<p>The llama-nemotron-colembed-vl-3b-v2 model was trained in a two-stage pipeline:  it was first fine-tuned with 12.5M textQA pairs, and subsequently fine-tuned with text‚Äìimage pairs. The nemotron-colembed-vl-8b-v2,  nemotron-colembed-vl-4b-v2 models were fine-tuned using only text-image pairs (2nd stage).</p>
<p>Our training datasets contain both text-only and text-image pairs, and we apply hard negative mining following the positive-aware hard negative mining methods presented in the NV-Retriever <a href="https://arxiv.org/pdf/2407.15831">paper</a> to improve retrieval performance.</p>
<p>‚ú® <strong>Key Improvements over V1:</strong></p>
<p>‚öóÔ∏è Advanced Model Merging: Utilizes post-training model merging to combine the strengths of multiple fine-tuned checkpoints. This delivers the accuracy stability of an ensemble without any additional inference latency.<br /></p>
<p>üåç Enhanced Synthetic Data: We significantly enriched our training mixture with diverse multilingual synthetic data, improving semantic alignment across languages and complex document types.</p>
<p><a href="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/J7JIKCDriMjztO1ULw0k3.png"><img alt="modelperfs_vidorev3" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/J7JIKCDriMjztO1ULw0k3.png" /></a></p>
<h2>
	<a href="#start-building-with-nemotron-colembed-v2">
		
	</a>
	<span>
		Start Building with Nemotron ColEmbed V2
	</span>
</h2>
<p>Nemotron ColEmbed V2 models mark a major step forward in high-accuracy text‚Äìimage retrieval, delivering state-of-the-art results on the ViDoRe V1, V2, and  V3 benchmarks. The availability of 3B, 4B and 8B model variants further establishes a solid foundation for future research and advanced experimentation in multimodal retrieval applications.</p>
<p>Get started with Nemotron ColEmbed V2 models by downloading the models: <a href="https://huggingface.co/nvidia/nemotron-colembed-vl-8b-v2">nemotron-colembed-vl-8b-v2</a>, <a href="https://huggingface.co/nvidia/nemotron-colembed-vl-4b-v2">nemotron-colembed-vl-4b-v2</a> and <a href="https://huggingface.co/nvidia/llama-nemotron-colembed-vl-3b-v2">llama-nemotron-colembed-vl-3b-v2</a>, available on Hugging Face. Check out our <a href="https://huggingface.co/nvidia/nemotron-colembed-vl-8b-v2/blob/main/nemotron_colembed_4b_8b_v2_example_vidore_v3.ipynb">example notebook</a> which demonstrates a simple indexing and retrieval pipeline using the nemotron-colembed-vl-4b-v2 and nemotron-colembed-vl-8b-v2 models. For a deep dive into the architectures, training methodology, and benchmarks, see our <a href="https://arxiv.org/abs/2602.03992">Nemotron ColEmbed V2 paper</a>.</p>
<p>Learn more about the NVIDIA NeMo Retriever family of Nemotron RAG models on the <a href="https://developer.nvidia.com/nemo-retriever">product page</a>, or access the microservice container from <a href="https://catalog.ngc.nvidia.com/orgs/nim/collections/nemo-retriever">NVIDIA NGC</a>. This is an excellent opportunity to explore state-of-the-art retrieval in your own applications and workflows. Try <a href="https://build.nvidia.com/nvidia/build-an-enterprise-rag-pipeline">NVIDIA Enterprise RAG Blueprint</a>, using the Nemotron RAG models that are powered by the same tech behind our ViDoRe V3 winning. </p>
</div></div>
  </div>
</body>
</html>