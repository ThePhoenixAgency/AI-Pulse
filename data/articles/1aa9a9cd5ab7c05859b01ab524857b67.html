<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Accelerate a World of LLMs on Hugging Face with NVIDIA NIM</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
</style>
</head>
<body>
  <h1>Accelerate a World of LLMs on Hugging Face with NVIDIA NIM</h1>
  <div class="metadata">
<<<<<<< HEAD
    Source: Hugging Face Blog | Date: 7/21/2025 8:01:30 PM | Lang: EN |
=======
    Source: Hugging Face Blog | Date: 7/21/2025 6:01:30 PM | Lang: EN |
>>>>>>> 48d6193da6f49976a64b6a30483399bfb54b1b8d
    <a href="https://huggingface.co/blog/nvidia/multi-llm-nim" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div> <p><span><span><a href="https://huggingface.co/nealv"><img alt="Neal Vaidya's avatar" src="https://huggingface.co/avatars/cfdfb7efad2d7c720075252ca14f77da.svg"></a> </span> </span></p> </div> <p>AI builders want a choice of the latest <a href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language models</a> (LLM) architectures and specialized variants for use in <a href="https://www.nvidia.com/en-us/glossary/ai-agents/">AI agents</a> and other apps, but handling all the diversity can slow testing and deployment pipelines. In particular, managing and optimizing different inference software frameworks to achieve best performance across varied LLMs and serving requirements is a time-consuming bottleneck to getting performant AI apps in the hands of end-users.</p>
<p>NVIDIA AI customers and ecosystem partners leverage <a href="https://developer.nvidia.com/nim">NVIDIA NIM</a> inference microservices to streamline deployment of the latest AI models on NVIDIA accelerated infrastructure, including LLMs, multi-modal and domain-specific models from NVIDIA, Meta, Mistral AI, Google and hundreds more innovative model builders. We’ve seen customers and partners deliver more innovation, faster, with a simplified, reliable approach to model deployment, and today we’re excited to unlock over 100,000 LLMs on Hugging Face for rapid, reliable deployment with NIM.</p>
<h2> <a href="#a-single-nim-microservice-for-deploying-a-broad-range-of-llms"> </a> <span> A Single NIM Microservice for Deploying a Broad Range of LLMs </span>
</h2>
<p>NIM now provides a single docker container for deploying a broad range of LLMs supported by leading inference frameworks from NVIDIA and the community including NVIDIA TensorRT-LLM, vLLM and SGLang. When an LLM is provided to the NIM container, it performs several steps for deployment and performance optimization, without manual configuration:</p>
<div> <table> <thead><tr>
<th>LLM Adaptation Phase</th>
<th>What NIM Does</th>
</tr> </thead><tbody><tr>
<td><em>Model Analysis</em></td>
<td>NIM automatically identifies the model's format, including Hugging Face models, TensorRT-LLM checkpoints, or pre-built TensorRT-LLM engines, ensuring compatibility.</td>
</tr>
<tr>
<td><em>Architecture and Quantization Detection</em></td>
<td>It identifies the model's architecture (e.g., Llama, Mistral) and quantization format (e.g., FP16, FP8, INT4).</td>
</tr>
<tr>
<td><em>Backend Selection</em></td>
<td>Based on this analysis, NIM selects an inference backend (NVIDIA TensorRT-LLM, vLLM, or SGLang).</td>
</tr>
<tr>
<td><em>Performance Setup</em></td>
<td>NIM applies pre-configured settings for the chosen model and backend and then starts the inference server, reducing manual tuning efforts.</td>
</tr>
</tbody> </table>
</div>
<p>Table 1. NVIDIA NIM LLM adaptation phases and functionality</p>
<p>The single NIM container supports common LLM weight formats, including:</p>
<ul>
<li><strong>Hugging Face Transformers Checkpoints</strong>: LLMs can be deployed directly from Hugging Face repositories with<code>.safetensors</code> files, removing the need for complex conversions. </li>
<li><strong>GGUF Checkpoints:</strong> Quantized GGUF checkpoints for supported model architectures can be deployed directly from HuggingFace or from locally downloaded files </li>
<li><strong>TensorRT-LLM Checkpoints</strong>: Models packaged within a <code>trtllm_ckpt</code> directory, optimized for TensorRT-LLM, can be deployed. </li>
<li><strong>TensorRT-LLM Engines</strong>: Pre-built TensorRT-LLM engines from a <code>trtllm_engine</code> directory can be used for peak performance on NVIDIA GPUs.</li>
</ul>
<h2> <a href="#getting-started"> </a> <span> Getting Started </span>
</h2>
<p>To use NIM, ensure your environment has NVIDIA GPUs with appropriate drivers (CUDA 12.1+), Docker installed, an <a href="https://catalog.ngc.nvidia.com/">NVIDIA NGC</a> Account and API Key for NIM Docker images, and a Hugging Face account and API token for models requiring authentication. Learn more about environment prerequisites in the <a href="https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html">NIM documentation</a>.</p>
<p>Environment setup involves setting environment variables and creating a persistent cache directory. Ensure the <code>nim_cache</code> directory has correct Unix permissions, ideally owned by the same Unix user launching the Docker container, to prevent permission issues. Commands use <code>-u $(id -u)</code> to manage this.</p>
<p>For ease of use, let’s store some of the frequently used information in environment variables.</p>
<pre><code><span># A variable for storing the NIM docker image specification</span>
NIM_IMAGE=llm-nim
<span># Populate with your Hugging Face API token.</span>
HF_TOKEN=&lt;your_huggingface_token&gt;
</code></pre>
<h3> <a href="#example-1-deploying-a-model"> </a> <span> Example 1: Deploying a Model </span>
</h3>
<p>Deploying an LLM from Hugging Face is demonstrated with Codestral-22B:</p>
<pre><code>docker run --<span>rm</span> --gpus all \ --shm-size=16GB \ --network=host \ -u $(<span>id</span> -u) \ -v $(<span>pwd</span>)/nim_cache:/opt/nim/.cache \ -v $(<span>pwd</span>):$(<span>pwd</span>) \ -e HF_TOKEN=<span>$HF_TOKEN</span> \ -e NIM_TENSOR_PARALLEL_SIZE=1 \ -e NIM_MODEL_NAME=<span>"hf://mistralai/Codestral-22B-v0.1"</span> \ <span>$NIM_IMAGE</span>
</code></pre>
<p>For locally downloaded models, point <code>NIM_MODEL_NAME</code> to the path and mount the directory:</p>
<pre><code>docker run --<span>rm</span> --gpus all \ --shm-size=16GB \ --network=host \ -u $(<span>id</span> -u) \ -v $(<span>pwd</span>)/nim_cache:/opt/nim/.cache \ -v $(<span>pwd</span>):$(<span>pwd</span>) \ -v /path/to/model/dir:/path/to/model/dir \ -e HF_TOKEN=<span>$HF_TOKEN</span> \ -e NIM_TENSOR_PARALLEL_SIZE=1 \ -e NIM_MODEL_NAME=<span>"/path/to/model/dir/mistralai-Codestral-22B-v0.1"</span> \ <span>$NIM_IMAGE</span>
</code></pre>
<p>While deploying a model, feel free to inspect the output logs to get a sense of the choices NIM made during model deployment. Deployed models are available at <code>http://localhost:8000</code>, with API endpoints at <a href="http://localhost:8000/docs"><code>http://localhost:8000/docs</code></a>.</p>
<p>Additional arguments are available by the underlying engine. You can inspect the full list of such arguments by running nim-run --help in the container, as shown below.</p>
<pre><code>docker run --<span>rm</span> --gpus all \ --network=host \ -u $(<span>id</span> -u) \ <span>$NIM_IMAGE</span> nim-run --<span>help</span>
</code></pre>
<h3> <a href="#example-2-specifying-a-backend"> </a> <span> Example 2: Specifying a Backend </span>
</h3>
<p>To inspect compatible backends or choose a specific one, use <code>list-model-profiles</code>:</p>
<pre><code>docker run --<span>rm</span> --gpus all \ --shm-size=16GB \ --network=host \ -u $(<span>id</span> -u) \ -v $(<span>pwd</span>)/nim_cache:/opt/nim/.cache \ -v $(<span>pwd</span>):$(<span>pwd</span>) \ -e HF_TOKEN=<span>$HF_TOKEN</span> \ <span>$NIM_IMAGE</span> list-model-profiles --model <span>"hf://meta-llama/Llama-3.1-8B-Instruct"</span>
</code></pre>
<p>This command shows compatible profiles, including for LoRA adapters. To deploy with a specific backend like vLLM, use the <code>NIM_MODEL_PROFILE</code> environment variable, using the output supplied by <code>list-model-profiles</code>:</p>
<pre><code>docker run --<span>rm</span> --gpus all \ --shm-size=16GB \ --network=host \ -u $(<span>id</span> -u) \ -v $(<span>pwd</span>)/nim_cache:/opt/nim/.cache \ -v $(<span>pwd</span>):$(<span>pwd</span>) \ -e HF_TOKEN=<span>$HF_TOKEN</span> \ -e NIM_TENSOR_PARALLEL_SIZE=1 \ -e NIM_MODEL_NAME=<span>"hf://meta-llama/Llama-3.1-8B-Instruct"</span> \ -e NIM_MODEL_PROFILE=<span>"e2f00b2cbfb168f907c8d6d4d40406f7261111fbab8b3417a485dcd19d10cc98"</span> \ <span>$NIM_IMAGE</span>
</code></pre>
<h3> <a href="#example-3-quantized-model-deployment"> </a> <span> Example 3: Quantized Model Deployment </span>
</h3>
<p>NIM facilitates deploying quantized models. It automatically detects the quantization format (e.g., GGUF, AWQ) and selects the appropriate backend using standard deployment commands:</p>
<pre><code><span># Choose a quantized model and populate the MODEL variable, for example:</span>
<span># MODEL="hf://modularai/Llama-3.1-8B-Instruct-GGUF"</span>
<span># or</span>
<span># MODEL="hf://Qwen/Qwen2.5-14B-Instruct-AWQ"</span>
docker run --<span>rm</span> --gpus all \ --shm-size=16GB \ --network=host \ -u $(<span>id</span> -u) \ -v $(<span>pwd</span>)/nim_cache:/opt/nim/.cache \ -v $(<span>pwd</span>):$(<span>pwd</span>) \ -e HF_TOKEN=<span>$HF_TOKEN</span> \ -e NIM_TENSOR_PARALLEL_SIZE=1 \ -e NIM_MODEL_NAME=<span>$MODEL</span> \ <span>$NIM_IMAGE</span>
</code></pre>
<p>For advanced users, NIM offers customization through environment variables such as <code>NIM_MAX_MODEL_LEN</code> for context length. For large LLMs, <code>NIM_TENSOR_PARALLEL_SIZE</code> enables multi-GPU deployment. Ensure <code>--shm-size=&lt;shared memory size&gt;</code> is passed to Docker for multi-GPU communication.</p>
<p>The NIM container supports a broad range of LLMs supported by NVIDIA TensorRT-LLM, vLLM and SGLang, including popular LLMs and specialized variants on Hugging Face. For more details on supported LLMs, see the <a href="https://docs.nvidia.com/nim/large-language-models/latest/supported-llm-agnostic-architectures.html">documentation</a>.</p>
<h2> <a href="#build-with-hugging-face-and-nvidia"> </a> <span> Build with Hugging Face and NVIDIA </span>
</h2>
<p>NIM is designed to simplify AI model deployment on NVIDIA accelerated infrastructure, speeding innovation and time to value for high performance AI builders and enterprise AI teams. We look forward to engagement and feedback from the Hugging Face Community.</p>
<p>Get started with a developer example in an NVIDIA-hosted computing environment at <a href="https://build.nvidia.com/nvidia/bring-llm-to-nim">build.nvidia.com</a>.</p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
  </script>
</body>
</html>