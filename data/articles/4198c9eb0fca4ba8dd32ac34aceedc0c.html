<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>GitHub - getzep/graphiti: Build Real-Time Knowledge Graphs for AI Agents</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>GitHub - getzep/graphiti: Build Real-Time Knowledge Graphs for AI Agents</h1>
  <div class="metadata">
    Source: GitHub Trending Python | Date: 2/20/2026 4:14:32 AM | <a href="https://github.com/getzep/graphiti" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: EN
  </div>
  <div class="content">
    <p> <a href="https://www.getzep.com/"> <img src="https://private-user-images.githubusercontent.com/131175/370887997-119c5682-9654-4257-8922-56b7cb8ffd73.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NzE1NjExNjUsIm5iZiI6MTc3MTU2MDg2NSwicGF0aCI6Ii8xMzExNzUvMzcwODg3OTk3LTExOWM1NjgyLTk2NTQtNDI1Ny04OTIyLTU2YjdjYjhmZmQ3My5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMjIwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDIyMFQwNDE0MjVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mYjY4N2I4OGI2ZTE5ODM5MGQ3ZGQ3NGYwYWE3OTIxZTM2OGVlMWMwZDI5ZDk3ZWUyNDhlNzA4NTM2Njg2ZjJmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.kCnqbdtLVw3Iyr0mkQ0_1oJKs9MlPHSO523L41CU6E8" alt="Zep Logo"> </a>
</p>
<div><h1>
Graphiti
</h1><a href="#graphiti"></a></div>
<div><h2> Build Real-Time Knowledge Graphs for AI Agents</h2><a href="#-build-real-time-knowledge-graphs-for-ai-agents"></a></div>
<div>
<p><a href="https://github.com/getzep/Graphiti/actions/workflows/lint.yml"><img src="https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat" alt="Lint"></a>
<a href="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml"><img src="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg" alt="Unit Tests"></a>
<a href="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml"><img src="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg" alt="MyPy Check"></a></p>
<p><a target="_blank" href="https://camo.githubusercontent.com/864306708704ba460b189a65239c82a05cdb4eb44a71f53aafebf971f88718a1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6765747a65702f6772617068697469"><img src="https://camo.githubusercontent.com/864306708704ba460b189a65239c82a05cdb4eb44a71f53aafebf971f88718a1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6765747a65702f6772617068697469" alt="GitHub Repo stars"></a>
<a href="https://discord.com/invite/W8Kw6bsgXQ"><img src="https://camo.githubusercontent.com/b9c9d6f0227a6549bc173032d801fe0fd0295e5e6e235c7e6155c05cbc961f15/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d2532333538363546322e7376673f266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Discord"></a>
<a href="https://arxiv.org/abs/2501.13956"><img src="https://camo.githubusercontent.com/0c4083153cd07b456b47b4e4dca3ff84e8b1eeeaabf8e2f2741304fa7c344990/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323530312e31333935362d6233316231622e7376673f7374796c653d666c6174" alt="arXiv"></a>
<a href="https://github.com/getzep/graphiti/releases"><img src="https://camo.githubusercontent.com/d950eb1d592a4e53b267f5b20e20c214198ecca08e1696c138437bb20f6f0472/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6765747a65702f67726170686974693f7374796c653d666c6174266c6162656c3d52656c6561736526636f6c6f723d6c696d65677265656e" alt="Release"></a></p>
</div>
<div>
<p><a href="https://trendshift.io/repositories/12986"><img src="https://camo.githubusercontent.com/9f7b9c668eb9c343af43793c76033eb809fe442811817cc3350948ecd7f6fd81/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f3132393836" alt="getzep%2Fgraphiti | Trendshift"></a></p>
</div>
<p> <em>Help us reach more developers and grow the Graphiti community. Star this repo!</em></p>
<br>
<div><p>Tip</p><p>Check out the new <a href="/getzep/graphiti/blob/main/mcp_server/README.md">MCP server for Graphiti</a>! Give Claude, Cursor, and other MCP clients powerful
Knowledge Graph-based memory.</p>
</div>
<pre><code>Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents
operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti
continuously integrates user interactions, structured and unstructured enterprise data, and external information into a
coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical
queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI
applications.</code></pre>
<p>Use Graphiti to:</p>
<ul>
<li>Integrate and maintain dynamic user interactions and business data.</li>
<li>Facilitate state-based reasoning and task automation for agents.</li>
<li>Query complex, evolving data with semantic, keyword, and graph-based search methods.</li>
</ul>
<br>
<p> <a target="_blank" href="/getzep/graphiti/blob/main/images/graphiti-graph-intro.gif"><img src="/getzep/graphiti/raw/main/images/graphiti-graph-intro.gif" alt="Graphiti temporal walkthrough"></a>
</p>
<br>
<pre><code>A knowledge graph is a network of interconnected facts, such as "Kendra loves Adidas shoes." Each fact is a "triplet"
represented by two entities, or
nodes ("Kendra", "Adidas shoes"), and their relationship, or edge ("loves"). Knowledge Graphs have been explored
extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph
while handling changing relationships and maintaining historical context.</code></pre>
<div><h2>Graphiti and Zep's Context Engineering Platform.</h2><a href="#graphiti-and-zeps-context-engineering-platform"></a></div>
<p>Graphiti powers the core of <a href="https://www.getzep.com">Zep's context engineering platform</a> for AI Agents. Zep
offers agent memory, Graph RAG for dynamic data, and context retrieval and assembly.</p>
<p>Using Graphiti, we've demonstrated Zep is
the <a href="https://blog.getzep.com/state-of-the-art-agent-memory/">State of the Art in Agent Memory</a>.</p>
<p>Read our paper: <a href="https://arxiv.org/abs/2501.13956">Zep: A Temporal Knowledge Graph Architecture for Agent Memory</a>.</p>
<p>We're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.</p>
<p> <a href="https://arxiv.org/abs/2501.13956"><img src="/getzep/graphiti/raw/main/images/arxiv-screenshot.png" alt="Zep: A Temporal Knowledge Graph Architecture for Agent Memory"></a>
</p>
<div><h2>Zep vs Graphiti</h2><a href="#zep-vs-graphiti"></a></div>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Zep</th>
<th>Graphiti</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>What they are</strong></td>
<td>Fully managed platform for context engineering and AI memory</td>
<td>Open-source graph framework</td>
</tr>
<tr>
<td><strong>User &amp; conversation management</strong></td>
<td>Built-in users, threads, and message storage</td>
<td>Build your own</td>
</tr>
<tr>
<td><strong>Retrieval &amp; performance</strong></td>
<td>Pre-configured, production-ready retrieval with sub-200ms performance at scale</td>
<td>Custom implementation required; performance depends on your setup</td>
</tr>
<tr>
<td><strong>Developer tools</strong></td>
<td>Dashboard with graph visualization, debug logs, API logs; SDKs for Python, TypeScript, and Go</td>
<td>Build your own tools</td>
</tr>
<tr>
<td><strong>Enterprise features</strong></td>
<td>SLAs, support, security guarantees</td>
<td>Self-managed</td>
</tr>
<tr>
<td><strong>Deployment</strong></td>
<td>Fully managed or in your cloud</td>
<td>Self-hosted only</td>
</tr>
</tbody>
</table>
<div><h3>When to choose which</h3><a href="#when-to-choose-which"></a></div>
<p><strong>Choose Zep</strong> if you want a turnkey, enterprise-grade platform with security, performance, and support baked in.</p>
<p><strong>Choose Graphiti</strong> if you want a flexible OSS core and you're comfortable building/operating the surrounding system.</p>
<div><h2>Why Graphiti?</h2><a href="#why-graphiti"></a></div>
<p>Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for
frequently changing data. Graphiti addresses these challenges by providing:</p>
<ul>
<li><strong>Real-Time Incremental Updates:</strong> Immediate integration of new data episodes without batch recomputation.</li>
<li><strong>Bi-Temporal Data Model:</strong> Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time
queries.</li>
<li><strong>Efficient Hybrid Retrieval:</strong> Combines semantic embeddings, keyword (BM25), and graph traversal to achieve
low-latency queries without reliance on LLM summarization.</li>
<li><strong>Custom Entity Definitions:</strong> Flexible ontology creation and support for developer-defined entities through
straightforward Pydantic models.</li>
<li><strong>Scalability:</strong> Efficiently manages large datasets with parallel processing, suitable for enterprise environments.</li>
</ul>
<p> <a target="_blank" href="/getzep/graphiti/blob/main/images/graphiti-intro-slides-stock-2.gif"><img src="/getzep/graphiti/raw/main/images/graphiti-intro-slides-stock-2.gif" alt="Graphiti structured + unstructured demo"></a>
</p>
<div><h2>Graphiti vs. GraphRAG</h2><a href="#graphiti-vs-graphrag"></a></div>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>GraphRAG</th>
<th>Graphiti</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Primary Use</strong></td>
<td>Static document summarization</td>
<td>Dynamic data management</td>
</tr>
<tr>
<td><strong>Data Handling</strong></td>
<td>Batch-oriented processing</td>
<td>Continuous, incremental updates</td>
</tr>
<tr>
<td><strong>Knowledge Structure</strong></td>
<td>Entity clusters &amp; community summaries</td>
<td>Episodic data, semantic entities, communities</td>
</tr>
<tr>
<td><strong>Retrieval Method</strong></td>
<td>Sequential LLM summarization</td>
<td>Hybrid semantic, keyword, and graph-based search</td>
</tr>
<tr>
<td><strong>Adaptability</strong></td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td><strong>Temporal Handling</strong></td>
<td>Basic timestamp tracking</td>
<td>Explicit bi-temporal tracking</td>
</tr>
<tr>
<td><strong>Contradiction Handling</strong></td>
<td>LLM-driven summarization judgments</td>
<td>Temporal edge invalidation</td>
</tr>
<tr>
<td><strong>Query Latency</strong></td>
<td>Seconds to tens of seconds</td>
<td>Typically sub-second latency</td>
</tr>
<tr>
<td><strong>Custom Entity Types</strong></td>
<td>No</td>
<td>Yes, customizable</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>Moderate</td>
<td>High, optimized for large datasets</td>
</tr>
</tbody>
</table>
<p>Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it
particularly suitable for applications requiring real-time interaction and precise historical queries.</p>
<div><h2>Installation</h2><a href="#installation"></a></div>
<p>Requirements:</p>
<ul>
<li>Python 3.10 or higher</li>
<li>Neo4j 5.26 / FalkorDB 1.1.2 / Kuzu 0.11.2 / Amazon Neptune Database Cluster or Neptune Analytics Graph + Amazon
OpenSearch Serverless collection (serves as the full text search backend)</li>
<li>OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)</li>
</ul>
<div><p>Important</p><p>Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).
Using other services may result in incorrect output schemas and ingestion failures. This is particularly
problematic when using smaller models.</p>
</div>
<p>Optional:</p> <div><p>Tip</p><p>The simplest way to install Neo4j is via <a href="https://neo4j.com/download/">Neo4j Desktop</a>. It provides a user-friendly
interface to manage Neo4j instances and databases.
Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:</p>
</div>
<div><pre>docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest
</pre></div>
<div><pre>pip install graphiti-core</pre></div>
<p>or</p>
<div><pre>uv add graphiti-core</pre></div>
<div><h3>Installing with FalkorDB Support</h3><a href="#installing-with-falkordb-support"></a></div>
<p>If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:</p>
<div><pre>pip install graphiti-core[falkordb] <span><span>#</span> or with uv</span>
uv add graphiti-core[falkordb]</pre></div>
<div><h3>Installing with Kuzu Support</h3><a href="#installing-with-kuzu-support"></a></div>
<p>If you plan to use Kuzu as your graph database backend, install with the Kuzu extra:</p>
<div><pre>pip install graphiti-core[kuzu] <span><span>#</span> or with uv</span>
uv add graphiti-core[kuzu]</pre></div>
<div><h3>Installing with Amazon Neptune Support</h3><a href="#installing-with-amazon-neptune-support"></a></div>
<p>If you plan to use Amazon Neptune as your graph database backend, install with the Amazon Neptune extra:</p>
<div><pre>pip install graphiti-core[neptune] <span><span>#</span> or with uv</span>
uv add graphiti-core[neptune]</pre></div>
<div><h3>You can also install optional LLM providers as extras:</h3><a href="#you-can-also-install-optional-llm-providers-as-extras"></a></div>
<div><pre><span><span>#</span> Install with Anthropic support</span>
pip install graphiti-core[anthropic] <span><span>#</span> Install with Groq support</span>
pip install graphiti-core[groq] <span><span>#</span> Install with Google Gemini support</span>
pip install graphiti-core[google-genai] <span><span>#</span> Install with multiple providers</span>
pip install graphiti-core[anthropic,groq,google-genai] <span><span>#</span> Install with FalkorDB and LLM providers</span>
pip install graphiti-core[falkordb,anthropic,google-genai] <span><span>#</span> Install with Amazon Neptune</span>
pip install graphiti-core[neptune]</pre></div>
<div><h2>Default to Low Concurrency; LLM Provider 429 Rate Limit Errors</h2><a href="#default-to-low-concurrency-llm-provider-429-rate-limit-errors"></a></div>
<p>Graphiti's ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM
Provider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below.</p>
<p>Concurrency controlled by the </p><pre><code>SEMAPHORE_LIMIT</code></pre> environment variable. By default, <pre><code>SEMAPHORE_LIMIT</code></pre> is set to <pre><code>10</code></pre>
concurrent operations to help prevent <pre><code>429</code></pre> rate limit errors from your LLM provider. If you encounter such errors, try
lowering this value.<p></p>
<p>If your LLM provider allows higher throughput, you can increase </p><pre><code>SEMAPHORE_LIMIT</code></pre> to boost episode ingestion
performance.<p></p>
<div><h2>Quick Start</h2><a href="#quick-start"></a></div>
<div><p>Important</p><p>Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an </p><pre><code>OPENAI_API_KEY</code></pre> is set in your
environment.
Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI
compatible APIs.<p></p>
</div>
<p>For a complete working example, see the <a href="/getzep/graphiti/blob/main/examples/quickstart/README.md">Quickstart Example</a> in the examples directory.
The quickstart demonstrates:</p>
<ol>
<li>Connecting to a Neo4j, Amazon Neptune, FalkorDB, or Kuzu database</li>
<li>Initializing Graphiti indices and constraints</li>
<li>Adding episodes to the graph (both text and structured JSON)</li>
<li>Searching for relationships (edges) using hybrid search</li>
<li>Reranking search results using graph distance</li>
<li>Searching for nodes using predefined search recipes</li>
</ol>
<p>The example is fully documented with clear explanations of each functionality and includes a comprehensive README with
setup instructions and next steps.</p>
<div><h3>Running with Docker Compose</h3><a href="#running-with-docker-compose"></a></div>
<p>You can use Docker Compose to quickly start the required services:</p>
<ul>
<li>
<p><strong>Neo4j Docker:</strong></p>
<div><pre>docker compose up</pre></div>
<p>This will start the Neo4j Docker service and related components.</p>
</li>
<li>
<p><strong>FalkorDB Docker:</strong></p>
<div><pre>docker compose --profile falkordb up</pre></div>
<p>This will start the FalkorDB Docker service and related components.</p>
</li>
</ul>
<div><h2>MCP Server</h2><a href="#mcp-server"></a></div>
<p>The </p><pre><code>mcp_server</code></pre> directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server
allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.<p></p>
<p>Key features of the MCP server include:</p>
<ul>
<li>Episode management (add, retrieve, delete)</li>
<li>Entity management and relationship handling</li>
<li>Semantic and hybrid search capabilities</li>
<li>Group management for organizing related data</li>
<li>Graph maintenance operations</li>
</ul>
<p>The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant
workflows.</p>
<p>For detailed setup instructions and usage examples, see the <a href="/getzep/graphiti/blob/main/mcp_server/README.md">MCP server README</a>.</p>
<div><h2>REST Service</h2><a href="#rest-service"></a></div>
<p>The </p><pre><code>server</code></pre> directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.<p></p>
<p>Please see the <a href="/getzep/graphiti/blob/main/server/README.md">server README</a> for more information.</p>
<div><h2>Optional Environment Variables</h2><a href="#optional-environment-variables"></a></div>
<pre><code>In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.
If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables
must be set.</code></pre>
<div><h3>Database Configuration</h3><a href="#database-configuration"></a></div>
<p>Database names are configured directly in the driver constructors:</p>
<ul>
<li><strong>Neo4j</strong>: Database name defaults to <pre><code>neo4j</code></pre> (hardcoded in Neo4jDriver)</li>
<li><strong>FalkorDB</strong>: Database name defaults to <pre><code>default_db</code></pre> (hardcoded in FalkorDriver)</li>
</ul>
<p>As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it
to the Graphiti constructor using the </p><pre><code>graph_driver</code></pre> parameter.<p></p>
<div><h4>Neo4j with Custom Database Name</h4><a href="#neo4j-with-custom-database-name"></a></div>
<div><pre><span>from</span> <span>graphiti_core</span> <span>import</span> <span>Graphiti</span>
<span>from</span> <span>graphiti_core</span>.<span>driver</span>.<span>neo4j_driver</span> <span>import</span> <span>Neo4jDriver</span> <span># Create a Neo4j driver with custom database name</span>
<span>driver</span> <span>=</span> <span>Neo4jDriver</span>( <span>uri</span><span>=</span><span>"bolt://localhost:7687"</span>, <span>user</span><span>=</span><span>"neo4j"</span>, <span>password</span><span>=</span><span>"password"</span>, <span>database</span><span>=</span><span>"my_custom_database"</span> <span># Custom database name</span>
) <span># Pass the driver to Graphiti</span>
<span>graphiti</span> <span>=</span> <span>Graphiti</span>(<span>graph_driver</span><span>=</span><span>driver</span>)</pre></div>
<div><h4>FalkorDB with Custom Database Name</h4><a href="#falkordb-with-custom-database-name"></a></div>
<div><pre><span>from</span> <span>graphiti_core</span> <span>import</span> <span>Graphiti</span>
<span>from</span> <span>graphiti_core</span>.<span>driver</span>.<span>falkordb_driver</span> <span>import</span> <span>FalkorDriver</span> <span># Create a FalkorDB driver with custom database name</span>
<span>driver</span> <span>=</span> <span>FalkorDriver</span>( <span>host</span><span>=</span><span>"localhost"</span>, <span>port</span><span>=</span><span>6379</span>, <span>username</span><span>=</span><span>"falkor_user"</span>, <span># Optional</span> <span>password</span><span>=</span><span>"falkor_password"</span>, <span># Optional</span> <span>database</span><span>=</span><span>"my_custom_graph"</span> <span># Custom database name</span>
) <span># Pass the driver to Graphiti</span>
<span>graphiti</span> <span>=</span> <span>Graphiti</span>(<span>graph_driver</span><span>=</span><span>driver</span>)</pre></div>
<div><h4>Kuzu</h4><a href="#kuzu"></a></div>
<div><pre><span>from</span> <span>graphiti_core</span> <span>import</span> <span>Graphiti</span>
<span>from</span> <span>graphiti_core</span>.<span>driver</span>.<span>kuzu_driver</span> <span>import</span> <span>KuzuDriver</span> <span># Create a Kuzu driver</span>
<span>driver</span> <span>=</span> <span>KuzuDriver</span>(<span>db</span><span>=</span><span>"/tmp/graphiti.kuzu"</span>) <span># Pass the driver to Graphiti</span>
<span>graphiti</span> <span>=</span> <span>Graphiti</span>(<span>graph_driver</span><span>=</span><span>driver</span>)</pre></div>
<div><h4>Amazon Neptune</h4><a href="#amazon-neptune"></a></div>
<div><pre><span>from</span> <span>graphiti_core</span> <span>import</span> <span>Graphiti</span>
<span>from</span> <span>graphiti_core</span>.<span>driver</span>.<span>neptune_driver</span> <span>import</span> <span>NeptuneDriver</span> <span># Create a FalkorDB driver with custom database name</span>
<span>driver</span> <span>=</span> <span>NeptuneDriver</span>( <span>host</span><span>=</span> <span>&lt;</span> <span>NEPTUNE</span>
<span>ENDPOINT</span> <span>&gt;</span>,
<span>aoss_host</span> <span>=</span> <span>&lt;</span> <span>Amazon</span>
<span>OpenSearch</span>
<span>Serverless</span>
<span>Host</span> <span>&gt;</span>,
<span>port</span> <span>=</span> <span>&lt;</span> <span>PORT</span> <span>&gt;</span> <span># Optional, defaults to 8182,</span> <span>aoss_port</span> <span>=</span> <span>&lt;</span> <span>PORT</span> <span>&gt;</span> <span># Optional, defaults to 443</span>
) <span>driver</span> <span>=</span> <span>NeptuneDriver</span>(<span>host</span><span>=</span><span>neptune_uri</span>, <span>aoss_host</span><span>=</span><span>aoss_host</span>, <span>port</span><span>=</span><span>neptune_port</span>) <span># Pass the driver to Graphiti</span>
<span>graphiti</span> <span>=</span> <span>Graphiti</span>(<span>graph_driver</span><span>=</span><span>driver</span>)</pre></div>
<div><h2>Graph Driver Architecture</h2><a href="#graph-driver-architecture"></a></div>
<p>Graphiti uses a pluggable driver architecture so the core framework is backend-agnostic. All database-specific logic
is encapsulated in driver implementations, allowing you to swap backends or add new ones without modifying the rest of
the framework.</p>
<div><h3>How Drivers are Integrated</h3><a href="#how-drivers-are-integrated"></a></div>
<p>The driver layer is organized into three tiers:</p>
<ol>
<li>
<p><strong></strong></p><pre><strong><code>GraphDriver</code></strong></pre><strong> ABC</strong> (<p></p><pre><code>graphiti_core/driver/driver.py</code></pre>) — the core interface every backend must implement. It
defines query execution, session management, index lifecycle, and exposes 11 operations interfaces as <pre><code>@property</code></pre>
accessors.<p></p>
</li>
<li>
<p><strong></strong></p><pre><strong><code>GraphProvider</code></strong></pre><strong> enum</strong> — identifies the backend (<p></p><pre><code>NEO4J</code></pre>, <pre><code>FALKORDB</code></pre>, <pre><code>KUZU</code></pre>, <pre><code>NEPTUNE</code></pre>). Query builders use this
enum in <pre><code>match/case</code></pre> statements to return dialect-specific query strings.<p></p>
</li>
<li>
<p><strong>11 Operations ABCs</strong> (</p><pre><code>graphiti_core/driver/operations/</code></pre>) — abstract interfaces covering all CRUD and search
operations for every graph element type:<p></p>
<ul>
<li><strong>Node ops:</strong> <pre><code>EntityNodeOperations</code></pre>, <pre><code>EpisodeNodeOperations</code></pre>, <pre><code>CommunityNodeOperations</code></pre>, <pre><code>SagaNodeOperations</code></pre></li>
<li><strong>Edge ops:</strong> <pre><code>EntityEdgeOperations</code></pre>, <pre><code>EpisodicEdgeOperations</code></pre>, <pre><code>CommunityEdgeOperations</code></pre>,
<pre><code>HasEpisodeEdgeOperations</code></pre>, <pre><code>NextEpisodeEdgeOperations</code></pre></li>
<li><strong>Search &amp; maintenance:</strong> <pre><code>SearchOperations</code></pre>, <pre><code>GraphMaintenanceOperations</code></pre></li>
</ul>
</li>
</ol>
<p>Each backend provides a concrete driver class and a matching </p><pre><code>operations/</code></pre> directory with implementations of all 11
ABCs. The key directories and files are shown below (simplified; see source for complete structure):<p></p>
<div><pre><code>graphiti_core/driver/
├── driver.py # GraphDriver ABC, GraphProvider enum
├── query_executor.py # QueryExecutor protocol
├── record_parsers.py # Shared record → model conversion
├── operations/ # 11 operation ABCs
│ ├── entity_node_ops.py
│ ├── episode_node_ops.py
│ ├── community_node_ops.py
│ ├── saga_node_ops.py
│ ├── entity_edge_ops.py
│ ├── episodic_edge_ops.py
│ ├── community_edge_ops.py
│ ├── has_episode_edge_ops.py
│ ├── next_episode_edge_ops.py
│ ├── search_ops.py
│ ├── graph_ops.py
│ └── graph_utils.py # Shared algorithms (e.g., label propagation)
├── graph_operations/ # Legacy graph operations interface
├── search_interface/ # Legacy search interface
├── neo4j_driver.py # Neo4jDriver
├── neo4j/operations/ # 11 Neo4j implementations
├── falkordb_driver.py # FalkorDriver
├── falkordb/operations/ # 11 FalkorDB implementations
├── kuzu_driver.py # KuzuDriver
├── kuzu/operations/ # 11 Kuzu implementations + record_parsers.py
├── neptune_driver.py # NeptuneDriver
└── neptune/operations/ # 11 Neptune implementations
</code></pre></div>
<p>Operations are decoupled from the driver itself — each operation method receives an </p><pre><code>executor: QueryExecutor</code></pre> parameter
(a protocol for running queries) rather than a concrete <pre><code>GraphDriver</code></pre>, which makes operations testable and
driver-agnostic. The driver class instantiates all 11 operation classes in its <pre><code>__init__</code></pre> and exposes them as
properties. The base <pre><code>GraphDriver</code></pre> ABC defines each property with an optional return type (<pre><code>| None</code></pre>, defaulting to
<pre><code>None</code></pre>); concrete drivers override these to return their implementations:<p></p>
<div><pre><span># In your concrete driver (e.g., Neo4jDriver):</span>
<span>@<span>property</span></span>
<span>def</span> <span>entity_node_ops</span>(<span>self</span>) <span>-&gt;</span> <span>EntityNodeOperations</span>: <span>return</span> <span>self</span>.<span>_entity_node_ops</span></pre></div>
<p>Provider-specific query strings are generated by shared query builders in </p><pre><code>graphiti_core/models/nodes/node_db_queries.py</code></pre>
and <pre><code>graphiti_core/models/edges/edge_db_queries.py</code></pre>, which use <pre><code>match/case</code></pre> on the <pre><code>GraphProvider</code></pre> enum to return the
correct dialect for each backend.<p></p>
<div><h3>Adding a New Graph Driver</h3><a href="#adding-a-new-graph-driver"></a></div>
<p>To integrate a new graph database backend, follow these steps:</p>
<ol>
<li>
<p><strong>Add to </strong></p><pre><strong><code>GraphProvider</code></strong></pre> — add your enum value in <p></p><pre><code>graphiti_core/driver/driver.py</code></pre>:<p></p>
<div><pre><span>class</span> <span>GraphProvider</span>(<span>Enum</span>): <span>NEO4J</span> <span>=</span> <span>'neo4j'</span> <span>FALKORDB</span> <span>=</span> <span>'falkordb'</span> <span>KUZU</span> <span>=</span> <span>'kuzu'</span> <span>NEPTUNE</span> <span>=</span> <span>'neptune'</span> <span>MY_BACKEND</span> <span>=</span> <span>'my_backend'</span> <span># New backend</span></pre></div>
</li>
<li>
<p><strong>Create directory structure</strong> — create </p><pre><code>graphiti_core/driver/&lt;backend&gt;/operations/</code></pre> with an <pre><code>__init__.py</code></pre> exporting
all 11 operation classes.<p></p>
</li>
<li>
<p><strong>Implement </strong></p><pre><strong><code>GraphDriver</code></strong></pre><strong> subclass</strong> — create <p></p><pre><code>graphiti_core/driver/&lt;backend&gt;_driver.py</code></pre>:<p></p>
<ul>
<li>Set <pre><code>provider = GraphProvider.&lt;BACKEND&gt;</code></pre></li>
<li>Implement the abstract methods: <pre><code>execute_query()</code></pre>, <pre><code>session()</code></pre>, <pre><code>close()</code></pre>,
<pre><code>build_indices_and_constraints()</code></pre>, <pre><code>delete_all_indexes()</code></pre></li>
<li>Instantiate all 11 operation classes in <pre><code>__init__</code></pre> and return them via <pre><code>@property</code></pre> overrides</li>
</ul>
</li>
<li>
<p><strong>Implement all 11 operation ABCs</strong> — one file per ABC in </p><pre><code>&lt;backend&gt;/operations/</code></pre>, each inheriting from the
corresponding ABC in <pre><code>graphiti_core/driver/operations/</code></pre>.<p></p>
</li>
<li>
<p><strong>Add query variants</strong> — add </p><pre><code>case GraphProvider.&lt;BACKEND&gt;:</code></pre> branches to
<pre><code>graphiti_core/models/nodes/node_db_queries.py</code></pre> and <pre><code>graphiti_core/models/edges/edge_db_queries.py</code></pre> for your
database's query dialect.<p></p>
</li>
<li>
<p><strong>Implement </strong></p><pre><strong><code>GraphDriverSession</code></strong></pre> — if your backend needs session or connection management, subclass
<p></p><pre><code>GraphDriverSession</code></pre> from <pre><code>driver.py</code></pre> and implement <pre><code>run()</code></pre>, <pre><code>close()</code></pre>, and <pre><code>execute_write()</code></pre>.<p></p>
</li>
<li>
<p><strong>Register as optional dependency</strong> — add an extras group in </p><pre><code>pyproject.toml</code></pre>:<p></p>
<div><pre>[<span>project</span>.<span>optional-dependencies</span>]
<span>my_backend</span> = [<span><span>"</span>my-backend-client&gt;=1.0.0<span>"</span></span>]</pre></div>
</li>
</ol>
<p>For reference implementations, look at:</p> <div><h2>Using Graphiti with Azure OpenAI</h2><a href="#using-graphiti-with-azure-openai"></a></div>
<p>Graphiti supports Azure OpenAI for both LLM inference and embeddings using Azure's OpenAI v1 API compatibility layer.</p>
<div><h3>Quick Start</h3><a href="#quick-start-1"></a></div>
<div><pre><span>from</span> <span>openai</span> <span>import</span> <span>AsyncOpenAI</span>
<span>from</span> <span>graphiti_core</span> <span>import</span> <span>Graphiti</span>
<span>from</span> <span>graphiti_core</span>.<span>llm_client</span>.<span>azure_openai_client</span> <span>import</span> <span>AzureOpenAILLMClient</span>
<span>from</span> <span>graphiti_core</span>.<span>llm_client</span>.<span>config</span> <span>import</span> <span>LLMConfig</span>
<span>from</span> <span>graphiti_core</span>.<span>embedder</span>.<span>azure_openai</span> <span>import</span> <span>AzureOpenAIEmbedderClient</span> <span># Initialize Azure OpenAI client using the standard OpenAI client</span>
<span># with Azure's v1 API endpoint</span>
<span>azure_client</span> <span>=</span> <span>AsyncOpenAI</span>( <span>base_url</span><span>=</span><span>"https://your-resource-name.openai.azure.com/openai/v1/"</span>, <span>api_key</span><span>=</span><span>"your-api-key"</span>,
) <span># Create LLM and Embedder clients</span>
<span>llm_client</span> <span>=</span> <span>AzureOpenAILLMClient</span>( <span>azure_client</span><span>=</span><span>azure_client</span>, <span>config</span><span>=</span><span>LLMConfig</span>(<span>model</span><span>=</span><span>"gpt-5-mini"</span>, <span>small_model</span><span>=</span><span>"gpt-5-mini"</span>) <span># Your Azure deployment name</span>
)
<span>embedder_client</span> <span>=</span> <span>AzureOpenAIEmbedderClient</span>( <span>azure_client</span><span>=</span><span>azure_client</span>, <span>model</span><span>=</span><span>"text-embedding-3-small"</span> <span># Your Azure embedding deployment name</span>
) <span># Initialize Graphiti with Azure OpenAI clients</span>
<span>graphiti</span> <span>=</span> <span>Graphiti</span>( <span>"bolt://localhost:7687"</span>, <span>"neo4j"</span>, <span>"password"</span>, <span>llm_client</span><span>=</span><span>llm_client</span>, <span>embedder</span><span>=</span><span>embedder_client</span>,
) <span># Now you can use Graphiti with Azure OpenAI</span></pre></div>
<p><strong>Key Points:</strong></p>
<ul>
<li>Use the standard <pre><code>AsyncOpenAI</code></pre> client with Azure's v1 API endpoint format: <pre><code>https://your-resource-name.openai.azure.com/openai/v1/</code></pre></li>
<li>The deployment names (e.g., <pre><code>gpt-5-mini</code></pre>, <pre><code>text-embedding-3-small</code></pre>) should match your Azure OpenAI deployment names</li>
<li>See <pre><code>examples/azure-openai/</code></pre> for a complete working example</li>
</ul>
<p>Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names.</p>
<div><h2>Using Graphiti with Google Gemini</h2><a href="#using-graphiti-with-google-gemini"></a></div>
<p>Graphiti supports Google's Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini,
you'll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.</p>
<p>Install Graphiti:</p>
<div><pre>uv add <span><span>"</span>graphiti-core[google-genai]<span>"</span></span> <span><span>#</span> or</span> pip install <span><span>"</span>graphiti-core[google-genai]<span>"</span></span></pre></div>
<div><pre><span>from</span> <span>graphiti_core</span> <span>import</span> <span>Graphiti</span>
<span>from</span> <span>graphiti_core</span>.<span>llm_client</span>.<span>gemini_client</span> <span>import</span> <span>GeminiClient</span>, <span>LLMConfig</span>
<span>from</span> <span>graphiti_core</span>.<span>embedder</span>.<span>gemini</span> <span>import</span> <span>GeminiEmbedder</span>, <span>GeminiEmbedderConfig</span>
<span>from</span> <span>graphiti_core</span>.<span>cross_encoder</span>.<span>gemini_reranker_client</span> <span>import</span> <span>GeminiRerankerClient</span> <span># Google API key configuration</span>
<span>api_key</span> <span>=</span> <span>"&lt;your-google-api-key&gt;"</span> <span># Initialize Graphiti with Gemini clients</span>
<span>graphiti</span> <span>=</span> <span>Graphiti</span>( <span>"bolt://localhost:7687"</span>, <span>"neo4j"</span>, <span>"password"</span>, <span>llm_client</span><span>=</span><span>GeminiClient</span>( <span>config</span><span>=</span><span>LLMConfig</span>( <span>api_key</span><span>=</span><span>api_key</span>, <span>model</span><span>=</span><span>"gemini-2.0-flash"</span> ) ), <span>embedder</span><span>=</span><span>GeminiEmbedder</span>( <span>config</span><span>=</span><span>GeminiEmbedderConfig</span>( <span>api_key</span><span>=</span><span>api_key</span>, <span>embedding_model</span><span>=</span><span>"embedding-001"</span> ) ), <span>cross_encoder</span><span>=</span><span>GeminiRerankerClient</span>( <span>config</span><span>=</span><span>LLMConfig</span>( <span>api_key</span><span>=</span><span>api_key</span>, <span>model</span><span>=</span><span>"gemini-2.5-flash-lite"</span> ) )
) <span># Now you can use Graphiti with Google Gemini for all components</span></pre></div>
<p>The Gemini reranker uses the </p><pre><code>gemini-2.5-flash-lite</code></pre> model by default, which is optimized for
cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI
reranker, leveraging Gemini's log probabilities feature to rank passage relevance.<p></p>
<div><h2>Using Graphiti with Ollama (Local LLM)</h2><a href="#using-graphiti-with-ollama-local-llm"></a></div>
<p>Graphiti supports Ollama for running local LLMs and embedding models via Ollama's OpenAI-compatible API. This is ideal
for privacy-focused applications or when you want to avoid API costs.</p>
<p><strong>Note:</strong> Use </p><pre><code>OpenAIGenericClient</code></pre> (not <pre><code>OpenAIClient</code></pre>) for Ollama and other OpenAI-compatible providers like LM Studio. The <pre><code>OpenAIGenericClient</code></pre> is optimized for local models with a higher default max token limit (16K vs 8K) and full support for structured outputs.<p></p>
<p>Install the models:</p>
<div><pre>ollama pull deepseek-r1:7b <span><span>#</span> LLM</span>
ollama pull nomic-embed-text <span><span>#</span> embeddings</span></pre></div>
<div><pre><span>from</span> <span>graphiti_core</span> <span>import</span> <span>Graphiti</span>
<span>from</span> <span>graphiti_core</span>.<span>llm_client</span>.<span>config</span> <span>import</span> <span>LLMConfig</span>
<span>from</span> <span>graphiti_core</span>.<span>llm_client</span>.<span>openai_generic_client</span> <span>import</span> <span>OpenAIGenericClient</span>
<span>from</span> <span>graphiti_core</span>.<span>embedder</span>.<span>openai</span> <span>import</span> <span>OpenAIEmbedder</span>, <span>OpenAIEmbedderConfig</span>
<span>from</span> <span>graphiti_core</span>.<span>cross_encoder</span>.<span>openai_reranker_client</span> <span>import</span> <span>OpenAIRerankerClient</span> <span># Configure Ollama LLM client</span>
<span>llm_config</span> <span>=</span> <span>LLMConfig</span>( <span>api_key</span><span>=</span><span>"ollama"</span>, <span># Ollama doesn't require a real API key, but some placeholder is needed</span> <span>model</span><span>=</span><span>"deepseek-r1:7b"</span>, <span>small_model</span><span>=</span><span>"deepseek-r1:7b"</span>, <span>base_url</span><span>=</span><span>"http://localhost:11434/v1"</span>, <span># Ollama's OpenAI-compatible endpoint</span>
) <span>llm_client</span> <span>=</span> <span>OpenAIGenericClient</span>(<span>config</span><span>=</span><span>llm_config</span>) <span># Initialize Graphiti with Ollama clients</span>
<span>graphiti</span> <span>=</span> <span>Graphiti</span>( <span>"bolt://localhost:7687"</span>, <span>"neo4j"</span>, <span>"password"</span>, <span>llm_client</span><span>=</span><span>llm_client</span>, <span>embedder</span><span>=</span><span>OpenAIEmbedder</span>( <span>config</span><span>=</span><span>OpenAIEmbedderConfig</span>( <span>api_key</span><span>=</span><span>"ollama"</span>, <span># Placeholder API key</span> <span>embedding_model</span><span>=</span><span>"nomic-embed-text"</span>, <span>embedding_dim</span><span>=</span><span>768</span>, <span>base_url</span><span>=</span><span>"http://localhost:11434/v1"</span>, ) ), <span>cross_encoder</span><span>=</span><span>OpenAIRerankerClient</span>(<span>client</span><span>=</span><span>llm_client</span>, <span>config</span><span>=</span><span>llm_config</span>),
) <span># Now you can use Graphiti with local Ollama models</span></pre></div>
<p>Ensure Ollama is running (</p><pre><code>ollama serve</code></pre>) and that you have pulled the models you want to use.<p></p>
<div><h2>Documentation</h2><a href="#documentation"></a></div>
<ul>
<li><a href="https://help.getzep.com/graphiti">Guides and API documentation</a>.</li>
<li><a href="https://help.getzep.com/graphiti/graphiti/quick-start">Quick Start</a></li>
<li><a href="https://help.getzep.com/graphiti/integrations/lang-graph-agent">Building an agent with LangChain's LangGraph and Graphiti</a></li>
</ul>
<div><h2>Telemetry</h2><a href="#telemetry"></a></div>
<p>Graphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for
everyone. We believe transparency is important, so here's exactly what we collect and why.</p>
<div><h3>What We Collect</h3><a href="#what-we-collect"></a></div>
<p>When you initialize a Graphiti instance, we collect:</p>
<ul>
<li><strong>Anonymous identifier</strong>: A randomly generated UUID stored locally in <pre><code>~/.cache/graphiti/telemetry_anon_id</code></pre></li>
<li><strong>System information</strong>: Operating system, Python version, and system architecture</li>
<li><strong>Graphiti version</strong>: The version you're using</li>
<li><strong>Configuration choices</strong>:
<ul>
<li>LLM provider type (OpenAI, Azure, Anthropic, etc.)</li>
<li>Database backend (Neo4j, FalkorDB, Kuzu, Amazon Neptune Database or Neptune Analytics)</li>
<li>Embedder provider (OpenAI, Azure, Voyage, etc.)</li>
</ul>
</li>
</ul>
<div><h3>What We Don't Collect</h3><a href="#what-we-dont-collect"></a></div>
<p>We are committed to protecting your privacy. We <strong>never</strong> collect:</p>
<ul>
<li>Personal information or identifiers</li>
<li>API keys or credentials</li>
<li>Your actual data, queries, or graph content</li>
<li>IP addresses or hostnames</li>
<li>File paths or system-specific information</li>
<li>Any content from your episodes, nodes, or edges</li>
</ul>
<div><h3>Why We Collect This Data</h3><a href="#why-we-collect-this-data"></a></div>
<p>This information helps us:</p>
<ul>
<li>Understand which configurations are most popular to prioritize support and testing</li>
<li>Identify which LLM and database providers to focus development efforts on</li>
<li>Track adoption patterns to guide our roadmap</li>
<li>Ensure compatibility across different Python versions and operating systems</li>
</ul>
<p>By sharing this anonymous information, you help us make Graphiti better for everyone in the community.</p>
<div><h3>View the Telemetry Code</h3><a href="#view-the-telemetry-code"></a></div>
<p>The Telemetry code <a href="/getzep/graphiti/blob/main/graphiti_core/telemetry/telemetry.py">may be found here</a>.</p>
<div><h3>How to Disable Telemetry</h3><a href="#how-to-disable-telemetry"></a></div>
<p>Telemetry is <strong>opt-out</strong> and can be disabled at any time. To disable telemetry collection:</p>
<p><strong>Option 1: Environment Variable</strong></p>
<div><pre><span>export</span> GRAPHITI_TELEMETRY_ENABLED=false</pre></div>
<p><strong>Option 2: Set in your shell profile</strong></p>
<div><pre><span><span>#</span> For bash users (~/.bashrc or ~/.bash_profile)</span>
<span>echo</span> <span><span>'</span>export GRAPHITI_TELEMETRY_ENABLED=false<span>'</span></span> <span>&gt;&gt;</span> <span>~</span>/.bashrc <span><span>#</span> For zsh users (~/.zshrc)</span>
<span>echo</span> <span><span>'</span>export GRAPHITI_TELEMETRY_ENABLED=false<span>'</span></span> <span>&gt;&gt;</span> <span>~</span>/.zshrc</pre></div>
<p><strong>Option 3: Set for a specific Python session</strong></p>
<div><pre><span>import</span> <span>os</span> <span>os</span>.<span>environ</span>[<span>'GRAPHITI_TELEMETRY_ENABLED'</span>] <span>=</span> <span>'false'</span> <span># Then initialize Graphiti as usual</span>
<span>from</span> <span>graphiti_core</span> <span>import</span> <span>Graphiti</span> <span>graphiti</span> <span>=</span> <span>Graphiti</span>(...)</pre></div>
<p>Telemetry is automatically disabled during test runs (when </p><pre><code>pytest</code></pre> is detected).<p></p>
<div><h3>Technical Details</h3><a href="#technical-details"></a></div>
<ul>
<li>Telemetry uses PostHog for anonymous analytics collection</li>
<li>All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti
functionality</li>
<li>The anonymous ID is stored locally and is not tied to any personal information</li>
</ul>
<div><h2>Status and Roadmap</h2><a href="#status-and-roadmap"></a></div>
<p>Graphiti is under active development. We aim to maintain API stability while working on:</p>
<ul>
<li> Supporting custom graph schemas:
<ul>
<li>Allow developers to provide their own defined node and edge classes when ingesting episodes</li>
<li>Enable more flexible knowledge representation tailored to specific use cases</li>
</ul>
</li>
<li> Enhancing retrieval capabilities with more robust and configurable options</li>
<li> Graphiti MCP Server</li>
<li> Expanding test coverage to ensure reliability and catch edge cases</li>
</ul>
<div><h2>Contributing</h2><a href="#contributing"></a></div>
<pre><code>We encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or
answering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer
to CONTRIBUTING.</code></pre>
<div><h2>Support</h2><a href="#support"></a></div>
<p>Join the <a href="https://discord.com/invite/W8Kw6bsgXQ">Zep Discord server</a> and make your way to the <strong>#Graphiti</strong> channel!</p>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>