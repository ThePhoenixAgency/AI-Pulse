<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="UTF-8">
<title>Inception lance Mercury 2, le LLM � raisonnement le plus rapide, qui serait 5 fois plus rapide que les principaux LLM optimis�s pour la vitesse, avec un co�t d'inf�rence r�duit</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>Inception lance Mercury 2, le LLM � raisonnement le plus rapide, qui serait 5 fois plus rapide que les principaux LLM optimis�s pour la vitesse, avec un co�t d'inf�rence r�duit</h1>
  <div class="metadata">
    Source: Developpez.com | Date: 2/26/2026 2:02:00 AM | <a href="https://intelligence-artificielle.developpez.com/actu/380593/Inception-lance-Mercury-2-le-LLM-a-raisonnement-le-plus-rapide-qui-serait-5-fois-plus-rapide-que-les-principaux-LLM-optimises-pour-la-vitesse-avec-un-cout-d-inference-reduit/" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: FR
  </div>
  <div class="content">
    <div><div> <p><img src="https://www.developpez.com/images/logos/intelligence-artificielle2.png"> <b>Alors que l'industrie de l'IA d�pense des milliards pour gagner quelques fractions de seconde sur les mod�les autor�gressifs jeton par jeton, la g�n�ration bas�e sur la diffusion d'Inception est une avanc�e architecturale qui rend le raisonnement � haut d�bit natif au mod�le. Fond�e par des chercheurs de Stanford, UCLA et Cornell � l'origine des travaux fondamentaux sur la diffusion, Inception a commercialis� la diffusion pour le texte et Mercury 2 �tend cette avanc�e � un raisonnement de niveau production con�u pour l'inf�rence dans le monde r�el. Mercury 2 est con�u pour les workflows de production � forte valeur ajout�e o� les performances d'inf�rence d�terminent l'adoption : boucles d'agents, voix et recherche en temps r�el, codage et �dition instantan�s � grande �chelle.</b></p><p>Tous les principaux LLM actuellement en production, y compris GPT, Claude et Gemini, reposent sur le m�me m�canisme de base : la g�n�ration autor�gressive. Ils produisent du texte de mani�re s�quentielle. Un. Jeton. �. La. Fois. Cette approche a un plafond bas, car la vitesse est finalement limit�e par la nature s�quentielle de la g�n�ration, et les contraintes s'aggravent � mesure que la profondeur du raisonnement augmente, ce qui augmente les co�ts de service et r�duit la r�activit�.</p><p>Limit�e par ce plafond, l'industrie a largement emprunt� trois voies pour am�liorer la vitesse : des puces sp�cialis�es, des piles de service optimis�es et la compression des mod�les, troquant la capacit� contre la vitesse. Les principaux laboratoires et fournisseurs d'infrastructures ont investi des milliards dans ces efforts afin de tirer le maximum de gains de performance de la m�me boucle de g�n�ration jeton par jeton.</p><p>Inception, la soci�t� � l'origine des premiers grands mod�les de langage commerciaux bas�s sur la diffusion (dLLM), a emprunt� une voie fondamentalement diff�rente, fond�e sur la diffusion, la m�me approche technique que celle utilis�e dans les syst�mes modernes de g�n�ration d'images et de vid�os, d�sormais appliqu�e au langage. Inception a annonc� le lancement de Mercury 2, le LLM de raisonnement le plus rapide et le premier dLLM de raisonnement. </p><p>Mercury 2 fait progresser cette base de diffusion vers un raisonnement de niveau production et �tablit une nouvelle norme de performance pour les LLM optimis�s en termes de vitesse, offrant un raisonnement rentable � un d�bit de 1 000 tokens par seconde avec des performances �quivalentes � celles de Claude 4.5 Haiku et GPT 5.2 Mini. Il en r�sulte un d�bit et une r�activit� qui proviennent du mod�le lui-m�me, permettant une inf�rence rapide et �volutive.</p><div>
<p></p>
</div>
<p><b><span>Comment fonctionnent les dLLM</span></b></p><p>Au lieu de pr�dire le prochain jeton d'une s�quence, Mercury 2 commence par une esquisse approximative de la sortie compl�te et l'affine de mani�re it�rative gr�ce � un processus appel� � d�noisage �, sur plusieurs jetons en parall�le. Chaque passage dans le mod�le modifie et am�liore simultan�ment plusieurs jetons, de sorte qu'une seule �valuation du r�seau neuronal produit un travail beaucoup plus utile � chaque �tape. L'avantage en termes de vitesse provient du mod�le lui-m�me, et non d'un mat�riel sp�cialis�. Et comme le mod�le affine de mani�re it�rative plut�t que de s'engager de mani�re permanente sur chaque jeton, il peut corriger les erreurs en cours de g�n�ration.</p><p>� <i>Les mod�les de raisonnement ne sont utiles que dans la mesure o� ils peuvent �tre utilis�s en production</i> �, a d�clar� Stefano Ermon, PDG et cofondateur d'Inception. � <i>Au cours des derni�res ann�es, nous avons constat� des progr�s incroyables dans les capacit�s des mod�les, mais beaucoup moins dans leur utilisation dans des cas d'utilisation � faible latence. Avec Mercury 2, nous avons construit un syst�me o� le raisonnement de haute qualit� fonctionne suffisamment rapidement et efficacement pour des applications en temps r�el. Lorsque la vitesse, le co�t et la qualit� fonctionnent ensemble, vous ouvrez de toutes nouvelles possibilit�s, et c'est ce qui nous enthousiasme le plus.</i> �</p><p><img src="https://www.developpez.net/forums/attachments/p674541d1/a/a/a"></p><p>
Dans les benchmarks standard, conform�ment � la m�thodologie d'Artificial Analysis, Mercury 2 atteint un d�bit de sortie d'environ 1 000 tokens par seconde, contre environ 89 tokens par seconde pour Claude 4.5 Haiku Reasoning et environ 71 tokens par seconde pour GPT-5 Mini. En termes de qualit�, Mercury 2 a obtenu un score de 91,1 sur AIME 2025, 73,6 sur GPQA, 71,3 sur IFBench, 67,3 sur LiveCodeBench, 38,4 sur SciCode et 52,9 sur Tau. Ces scores placent Mercury 2 dans la fourchette concurrentielle de Claude 4.5 Haiku et GPT 5.2 Mini en termes de qualit�, tout en offrant un d�bit environ 10 fois sup�rieur.</p><p>� <i>La plupart des �quipes consid�rent l'inf�rence comme un exercice d'optimisation autour de la pile autor�gressive, mais Inception est parti d'un principe plus fondamental : la diffusion pour le langage</i> �, a d�clar� Tim Tully, associ� chez Menlo Ventures. � <i>Mercury 2 montre ce qui se passe lorsque cette base est associ�e � une approche s�rieuse du raisonnement et du d�ploiement, et pas seulement � des d�monstrations. Nous pensons que la feuille de route bas�e sur la diffusion d'Inception a le potentiel de red�finir les attentes en mati�re de rapidit� et d'�volutivit� des mod�les de raisonnement.</i> �</p><p><img src="https://www.developpez.net/forums/attachments/p674542d1/a/a/a"></p><p>
S'appuyant sur le principe de diffusion prioritaire d'Inception, Mercury 2 offre les cas d'utilisation suivants :</p><p>- <b>Boucles d'agents rapides et � haut volume</b> : Mercury 2 transforme les agents de � d�monstration sympa � en � syst�me de production fiable � en r�duisant la p�nalit� de latence qui s'accumule dans les workflows en plusieurs �tapes. Cela signifie que les agents de code, le triage IT et SecOps, et les boucles d'automatisation back-office en plusieurs �tapes peuvent ex�cuter davantage d'�tapes avec des cycles de r�troaction plus courts, am�liorant ainsi directement la contr�labilit� et la confiance.</p><p>- <b>Recherche et voix</b> : Mercury 2 permet d'int�grer le raisonnement dans des SLA en temps r�el stricts, o� les latences p95 et p99 d�terminent si l'exp�rience semble naturelle. Cela permet de renforcer les applications telles que les agents vocaux d'assistance et de vente, les copilotes d'assistance � la client�le, les questions-r�ponses interactives de tutorat et la traduction en temps r�el.</p><p>- <b>Codage et �dition instantan�s</b> : Mercury 2 alimente la boucle de codage it�rative, permettant aux utilisateurs de demander, de r�viser et de modifier rapidement.</p><p>Mercury 2 offre �galement des capacit�s difficiles � obtenir avec une g�n�ration strictement s�quentielle. Le raffinement it�ratif prend en charge la correction des erreurs pendant la g�n�ration et des sorties plus contr�lables, notamment des r�ponses structur�es pour l'orchestration des agents, les modifications de code et l'appel de fonctions, ce qui aide les �quipes � maintenir la coh�rence et la supervision lorsqu'elles passent des prototypes � la production.</p><p><img src="https://www.developpez.net/forums/attachments/p674543d1/a/a/a"></p><p>
Inception a �t� fond�e par des chercheurs de Stanford, de l'UCLA et de Cornell qui ont contribu� aux travaux fondamentaux sur les mod�les de diffusion et d'autres techniques d'IA essentielles, notamment l'attention flash, les transformateurs de d�cision et l'optimisation directe des pr�f�rences. Le PDG Stefano Ermon est le co-inventeur des m�thodes de diffusion largement utilis�es dans les syst�mes modernes de g�n�ration d'images et de vid�os. Les mod�les Mercury 2 sont disponibles d�s aujourd'hui via l'API Inception.</p><p>Voici un extrait de l'annonce de Mercury 2 :</p><p><b><span>Ce que Mercury 2 apporte � la production</span></b></p><p>Mercury 2 excelle dans les applications sensibles � la latence o� l'exp�rience utilisateur est non n�gociable.</p><p><b>1. Codage et �dition</b></p><p>Saisie semi-automatique, suggestions de modification suivante, refactorisations, agents de code interactifs : autant de workflows o� le d�veloppeur est dans la boucle et o� toute pause interrompt le flux.</p><p>� <i>Les suggestions arrivent suffisamment vite pour donner l'impression de faire partie de votre propre r�flexion, et non pas d'�tre quelque chose que vous devez attendre.</i> � Max Brunsfeld, cofondateur, Zed</p><p><b>2. Boucles agentiques</b></p><p>Les flux de travail agentiques encha�nent des dizaines d'appels d'inf�rence par t�che. R�duire la latence par appel ne permet pas seulement de gagner du temps, cela modifie �galement le nombre d'�tapes que vous pouvez vous permettre d'ex�cuter et la qualit� du r�sultat final.</p><p>� <i>Nous exploitons d�sormais le dernier mod�le Mercury pour optimiser intelligemment l'ex�cution des campagnes � grande �chelle. En faisant �merger des informations et en am�liorant dynamiquement la diffusion en temps r�el, nous obtenons de meilleures performances, une plus grande efficacit� et un �cosyst�me publicitaire plus r�silient...</i></p><p>La fin de cet article est r�serv�e aux abonn�s. Soutenez le Club Developpez.com en <a href="https://premium.developpez.com/abonnement">prenant un abonnement</a> pour que nous puissions continuer � vous proposer des publications.</p> </div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>