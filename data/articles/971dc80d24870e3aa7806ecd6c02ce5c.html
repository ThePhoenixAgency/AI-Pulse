<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>GitHub - karpathy/nanochat: The best ChatGPT that $100 can buy.</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.8; color: #e2e8f0; max-width: 800px; margin: 40px auto; padding: 0 20px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.5em; }
  .metadata { color: #94a3b8; font-size: 0.9em; margin-bottom: 2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 1em; }
  img { max-width: 100%; height: auto; border-radius: 8px; }
  a { color: #00d9ff; }
  p { margin-bottom: 1em; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 15px; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 15px; border-radius: 6px; overflow-x: auto; }
</style>
</head>
<body>
  <h1>GitHub - karpathy/nanochat: The best ChatGPT that $100 can buy.</h1>
  <div class="metadata">
    Source: GitHub Trending Python | Date: Invalid Date | Lang: EN |
    <a href="https://github.com/karpathy/nanochat" target="_blank">Original Article</a>
  </div>
  <div class="content">
    <div><div><article><p></p><h2>nanochat</h2><a href="#nanochat"></a><p></p>
<p><a target="_blank" href="https://github.com/karpathy/nanochat/blob/master/dev/nanochat.png"><img src="https://github.com/karpathy/nanochat/raw/master/dev/nanochat.png" alt="nanochat logo" /></a>
<a target="_blank" href="https://github.com/karpathy/nanochat/blob/master/dev/scaling_laws_jan26.png"><img src="https://github.com/karpathy/nanochat/raw/master/dev/scaling_laws_jan26.png" alt="scaling laws" /></a></p>
<p>nanochat is the simplest experimental harness for training LLMs. It is designed to run on a single GPU node, the code is minimal/hackable, and it covers all major LLM stages including tokenization, pretraining, finetuning, evaluation, inference, and a chat UI. For example, you can train your own GPT-2 capability LLM (which cost ~$43,000 to train in 2019) for only $72 (~3 hours of 8XH100 GPU node) and then talk to it in a familiar ChatGPT-like web UI. On a spot instance, the total cost can be closer to ~$20. More generally, nanochat is configured out of the box to train an entire miniseries of compute-optimal models by setting one single complexity dial: <code>--depth</code>, the number of layers in the GPT transformer model (GPT-2 capability happens to be approximately depth 26). All other hyperparameters (the width of the transformer, number of heads, learning rate adjustments, training horizons, weight decays, ...) are calculated automatically in an optimal way.</p>
<p>For questions about the repo, I recommend either using <a href="https://deepwiki.com/karpathy/nanochat">DeepWiki</a> from Devin/Cognition to ask questions about the repo, or use the <a href="https://github.com/karpathy/nanochat/discussions">Discussions tab</a>, or come by the <a href="https://discord.com/channels/1020383067459821711/1427295580895314031">#nanochat</a> channel on Discord.</p>
<p></p><h2>Time-to-GPT-2 Leaderboard</h2><a href="#time-to-gpt-2-leaderboard"></a><p></p>
<p>Presently, the main focus of development is on tuning the pretraining stage, which takes the most amount of compute. Inspired by the modded-nanogpt repo and to incentivise progress and community collaboration, nanochat maintains a leaderboard for a "GPT-2 speedrun", which is the wall-clock time required to train a nanochat model to GPT-2 grade capability, as measured by the DCLM CORE score. The <a href="https://github.com/karpathy/nanochat/blob/master/runs/speedrun.sh">runs/speedrun.sh</a> script always reflects the reference way to train GPT-2 grade model and talk to it. The current leaderboard looks as follows:</p>
<table>
<thead>
<tr>
<th>#</th>
<th>time</th>
<th>val_bpb</th>
<th>CORE</th>
<th>Description</th>
<th>Date</th>
<th>Commit</th>
<th>Contributors</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>168 hours</td>
<td>-</td>
<td>0.2565</td>
<td>Original OpenAI GPT-2 checkpoint</td>
<td>2019</td>
<td>-</td>
<td>OpenAI</td>
</tr>
<tr>
<td>1</td>
<td>3.04</td>
<td>0.74833</td>
<td>0.2585</td>
<td>d24 baseline, slightly overtrained</td>
<td>Jan 29 2026</td>
<td>348fbb3</td>
<td>@karpathy</td>
</tr>
<tr>
<td>2</td>
<td>2.91</td>
<td>0.74504</td>
<td>0.2578</td>
<td>d26 slightly undertrained <strong>+fp8</strong></td>
<td>Feb 2 2026</td>
<td>a67eba3</td>
<td>@karpathy</td>
</tr>
<tr>
<td>3</td>
<td>2.76</td>
<td>0.74645</td>
<td>0.2602</td>
<td>bump total batch size to 1M tokens</td>
<td>Feb 5 2026</td>
<td>2c062aa</td>
<td>@karpathy</td>
</tr>
</tbody>
</table>
<p>The primary metric we care about is "time to GPT-2" - the wall clock time needed to outperform the GPT-2 (1.6B) CORE metric on an 8XH100 GPU node. The GPT-2 CORE score is 0.256525. In 2019, the training of GPT-2 cost approximately $43,000 so it is incredible that due to many advances over 7 years across the stack, we can now do so much faster and for well below $100 (e.g. at the current ~$3/GPU/hr, an 8XH100 node is ~$24/hr, so 3 hours is ~$72).</p>
<p>See <a href="https://github.com/karpathy/nanochat/blob/master/dev/LEADERBOARD.md">dev/LEADERBOARD.md</a> for more docs on how to interpret and contribute to the leaderboard.</p>
<p></p><h2>Getting started</h2><a href="#getting-started"></a><p></p>
<p></p><h3>Reproduce and talk to GPT-2</h3><a href="#reproduce-and-talk-to-gpt-2"></a><p></p>
<p>The most fun you can have is to train your own GPT-2 and talk to it. The entire pipeline to do so is contained in the single file <a href="https://github.com/karpathy/nanochat/blob/master/runs/speedrun.sh">runs/speedrun.sh</a>, which is designed to be run on an 8XH100 GPU node. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like <a href="https://lambda.ai/service/gpu-cloud">Lambda</a>), and kick off the training script:</p>
<div><pre>bash runs/speedrun.sh</pre></div>
<p>You may wish to do so in a screen session as this will take ~3 hours to run. Once it's done, you can talk to it via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run <code>source .venv/bin/activate</code>), and serve it:</p>
<div><pre>python -m scripts.chat_web</pre></div>
<p>And then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example <a href="http://209.20.xxx.xxx:8000/">http://209.20.xxx.xxx:8000/</a>, etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).</p>
<hr />
<a target="_blank" href="https://private-user-images.githubusercontent.com/241138/500544377-ed39ddf8-2370-437a-bedc-0f39781e76b5.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NzEwNjg5OTAsIm5iZiI6MTc3MTA2ODY5MCwicGF0aCI6Ii8yNDExMzgvNTAwNTQ0Mzc3LWVkMzlkZGY4LTIzNzAtNDM3YS1iZWRjLTBmMzk3ODFlNzZiNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMjE0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDIxNFQxMTMxMzBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1iZmZhMmQzYWVkZWE5YjE2NzFmMGVhMmFjMjlmNzdmZTljZDliNGY1NmRjOGJjYmZmZDY4YTFmY2ZhMTNhOGMwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.emCmaNXs9vsiop19GBuBzoXszUxUGcVf27fs1BrjsLE"><img alt="image" src="https://private-user-images.githubusercontent.com/241138/500544377-ed39ddf8-2370-437a-bedc-0f39781e76b5.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NzEwNjg5OTAsIm5iZiI6MTc3MTA2ODY5MCwicGF0aCI6Ii8yNDExMzgvNTAwNTQ0Mzc3LWVkMzlkZGY4LTIzNzAtNDM3YS1iZWRjLTBmMzk3ODFlNzZiNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjYwMjE0JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI2MDIxNFQxMTMxMzBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1iZmZhMmQzYWVkZWE5YjE2NzFmMGVhMmFjMjlmNzdmZTljZDliNGY1NmRjOGJjYmZmZDY4YTFmY2ZhMTNhOGMwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.emCmaNXs9vsiop19GBuBzoXszUxUGcVf27fs1BrjsLE" /></a>
<hr />
<p>A few more notes:</p>
<ul>
<li>The code will run just fine on the Ampere 8XA100 GPU node as well, but a bit slower.</li>
<li>All code will run just fine on even a single GPU by omitting <code>torchrun</code>, and will produce ~identical results (code will automatically switch to gradient accumulation), but you'll have to wait 8 times longer.</li>
<li>If your GPU(s) have less than 80GB, you'll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for <code>--device_batch_size</code> in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. Less than that you'll have to know a bit more what you're doing and get more creative.</li>
<li>Most of the code is fairly vanilla PyTorch so it should run on anything that supports that - xpu, mps, or etc, but I haven't personally exercised all of these code paths so there might be sharp edges.</li>
</ul>
<p></p><h2>Research</h2><a href="#research"></a><p></p>
<p>If you are a researcher and wish to help improve nanochat, two scripts of interest are <a href="https://github.com/karpathy/nanochat/blob/master/runs/scaling_laws.sh">runs/scaling_laws.sh</a> and <a href="https://github.com/karpathy/nanochat/blob/master/runs/miniseries.sh">runs/miniseries.sh</a>. See <a href="https://github.com/karpathy/nanochat/discussions/420">Jan 7 miniseries v1</a> for related documentation. For quick experimentation (~5 min pretraining runs) my favorite scale is to train a 12-layer model (GPT-1 sized), e.g. like this:</p>
<div><pre><code>OMP_NUM_THREADS=1 torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- \
    --depth=12 \
    --run="d12" \
    --model-tag="d12" \
    --core-metric-every=999999 \
    --sample-every=-1 \
    --save-every=-1 \
</code></pre></div>
<p>This uses wandb (run name "d12"), only runs the CORE metric on last step, and it doesn't sample and save intermediate checkpoints. I like to change something in the code, re-run a d12 (or a d16 etc) and see if it helped, in an iteration loop. To see if a run helps, I like to monitor the wandb plots for:</p>
<ol>
<li><code>val_bpb</code> (validation loss in vocab-size-invariant units of bits per byte) as a function of <code>step</code>, <code>total_training_time</code> and <code>total_training_flops</code>.</li>
<li><code>core_metric</code> (the DCLM CORE socre)</li>
<li>VRAM utilization, <code>train/mfu</code> (Model FLOPS utilization), <code>train/tok_per_sec</code> (training throughput)</li>
</ol>
<p>See an example <a href="https://github.com/karpathy/nanochat/pull/498#issuecomment-3850720044">here</a>.</p>
<p>The important thing to note is that nanochat is written and configured around one single dial of complexity - the depth of the transformer. This single integer automatically determines all other hyperparameters (the width of the transformer, number of heads, learning rate adjustments, training horizons, weight decays, ...) so that the trained model comes out compute optimal. The idea is that the user doesn't have to think about or set any of this, they are simply asking for a smaller or bigger model using <code>--depth</code>, and everything "just works". By sweeping out the depth, you achieve the nanochat miniseries of compute optimal models at various sizes. GPT-2 capability model (which is of most interest at the moment) happens to be somewhere around d24-d26 range with the current code. But any candidate changes to the repo have to be principled enough that they work for all settings of depth.</p>
<p></p><h2>Running on CPU / MPS</h2><a href="#running-on-cpu--mps"></a><p></p>
<p>The script <a href="https://github.com/karpathy/nanochat/blob/master/runs/runcpu.sh">runs/runcpu.sh</a> shows a very simple example of running on CPU or Apple Silicon. It dramatically shrinks the LLM that is being trained to make things fit into a reasonable time interval of a few ten minutes of training. You will not get strong results in this way.</p>
<p></p><h2>Guides</h2><a href="#guides"></a><p></p>
<p>I've published a number of guides that might contain helpful information, most recent to least recent:</p>
<ul>
<li><a href="https://github.com/karpathy/nanochat/discussions/481">Feb 1 2026: Beating GPT-2 for &lt;&lt;$100: the nanochat journey</a></li>
<li><a href="https://github.com/karpathy/nanochat/discussions/420">Jan 7 miniseries v1</a> documents the first nanochat miniseries of models.</li>
<li>To add new abilities to nanochat, see <a href="https://github.com/karpathy/nanochat/discussions/164">Guide: counting r in strawberry (and how to add abilities generally)</a>.</li>
<li>To customize your nanochat, see <a href="https://github.com/karpathy/nanochat/discussions/139">Guide: infusing identity to your nanochat</a> in Discussions, which describes how you can tune your nanochat's personality through synthetic data generation and mixing that data into the SFT stage.</li>
<li><a href="https://github.com/karpathy/nanochat/discussions/1">Oct 13 2025: original nanochat post</a> introducing nanochat, though now it contains some deprecated information and the model is a lot older (with worse results) than current master.</li>
</ul>
<p></p><h2>File structure</h2><a href="#file-structure"></a><p></p>
<div><pre><code>.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ dev
â”‚   â”œâ”€â”€ gen_synthetic_data.py       # Example synthetic data for identity
â”‚   â”œâ”€â”€ generate_logo.html
â”‚   â”œâ”€â”€ nanochat.png
â”‚   â””â”€â”€ repackage_data_reference.py # Pretraining data shard generation
â”œâ”€â”€ nanochat
â”‚   â”œâ”€â”€ __init__.py                 # empty
â”‚   â”œâ”€â”€ checkpoint_manager.py       # Save/Load model checkpoints
â”‚   â”œâ”€â”€ common.py                   # Misc small utilities, quality of life
â”‚   â”œâ”€â”€ core_eval.py                # Evaluates base model CORE score (DCLM paper)
â”‚   â”œâ”€â”€ dataloader.py               # Tokenizing Distributed Data Loader
â”‚   â”œâ”€â”€ dataset.py                  # Download/read utils for pretraining data
â”‚   â”œâ”€â”€ engine.py                   # Efficient model inference with KV Cache
â”‚   â”œâ”€â”€ execution.py                # Allows the LLM to execute Python code as tool
â”‚   â”œâ”€â”€ gpt.py                      # The GPT nn.Module Transformer
â”‚   â”œâ”€â”€ logo.svg
â”‚   â”œâ”€â”€ loss_eval.py                # Evaluate bits per byte (instead of loss)
â”‚   â”œâ”€â”€ optim.py                    # AdamW + Muon optimizer, 1GPU and distributed
â”‚   â”œâ”€â”€ report.py                   # Utilities for writing the nanochat Report
â”‚   â”œâ”€â”€ tokenizer.py                # BPE Tokenizer wrapper in style of GPT-4
â”‚   â””â”€â”€ ui.html                     # HTML/CSS/JS for nanochat frontend
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ runs
â”‚   â”œâ”€â”€ miniseries.sh               # Miniseries training script
â”‚   â”œâ”€â”€ runcpu.sh                   # Small example of how to run on CPU/MPS
â”‚   â”œâ”€â”€ scaling_laws.sh             # Scaling laws experiments
â”‚   â””â”€â”€ speedrun.sh                 # Train the ~$100 nanochat d20
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ base_eval.py                # Base model: CORE score, bits per byte, samples
â”‚   â”œâ”€â”€ base_train.py               # Base model: train
â”‚   â”œâ”€â”€ chat_cli.py                 # Chat model: talk to over CLI
â”‚   â”œâ”€â”€ chat_eval.py                # Chat model: eval tasks
â”‚   â”œâ”€â”€ chat_rl.py                  # Chat model: reinforcement learning
â”‚   â”œâ”€â”€ chat_sft.py                 # Chat model: train SFT
â”‚   â”œâ”€â”€ chat_web.py                 # Chat model: talk to over WebUI
â”‚   â”œâ”€â”€ tok_eval.py                 # Tokenizer: evaluate compression rate
â”‚   â””â”€â”€ tok_train.py                # Tokenizer: train it
â”œâ”€â”€ tasks
â”‚   â”œâ”€â”€ arc.py                      # Multiple choice science questions
â”‚   â”œâ”€â”€ common.py                   # TaskMixture | TaskSequence
â”‚   â”œâ”€â”€ customjson.py               # Make Task from arbitrary jsonl convos
â”‚   â”œâ”€â”€ gsm8k.py                    # 8K Grade School Math questions
â”‚   â”œâ”€â”€ humaneval.py                # Misnomer; Simple Python coding task
â”‚   â”œâ”€â”€ mmlu.py                     # Multiple choice questions, broad topics
â”‚   â”œâ”€â”€ smoltalk.py                 # Conglomerate dataset of SmolTalk from HF
â”‚   â””â”€â”€ spellingbee.py              # Task teaching model to spell/count letters
â”œâ”€â”€ tests
â”‚   â””â”€â”€ test_engine.py
â””â”€â”€ uv.lock
</code></pre></div>
<p></p><h2>Contributing</h2><a href="#contributing"></a><p></p>
<p>The goal of nanochat is to improve the state of the art in micro models that are accessible to work with end to end on budgets of &lt; $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM "framework"; there are no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable "strong baseline" codebase designed to run start to end and produce a ChatGPT model you can talk to. Currently, the most interesting part personally is speeding up the latency to GPT-2 (i.e. getting a CORE score above 0.256525). Currently this takes ~3 hours, but by improving the pretraining stage we can improve this further.</p>
<p>Current AI policy: disclosure. When submitting a PR, please declare any parts that had substantial LLM contribution and that you have not written or that you do not fully understand.</p>
<p></p><h2>Acknowledgements</h2><a href="#acknowledgements"></a><p></p>
<ul>
<li>The name (nanochat) derives from my earlier project <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>, which only covered pretraining.</li>
<li>nanochat is also inspired by <a href="https://github.com/KellerJordan/modded-nanogpt">modded-nanoGPT</a>, which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.</li>
<li>Thank you to <a href="https://huggingface.co/">HuggingFace</a> for fineweb and smoltalk.</li>
<li>Thank you <a href="https://lambda.ai/service/gpu-cloud">Lambda</a> for the compute used in developing this project.</li>
<li>Thank you to chief LLM whisperer ğŸ§™â€â™‚ï¸ Alec Radford for advice/guidance.</li>
<li>Thank you to the repo czar Sofie <a href="https://github.com/svlandeg">@svlandeg</a> for help with managing issues, pull requests and discussions of nanochat.</li>
</ul>
<p></p><h2>Cite</h2><a href="#cite"></a><p></p>
<p>If you find nanochat helpful in your research cite simply as:</p>
<div><pre><span>@misc</span>{<span>nanochat</span>,
  <span>author</span> = <span><span>{</span>Andrej Karpathy<span>}</span></span>,
  <span>title</span> = <span><span>{</span>nanochat: The best ChatGPT that \$100 can buy<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2025<span>}</span></span>,
  <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
  <span>url</span> = <span><span>{</span>https://github.com/karpathy/nanochat<span>}</span></span>
}</pre></div>
<p></p><h2>License</h2><a href="#license"></a><p></p>
<p>MIT</p>
</article></div></div>
  </div>
</body>
</html>