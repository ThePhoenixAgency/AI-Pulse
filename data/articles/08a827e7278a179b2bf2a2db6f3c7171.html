<!DOCTYPE html>
<html lang="it">
<head>
<meta charset="UTF-8">
<title>New in llama.cpp: Model Management</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.55; color: #e2e8f0; max-width: 800px; margin: 26px auto; padding: 0 18px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.35em; line-height: 1.22; font-size: clamp(1.45rem, 2.1vw, 1.95rem); font-weight: 700; }
  h2, h3 { line-height: 1.28; margin: 1.1em 0 0.45em; }
  .metadata { color: #94a3b8; font-size: 0.86em; margin-bottom: 1.2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 0.7em; }
  img { max-width: 100%; width: auto !important; height: auto !important; object-fit: contain !important; border-radius: 8px; display: block; margin: 0.6em auto; }
  a { color: #00d9ff; }
  p { margin-bottom: 0.72em; line-height: 1.58; }
  ul, ol { margin: 0.5em 0 0.9em 1.1em; }
  li { margin: 0.18em 0; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 12px; margin: 0.8em 0; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 12px; border-radius: 6px; overflow-x: auto; }
  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }
  [id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"],
  [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"],
  [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"] {
    display: none !important;
    visibility: hidden !important;
    pointer-events: none !important;
  }
</style>
</head>
<body>
  <h1>New in llama.cpp: Model Management</h1>
  <div class="metadata">
    Source: Hugging Face Blog | Date: 12/11/2025 3:47:44 PM | <a href="https://huggingface.co/blog/ggml-org/model-management-in-llamacpp" target="_blank" rel="noopener noreferrer">Lien</a> | Lang: IT
  </div>
  <div class="content">
    <div><div> <p><a href="https://huggingface.co/blog"> Back to Articles</a></p> <div><div> <p><span><span><a href="https://huggingface.co/ngxson"><img alt="Xuan-Son Nguyen's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674191139776-noauth.png"></a> </span> </span></p> </div><div> <p><span><span><a href="https://huggingface.co/victor"><img alt="Victor Mustar's avatar" src="https://cdn-avatars.huggingface.co/v1/production/uploads/5f17f0a0925b9863e28ad517/fXIY5i9RLsIa1v3CCuVtt.jpeg"></a> </span> </span></p> </div></div> <p><a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server">llama.cpp server</a> now ships with <strong>router mode</strong>, which lets you dynamically load, unload, and switch between multiple models without restarting.</p>
<blockquote>
<p>Reminder: llama.cpp server is a lightweight, OpenAI-compatible HTTP server for running LLMs locally.</p>
</blockquote>
<p>This feature was a popular request to bring Ollama-style model management to llama.cpp. It uses a multi-process architecture where each model runs in its own process, so if one model crashes, others remain unaffected.</p>
<h2> <a href="#quick-start"> </a> <span> Quick Start </span>
</h2>
<p>Start the server in router mode by <strong>not specifying a model</strong>:</p>
<pre><code>llama-server
</code></pre>
<p>This auto-discovers models from your llama.cpp cache (<code>LLAMA_CACHE</code> or <code>~/.cache/llama.cpp</code>). If you've previously downloaded models via <code>llama-server -hf user/model</code>, they'll be available automatically.</p>
<p>You can also point to a local directory of GGUF files:</p>
<pre><code>llama-server --models-dir ./my-models
</code></pre>
<h2> <a href="#features"> </a> <span> Features </span>
</h2>
<ol>
<li><strong>Auto-discovery</strong>: Scans your llama.cpp cache (default) or a custom <code>--models-dir</code> folder for GGUF files</li>
<li><strong>On-demand loading</strong>: Models load automatically when first requested</li>
<li><strong>LRU eviction</strong>: When you hit <code>--models-max</code> (default: 4), the least-recently-used model unloads</li>
<li><strong>Request routing</strong>: The <code>model</code> field in your request determines which model handles it</li>
</ol>
<h2> <a href="#examples"> </a> <span> Examples </span>
</h2>
<h3> <a href="#chat-with-a-specific-model"> </a> <span> Chat with a specific model </span>
</h3>
<pre><code>curl http://localhost:8080/v1/chat/completions \ -H <span>"Content-Type: application/json"</span> \ -d <span>'{</span>
<span> "model": "ggml-org/gemma-3-4b-it-GGUF:Q4_K_M",</span>
<span> "messages": [{"role": "user", "content": "Hello!"}]</span>
<span> }'</span>
</code></pre>
<p>On the first request, the server automatically loads the model into memory (loading time depends on model size). Subsequent requests to the same model are instant since it's already loaded.</p>
<h3> <a href="#list-available-models"> </a> <span> List available models </span>
</h3>
<pre><code>curl http://localhost:8080/models
</code></pre>
<p>Returns all discovered models with their status (<code>loaded</code>, <code>loading</code>, or <code>unloaded</code>).</p>
<h3> <a href="#manually-load-a-model"> </a> <span> Manually load a model </span>
</h3>
<pre><code>curl -X POST http://localhost:8080/models/load \ -H <span>"Content-Type: application/json"</span> \ -d <span>'{"model": "my-model.gguf"}'</span>
</code></pre>
<h3> <a href="#unload-a-model-to-free-vram"> </a> <span> Unload a model to free VRAM </span>
</h3>
<pre><code>curl -X POST http://localhost:8080/models/unload \ -H <span>"Content-Type: application/json"</span> \ -d <span>'{"model": "my-model.gguf"}'</span>
</code></pre>
<h2> <a href="#key-options"> </a> <span> Key Options </span>
</h2>
<div> <table> <thead><tr>
<th>Flag</th>
<th>Description</th>
</tr> </thead><tbody><tr>
<td><code>--models-dir PATH</code></td>
<td>Directory containing your GGUF files</td>
</tr>
<tr>
<td><code>--models-max N</code></td>
<td>Max models loaded simultaneously (default: 4)</td>
</tr>
<tr>
<td><code>--no-models-autoload</code></td>
<td>Disable auto-loading; require explicit <code>/models/load</code> calls</td>
</tr>
</tbody> </table>
</div>
<p>All model instances inherit settings from the router:</p>
<pre><code>llama-server --models-dir ./models -c 8192 -ngl 99
</code></pre>
<p>All loaded models will use 8192 context and full GPU offload. You can also define per-model settings using <a href="https://github.com/ggml-org/llama.cpp/pull/17859">presets</a>:</p>
<pre><code>llama-server --models-preset config.ini
</code></pre>
<pre><code><span>[my-model]</span>
<span>model</span> = /path/to/model.gguf
<span>ctx-size</span> = <span>65536</span>
<span>temp</span> = <span>0.7</span>
</code></pre>
<h2> <a href="#also-available-in-the-web-ui"> </a> <span> Also available in the Web UI </span>
</h2>
<p>The <a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server/webui">built-in web UI</a> also supports model switching. Just select a model from the dropdown and it loads automatically.</p>
<h2> <a href="#join-the-conversation"> </a> <span> Join the Conversation </span>
</h2>
<p>We hope this feature makes it easier to A/B test different model versions, run multi-tenant deployments, or simply switch models during development without restarting the server.</p>
<p>Have questions or feedback? Drop a comment below or open an issue on <a href="https://github.com/ggml-org/llama.cpp/issues">GitHub</a>.</p>
</div></div>
  </div>
  <div class="article-elevator" aria-label="Navigation article">
    <button class="article-elevator-btn" type="button" onclick="history.back()" title="Retour">←</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToTop()">▲</button>
    <button class="article-elevator-btn" type="button" onclick="scrollToBottom()">▼</button>
  </div>
  <script>
    function stripBlockingPanels() {
      const selector = '[id*="overlay"], [class*="overlay"], [id*="modal"], [class*="modal"], [id*="popup"], [class*="popup"], [id*="paywall"], [class*="paywall"], [id*="subscribe"], [class*="subscribe"], [id*="cookie"], [class*="cookie"], [id*="consent"], [class*="consent"], [id*="gdpr"], [class*="gdpr"], [role="dialog"], [aria-modal="true"]';
      const textPattern = /\b(cookie|consent|gdpr|subscribe|subscription|paywall|abonnez[-\s]?vous|inscrivez[-\s]?vous|continue reading|continuez la lecture)\b/i;
      document.querySelectorAll(selector).forEach((node) => node.remove());
      document.querySelectorAll('div, section, aside').forEach((node) => {
        const styleAttr = String(node.getAttribute('style') || '').toLowerCase();
        const classAndId = String(node.className || '').toLowerCase() + ' ' + String(node.id || '').toLowerCase();
        const text = String(node.textContent || '').slice(0, 800);
        const hasKeyword = textPattern.test(classAndId) || textPattern.test(text);
        const looksFixed = /(position\s*:\s*(fixed|sticky)|inset\s*:|top\s*:|left\s*:|right\s*:|bottom\s*:)/.test(styleAttr);
        const hasPriority = /(z-index\s*:\s*[1-9]\d{1,}|backdrop-filter|overflow\s*:\s*hidden)/.test(styleAttr);
        if (hasKeyword && (looksFixed || hasPriority)) node.remove();
      });
    }
    function scrollToTop() {
      window.scrollTo({ top: 0, behavior: 'auto' });
    }
    function scrollToBottom() {
      window.scrollTo({ top: document.documentElement.scrollHeight, behavior: 'auto' });
    }
    window.addEventListener('message', (event) => {
      const data = event && event.data;
      if (!data || data.type !== 'AI_PULSE_SCROLL') return;
      if (data.direction === 'up' || data.direction === 'top') scrollToTop();
      if (data.direction === 'down' || data.direction === 'bottom') scrollToBottom();
    });
    stripBlockingPanels();
    setTimeout(stripBlockingPanels, 60);
    setTimeout(stripBlockingPanels, 220);
    setTimeout(stripBlockingPanels, 650);
  </script>
</body>
</html>