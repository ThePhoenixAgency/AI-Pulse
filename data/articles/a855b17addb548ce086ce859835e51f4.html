<!DOCTYPE html><html lang="en"><head>
<meta charset="UTF-8">
<title>Opik: Your Agent's Black Box Flight Recorder</title>
<style>
  body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.8; color: #e2e8f0; max-width: 800px; margin: 40px auto; padding: 0 20px; background: #0a0e27; }
  h1 { color: #00d9ff; margin-bottom: 0.5em; }
  .metadata { color: #94a3b8; font-size: 0.9em; margin-bottom: 2em; border-bottom: 1px solid rgba(0,217,255,0.2); padding-bottom: 1em; }
  img { max-width: 100%; height: auto; border-radius: 8px; }
  a { color: #00d9ff; }
  p { margin-bottom: 1em; }
  blockquote { border-left: 3px solid #825ee4; padding-left: 15px; color: #94a3b8; }
  code { background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px; color: #ff79c6; }
  pre { background: rgba(0,0,0,0.4); padding: 15px; border-radius: 6px; overflow-x: auto; }

  .article-elevator { position: fixed; right: 14px; bottom: 14px; display: flex; flex-direction: column; gap: 8px; z-index: 9999; }
  .article-elevator-btn { width: 36px; height: 36px; border: 1px solid rgba(0,217,255,0.35); border-radius: 10px; background: rgba(10,14,39,0.88); color: #00d9ff; cursor: pointer; font-size: 16px; line-height: 1; }
  .article-elevator-btn:hover { background: rgba(10,14,39,1); }

</style>
</head>
<body>
  <h1>Opik: Your Agent's Black Box Flight Recorder</h1>
  <div class="metadata">
    Source: Dev.to Open Source | Date: 2/14/2026 | Lang: EN
  </div>
  <div class="content">
    <div><div>
                <p>Building LLM agents that actually work reliably is hard. Really hard.</p>

<p>You've probably experienced this cycle: your agent works perfectly in three test cases, fails spectacularly in production, you tweak a prompt, it fixes one problem but creates two others. Rinse and repeat.</p>

<p>This is where <strong>Opik</strong> comes in. Built by Comet, Opik is an open-source platform that brings systematic evaluation and optimization to LLM development. Let me show you how to use it to build better agents.</p>

<h2>
  <a name="why-traditional-testing-fails-for-agents" href="#why-traditional-testing-fails-for-agents">
  </a>
  Why Traditional Testing Fails for Agents
</h2>

<p>Before diving into Opik, let's understand why agent testing is uniquely challenging:</p>

<ol>
<li>
<strong>Non-deterministic outputs</strong> - The same input can produce different responses</li>
<li>
<strong>Multi-step reasoning</strong> - Errors compound across tool calls and decision points</li>
<li>
<strong>No single "right answer"</strong> - Multiple valid approaches exist</li>
<li>
<strong>Integration complexity</strong> - Agents interact with real APIs and databases</li>
</ol>

<p>Traditional unit tests can't capture this complexity. You need a different approach.</p>

<h2>
  <a name="enter-opik-evaluationfirst-development" href="#enter-opik-evaluationfirst-development">
  </a>
  Enter Opik: Evaluation-First Development
</h2>

<p>Opik treats evaluation as a first-class concern. The core workflow:<br>
</p>

<div>
<pre><code>Collect traces → Define metrics → Run evaluations → Optimize → Deploy
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<p>Let me walk through a practical example of optimizing a customer support agent.</p>

<h2>
  <a name="example-building-a-resilient-support-agent" href="#example-building-a-resilient-support-agent">
  </a>
  Example: Building a Resilient Support Agent
</h2>

<p>We'll build an agent that handles refund requests. It needs to check order history, verify refund eligibility, and process requests - all while maintaining a helpful tone.</p>

<h3>
  <a name="step-1-instrument-your-agent" href="#step-1-instrument-your-agent">
  </a>
  Step 1: Instrument Your Agent
</h3>

<p>First, add Opik instrumentation to capture everything:<br>
</p>

<div>
<pre><code><span>from</span> <span>opik</span> <span>import</span> <span>opik_context</span><span>,</span> <span>track</span>
<span>from</span> <span>opik.integrations.openai</span> <span>import</span> <span>track_openai</span>
<span>import</span> <span>openai</span>

<span># Track OpenAI calls automatically
</span><span>openai_client</span> <span>=</span> <span>track_openai</span><span>(</span><span>openai</span><span>.</span><span>OpenAI</span><span>())</span>

<span>class</span> <span>SupportAgent</span><span>:</span>
    <span>@track</span><span>(</span><span>name</span><span>=</span><span>"</span><span>process_refund_request</span><span>"</span><span>)</span>
    <span>def</span> <span>process</span><span>(</span><span>self</span><span>,</span> <span>user_message</span><span>:</span> <span>str</span><span>,</span> <span>user_id</span><span>:</span> <span>str</span><span>):</span>
        <span># Get conversation history
</span>        <span>history</span> <span>=</span> <span>self</span><span>.</span><span>get_conversation_history</span><span>(</span><span>user_id</span><span>)</span>

        <span># Track this as a conversation
</span>        <span>opik_context</span><span>.</span><span>update_current_trace</span><span>(</span>
            <span>name</span><span>=</span><span>"</span><span>customer_support</span><span>"</span><span>,</span>
            <span>metadata</span><span>=</span><span>{</span>
                <span>"</span><span>user_id</span><span>"</span><span>:</span> <span>user_id</span><span>,</span>
                <span>"</span><span>conversation_length</span><span>"</span><span>:</span> <span>len</span><span>(</span><span>history</span><span>)</span>
            <span>}</span>
        <span>)</span>

        <span># Step 1: Understand intent
</span>        <span>intent</span> <span>=</span> <span>self</span><span>.</span><span>classify_intent</span><span>(</span><span>user_message</span><span>)</span>

        <span># Step 2: If refund-related, check eligibility
</span>        <span>if</span> <span>intent</span> <span>==</span> <span>"</span><span>refund</span><span>"</span><span>:</span>
            <span>order_info</span> <span>=</span> <span>self</span><span>.</span><span>check_order_history</span><span>(</span><span>user_id</span><span>)</span>
            <span>eligibility</span> <span>=</span> <span>self</span><span>.</span><span>check_refund_eligibility</span><span>(</span><span>order_info</span><span>)</span>

            <span># Track tool usage
</span>            <span>opik_context</span><span>.</span><span>log_tool_call</span><span>(</span>
                <span>name</span><span>=</span><span>"</span><span>check_refund_eligibility</span><span>"</span><span>,</span>
                <span>input</span><span>=</span><span>order_info</span><span>,</span>
                <span>output</span><span>=</span><span>eligibility</span>
            <span>)</span>

        <span># Step 3: Generate response
</span>        <span>response</span> <span>=</span> <span>self</span><span>.</span><span>generate_response</span><span>(</span><span>intent</span><span>,</span> <span>eligibility</span><span>)</span>
        <span>return</span> <span>response</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<h3>
  <a name="step-2-define-what-good-looks-like" href="#step-2-define-what-good-looks-like">
  </a>
  Step 2: Define What "Good" Looks Like
</h3>

<p>This is where Opik shines. Instead of writing brittle assertions, define metrics that capture agent quality:<br>
</p>

<div>
<pre><code><span>from</span> <span>opik.evaluation.metrics</span> <span>import</span> <span>(</span>
    <span>IsJson</span><span>,</span> 
    <span>ContainsAny</span><span>,</span> 
    <span>Hallucination</span><span>,</span> 
    <span>ToolCallCorrectness</span><span>,</span>
    <span>BaseMetric</span>
<span>)</span>

<span>class</span> <span>ToneAppropriateness</span><span>(</span><span>BaseMetric</span><span>):</span>
    <span>"""</span><span>Custom metric for customer service tone</span><span>"""</span>

    <span>def</span> <span>evaluate</span><span>(</span><span>self</span><span>,</span> <span>output</span><span>:</span> <span>str</span><span>,</span> <span>reference</span><span>:</span> <span>str</span> <span>=</span> <span>None</span><span>):</span>
        <span># Use an LLM judge to evaluate tone
</span>        <span>prompt</span> <span>=</span> <span>f</span><span>"""</span><span>
        Rate the professionalism and helpfulness of this support response (1-5):

        Response: </span><span>{</span><span>output</span><span>}</span><span>

        Return only a number.
        </span><span>"""</span>

        <span>score</span> <span>=</span> <span>int</span><span>(</span><span>llm_client</span><span>.</span><span>complete</span><span>(</span><span>prompt</span><span>))</span>
        <span>return</span> <span>{</span>
            <span>"</span><span>score</span><span>"</span><span>:</span> <span>score</span><span>,</span>
            <span>"</span><span>reason</span><span>"</span><span>:</span> <span>f</span><span>"</span><span>Tone rated </span><span>{</span><span>score</span><span>}</span><span>/5</span><span>"</span><span>,</span>
            <span>"</span><span>name</span><span>"</span><span>:</span> <span>"</span><span>tone_appropriateness</span><span>"</span>
        <span>}</span>

<span># Define evaluation criteria
</span><span>metrics</span> <span>=</span> <span>[</span>
    <span>Hallucination</span><span>(</span><span>threshold</span><span>=</span><span>0.3</span><span>),</span>  <span># Penalize making up facts
</span>    <span>ContainsAny</span><span>([</span><span>"</span><span>refund</span><span>"</span><span>,</span> <span>"</span><span>credit</span><span>"</span><span>,</span> <span>"</span><span>process</span><span>"</span><span>],</span> <span>min_count</span><span>=</span><span>1</span><span>),</span>  <span># Keywords present
</span>    <span>ToolCallCorrectness</span><span>(),</span>  <span># Tools used appropriately
</span>    <span>ToneAppropriateness</span><span>(</span><span>min_score</span><span>=</span><span>4</span><span>)</span>
<span>]</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<h3>
  <a name="step-3-create-a-test-dataset" href="#step-3-create-a-test-dataset">
  </a>
  Step 3: Create a Test Dataset
</h3>

<p>Good evaluations need good data. Opik lets you create datasets from production traces:<br>
</p>

<div>
<pre><code><span>from</span> <span>opik</span> <span>import</span> <span>Opik</span>

<span>client</span> <span>=</span> <span>Opik</span><span>()</span>
<span>dataset</span> <span>=</span> <span>client</span><span>.</span><span>create_dataset</span><span>(</span><span>"</span><span>refund_requests</span><span>"</span><span>)</span>

<span># Add edge cases you've encountered
</span><span>dataset</span><span>.</span><span>insert</span><span>([</span>
    <span>{</span>
        <span>"</span><span>input</span><span>"</span><span>:</span> <span>"</span><span>I want a refund for order #12345</span><span>"</span><span>,</span>
        <span>"</span><span>expected_output</span><span>"</span><span>:</span> <span>"</span><span>Check eligibility and process if valid</span><span>"</span><span>,</span>
        <span>"</span><span>user_id</span><span>"</span><span>:</span> <span>"</span><span>user_1</span><span>"</span><span>,</span>
        <span>"</span><span>order_exists</span><span>"</span><span>:</span> <span>True</span><span>,</span>
        <span>"</span><span>eligible</span><span>"</span><span>:</span> <span>True</span>
    <span>},</span>
    <span>{</span>
        <span>"</span><span>input</span><span>"</span><span>:</span> <span>"</span><span>Give me my money back!!!</span><span>"</span><span>,</span>  <span># Emotional customer
</span>        <span>"</span><span>expected_output</span><span>"</span><span>:</span> <span>"</span><span>De-escalate and check order</span><span>"</span><span>,</span>
        <span>"</span><span>user_id</span><span>"</span><span>:</span> <span>"</span><span>user_2</span><span>"</span><span>,</span> 
        <span>"</span><span>order_exists</span><span>"</span><span>:</span> <span>True</span><span>,</span>
        <span>"</span><span>eligible</span><span>"</span><span>:</span> <span>False</span>  <span># Past return window
</span>    <span>},</span>
    <span>{</span>
        <span>"</span><span>input</span><span>"</span><span>:</span> <span>"</span><span>Refund for order that never arrived</span><span>"</span><span>,</span>
        <span>"</span><span>expected_output</span><span>"</span><span>:</span> <span>"</span><span>Check delivery status, offer replacement</span><span>"</span><span>,</span>
        <span>"</span><span>user_id</span><span>"</span><span>:</span> <span>"</span><span>user_3</span><span>"</span><span>,</span>
        <span>"</span><span>order_exists</span><span>"</span><span>:</span> <span>True</span><span>,</span>
        <span>"</span><span>eligible</span><span>"</span><span>:</span> <span>True</span>
    <span>}</span>
<span>])</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<h3>
  <a name="step-4-run-systematic-evaluations" href="#step-4-run-systematic-evaluations">
  </a>
  Step 4: Run Systematic Evaluations
</h3>

<p>Now the magic happens. Run your agent against the dataset and Opik automatically evaluates each response:<br>
</p>

<div>
<pre><code><span>from</span> <span>opik.evaluation</span> <span>import</span> <span>evaluate</span>

<span>def</span> <span>evaluation_task</span><span>(</span><span>x</span><span>):</span>
    <span>agent</span> <span>=</span> <span>SupportAgent</span><span>()</span>
    <span>response</span> <span>=</span> <span>agent</span><span>.</span><span>process</span><span>(</span><span>x</span><span>[</span><span>"</span><span>input</span><span>"</span><span>],</span> <span>x</span><span>[</span><span>"</span><span>user_id</span><span>"</span><span>])</span>
    <span>return</span> <span>{</span>
        <span>"</span><span>output</span><span>"</span><span>:</span> <span>response</span><span>,</span>
        <span>"</span><span>reference</span><span>"</span><span>:</span> <span>x</span><span>[</span><span>"</span><span>expected_output</span><span>"</span><span>],</span>
        <span>"</span><span>metadata</span><span>"</span><span>:</span> <span>{</span><span>"</span><span>user_id</span><span>"</span><span>:</span> <span>x</span><span>[</span><span>"</span><span>user_id</span><span>"</span><span>]}</span>
    <span>}</span>

<span>results</span> <span>=</span> <span>evaluate</span><span>(</span>
    <span>dataset</span><span>=</span><span>"</span><span>refund_requests</span><span>"</span><span>,</span>
    <span>task</span><span>=</span><span>evaluation_task</span><span>,</span>
    <span>metrics</span><span>=</span><span>metrics</span>
<span>)</span>

<span>print</span><span>(</span><span>f</span><span>"</span><span>Overall score: </span><span>{</span><span>results</span><span>.</span><span>score</span><span>}</span><span>"</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"</span><span>Failed examples: </span><span>{</span><span>results</span><span>.</span><span>failures</span><span>}</span><span>"</span><span>)</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<h3>
  <a name="step-5-identify-failure-patterns" href="#step-5-identify-failure-patterns">
  </a>
  Step 5: Identify Failure Patterns
</h3>

<p>Here's where you get real insights. Opik's dashboard shows you:</p>

<ul>
<li>
<strong>Low-scoring traces</strong> - Which conversations performed poorly</li>
<li>
<strong>Metric breakdowns</strong> - Is tone consistently bad? Tool usage failing?</li>
<li>
<strong>Clustering</strong> - Similar failures grouped together</li>
</ul>

<p>In my experience, you'll typically find patterns like:<br>
</p>

<div>
<pre><code>1. Tool call errors: Agent tries to process refunds without checking eligibility
2. Tone failures: Responses become robotic when handling angry customers
3. Context loss: Agent forgets conversation history after long exchanges
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<h3>
  <a name="step-6-optimize-iteratively" href="#step-6-optimize-iteratively">
  </a>
  Step 6: Optimize Iteratively
</h3>

<p>Now you optimize based on evidence, not intuition:</p>

<p><strong>Iteration 1: Fix tool usage</strong><br>
</p>

<div>
<pre><code><span># Problem: Agent called process_refund before eligibility check
# Solution: Explicit system prompt
</span>
<span>system_prompt</span> <span>=</span> <span>"""</span><span>
You are a customer support agent. Follow this order:
1. ALWAYS check eligibility before processing refunds
2. Call check_eligibility() first
3. Only call process_refund() if eligibility confirmed
</span><span>"""</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<p><strong>Iteration 2: Fix tone for edge cases</strong><br>
</p>

<div>
<pre><code><span># Problem: Angry customers get cold, scripted responses
# Solution: Tone guidelines in system prompt
</span>
<span>tone_guidelines</span> <span>=</span> <span>"""</span><span>
For frustrated customers:
- Acknowledge their frustration: </span><span>"</span><span>I understand this is frustrating...</span><span>"</span><span>
- Show empathy before solving
- Use softer language: </span><span>"</span><span>I</span><span>'</span><span>d be happy to help</span><span>"</span><span> vs </span><span>"</span><span>I will help</span><span>"</span><span>
</span><span>"""</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<p><strong>Iteration 3: Add safety checks</strong><br>
</p>

<div>
<pre><code><span># Problem: Agent hallucinated refund policies
# Solution: Add factual grounding
</span>
<span>@track</span><span>(</span><span>name</span><span>=</span><span>"</span><span>check_policy</span><span>"</span><span>)</span>
<span>def</span> <span>get_policy</span><span>(</span><span>order_date</span><span>):</span>
    <span># Pull from actual database, not model memory
</span>    <span>return</span> <span>db</span><span>.</span><span>get_refund_policy</span><span>(</span><span>order_date</span><span>)</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<h3>
  <a name="step-7-continuous-evaluation" href="#step-7-continuous-evaluation">
  </a>
  Step 7: Continuous Evaluation
</h3>

<p>Don't just evaluate once. Set up continuous evaluation:<br>
</p>

<div>
<pre><code><span># GitHub Action / CI Pipeline
# .github/workflows/evaluate-agent.yml
</span>
<span>name</span><span>:</span> <span>Evaluate</span> <span>Agent</span>
<span>on</span><span>:</span> <span>[</span><span>push</span><span>,</span> <span>pull_request</span><span>]</span>

<span>jobs</span><span>:</span>
  <span>evaluate</span><span>:</span>
    <span>runs</span><span>-</span><span>on</span><span>:</span> <span>ubuntu</span><span>-</span><span>latest</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>name</span><span>:</span> <span>Run</span> <span>evaluations</span>
        <span>run</span><span>:</span> <span>python</span> <span>evaluate_agent</span><span>.</span><span>py</span>

      <span>-</span> <span>name</span><span>:</span> <span>Compare</span> <span>with</span> <span>baseline</span>
        <span>run</span><span>:</span> <span>|</span>
          <span>current_score</span> <span>=</span> <span>get_current_score</span><span>()</span>
          <span>baseline_score</span> <span>=</span> <span>get_baseline_score</span><span>()</span>
          <span>assert</span> <span>current_score</span> <span>&gt;=</span> <span>baseline_score</span> <span>-</span> <span>0.05</span>

      <span>-</span> <span>name</span><span>:</span> <span>Upload</span> <span>results</span> <span>to</span> <span>Opik</span>
        <span>run</span><span>:</span> <span>opik</span> <span>upload_results</span> <span>--</span><span>dataset</span> <span>refund_requests</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<h2>
  <a name="real-impact-what-you-gain" href="#real-impact-what-you-gain">
  </a>
  Real Impact: What You Gain
</h2>

<p>After implementing this workflow with Opik, I've consistently seen:</p>

<p><strong>50-70% reduction in regression bugs</strong> - Each change is evaluated against 100+ test cases automatically</p>

<p><strong>2-3x faster iteration cycles</strong> - No more manual testing of every edge case</p>

<p><strong>Clear success metrics</strong> - You know exactly when your agent is ready for production</p>

<p><strong>Traceability</strong> - When something fails in production, you can trace it back to the exact prompt and tool call</p>

<h2>
  <a name="getting-started" href="#getting-started">
  </a>
  Getting Started
</h2>

<ol>
<li>
<strong>Install Opik</strong>:
</li>
</ol>

<div>
<pre><code>pip <span>install </span>opik
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<ol>
<li>
<strong>Start the platform</strong> (local or cloud):
</li>
</ol>

<div>
<pre><code>opik <span>local </span>start
<span># or sign up at comet.com/opik</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<ol>
<li>
<strong>Instrument your first agent</strong>:
</li>
</ol>

<div>
<pre><code><span>import</span> <span>opik</span>
<span>opik</span><span>.</span><span>configure</span><span>()</span>
</code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<ol>
<li>
<strong>Run your first evaluation</strong>:
</li>
</ol>

<div>
<pre><code><span>from</span> <span>opik.evaluation</span> <span>import</span> <span>evaluate</span>
<span># Follow the examples above
</span></code></pre>
<div>
<p>
    Enter fullscreen mode
    

    Exit fullscreen mode
    

</p>
</div>
</div>

<h2>
  <a name="the-bottom-line" href="#the-bottom-line">
  </a>
  The Bottom Line
</h2>

<p>Building reliable LLM agents isn't about perfect prompts or the latest model. It's about having a systematic way to measure quality, identify issues, and verify improvements.</p>

<p>Opik gives you that system. It's not magic - you still need to iterate and think critically about your agent's behavior. But it transforms agent optimization from guesswork into engineering.</p>

<p>The LLM space is moving fast. The teams that win won't be the ones with the cleverest prompts - they'll be the ones who can iterate fastest while maintaining quality. That's what Opik enables.</p>

<p><strong>Your turn</strong>: Pick one agent you're currently building or maintaining. Instrument it with Opik this week. Run one evaluation. I guarantee you'll find something you didn't expect.</p>

<p><em>Have you tried systematic evaluation for your agents? What challenges are you facing? Let me know in the comments.</em></p>

            </div></div>
  </div>

</body></html>